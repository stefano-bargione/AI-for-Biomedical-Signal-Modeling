{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b81901-633c-440c-9416-79970e949f1a",
   "metadata": {},
   "source": [
    "# Deep Learning Analyses - P300 Decoding from 2D EEG Data Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918016f4-58e6-4223-a313-cf06f35e9cba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Impostazione **Weight & Biases DL Training** con **Rappresentazione Time-Frequency Signal (2D)** \n",
    "\n",
    "## Optimization Weight and Biases - EEG Spectrograms - Frequency x Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5c002-ee78-4174-ab90-2ba446f104d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure FINAL SEQUENCE OF STEPS - EEG Spectrograms - Time x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d435b59-674d-49de-bb55-3b185db84442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Importing \n",
    "    \n",
    "import os\n",
    "import math\n",
    "import copy as cp \n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random \n",
    "\n",
    "#import mne \n",
    "import scipy\n",
    "\n",
    "import numpy as np  # NumPy per operazioni numeriche\n",
    "import matplotlib.pyplot as plt  # Matplotlib per la visualizzazione dei dati\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b8ea4-32c2-4bf5-b8eb-4c25d4c01063",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Utils Functions - EEG Spectrograms - Time x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b28464-a2c6-4285-aac7-d13040822b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "QUI DENTRO HO CONFIGURATO \n",
    "LE FUNZIONI DI CONTROLLO DELLE STRINGHE \n",
    "PER IL SALVATAGGIO DELLE PERFORMANCE DEL MODELLO\n",
    "NELLE RELATIVE SUBFOLDERS\n",
    "\n",
    "(I.E., get_subfolder_from_key, get_subfolder_from_key_hyper)\n",
    "\n",
    "IN MODO CHE SI LEGHINO ALLA CHIAVE 'STANDARDIZATION' DELL'OGGETTO SWEEP_CONFIG\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data_hyper(data_type, category, wavelet_level=None, condition= \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Carica i dati EEG dalla directory appropriata, seleziona la finestra temporale (50°-300° punto, ossia 0-1000 mms).\n",
    "\n",
    "    Parameters:\n",
    "    - data_type: str, \"1_20\", \"1_45\" o \"wavelet\"\n",
    "    - category: str, \"familiar\" o \"unfamiliar\"\n",
    "    - wavelet_level: str, \"theta\", \"delta\", ecc. (solo per dati wavelet)\n",
    "    - condition: str, condizione sperimentale da selezionare\n",
    "\n",
    "    Returns:\n",
    "    - X: Dati EEG sotto-selezionati (50°-300° punto)\n",
    "    - y: Etichette corrispondenti\n",
    "    \"\"\"\n",
    "    # Definizione dei percorsi base\n",
    "    base_paths = {\n",
    "        \"1_20\": {\n",
    "            \"familiar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_20/hyper_dataset_EEG_preprocessed_1_20_familiar_{condition}.pkl\",\n",
    "            \"unfamiliar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_20/hyper_dataset_EEG_preprocessed_1_20_unfamiliar_{condition}.pkl\"\n",
    "        },\n",
    "        \"1_45\": {\n",
    "            \"familiar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_45/hyper_dataset_EEG_preprocessed_1_45_familiar_{condition}.pkl\",\n",
    "            \"unfamiliar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_45/hyper_dataset_EEG_preprocessed_1_45_unfamiliar_{condition}.pkl\"\n",
    "        },\n",
    "        \"wavelet\": {\n",
    "            \"familiar\": \"/home/stefano/Interrogait/all_datas/Hyper_Datasets_Wavelet_Reconstructions/hyper_dataset_wavelet_familiar.pkl\",\n",
    "            \"unfamiliar\": \"/home/stefano/Interrogait/all_datas/Hyper_Datasets_Wavelet_Reconstructions/hyper_dataset_wavelet_unfamiliar.pkl\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Seleziona il path corretto\n",
    "    filepath = base_paths[data_type][category]\n",
    "    \n",
    "    # Caricamento del file\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Selezione della finestra temporale e delle etichette\n",
    "    X = data[wavelet_level][condition][\"data\"][:, :, 125:200] if data_type == \"wavelet\" else data[\"data\"][:, :, 50:300]\n",
    "    y = data[wavelet_level][condition][\"labels\"] if data_type == \"wavelet\" else data[\"labels\"]\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_type, category, subject_type, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Carica i dati EEG dalla directory appropriata, già salvati con la finestra temporale (50°-300° punto)\n",
    "\n",
    "    Parameters:\n",
    "    - data_type: str, \"spectrograms\",\n",
    "    - category: str, \"familiar\" o \"unfamiliar\"\n",
    "    - subject_type: str, \"th\" (terapisti) o \"pt\" (pazienti)\n",
    "    - condition: str, condizione sperimentale da selezionare\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - X: Dati EEG sotto-selezionati (50°-300° punto e canali selezionati se applicabile)\n",
    "    - y: Etichette corrispondenti\n",
    "    \"\"\"\n",
    "\n",
    "    # Definizione dei percorsi base\n",
    "    base_paths = {\n",
    "        \"spectrograms\": {\n",
    "            \"familiar\": \"/home/stefano/Interrogait/all_datas/Familiar_Spectrograms/\",\n",
    "            \"unfamiliar\": \"/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms/\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Seleziona il path corretto\n",
    "    base_path = base_paths[data_type][category]\n",
    "\n",
    "    # Determina il nome del file corretto\n",
    "    if data_type in [\"spectrograms\"]:\n",
    "        filename = f\"new_all_{subject_type}_concat_spectrograms_coupled_exp.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"data_type non valido!\")\n",
    "        \n",
    "    # Caricamento del file\n",
    "    filepath = base_path + filename\n",
    "    \n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    '''\n",
    "    Per i dati spectrogram, la funzione seleziona la condizione desiderata (i.e., condition = \"th_resp_vs_pt_resp\") \n",
    "    e preleva i dati e le etichette associati a quella condizione.\n",
    "    '''\n",
    "    \n",
    "    # Selezione della finestra temporale e delle etichette\n",
    "    X = data[condition][\"data\"]\n",
    "    y = data[condition][\"labels\"]\n",
    "\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def select_channels(data, channels=[12, 30, 48]):\n",
    "    \"\"\"\n",
    "    Seleziona i canali EEG specificati SOLO per i dati 1-20 e 1-45.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array NumPy, dati EEG con shape (n_trials, n_channels, n_timepoints)\n",
    "    - channels: list, indici dei canali da selezionare\n",
    "\n",
    "    Returns:\n",
    "    - data filtrato sui canali specificati\n",
    "    \"\"\"\n",
    "    return data[:, channels, :]\n",
    "\n",
    "\n",
    "# Funzione per train-test split\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "'''ATTENZIONE MODIFICATA FUNZIONE DI STANDARDIZZAZIONE'''\n",
    "# Funzione per standardizzare i dati\n",
    "# Con questa modifica eviti che std==0 produca NaN e i tuoi loss torneranno numeri sensati.\n",
    "def standardize_data(X_train, X_val, X_test, eps = 1e-8):\n",
    "    \n",
    "    mean = X_train.mean(axis=0, keepdims=True)\n",
    "    std = X_train.std(axis=0, keepdims=True)\n",
    "    \n",
    "    #aggiungo eps per evitare divisione per zero\n",
    "    X_train = (X_train - mean) / (std + eps)\n",
    "    X_val = (X_val - mean) / (std + eps)\n",
    "    X_test = (X_test - mean) / (std + eps)\n",
    "    \n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "# Import modelli (definisci le classi CNN1D, ReadMEndYou, ReadMYMind)\n",
    "#from models import CNN1D, ReadMEndYou, ReadMYMind  # Assicurati di avere i modelli definiti in 'models.py'\n",
    "\n",
    "# Funzione per inizializzare i modelli\n",
    "def initialize_models():\n",
    "    #model = CNN1D(input_channels=3, num_classes=2)\n",
    "    model_CNN = CNN2D(input_channels=3, num_classes=2)\n",
    "    #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    model_LSTM = ReadMEndYou(input_size=3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "    model_Transformer = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "    \n",
    "    return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "'''\n",
    "Questa funzione prende in input i dati di training, validation e test, \n",
    "il tipo di modello scelto e la dimensione del batch. Si occupa di:\n",
    "\n",
    "Calcolare i pesi delle classi.\n",
    "Convertire i dati in tensori PyTorch, con le opportune trasformazioni per CNN, LSTM o Transformer.\n",
    "Creare i dataset e i dataloader per il training.\n",
    "'''\n",
    "\n",
    "\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48):\n",
    "    \n",
    "    # Calcolo dei pesi delle classi\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                         classes=np.unique(y_train), \n",
    "                                         y=y_train)\n",
    "    \n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    class_weights_tensor = class_weights_tensor.to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch con permutazione se necessario\n",
    "    \n",
    "    '''OCCHIO QUI CAMBIATO'''\n",
    "    #if model_type == \"CNN2D\":\n",
    "    \n",
    "    if model_type == \"CNN2D_LSTM_TF\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #BiLSTM (ReadMEndYou):\n",
    "    #Ora il modello si aspetta l’input con shape (batch, canali, frequenze, tempo) \n",
    "    #e, al suo interno, \n",
    "    #esegue la permutazione per avere il tempo come dimensione sequenziale. \n",
    "    #Non serve quindi applicare una permutazione anche qui.\n",
    "    \n",
    "    elif model_type == \"BiLSTM\":\n",
    "            \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #Transformer (ReadMYMind):\n",
    "    #Analogamente, il modello gestisce internamente la riorganizzazione dell’input, quindi lasciamo i dati nella loro forma originale.\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Modello non riconosciuto. Scegli tra 'CNN', 'LSTM' o 'Transformer'.\")\n",
    "    \n",
    "    # Conversione delle etichette in tensori\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Creazione dei dataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Creazione dei dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "\n",
    "\n",
    "'''\n",
    "QUESTE DUE FUNZIONI PRESE DA \n",
    "\n",
    "EEG Motor Movement - Imagery Dataset (EEGMMIDB) - TASK 1 - 2D GRID - ALL FREQS + 3D CONV CONV SEP.ipynb\n",
    "\n",
    "DA SEZIONE\n",
    "\n",
    "## Impostazione **Weight & Biases DL Training** con **Rappresentazione Tempo-Frequenza dei miei dati EEG** \n",
    "a seconda del Dataset del Task scelto\n",
    "'''\n",
    "\n",
    "\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, model_standardization):\n",
    "    \n",
    "    #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "    if '_familiar_th' in key:\n",
    "        return 'th_fam'\n",
    "    elif '_unfamiliar_th' in key:\n",
    "        return 'th_unfam'\n",
    "    elif '_familiar_pt' in key:\n",
    "        return 'pt_fam'\n",
    "    elif '_unfamiliar_pt' in key:\n",
    "        return 'pt_unfam'\n",
    "    else:\n",
    "        return None\n",
    "     \n",
    "   \n",
    "    \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, my_train_results, my_test_results, key, data_type, sweep_config, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, sweep_config)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "     # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"spectrograms\" in key:\n",
    "        data_type_str = \"spectrograms\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    file_name = f\"{model_name}_performances_{condition}_{subfolder}_{data_type_str}.pkl\"\n",
    "    folder_path = os.path.join(base_folder, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo ✅ in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei risultati: {e}\")\n",
    "        \n",
    "        \n",
    "'''\n",
    "\n",
    "QUESTE FUNZIONI TROVATE COME ERANO NEL NOTEBOOK\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, sweep_config):\n",
    "    \n",
    "    \n",
    "    #Mi richiamo la chiave 'standardization' che ho impostato nella configurazione dell'oggetto weight and biases\n",
    "    #(i.e., sweep_config['standardization']) e eseguo una procedura condizionale \n",
    "    \n",
    "    #ossia che, se risulta o True o False, lui cambi le condizioni di gestione \n",
    "    #della costruzione delle path di salvataggio \n",
    "    \n",
    "    \n",
    "    # Controlla se i dati sono standardizzati\n",
    "    if sweep_config['standardization']:\n",
    "    \n",
    "        #PER I DATI SCALED\n",
    "        if '_familiar_th' in key:\n",
    "            return 'TH_FAM'\n",
    "        elif '_unfamiliar_th' in key:\n",
    "            return 'TH_UNFAM'\n",
    "        elif '_familiar_pt' in key:\n",
    "            return 'PT_FAM'\n",
    "        elif '_unfamiliar_pt' in key:\n",
    "            return 'PT_UNFAM'\n",
    "        else:\n",
    "            return None\n",
    "    else: \n",
    "        #PER I DATI UNSCALED\n",
    "\n",
    "        if '_familiar_th' in key:\n",
    "            return 'TH_FAM_UNSCALED'\n",
    "        elif '_unfamiliar_th' in key:\n",
    "            return 'TH_UNFAM_UNSCALED'\n",
    "        elif '_familiar_pt' in key:\n",
    "            return 'PT_FAM_UNSCALED'\n",
    "        elif '_unfamiliar_pt' in key:\n",
    "            return 'PT_UNFAM_UNSCALED'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "     \n",
    "   \n",
    "    \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, my_train_results, my_test_results, key, data_type, sweep_config, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, sweep_config)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"wavelet\" in key:\n",
    "        data_type_str = \"wavelet_delta\"\n",
    "    elif \"1_20\" in key:\n",
    "        data_type_str = \"1_20\"\n",
    "    elif \"1_45\" in key:\n",
    "        data_type_str = \"1_45\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    file_name = f\"{model_name}_performances_{condition}_{subfolder}_{data_type_str}.pkl\"\n",
    "    folder_path = os.path.join(base_folder, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo ✅ in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei risultati: {e}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key_hyper(key, sweep_config):\n",
    "    \n",
    "    \n",
    "    #Mi richiamo la chiave 'standardization' che ho impostato nella configurazione dell'oggetto weight and biases\n",
    "    #(i.e., sweep_config['standardization']) e eseguo una procedura condizionale \n",
    "    \n",
    "    #ossia che, se risulta o True o False, lui cambi le condizioni di gestione \n",
    "    #della costruzione delle path di salvataggio \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Controlla se i dati sono standardizzati\n",
    "    if sweep_config['standardization']:\n",
    "        \n",
    "        #PER I DATI SCALED\n",
    "            \n",
    "        if '_familiar' in key:\n",
    "            return 'HYPER_FAM'\n",
    "        elif '_unfamiliar' in key:\n",
    "            return 'HYPER_UNFAM'\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        \n",
    "        #PER I DATI UNSCALED\n",
    "        if '_familiar' in key:\n",
    "            return 'HYPER_FAM_UNSCALED'\n",
    "        elif '_unfamiliar' in key:\n",
    "            return 'HYPER_UNFAM_UNSCALED'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results_hyper(model_name, my_train_results, my_test_results, key, data_type, sweep_config, condition= \"th_resp_vs_pt_resp\"):\n",
    "    \n",
    "    #Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key_hyper(key, sweep_config)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"wavelet\" in key:\n",
    "        data_type_str = \"wavelet_delta\"\n",
    "    elif \"1_20\" in key:\n",
    "        data_type_str = \"1_20\"\n",
    "    elif \"1_45\" in key:\n",
    "        data_type_str = \"1_45\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    file_name = f\"{model_name}_performances_{condition}_{subfolder}_{data_type_str}.pkl\"\n",
    "    folder_path = os.path.join(base_folder, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo ✅ in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei risultati: {e}\")\n",
    "        \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98783273-d297-436e-ae54-32679f5ef308",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **DL Models - EEG Spectrograms - Time x Frequencies (OLD VERSIONS BEFORE SEPTEMBER 2025!)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "781d8b48-bb26-4e40-9b5b-ef9c7e14608a",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI OLD VERSIONS\n",
    "'''\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_channels, 16, kernel_size=8, stride=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)  # Batch Normalization\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=6, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)  # Batch Normalization\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2)      \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(32, 48, kernel_size=4, stride=1)\n",
    "        self.bn3 = nn.BatchNorm1d(48)  # Batch Normalization\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size=2)      \n",
    "\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)  # 50% di dropout\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Iperparametri CNN\n",
    "#num_classes = 2\n",
    "\n",
    "# Creazione del modello CNN\n",
    "#model = CNN1D(input_channels = 3, num_classes = num_classes)\n",
    "\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, num_layers=3, dropout=0.5, bidirectional=False):\n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional  # Impostazione della bidirezionalità\n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        \n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        # Definizione dei layer LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        \n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        \n",
    "        # Dropout layer tra gli LSTM e prima del fully connected\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer finale per la classificazione\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "       \n",
    "        # Estrazione dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "        \n",
    "# Parametri\n",
    "#I 3 canali EEG\n",
    "#input_sizes = 3 \n",
    "\n",
    "#hidden_sizes = [24, 48, 62]\n",
    "\n",
    "#output_sizes = 2  # Ad esempio, classificazione binaria\n",
    "\n",
    "# Creazione del modello\n",
    "#model = ReadMEndYou(input_size=input_sizes, hidden_sizes=hidden_sizes, output_size=output_sizes, bidirectional= True)\n",
    "\n",
    "# Visualizzare il modello\n",
    "#print(model)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "    def __init__(self, num_channels, seq_length, d_model, num_heads, num_layers, num_classes):\n",
    "        super(ReadMYMind, self).__init__()\n",
    "        \n",
    "        # Embedding per portare i dati EEG in uno spazio latente\n",
    "        \n",
    "        # da # (batch_size, 3, 300) -> (batch_size, 3, 16)\n",
    "        self.embedding = nn.Linear(seq_length, d_model)  \n",
    "        \n",
    "        #Il modulo nn.TransformerEncoder si aspetta un input di forma:\n",
    "        #(seq_length, batch_size, embedding_dim)\n",
    "        \n",
    "        # Attenzione Spaziale\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Attenzione Temporale\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Cross-attention per combinare spaziale e temporale iterativamente\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Layer finale di fusione e classificazione\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #SHAPE DATI PRIMA DELLA TRASFORMAZIONE SPAZIALE\n",
    "        #print(f\"x shape before embedding: {x.shape}\") \n",
    "        #(batch_size, 3, 300)  # già nella forma corretta\n",
    "        \n",
    "        x = self.embedding(x)   # da # (batch_size, 3, 300) -> (batch_size, 3, 16)\n",
    "        \n",
    "        #print(f\"x shape after embedding: {x.shape}\")\n",
    "        \n",
    "        #Permuta per il TransformerEncoder, che vuole\n",
    "        #(seq_length, batch_size, embedding_dim)\n",
    "        \n",
    "        # da (batch_size, 3, 16) -> (16, batch_size, 3) \n",
    "        # quindi da (batch_size, 3, 16) sposta l'ultima dimensione (16) alla prima posizione, \n",
    "        # perché ho rimappato i 300 punti temporali in uno spazio latente 16 dimensionale.\n",
    "        # Dopodiché, il batch size lo mantengo alla dimensione 2 \n",
    "        # E alla dimensione 3 metto invece 3 (che erano i miei canali e che diventano invece l'embedding_dim)\n",
    "        \n",
    "        #x = x.permute(2, 0, 1)  \n",
    "        \n",
    "        #print(f\"X shape before spatial_transformer: {x.shape}\") # (seq_length = 16, batch_size, embedding_dim = 3)\n",
    "        \n",
    "        x_spatial = self.spatial_transformer(x)  # (seq_length = 16, batch_size, embedding_dim = 3)\n",
    "            \n",
    "        #print(f\"X shape after spatial_transformer: {x_spatial.shape}\") # (seq_length = 16, batch_size, embedding_dim = 3)\n",
    "    \n",
    "        # Prima iterazione: attenzione temporale per ogni canale\n",
    "        \n",
    "        #print(f\"X_spatial shape before temporal attention: {x_spatial.shape}\") # (seq_length = 16, batch_size, embedding_dim = 3)\n",
    "    \n",
    "        x_temporal = self.temporal_transformer(x_spatial) #batch_size, d_model, num_channels\n",
    "        \n",
    "        #print(f\"X shape after temporal_transformer: {x_temporal.shape}\") # (seq_length = 16, batch_size, embedding_dim = 3)\n",
    "        \n",
    "        #Nel caso della nn.MultiheadAttention, la shape corretta dell'input è:\n",
    "        \n",
    "        #Query (Q): (seq_length, batch_size, embedding_dim)\n",
    "        #Key (K) e Value (V): (seq_length, batch_size, embedding_dim)\n",
    "        \n",
    "        # Cross-attention: combinazione spaziale-temporale\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Seconda iterazione: raffinamento con attenzione spaziale\n",
    "        #x_spatial_refined = self.spatial_transformer(x_cross)\n",
    "       \n",
    "        # Seconda iterazione: raffinamento con attenzione temporale\n",
    "        #x_temporal_refined = self.temporal_transformer(x_cross)\n",
    "       \n",
    "        # Fusione finale delle informazioni\n",
    "        #x_fused = self.fc_fusion((x_spatial_refined + x_temporal_refined).mean(dim=1))  # (batch_size, d_model)\n",
    "        \n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=1))  # (batch_size, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # (batch_size, num_classes)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b3a5cab-4b0a-4ce4-bbe0-c22fe79ab731",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI (PRE-LUGLIO 2025!)\n",
    "'''\n",
    "\n",
    "\n",
    "'''CNN2D\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, canali, altezza, larghezza). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (38)\n",
    "e la \"larghezza\" come le finestre temporali (6).\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, canali, frequenze, tempo)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "'''\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 3 * 26 = 78)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "'''\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "self.embedding = nn.Linear(seq_length, d_model)\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Scelta: In questa implementazione abbiamo deciso di usare il tempo come sequenza.\n",
    "#In alternativa, potresti scegliere i bin di frequenza come sequenza, \n",
    "#ma ciò richiederebbe una diversa riorganizzazione delle dimensioni \n",
    "#(ad esempio, un permute diverso).\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=3, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0719bf2d-d834-4535-a021-585543368e78",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI (LUGLIO 2025!)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Ecco una versione di CNN2D parametrizzata esattamente come la tua CNN1D, con:\n",
    "\n",
    "conv_out_channels, conv_kernel_size, conv_stride,\n",
    "\n",
    "pool_type, pool_kernel_size,\n",
    "\n",
    "fc1_units, dropout,\n",
    "\n",
    "activations (tupla di 3 stringhe),\n",
    "\n",
    "e scelta casuale di una delle 3 attivazioni prima di fc2.\n",
    "\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, canali, altezza, larghezza). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (38)\n",
    "e la \"larghezza\" come le finestre temporali (6).\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, canali, frequenze, tempo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "        conv_out_channels: int,\n",
    "        \n",
    "        # ora tre tuple per i 3 layer conv\n",
    "        conv_kernel_size: tuple,    # es. ((3,3),(5,5),(7,7))\n",
    "        conv_stride: tuple,         # es. ((1,1),(1,2),(2,2))\n",
    "        \n",
    "        pool_type: str,             # \"max\" o \"avg\"\n",
    "        \n",
    "        # tre tuple per i 3 layer di pooling\n",
    "        pool_kernel_size: tuple,    # es. (2,2)\n",
    "        \n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "        \n",
    "        activations: tuple          # es. ('relu','selu','elu')\n",
    "    ):\n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # mappatura stringa → funzione\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        \n",
    "        self.act_fns = [mapping[a] for a in activations]\n",
    "        \n",
    "        # estrai i parametri per ciascun layer\n",
    "        \n",
    "        (k1_h, k1_w), (k2_h, k2_w), (k3_h, k3_w) = conv_kernel_size   # <-- estrai i tre valoro della grandezza del kernel size per ogni layer convolutivo\n",
    "        \n",
    "        (s1_h, s1_w), (s2_h, s2_w), (s3_h, s3_w) = conv_stride   # <-- estrai i tre valori dello stride del kernel convolutivo per ogni layer convolutivo\n",
    "    \n",
    "        (p1_h, p1_w), (p2_h, p2_w), (p3_h, p3_w) = pool_kernel_size   # <-- estrai i tre valori del kernel dello strato di pooling per ogni layer di pooling\n",
    "        \n",
    "        \n",
    "        # Primo blocco Conv2d\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels =input_channels,\n",
    "            out_channels =conv_out_channels,\n",
    "            kernel_size =(k1_h, k1_w),\n",
    "            stride = (s1_h, s1_w), \n",
    "            padding ='same'\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = nn.MaxPool2d((p1_h, p1_w)) if pool_type =='max' else nn.AvgPool2d((p1_h, p1_w))\n",
    "\n",
    "        # Secondo blocco Conv2d (doppio numero di feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = conv_out_channels,\n",
    "            out_channels = conv_out_channels*2,\n",
    "            kernel_size = (k2_h, k2_w),\n",
    "            stride = (s2_h, s2_w),\n",
    "            padding='same'\n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = nn.MaxPool2d((p2_h, p2_w)) if pool_type=='max' else nn.AvgPool2d((p2_h, p2_w))\n",
    "\n",
    "        # Terzo blocco Conv2d (triplo numero di feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels = conv_out_channels*2,\n",
    "            out_channels = conv_out_channels*3,\n",
    "            kernel_size = (k3_h, k3_w),\n",
    "            stride = (s3_h, s3_w),\n",
    "            padding = 'same'\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = nn.MaxPool2d((p3_h, p3_w)) if pool_type =='max' else nn.AvgPool2d((p3_h, p3_w))\n",
    "\n",
    "        # Fully-connected finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fns[0](x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act_fns[1](x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act_fns[2](x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # attivazione casuale sul primo FC\n",
    "        act_fn = random.choice(self.act_fns)\n",
    "        \n",
    "        x = act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "'''\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 3 * 26 = 78)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.5,\n",
    "        bidirectional: bool = False\n",
    "    ):\n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        base_sizes = [hidden_size] * num_layers\n",
    "        self.hidden_sizes = [h * 2 if bidirectional else h for h in base_sizes]\n",
    "        \n",
    "        # Definizione LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,\n",
    "                             hidden_size=self.hidden_sizes[0],\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * (2 if bidirectional else 1),\n",
    "                             hidden_size=self.hidden_sizes[1],\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * (2 if bidirectional else 1),\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * (2 if bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        x = x.permute(0, 3, 1, 2)              # -> (batch, tempo, canali, frequenze)\n",
    "        b, t, c, f = x.shape\n",
    "        x = x.reshape(b, t, c * f)            # -> (batch, tempo, input_size)\n",
    "\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "    \n",
    "    \n",
    "\n",
    "'''\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "self.embedding = nn.Linear(seq_length, d_model)\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=3, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Scelta: In questa implementazione abbiamo deciso di usare il tempo come sequenza.\n",
    "#In alternativa, potresti scegliere i bin di frequenza come sequenza, \n",
    "#ma ciò richiederebbe una diversa riorganizzazione delle dimensioni \n",
    "#(ad esempio, un permute diverso).\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        freq_bins: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        num_classes: int\n",
    "    ):\n",
    "        super(ReadMYMind, self).__init__()\n",
    "        \n",
    "        # Input size: canali * frequenze\n",
    "        self.input_size = num_channels * freq_bins\n",
    "        \n",
    "        # Embedding layer:\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(self.input_size, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        \n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Permuta per avere tempo come seq dim\n",
    "        x = x.permute(0, 3, 1, 2)        # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        #batch, time, channels, freqs = x.shape\n",
    "        b, t, c, f = x.shape\n",
    "        \n",
    "        #batch, tempo, channels*frequencies\n",
    "        x = x.reshape(b, t, c * f)      # -> (batch, tempo, input_size)\n",
    "\n",
    "        # Embedding\n",
    "        x = self.embedding(x)           # -> (batch, tempo, d_model)\n",
    "        x = x.permute(1, 0, 2)          # -> (tempo, batch, d_model)\n",
    "\n",
    "        # Transformer layers\n",
    "        \n",
    "        #Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        \n",
    "        #Classification\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))\n",
    "        return self.fc_classify(x_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79304bb3-51b3-45b6-bfce-ce5deada0540",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **DEFINIZIONE DEI MODELLI NUOVI PER P300 FROM 2D TIME-FREQUENCY SIGNAL  - DA LUGLIO A SETTEMBRE 2025** \n",
    "\n",
    "PS: \n",
    "\n",
    "La **vecchia CNN2D** (creata a **LUGLIO 2025**) con LE FUNZIONI DI ATTIVAZIONI DINAMICHE (LAYER CONVOLUTIVI + FC1) E DINAMISMO DEI KERNEL SIZE DI CONV e POOL LAYER e STRIDE è stata **SOSTITUITA DA QUELLA CNN2D_LSTM_TF**, USATA PER **BRAIN DECODING DEL MOTOR TASK, PER LA RAPPRESENTAZIONE TEMPO x FREQUENZA!**\n",
    "\n",
    "(**SALTA QUESTA PRIMA CELLA DI CODICE QUI SOTTO**, DOVE CI SAREBBERO **LA VECCHIE RETI CNN2D, BILSTM e TRANSFORMER DI LUGLIO 2025** CON O**LE FUNZIONI DI ATTIVAZIONI DINAMICHE (LAYER CONVOLUTIVI + FC1) E DINAMISMO DEI KERNEL SIZE DI CONV e POOL LAYER e STRIDE**!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aca851f9-ac1c-4737-881f-a93fead16147",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NUOVI PER P300 FROM 2D TIME-FREQUENCY SIGNAL  - LUGLIO 2025\n",
    "\n",
    "\n",
    "ATTENZIONE CHE ORA RISPETTO A PRIMA (PRE-LUGLIO 2025)\n",
    "\n",
    "\n",
    "\n",
    "Ora però, ragionandoci, potrei inserire dei valori da cui pescare, \n",
    "\n",
    "durante l'ottimizzazione degli iper-parametri della mia rete, che si riferiscono \n",
    "\n",
    "1) a valori di alcuni parametri generale dell'apprendimento delle reti\n",
    "2) a valori dei parametri architetturali di ciascuna delle mie singole reti neurali testate\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN1D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***OLD CNN2D***\n",
    "\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, canali, altezza, larghezza). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (38)\n",
    "e la \"larghezza\" come le finestre temporali (6).\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, canali, frequenze, tempo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''CNN2D CON LE FUNZIONI DI ATTIVAZIONI DINAMICHE (LAYER CONVOLUTIVI + FC1) E DINAMISMO DEI KERNEL SIZE DI CONV e POOL LAYER e STRIDE'''\n",
    "\n",
    "\n",
    "#https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, freq, tempo)\n",
    "        x = self.pool1(self.act_fns[0]( self.bn1(self.conv1(x)) ))\n",
    "        x = self.pool2(self.act_fns[1]( self.bn2(self.conv2(x)) ))\n",
    "        x = self.pool3(self.act_fns[2]( self.bn3(self.conv3(x)) ))\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #x = self.dropout( random.choice(self.act_fns)( self.fc1(x) ) )\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "  \n",
    "  \n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***BILSTM NEW*** \n",
    "\n",
    "\n",
    "Per la rete BiLSTM, bisogna configurare hidden_size e dropout come indicato nel sweep_config, \n",
    "insieme alla possibilità di scegliere se utilizzare o meno la bidirezionalità.\n",
    "\n",
    "\n",
    "1) il valore di hidden_sizes ossia dello spazio di embedding multidimensionale dei miei punti temporali \n",
    "del dato EEG (tutti i valori tra 16 e 32 con step di 2, ossia 16, 18, 20.. e così via)\n",
    "\n",
    "2) la scelta sulla bidirezionalità o meno (True o False)\n",
    "\n",
    "3) il valore di dropout (tra 0.0 e 0.5)\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - BILSTM\n",
    "\n",
    "\n",
    "| Iper-parametro  | Descrizione                                       | Valori possibili                 |\n",
    "| --------------- | ------------------------------------------------- | -------------------------------- |\n",
    "| `hidden_size`   | Dimensione dello stato nascosto per layer LSTM    | `[16, 18, …, 32]` (passo 2)      |\n",
    "| `bidirectional` | Se usare LSTM bidirezionale (0 → False, 1 → True) | `[0, 1]`                         |\n",
    "| **+ comune**    | `dropout`                                         | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***BILSTM OLD***\n",
    "\n",
    "\n",
    "\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 3 * 26 = 78)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''BILSTM HIDDEN SIZE, DROPOUT e BIDIREZIONALITA' DINAMICI\n",
    "\n",
    "Ecco come potresti adattare la tua BiLSTM “1D” in modo che lavori sul “2D” \n",
    "(canali×frequenze come feature per ciascun time‑step),\n",
    "mantenendo esattamente lo stesso schema iper‑parametrico che stai usando nello sweep:\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # invece di passare input_size in unità temporali, lo passi già come canali*frequenze\n",
    "        input_size:   int,   # = num_channels * num_freqs\n",
    "        hidden_size:  int,\n",
    "        output_size:  int,\n",
    "        num_layers:   int,   # --> ricordati qui è 3!\n",
    "        dropout:      float,\n",
    "        \n",
    "        # da sweep: bidirectional come 0/1\n",
    "        bidirectional: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bool(bidirectional)\n",
    "        \n",
    "        nd = 2 if self.bidirectional else 1\n",
    "        \n",
    "        # costruisci la lista degli hidden sizes\n",
    "        #base_sizes = [hidden_size] * num_layers\n",
    "        \n",
    "        #self.hidden_sizes = [\n",
    "            #h * (2 if self.bidirectional else 1)\n",
    "            #for h in base_sizes\n",
    "        #]\n",
    "        \n",
    "        # 1) hidden_size “per direzione” (non raddoppiato qui)\n",
    "        self.hidden_sizes = [hidden_size] * num_layers  # [24,24,24]\n",
    "        \n",
    "        # primo LSTM\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_sizes[0],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        # secondo LSTM\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[0]* nd,\n",
    "            hidden_size=self.hidden_sizes[1],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        # terzo LSTM\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[1]* nd,\n",
    "            hidden_size=self.hidden_sizes[2],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # fully‑connected sullo stato finale\n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_sizes[2]* nd,\n",
    "            output_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x ha shape (batch, canali, frequenze, tempo)\n",
    "        # 1) permuta per avere tempo come seq‑dim\n",
    "        #    (batch, tempo, canali, freq)\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        b, t, c, f = x.shape\n",
    "        \n",
    "        # 2) appiattisci canali×freq in feature vector\n",
    "        x = x.reshape(b, t, c * f)  # -> (batch, tempo, input_size)\n",
    "        \n",
    "        # 3) scorri gli LSTM + dropout\n",
    "        out, _ = self.lstm1(x); out = self.dropout(out)\n",
    "        out, _ = self.lstm2(out); out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out); out = self.dropout(out)\n",
    "        \n",
    "        # 4) prendi l'ultimo time‑step\n",
    "        out = out[:, -1, :]        # (batch, hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)        # (batch, output_size)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***TRANSFORMER NEW***\n",
    "                                                                \n",
    "Per il Transformer varieremo                                                        \n",
    "\n",
    "1) il valore dell'embedding, ossia \"d_model\" (con valori tra 8 e 64 con step di 8)\n",
    "2) il valore di head attenzionali, ossia \"num_heads\" (con valori tra 2 e 12 con step di 2) \n",
    "3) il valore di fully connected layers (con valori tra 1 e 3) dell\n",
    "\n",
    "4) il valore del feed_forward multiplier: descrive esattamente il suo ruolo, ossia è un moltiplicatore (mult) applicato\n",
    "alla dimensione del modello (d_model)per fissare l’ampiezza dell’FFN\n",
    "Ossia, il fattore con cui moltiplichi il tuo d_model per ottenere la dimensione interna del blocco feed-forward nel Transformer!\n",
    "\n",
    "In pratica, lo sweep esplora solo due moltiplicatori [2,4] invece di decine di valori hard-coded.\n",
    "Il modello transformer calcola internamente ogni run il corretto dim_feedforward = ff_mult * d_model.\n",
    "        \n",
    "5) il valore (stringa) della funzione di attivazione del layer fully connected (tra relu e gelu)\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "TABELLA FINALE RIASSUNTIVA - TRANSFORMER\n",
    "\n",
    "\n",
    "| Iper-parametro            | Descrizione                                                     | Valori possibili                 |\n",
    "| ------------------------- | --------------------------------------------------------------- | -------------------------------- |\n",
    "| `d_model`                 | Dimensione dell’embedding (modello)                             | `[8, 16, 24, …, 64]` (step 8)    |\n",
    "| `num_heads`               | Numero di teste di attenzione                                   | `[2, 4, 6, 8]`                   |\n",
    "| `num_layers`              | Numero di blocchi encoder                                       | `[1, 2, 3]`                      |\n",
    "| `ff_mult`                 | Moltiplicatore per la dimensione interna del feed-forward (FFN) | `[2, 4]`                         |\n",
    "| `transformer_activations` | Funzione di attivazione nel layer FFN                           | `[\"relu\", \"gelu\"]`               |\n",
    "| **+ comune**              | `dropout`                                                       | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    " \n",
    "                                                                ***TRANSFORMER OLD***\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "--> self.embedding = nn.Linear(seq_length, d_model)\n",
    "\n",
    "\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "--> self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "\n",
    "- Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "- Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=3, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Ecco una versione “2D” del tuo Transformer, \n",
    "che mantiene esattamente gli stessi iper‑parametri (d_model, num_heads, num_layers, ff_mult, dropout, transformer_activations)\n",
    "\n",
    "ma lavora su input di shape (batch, canali, frequenze, tempo).\n",
    "\n",
    "\n",
    "                                                                    ***TRANSFORMER PENULTIMO OLD***\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        num_channels: int,\n",
    "        num_freqs: int,\n",
    "        \n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        \n",
    "        num_layers: int,\n",
    "        num_classes: int,\n",
    "        \n",
    "        ff_mult: int, #<–– nuovo: moltiplicatore per ottenere dimensione del feedforward layer Transformer\n",
    "        dropout: float,\n",
    "        transformer_activations: str,  # \"relu\" o \"gelu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1) layer di embedding: da (channels*freqs) a d_model\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        # self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        self.embedding = nn.Linear(num_channels * num_freqs, d_model)\n",
    "        \n",
    "        # 2) Feed‑forward interno al Fully Connected Layer dell'Encoder Layer del Transformer\n",
    "        dim_feedforward = ff_mult * d_model\n",
    "\n",
    "        # Costruiamo un singolo encoder layer riusabile\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = d_model,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout,\n",
    "            activation = transformer_activations\n",
    "        )\n",
    "        # 3) due encoder: spaziale e temporale\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer  = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 4) # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # 5) # Fusione e classificazione finale\n",
    "        self.fc_fusion   = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # → permutiamo per avere il tempo come sequenza\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2) # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "\n",
    "        # → appiattiamo canali×freqs in feature vector\n",
    "        #    (batch, tempo, c*f)\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "\n",
    "        # 1) embedding → (batch, tempo, d_model)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "    \n",
    "        # 2) Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)  # → (tempo, batch, d_model)\n",
    "\n",
    "        # 3) Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # 4) Applichiamo il Transformer per l'attenzione temporale\n",
    "        \n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # 5) Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "\n",
    "        # 6) Fusione: per esempio, facciamo una media sul tempo (dimensione 0) \n",
    "        #    (tempo, batch, d_model) → media su dim 0 → (batch, d_model)\n",
    "        \n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0)) # -> (batch, d_model)\n",
    "\n",
    "        # 7) classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Versione 2D del tuo Transformer, ReadMYMind, che tratta i dati EEG in forma di spettrogramma (batch, canali, frequenze, tempo):\n",
    "\n",
    "Ramo temporale: \n",
    "\n",
    "Token length = T (time steps).\n",
    "Feature vector per token = C × F (canali × frequenze).\n",
    "\n",
    "Ramo frequenziale: \n",
    "\n",
    "Token length = F (frequenze).\n",
    "Feature vector per token = C × T (canali × time steps).\n",
    "\n",
    "Cross‑attention fra le due viste, somma e media sui time steps, e infine due FC per fusione e classificazione.\n",
    "\n",
    "\n",
    "\n",
    "L'architettura che hai postato per ReadMYMind si adatta perfettamente a una rappresentazione 2D time–frequency dei tuoi dati EEG. In breve:\n",
    "\n",
    "Due branche di embedding e encoding\n",
    "\n",
    "Temporal branch: tratta la sequenza lungo la dimensione tempo (T token), \n",
    "mappando per ciascun time‑step l’intero vettore “canali × frequenze” in uno spazio di dimensione d_model.\n",
    "\n",
    "Frequency branch: analogamente, tratti i token lungo la dimensione frequenza (F token), \n",
    "mappando per ciascun bin spettrale l’intero vettore “canali × time‑step” in d_model.\n",
    "\n",
    "TransformerEncoder separati\n",
    "\n",
    "Entrambi i transformer encoder hanno la stessa configurazione (d_model, nhead, dim_feedforward), \n",
    "ma uno gira su sequenze lunghe T e l’altro su sequenze lunghe F. \n",
    "\n",
    "In questo modo impari pattern temporali e pattern spettrali in parallelo.\n",
    "\n",
    "Cross‑attention\n",
    "\n",
    "Usi l’output temporale (xt) come query e l’output spettrale (xf) come key/value.\n",
    "In pratica, per ogni time‑step il modello “chiede” quali frequenze (e in quale misura) sono rilevanti, fondendo le due viste.\n",
    "\n",
    "Fusione e classificazione\n",
    "\n",
    "Sommi element‑wise xt + x_cross, fai la media sui T token (riassumendo l’intera sequenza temporale) e infine\n",
    "passi attraverso due layer lineari (fc_fusion e fc_classify).\n",
    "\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        num_channels: int,\n",
    "        num_freqs: int,\n",
    "        seq_length: int,\n",
    "        \n",
    "        # sweep params:\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        \n",
    "        num_layers: int,\n",
    "        num_classes: int,\n",
    "        \n",
    "        ff_mult: int, #<–– nuovo: moltiplicatore per ottenere dimensione del feedforward layer Transformer\n",
    "        dropout: float,\n",
    "        transformer_activations: str, # \"relu\" o \"gelu\"\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_feedforward = ff_mult * d_model\n",
    "        \n",
    "        # temporal embedding: features=C*F at each time-step\n",
    "        self.embedding_temporal = nn.Linear(num_channels * num_freqs, d_model)\n",
    "        \n",
    "        # frequency embedding: features=C*T at each frequency\n",
    "        self.embedding_frequency = nn.Linear(num_channels * seq_length, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=transformer_activations\n",
    "        )\n",
    "        # Transformer over time tokens\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Transformer over frequency tokens\n",
    "        self.frequency_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, F, T)\n",
    "        B, C, F, T = x.shape\n",
    "        \n",
    "        # temporal branch: tokens over T\n",
    "        xt = x.permute(0, 3, 1, 2).reshape(B, T, C * F)  # (B, T, C*F)\n",
    "        xt = self.embedding_temporal(xt)                  # (B, T, d_model)\n",
    "        xt = xt.permute(1, 0, 2)                          # (T, B, d_model)\n",
    "        xt = self.temporal_transformer(xt)                # (T, B, d_model)\n",
    "        \n",
    "        # frequency branch: tokens over F\n",
    "        xf = x.permute(0, 2, 1, 3).reshape(B, F, C * T)   # (B, F, C*T)\n",
    "        xf = self.embedding_frequency(xf)                 # (B, F, d_model)\n",
    "        xf = xf.permute(1, 0, 2)                          # (F, B, d_model)\n",
    "        xf = self.frequency_transformer(xf)               # (F, B, d_model)\n",
    "        \n",
    "        # cross-attention: query=temporal, key/value=frequency\n",
    "        x_cross, _ = self.cross_attention(xt, xf, xf)     # (T, B, d_model)\n",
    "        \n",
    "        # fuse and classify: sum+mean over time\n",
    "        x_fused = (xt + x_cross).mean(dim=0)              # (B, d_model)\n",
    "        x_fused = self.fc_fusion(x_fused)                 # (B, d_model)\n",
    "        out = self.fc_classify(x_fused)                   # (B, num_classes)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "51e8234a-75e9-41c0-b4c3-f7315bc27047",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Ecco un codice che fornisce dati di input fittizi a ciascuna rete neurale, \n",
    "stampa le dimensioni a ogni passaggio e verifica che gli output abbiano le forme attese.\n",
    "'''\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Testing shapes\n",
    "batch, channels, frequency, time, num_classes = 44, 61, 26, 11,2\n",
    "\n",
    "x = torch.randn(batch, channels, frequency, time)\n",
    "print(\"Input:\", x.shape)\n",
    "\n",
    "cnn = CNN2D(channels, num_classes,\n",
    "            conv_out_channels=16,\n",
    "            conv_k1_h=3,conv_k1_w=5,\n",
    "            conv_k2_h=3,conv_k2_w=5,\n",
    "            conv_k3_h=3,conv_k3_w=5,\n",
    "            conv_s1_h=1,conv_s1_w=2,\n",
    "            conv_s2_h=1,conv_s2_w=2,\n",
    "            conv_s3_h=1,conv_s3_w=2,\n",
    "            pool_p1_h=1,pool_p1_w=2,\n",
    "            pool_p2_h=1,pool_p2_w=2,\n",
    "            pool_p3_h=1,pool_p3_w=1,\n",
    "            pool_type='max',fc1_units=10,dropout=0.1,\n",
    "            cnn_act1='relu',cnn_act2='relu',cnn_act3='elu')\n",
    "\n",
    "out_cnn = cnn(x)\n",
    "print(\"CNN2D output:\", out_cnn.shape)\n",
    "\n",
    "\n",
    "lstm = ReadMEndYou(input_size=channels*frequency, hidden_size=24, output_size=num_classes,\n",
    "                   num_layers=3, dropout=0.1, bidirectional=1)\n",
    "out_lstm = lstm(x)\n",
    "print(\"LSTM output:\", out_lstm.shape)\n",
    "\n",
    "trans = ReadMYMind(num_channels=channels, num_freqs= frequency, seq_length=time,\n",
    "                   d_model=64, num_heads=8, num_layers=2, num_classes=num_classes,\n",
    "                   ff_mult=2, dropout=0.1, transformer_activations='relu')\n",
    "out_trans = trans(x)\n",
    "print(\"Transformer output:\", out_trans.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64773a87-13fd-4f4e-9332-47af595e274c",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Ecco un codice che fornisce dati di input fittizi a ciascuna rete neurale, \n",
    "stampa le dimensioni a ogni passaggio e verifica che gli output abbiano le forme attese.\n",
    "\n",
    "Ho mantenuto le forme coerenti con i tuoi parametri:\n",
    "\n",
    "\n",
    "Batch size: 8\n",
    "Numero di canali EEG: 3\n",
    "Numero di frequenze: 38\n",
    "Numero di timepoints (campioni temporali): 100\n",
    "Numero di classi: 2\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parametri\n",
    "batch_size = 44\n",
    "input_channels = 61   # Canali EEG\n",
    "num_freqs = 26       # Numero di frequenze\n",
    "num_timepoints = 11  # Numero di campioni temporali\n",
    "num_classes = 2       # Numero di classi\n",
    "\n",
    "# Creazione di dati fittizi per il test\n",
    "x = torch.randn(batch_size, input_channels, num_freqs, num_timepoints)  # (batch, canali, frequenze, tempo)\n",
    "print(f\"Input iniziale: {x.shape}\\n\")\n",
    "\n",
    "# ---- CNN2D ----\n",
    "cnn_model = CNN2D(input_channels=input_channels, num_classes=num_classes)\n",
    "cnn_output = cnn_model(x)\n",
    "print(f\"Output CNN2D: {cnn_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n",
    "# ---- ReadMEndYou (LSTM) ----\n",
    "hidden_sizes = [24, 48, 62]\n",
    "lstm_model = ReadMEndYou(input_size=input_channels * num_freqs, hidden_sizes=hidden_sizes, output_size=num_classes)\n",
    "lstm_output = lstm_model(x)\n",
    "print(f\"Output ReadMEndYou (LSTM): {lstm_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n",
    "# ---- ReadMYMind (Transformer) ----\n",
    "d_model = 64   # Dimensione embedding\n",
    "num_heads = 8   # Numero di teste di attenzione\n",
    "num_layers = 3  # Numero di strati Transformer\n",
    "\n",
    "transformer_model = ReadMYMind(d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes)\n",
    "transformer_output = transformer_model(x)\n",
    "print(f\"Output ReadMYMind (Transformer): {transformer_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4012c1c4-31f6-426e-9423-3f3031561666",
   "metadata": {
    "tags": []
   },
   "source": [
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, freq, tempo)\n",
    "        x = self.pool1(self.act_fns[0]( self.bn1(self.conv1(x)) ))\n",
    "        x = self.pool2(self.act_fns[1]( self.bn2(self.conv2(x)) ))\n",
    "        x = self.pool3(self.act_fns[2]( self.bn3(self.conv3(x)) ))\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #x = self.dropout( random.choice(self.act_fns)( self.fc1(x) ) )\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # invece di passare input_size in unità temporali, lo passi già come canali*frequenze\n",
    "        input_size:   int,   # = num_channels * num_freqs\n",
    "        hidden_size:  int,\n",
    "        output_size:  int,\n",
    "        num_layers:   int,   # --> ricordati qui è 3!\n",
    "        dropout:      float,\n",
    "        \n",
    "        # da sweep: bidirectional come 0/1\n",
    "        bidirectional: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bool(bidirectional)\n",
    "        \n",
    "        # costruisci la lista degli hidden sizes\n",
    "        base_sizes = [hidden_size] * num_layers\n",
    "        self.hidden_sizes = [\n",
    "            h * (2 if self.bidirectional else 1)\n",
    "            for h in base_sizes\n",
    "        ]\n",
    "        # primo LSTM\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_sizes[0],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        # secondo LSTM\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[0],\n",
    "            hidden_size=self.hidden_sizes[1],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        # terzo LSTM\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[1],\n",
    "            hidden_size=self.hidden_sizes[2],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # fully‑connected sullo stato finale\n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_sizes[2],\n",
    "            output_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x ha shape (batch, canali, frequenze, tempo)\n",
    "        # 1) permuta per avere tempo come seq‑dim\n",
    "        #    (batch, tempo, canali, freq)\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        b, t, c, f = x.shape\n",
    "        \n",
    "        # 2) appiattisci canali×freq in feature vector\n",
    "        x = x.reshape(b, t, c * f)  # -> (batch, tempo, input_size)\n",
    "        \n",
    "        # 3) scorri gli LSTM + dropout\n",
    "        out, _ = self.lstm1(x); out = self.dropout(out)\n",
    "        out, _ = self.lstm2(out); out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out); out = self.dropout(out)\n",
    "        \n",
    "        # 4) prendi l'ultimo time‑step\n",
    "        out = out[:, -1, :]        # (batch, hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)        # (batch, output_size)\n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        num_channels: int,\n",
    "        num_freqs: int,\n",
    "        seq_length: int,\n",
    "        \n",
    "        # sweep params:\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        \n",
    "        num_layers: int,\n",
    "        num_classes: int,\n",
    "        \n",
    "        ff_mult: int, #<–– nuovo: moltiplicatore per ottenere dimensione del feedforward layer Transformer\n",
    "        dropout: float,\n",
    "        transformer_activations: str, # \"relu\" o \"gelu\"\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_feedforward = ff_mult * d_model\n",
    "        \n",
    "        # temporal embedding: features=C*F at each time-step\n",
    "        self.embedding_temporal = nn.Linear(num_channels * num_freqs, d_model)\n",
    "        \n",
    "        # frequency embedding: features=C*T at each frequency\n",
    "        self.embedding_frequency = nn.Linear(num_channels * seq_length, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=transformer_activations\n",
    "        )\n",
    "        # Transformer over time tokens\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Transformer over frequency tokens\n",
    "        self.frequency_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, F, T)\n",
    "        B, C, F, T = x.shape\n",
    "        \n",
    "        # temporal branch: tokens over T\n",
    "        xt = x.permute(0, 3, 1, 2).reshape(B, T, C * F)  # (B, T, C*F)\n",
    "        xt = self.embedding_temporal(xt)                  # (B, T, d_model)\n",
    "        xt = xt.permute(1, 0, 2)                          # (T, B, d_model)\n",
    "        xt = self.temporal_transformer(xt)                # (T, B, d_model)\n",
    "        \n",
    "        # frequency branch: tokens over F\n",
    "        xf = x.permute(0, 2, 1, 3).reshape(B, F, C * T)   # (B, F, C*T)\n",
    "        xf = self.embedding_frequency(xf)                 # (B, F, d_model)\n",
    "        xf = xf.permute(1, 0, 2)                          # (F, B, d_model)\n",
    "        xf = self.frequency_transformer(xf)               # (F, B, d_model)\n",
    "        \n",
    "        # cross-attention: query=temporal, key/value=frequency\n",
    "        x_cross, _ = self.cross_attention(xt, xf, xf)     # (T, B, d_model)\n",
    "        \n",
    "        # fuse and classify: sum+mean over time\n",
    "        x_fused = (xt + x_cross).mean(dim=0)              # (B, d_model)\n",
    "        x_fused = self.fc_fusion(x_fused)                 # (B, d_model)\n",
    "        out = self.fc_classify(x_fused)                   # (B, num_classes)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ee67625-abe0-40fa-8492-33d9dc03a6f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "\n",
    "Se volessi usare \n",
    "\n",
    "CNN2D_LSTM_FT (come quella del BRAIN DECODING MOTORIO updated a settembre 2025),\n",
    "BiLSTM e Transformers (queste ultime due di LUGLIO 2025!)\n",
    "\n",
    "\n",
    "\n",
    "Note veloci:\n",
    "\n",
    "Va benissimo channels=61: la BatchNorm2d(input_channels) della CNN accetta qualsiasi C.\n",
    "Con tre pooling 2×2, assicurati che F e T siano abbastanza grandi da non collassare a 0 (con 26×11 sei OK: diventano ~3×1).\n",
    "Se vuoi provare su GPU, aggiungi device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'), poi .to(device) su modello e x.\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# MODELLI (già definiti da te)\n",
    "# -----------------------------\n",
    "# CNN2D_LSTM_TF, ReadMEndYou, ReadMYMind\n",
    "# (qui si assume che le classi che hai incollato sopra siano già nel namespace)\n",
    "\n",
    "# -----------------------------\n",
    "# SANITY CHECK: forward dummy\n",
    "# -----------------------------\n",
    "\n",
    "# dimensioni di test (come nel tuo snippet)\n",
    "batch, channels, frequency, time, num_classes = 44, 61, 26, 11, 2\n",
    "\n",
    "# input finto: (B, C, F, T)\n",
    "x = torch.randn(batch, channels, frequency, time)\n",
    "print(\"Input:\", x.shape)\n",
    "\n",
    "# 1) CNN2D + LSTM + FC\n",
    "#    Nota: il modello usa 3 MaxPool2d(2,2). Con F=26 e T=11 dopo i pool avrai ~ F=3, T=1,\n",
    "#    quindi l'LSTM riceve una sequenza di lunghezza 3 con input_size=128 → OK.\n",
    "cnn = CNN2D_LSTM_TF(input_channels=channels, num_classes=num_classes, dropout=0.1)\n",
    "out_cnn = cnn(x)\n",
    "print(\"CNN2D_LSTM_TF output:\", out_cnn.shape)  # atteso: (batch, num_classes)\n",
    "\n",
    "# 2) BiLSTM 2D (tempo come seq; feature = C*F)\n",
    "#    La classe ReadMEndYou appiattisce (C*F) per timestep → input_size = channels*frequency\n",
    "lstm = ReadMEndYou(\n",
    "    input_size=channels * frequency,\n",
    "    hidden_size=24,\n",
    "    output_size=num_classes,\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    "    bidirectional=1\n",
    ")\n",
    "out_lstm = lstm(x)\n",
    "print(\"BiLSTM (ReadMEndYou) output:\", out_lstm.shape)  # atteso: (batch, num_classes)\n",
    "\n",
    "# 3) Transformer tempo×frequenza\n",
    "#    d_model deve essere divisibile per num_heads (64 % 8 == 0)\n",
    "trans = ReadMYMind(\n",
    "    num_channels=channels,\n",
    "    num_freqs=frequency,\n",
    "    seq_length=time,\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes,\n",
    "    ff_mult=2,\n",
    "    dropout=0.1,\n",
    "    transformer_activations='relu'\n",
    ")\n",
    "out_trans = trans(x)\n",
    "print(\"Transformer (ReadMYMind) output:\", out_trans.shape)  # atteso: (batch, num_classes)\n",
    "\n",
    "# opzionale: piccole asserzioni\n",
    "assert out_cnn.shape  == (batch, num_classes)\n",
    "assert out_lstm.shape == (batch, num_classes)\n",
    "assert out_trans.shape== (batch, num_classes)\n",
    "print(\"✔️ Tutti i forward pass hanno prodotto la shape attesa.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7f84e-f3dc-4c6b-984a-2a1cc464cfd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **DL Models - EEG Spectrograms - Time x Frequencies NEW VERSIONS (SAME OF MOTOR TASKS!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b6dd9-44e0-40ed-ae52-3c5ecbd21a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NUOVI PER P300 FROM 2D TIME-FREQUENCY SIGNAL  - AGGIORNATI A SETTEMBRE 2025 COME QUELLI DEL TASK MOTORIO!\n",
    "\n",
    "\n",
    "                                                                ***CNN2D_LSTM_TF*** \n",
    "\n",
    "\n",
    "Uso la stessa rete neurale usata per Brain Decoding Task Motorio\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN2D_LSTM_TF(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels=61, num_classes=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # --- Block 1 ---\n",
    "        self.bn1   = nn.BatchNorm2d(input_channels)    # normalizza 64 canali\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # --- Block 2 (residual) ---\n",
    "        # Proiezione 1×1 per riallineare i canali di skip (32→64)\n",
    "        self.res_conv = nn.Conv2d(32, 64, kernel_size=1, bias=False)\n",
    "        self.res_bn   = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a   = nn.BatchNorm2d(32)\n",
    "        self.conv2a = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2b   = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "\n",
    "        # --- Head: Dropout + LSTM + FC finale ---\n",
    "        self.dropout     = nn.Dropout(dropout)\n",
    "        self.hidden_size = 64\n",
    "        \n",
    "        # dopo 3 pool: freq da 81→10, time da 9→1 → feature per timestep = 128×1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,64,81,9)\n",
    "\n",
    "        # --- Block 1 ---\n",
    "        x = self.bn1(x)                   # → (B,64,81,9)\n",
    "        x = F.relu(self.conv1(x))         # → (B,32,81,9)\n",
    "        x = self.pool(x)                  # → (B,32,40,4)\n",
    "\n",
    "        # --- Block 2 (residuo) ---\n",
    "        res = x                           # skip: (B,32,40,4)\n",
    "        res = self.res_conv(res)          # progetto: → (B,64,40,4)\n",
    "        res = self.res_bn(res)            # → (B,64,40,4)\n",
    "\n",
    "        # main path\n",
    "        x = self.bn2a(x)                  # → (B,32,40,4)\n",
    "        x = F.relu(self.conv2a(x))        # → (B,64,40,4)\n",
    "        x = self.bn2b(x)                  # → (B,64,40,4)\n",
    "        x = self.conv2b(x)                # → (B,64,40,4)\n",
    "\n",
    "        x = x + res                       # somma residua valida → (B,64,40,4)\n",
    "        x = F.relu(x)                     \n",
    "        x = self.pool(x)                  # → (B,64,20,2)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # → (B,128,20,2)\n",
    "        x = self.pool(x)                     # → (B,128,10,1)\n",
    "\n",
    "        # --- Prepara per LSTM ---\n",
    "        x = x.permute(0, 2, 1, 3)         # → (B,10,128,1)\n",
    "        b, seq, ch, tw = x.size()        \n",
    "        x = x.reshape(b, seq, ch * tw)    # → (B,10,128)\n",
    "\n",
    "        # --- LSTM + classificazione ---\n",
    "        out, _ = self.lstm(self.dropout(x))  # → out: (B,10,64)\n",
    "        last = out[:, -1, :]                 # prendo l’ultima uscita → (B,64)\n",
    "        logits = self.classifier(last)       # → (B,2)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "'''\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "'''\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 64 * 81 = 7471)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "'''\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "self.embedding = nn.Linear(seq_length, d_model)\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Scelta: In questa implementazione abbiamo deciso di usare il tempo come sequenza.\n",
    "#In alternativa, potresti scegliere i bin di frequenza come sequenza, ma ciò richiederebbe una diversa riorganizzazione delle dimensioni \n",
    "#(ad esempio, un permute diverso).\n",
    "\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=61, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71292815-fefe-42cf-8523-4d4ca9791cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ecco un codice che fornisce dati di input fittizi a ciascuna rete neurale, \n",
    "stampa le dimensioni a ogni passaggio e verifica che gli output abbiano le forme attese.\n",
    "\n",
    "Ho mantenuto le forme coerenti con i tuoi parametri:\n",
    "\n",
    "\n",
    "Batch size: 8\n",
    "Numero di canali EEG: 3\n",
    "Numero di frequenze: 38\n",
    "Numero di timepoints (campioni temporali): 100\n",
    "Numero di classi: 2\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parametri\n",
    "batch_size = 45\n",
    "input_channels = 61   # Canali EEG\n",
    "num_freqs = 26       # Numero di frequenze\n",
    "num_timepoints = 11  # Numero di campioni temporali\n",
    "\n",
    "num_classes = 2       # Numero di classi\n",
    "\n",
    "dropout = 0.2\n",
    "\n",
    "# Creazione di dati fittizi per il test\n",
    "x = torch.randn(batch_size, input_channels, num_freqs, num_timepoints)  # (batch, canali, frequenze, tempo)\n",
    "print(f\"Input iniziale: {x.shape}\\n\")\n",
    "\n",
    "# ---- CNN2D ----\n",
    "#cnn_model = CNN2D(input_channels=input_channels, num_classes=num_classes)\n",
    "\n",
    "cnn_model = CNN2D_LSTM_TF(input_channels = input_channels, num_classes =num_classes, dropout = dropout)\n",
    "cnn_output = cnn_model(x)\n",
    "print(f\"Output CNN2D_LSTM_TF: {cnn_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n",
    "\n",
    "# ---- ReadMEndYou (LSTM) ----\n",
    "hidden_sizes = [24, 48, 62]\n",
    "lstm_model = ReadMEndYou(input_size=input_channels * num_freqs, hidden_sizes=hidden_sizes, output_size=num_classes)\n",
    "lstm_output = lstm_model(x)\n",
    "print(f\"Output ReadMEndYou (LSTM): {lstm_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n",
    "# ---- ReadMYMind (Transformer) ----\n",
    "d_model = 64   # Dimensione embedding\n",
    "num_heads = 8   # Numero di teste di attenzione\n",
    "num_layers = 3  # Numero di strati Transformer\n",
    "\n",
    "transformer_model = ReadMYMind(d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes)\n",
    "transformer_output = transformer_model(x)\n",
    "print(f\"Output ReadMYMind (Transformer): {transformer_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677eff9-edee-40ba-a838-e7d453e5a236",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Early Stopping - EEG Spectrograms - Time x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882afa8-4d82-408c-af34-1f954977f535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE EARLY STOPPING\n",
    "'''\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5938fc5-c99a-4cfa-b3df-f5f9ad447ae6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login - EEG Spectrograms - Time x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f014952-9a43-4459-a583-93093e3a280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "WEIGHT AND BIASES LOGIN\n",
    "\n",
    "Il messaggio che stai ricevendo indica \n",
    "che sei già connesso al tuo account Weights & Biases (wandb).\n",
    "\n",
    "Se vuoi forzare il login, puoi usare il comando suggerito:\n",
    "\n",
    "wandb login --relogin\n",
    "\n",
    "Questo comando ti permetterà di reinserire le credenziali e riconnetterti al tuo account.\n",
    "Se non hai bisogno di disconnetterti o di cambiare l'account,\n",
    "puoi semplicemente continuare a usare wandb senza ulteriori passaggi. \n",
    "Hai bisogno di ulteriore assistenza con wandb o con il tuo progetto?\n",
    "'''\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"✅ Weights & Biases login effettuato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75d82e-5dd4-474d-9ca8-683d86af2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Per modificare il percorso in cui W&B salva i dati localmente,\n",
    "puoi configurare la variabile di ambiente WANDB_DIR.\n",
    "\n",
    "Questo ti permette di specificare una directory personalizzata in cui W&B salva tutti i file associati al tuo run, inclusi i dati e i log.\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "# Imposta la directory per i dati W&B:\n",
    "# Questo cambierà la cartella in cui W&B salva i dati per quella sessione di esecuzione\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''ATTENZIONE CHE QUI HO AGGIUNTO --> \"_time_frequency_\" alla WB_dir!'''\n",
    "\n",
    "# Definisci la cartella principale\n",
    "WB_dir = \"/home/stefano/Interrogait/WB_spectrograms_time_frequency_analyses\"\n",
    "os.makedirs(WB_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DIR\"] = WB_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9696a-9322-4c67-8f37-1ab3f4e4d859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Apri il file in modalità lettura binaria ('rb')\n",
    "\n",
    "#path = '/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms/'\n",
    "path = '/home/stefano/Interrogait/all_datas/Familiar_Spectrograms/'\n",
    "\n",
    "with open(f\"{path}new_all_th_concat_spectrograms_coupled_exp.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb4c55-71c9-4625-9982-d39722468c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Itera sulle chiavi del dizionario principale\n",
    "for condition, values in data.items():\n",
    "    if isinstance(values, dict) and \"data\" in values and \"labels\" in values:\n",
    "        X_shape = values[\"data\"].shape\n",
    "        y_length = len(values[\"labels\"])\n",
    "        print(f\"🔹 Condizione: {condition}\")\n",
    "        print(f\"   ➡ Shape dati: {X_shape}\")\n",
    "        print(f\"   ➡ Lunghezza labels: {y_length}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d572ec-951a-4201-95a0-56c5e06056db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login PRECEDURA CORRETTA ✅ (CANCELLAZIONE RUNS e NON SWEEPS!) - EEG Spectrograms - Time x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b9cb6-a312-4aa5-96bd-a127ad3d6448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Sì, è perfettamente normale: in W&B un progetto è semplicemente un contenitore di run e di sweep, \n",
    "e rimane visibile (anche se vuoto) fintanto che non lo archivi o lo cancelli esplicitamente. \n",
    "\n",
    "Cancellare tutte le run e tutti i sweep in un progetto non elimina il progetto stesso — lascia solo un progetto “vuoto” con zero run/sweep.\n",
    "\n",
    "\n",
    "Come rimuovere (o archiviare) anche i progetti\n",
    "\n",
    "\n",
    "1) Dal web UI\n",
    "\n",
    "Vai nella pagina del progetto che vuoi rimuovere.\n",
    "\n",
    "Clicca sui tre puntini (⋯) accanto al nome del progetto.\n",
    "Scegli “Archive project” (per nasconderlo) o, se disponibile, “Delete project” per eliminarlo definitivamente.\n",
    "\n",
    "\n",
    "2) Da CLI\n",
    "Se hai una versione recente del wandb CLI, puoi usare (da terminale):\n",
    "\n",
    "\n",
    "wandb project delete <ENTITY>/<PROJECT_NAME>\n",
    "\n",
    "Esempio:\n",
    "\n",
    "wandb project delete stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_1_20_familiar_th\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Programmaticamente\n",
    "Attualmente W&B non espone un’API Python ufficiale per cancellare progetti; l’opzione più rapida resta il CLI o la UI.\n",
    "\n",
    "In sintesi\n",
    "Cancellare run/sweep → lascia il progetto “vuoto” (0 run, 0 sweep).\n",
    "\n",
    "Progetto visibile → finché non lo archivi o elimini esplicitamente, lo vedrai nella lista.\n",
    "\n",
    "Per toglierlo di mezzo → archivia o cancella il progetto tramite i tre puntini in UI, oppure con wandb project delete da CLI.\n",
    "\n",
    "\n",
    "\n",
    "IO:\n",
    "ok ma quindi se ricreo il progetto con lo stesso nome semplicemente ripopola quel progetto ma con sweeps e runs nuovi giusto?\n",
    "\n",
    "\n",
    "ChatGPT:\n",
    "\n",
    "Esatto. In W&B il progetto è identificato dal solo nome (più l’entity) e non ha uno “stato” permanente di esistenza: se tu:\n",
    "\n",
    "Elimini o archivi totalmente il progetto (tramite UI o wandb project delete),\n",
    "\n",
    "E poi lanci di nuovo uno wandb.init(..., project=\"quel_nome\"),\n",
    "\n",
    "W&B creerà un nuovo progetto con lo stesso nome, vuoto all’inizio, in cui andranno a confluire i tuoi nuovi sweep e run.\n",
    "\n",
    "Quindi:\n",
    "\n",
    "Se archivi il vecchio progetto, è solo nascosto ma rimane là;\n",
    "\n",
    "Se lo cancelli definitivamente, sparisce e puoi ricrearlo identico per nome, e in quel momento sarà “pulito” (0 sweep, 0 run)\n",
    "pronto a riempirsi con i tuoi esperimenti successivi.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from itertools import product\n",
    "import wandb\n",
    "\n",
    "# 1) Parti delle stringhe da combinare\n",
    "prefixes = [\n",
    "    \"th_resp_vs_pt_resp\",\n",
    "    \"th_resp_vs_shared_resp\",\n",
    "    \"pt_resp_vs_shared_resp\",\n",
    "]\n",
    "middles = [\n",
    "    \"spectrograms_time_frequency\",\n",
    "]\n",
    "\n",
    "suffixes = [\n",
    "    \"familiar_th\",\n",
    "    \"familiar_pt\",\n",
    "    \"unfamiliar_th\",\n",
    "    \"unfamiliar_pt\",\n",
    "]\n",
    "\n",
    "# 2) Genera tutti i nomi di progetto\n",
    "projects = [\n",
    "    f\"{p}_{m}_{s}\"\n",
    "    for p, m, s in product(prefixes, middles, suffixes)\n",
    "]\n",
    "\n",
    "# 3) Configura l’API e l’entity\n",
    "entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "api = wandb.Api()\n",
    "\n",
    "# 4) Itera su ogni progetto: svuota le run e poi cancella gli sweep\n",
    "for proj in projects:\n",
    "    project_path = f\"{entity}/{proj}\"\n",
    "    print(f\"\\n→ Progetto: {project_path}\")\n",
    "\n",
    "    # 4.1 Cancella tutte le run via Python API\n",
    "    try:\n",
    "        runs = api.runs(project_path)\n",
    "        if runs:\n",
    "            print(f\"   • Eliminando {len(runs)} run…\")\n",
    "            for run in runs:\n",
    "                try:\n",
    "                    run.delete()\n",
    "                except Exception as e:\n",
    "                    print(f\"     – Errore cancellando run {run.id}: {e}\")\n",
    "                else:\n",
    "                    print(f\"     – run {run.id} eliminata\")\n",
    "        else:\n",
    "            print(\"   (nessuna run trovata)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Impossibile caricare le run: {e}\")\n",
    "\n",
    "    # 4.2 Cancella tutti gli sweep via CLI Python module\n",
    "    #    Evitiamo di chiamare un eseguibile esterno, usiamo `python -m wandb`\n",
    "    #    Lo stesso interprete che esegue questo script è in sys.executable\n",
    "    cmd_list = [\n",
    "        sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "        \"--project\", project_path, \"--list\"\n",
    "    ]\n",
    "    res = subprocess.run(cmd_list, capture_output=True, text=True)\n",
    "\n",
    "    if res.returncode != 0 or not res.stdout.strip():\n",
    "        print(\"   • Nessuno sweep trovato o progetto inesistente\")\n",
    "        continue\n",
    "\n",
    "    # Ogni riga di res.stdout ha uno sweep_id come primo token\n",
    "    for line in res.stdout.splitlines():\n",
    "        sweep_id = line.split()[0]\n",
    "        print(f\"   • Cancello sweep {sweep_id}\")\n",
    "        cmd_delete = [\n",
    "            sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "            \"--delete\", f\"{project_path}/{sweep_id}\"\n",
    "        ]\n",
    "        subprocess.run(cmd_delete, check=False)\n",
    "\n",
    "    print(f\"  ✅ Run e sweep eliminati per {project_path}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ba671ab-3ccd-4797-8c9e-86c1cb64092a",
   "metadata": {},
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from itertools import product\n",
    "import wandb\n",
    "\n",
    "entity   = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "api      = wandb.Api()\n",
    "\n",
    "# il solo progetto di cui voglio ripulire sweep+run\n",
    "target = \"th_resp_vs_pt_resp_spectrograms_time_frequency_familiar_th\"\n",
    "project_path = f\"{entity}/{target}\"\n",
    "\n",
    "print(f\"\\n→ Progetto: {project_path}\")\n",
    "\n",
    "# 4.1 Cancella tutte le run via Python API\n",
    "try:\n",
    "    runs = api.runs(project_path)\n",
    "    if runs:\n",
    "        print(f\"   • Eliminando {len(runs)} run…\")\n",
    "        for run in runs:\n",
    "            try:\n",
    "                run.delete()\n",
    "            except Exception as e:\n",
    "                print(f\"     – Errore cancellando run {run.id}: {e}\")\n",
    "            else:\n",
    "                print(f\"     – run {run.id} eliminata\")\n",
    "    else:\n",
    "        print(\"   (nessuna run trovata)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Impossibile caricare le run: {e}\")\n",
    "\n",
    "# 4.2 Cancella tutti gli sweep via CLI (usando python -m wandb)\n",
    "cmd_list = [\n",
    "    sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "    \"--project\", project_path, \"--list\"\n",
    "]\n",
    "res = subprocess.run(cmd_list, capture_output=True, text=True)\n",
    "\n",
    "if res.returncode != 0 or not res.stdout.strip():\n",
    "    print(\"   • Nessuno sweep trovato o progetto inesistente\")\n",
    "else:\n",
    "    for line in res.stdout.splitlines():\n",
    "        sweep_id = line.split()[0]\n",
    "        print(f\"   • Cancello sweep {sweep_id}\")\n",
    "        cmd_delete = [\n",
    "            sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "            \"--delete\", f\"{project_path}/{sweep_id}\"\n",
    "        ]\n",
    "        subprocess.run(cmd_delete, check=False)\n",
    "\n",
    "print(f\"  ✅ Run e sweep eliminati per {project_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ddd51-841c-4bed-9029-fa5d52037246",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login PRECEDURA CORRETTA ✅ (CANCELLAZIONE RUNS e ANCHE SWEEPS!) - EEG Spectrograms - Time x Frequencies V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335941c-2282-4263-a456-6e736134bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perfetto, hai due script:\n",
    "\n",
    "uno funzionale e robusto che elimina run e sweep, ma usa pattern statici,\n",
    "\n",
    "uno che usa prefisso, medio e suffisso per generare i nomi dei progetti, ma è meno dettagliato.\n",
    "\n",
    "Ti creo una versione unificata che:\n",
    "\n",
    "Usa prefissi, medii e suffissi per generare i nomi dei progetti (come nel secondo script).\n",
    "\n",
    "Per ogni progetto, cancella tutte le run (come nel primo script).\n",
    "\n",
    "Cancella tutti gli sweep usando wandb sweep --delete.\n",
    "\n",
    "Verifica che gli sweep siano stati eliminati correttamente.\n",
    "\n",
    "\n",
    "✅ Script finale combinato:\n",
    "    \n",
    "✅ Cosa fa questo script:\n",
    "\n",
    "Genera i progetti combinando prefix, middle, suffix.\n",
    "\n",
    "Cancella le run usando l’API Python (run.delete()).\n",
    "\n",
    "Cancella gli sweep usando wandb sweep --delete, richiamato tramite subprocess.\n",
    "\n",
    "Verifica se gli sweep sono stati realmente eliminati, usando proj.sweeps().\n",
    "\n",
    "📝 Dipendenze e prerequisiti:\n",
    "\n",
    "wandb dev'essere installato e autenticato (wandb login)\n",
    "\n",
    "Lo script va eseguito in un ambiente con accesso alla CLI di wandb (es. terminale Python)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from itertools import product\n",
    "import wandb\n",
    "\n",
    "# --- Configurazione ---\n",
    "entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "api = wandb.Api()\n",
    "\n",
    "# --- Parti del nome progetto ---\n",
    "prefixes = [\n",
    "    \"th_resp_vs_pt_resp\",\n",
    "    \"th_resp_vs_shared_resp\",\n",
    "    \"pt_resp_vs_shared_resp\",\n",
    "]\n",
    "middles = [\n",
    "    \"spectrograms_time_frequency\",\n",
    "]\n",
    "suffixes = [\n",
    "    \"familiar_th\",\n",
    "    \"familiar_pt\",\n",
    "    \"unfamiliar_th\",\n",
    "    \"unfamiliar_pt\",\n",
    "]\n",
    "\n",
    "# --- Genera i nomi dei progetti ---\n",
    "project_names = [\n",
    "    f\"{p}_{m}_{s}\"\n",
    "    for p, m, s in product(prefixes, middles, suffixes)\n",
    "]\n",
    "\n",
    "# --- Itera su ogni progetto ---\n",
    "for proj_name in project_names:\n",
    "    path = f\"{entity}/{proj_name}\"\n",
    "    print(f\"\\n→ Progetto: {path}\")\n",
    "\n",
    "    # --- 1. Elimina tutte le run ---\n",
    "    try:\n",
    "        runs = api.runs(path, per_page=None)\n",
    "        runs = list(runs)\n",
    "        if runs:\n",
    "            print(f\"   • Eliminando {len(runs)} run…\")\n",
    "            for run in runs:\n",
    "                try:\n",
    "                    run.delete()\n",
    "                    print(f\"     – Run {run.id} eliminata\")\n",
    "                except Exception as e:\n",
    "                    print(f\"     – Errore eliminando run {run.id}: {e}\")\n",
    "        else:\n",
    "            print(\"   (nessuna run trovata)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Errore caricando le run: {e}\")\n",
    "        continue  # salta alla prossima\n",
    "\n",
    "    # --- 2. Ottieni e cancella gli sweep tramite CLI ---\n",
    "    cmd_list = [\n",
    "        sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "        \"--project\", path, \"--list\"\n",
    "    ]\n",
    "    res = subprocess.run(cmd_list, capture_output=True, text=True)\n",
    "\n",
    "    if res.returncode != 0 or not res.stdout.strip():\n",
    "        print(\"   • Nessuno sweep trovato o progetto inesistente\")\n",
    "        continue\n",
    "\n",
    "    sweep_ids = []\n",
    "    for line in res.stdout.strip().splitlines():\n",
    "        sweep_id = line.split()[0]\n",
    "        sweep_ids.append(sweep_id)\n",
    "        print(f\"   • Cancello sweep {sweep_id}\")\n",
    "        cmd_delete = [\n",
    "            sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "            \"--delete\", f\"{path}/{sweep_id}\"\n",
    "        ]\n",
    "        subprocess.run(cmd_delete, check=False)\n",
    "\n",
    "    # --- 3. Verifica cancellazione sweep ---\n",
    "    print(\"   • Verifica sweep attivi dopo la cancellazione...\")\n",
    "    try:\n",
    "        project_obj = next(p for p in api.projects(entity=entity) if p.name == proj_name)\n",
    "        remaining_sweeps = project_obj.sweeps()\n",
    "        if not remaining_sweeps:\n",
    "            print(\"   ✅ Nessuno sweep attivo trovato.\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Sweep ancora attivi: {remaining_sweeps}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Errore nella verifica sweep: {e}\")\n",
    "\n",
    "    print(f\"  ✅ Run e sweep eliminati per {path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469b02e-5ef7-4192-8152-d322e754710c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Datasets Loading - EEG Spectrograms - Time x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69cd05-3b74-4ab6-93f8-de891a9917fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### CODICE UFFICIALE DEL 04/03/2025 ORE 9:30 #####################\n",
    "                                 ##################### SENZA DETTAGLI SCRITTI V°3 #####################\n",
    "        \n",
    "'''ATTENZIONE: \n",
    "\n",
    "HO SOSTITUITO LE VARIABILI DI \n",
    "\n",
    "    1) DATASET_TRAIN_LOADER -->  TRAIN_LOADER\n",
    "    2) DATASET_VAL_LOADER -->  VAL_LOADER\n",
    "\n",
    "    VEDI FUNZIONE 'PREPARE_DATA_FOR_MODEL --> NOMI DELLE VARIABILI DEI TORCH TENSOR DATASET LOADER SON  'TRAIN_LOADER' E VAL_LOADER!!!  \n",
    "\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "import copy as cp\n",
    "\n",
    "\n",
    "# Definisci le lista delle coppie di condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Inizializza il dizionario per caricare i dati\n",
    "data_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''ATTENZIONE CHE QUI HO AGGIUNTO --> \"_time_frequency_\" alla base_dir!'''\n",
    "\n",
    "# Definisci la cartella principale\n",
    "base_dir = \"/home/stefano/Interrogait/WB_spectrograms_best_results_time_frequency\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "'''LOOP DI CARICAMENTO DATI'''\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    # Crea la cartella per la condizione sperimentale\n",
    "    condition_dir = os.path.join(base_dir, condition)\n",
    "    os.makedirs(condition_dir, exist_ok=True)\n",
    "    \n",
    "    # Aggiungi un livello di annidamento per ogni condizione\n",
    "    data_dict[condition] = {}\n",
    "    \n",
    "    for data_type in [\"spectrograms\"]:\n",
    "        \n",
    "        # Crea la cartella per il tipo di dato\n",
    "        data_dir = os.path.join(condition_dir, data_type)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "        for category in [\"familiar\", \"unfamiliar\"]:\n",
    "            # Crea la cartella per la categoria\n",
    "            #category_dir = os.path.join(data_dir, category)\n",
    "            #os.makedirs(category_dir, exist_ok=True)\n",
    "            \n",
    "            for subject_type in [\"th\", \"pt\"]:\n",
    "                # Caricamento e suddivisione dei dati\n",
    "                \n",
    "                #if data_type == \"spectrograms\":\n",
    "                    \n",
    "                print(f\"Caricamento dati per: {condition} - {data_type} - {category}_{subject_type}\")\n",
    "                X, y = load_data(data_type, category, subject_type, condition=condition)\n",
    "                \n",
    "                \n",
    "                # Creazione della chiave per il dizionario annidato\n",
    "                data_dict[condition][data_type] = data_dict[condition].get(data_type, {})\n",
    "                data_dict[condition][data_type][f\"{category}_{subject_type}\"] = (X, y)\n",
    "                \n",
    "                # Stampa di conferma\n",
    "                print(f\"Dataset caricato: \\033[1m{condition}\\033[0m_\\033[1m{data_type}\\033[0m_\\033[1m{category}_{subject_type}\\033[0m - Shape X: \\033[1m{X.shape}\\033[0m, Shape y: \\033[1m{len(y)}\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8aa9a-2fc4-4860-b9b8-e66dc047b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c7c44-f253-452d-8736-413d3a191c4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Sweep Configuration - EEG Spectrograms - Time x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc972b-5cb0-4c8d-b197-ef3a4e3227f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "N.B. \n",
    "\n",
    "PER SAPERE A QUALE COMBINAZIONE DI FATTORI CORRISPONDONO I DATI (i.e, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "MI CREO UN DIZIONARIO ULTERIORE, 'DATA_DICT_PREPROCESSED' CHE CONTIENE PER OGNI COMBINAZIONE DI FATTORI I DATI SPLITTATI\n",
    "\n",
    "IN QUESTO MODO, QUANDO FORNISCO ALLA FUNZIONE 'TRAINING_SWEEP' LA TUPLA CON I VARI DATI ((TRAIN, VAL E TEST))\n",
    "IO POSSO CAPIRE A QUALE COMBINAZIONI DI FATTORI CORRISPONDE QUELLA TUPLA DI DATI (TRAIN, VAL E TEST)\n",
    "\n",
    "\n",
    "INOLTRE,\n",
    "MI CREO ANCHE UNA LISTA DI TUPLE DI STRINGHE, DOVE OGNI TUPLA CONTIENE LE STRINGHE DELLE CHIAVI USATE \n",
    "PER LA GENERAZIONE DI DATA_DICT_PREPROCESSED.\n",
    "\n",
    "IN QUESTO MODO, MI ASSICURO CHE SIA UNA COERENZA TRA LA CREAZIONE DEI 'NAME' E 'TAG' DELLA RUN\n",
    "E\n",
    "LA CORRETTA ESTRAZIONE DEI DATI (OSSIA I DATI DI QUALE CONDIZIONE SPERIMENTALE, QUALI EEG INPUT, E DA CHI PROVENGONO!)  \n",
    "\n",
    "\n",
    "Questo approccio permette di garantire la corrispondenza tra \n",
    "\n",
    "1) le chiavi dei dati pre‐processati e \n",
    "2) la configurazione delle runs su W&B\n",
    "\n",
    "andando a creare due strutture in parallelo:\n",
    "\n",
    "- data_dict_preprocessed – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "                            la tupla dei dati già suddivisi (X_train, X_val, X_test, y_train, y_val, y_test);\n",
    "                            \n",
    "- sweeps_id – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "              sia la stringa univoca dello sweep ID, che l'insieme delle stringhe che formano la combinazione (condition, data_type, category_subject)\n",
    "\n",
    "\n",
    "\n",
    "LOOP DI PREPARAZIONE DATI (FINO A DATASET SPLITTING)\n",
    "'''\n",
    "\n",
    "#A QUESTO PUNTO PER OGNI DATASET, FACCIO STEP PRIMA DELLO SWEEP\n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Seleziona il dispositivo (GPU o CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dizionario per salvare gli sweep ID associati a ogni condizione sperimentale\n",
    "\n",
    "'''sweep_ids_for_models contiene la struttura che mi serve da copiare per best_models''' \n",
    "sweep_ids_for_models = {}\n",
    "\n",
    "'''sweep_ids contiene la struttura che mi serve da copiare per iterare sui singoli swweps di ogni combinazione di fattori'''\n",
    "sweep_ids = {}  \n",
    "\n",
    "'''DIZIONARIO CHE VIENE FORNITO IN INGRESSO A TRAINING_SWEEP'''\n",
    "# Dizionario per salvare la tupla di dati già preprocessati\n",
    "data_dict_preprocessed = {}\n",
    "\n",
    "\n",
    "# Loop di addestramento e test per ogni condizione sperimentale\n",
    "for condition, data_types in data_dict.items():  # Itera sulle condizioni sperimentali\n",
    "    \n",
    "    data_dict_preprocessed[condition] = {}\n",
    "    \n",
    "    # Aggiungi al dizionario sweep_ids\n",
    "    if condition not in sweep_ids:\n",
    "        sweep_ids[condition] = {}\n",
    "        \n",
    "        '''sweep_ids_for_models'''\n",
    "        sweep_ids_for_models[condition] = {}\n",
    "        \n",
    "    for data_type, categories in data_types.items():  # Itera sui tipi di dati (1_20, 1_45, wavelet)\n",
    "        \n",
    "        data_dict_preprocessed[condition][data_type] = {}\n",
    "        \n",
    "        if data_type not in sweep_ids[condition]:\n",
    "            sweep_ids[condition][data_type] = {}\n",
    "            \n",
    "            '''sweep_ids_for_models'''\n",
    "            sweep_ids_for_models[condition][data_type] = {}\n",
    "            \n",
    "        for category_subject, (X_data, y_data) in categories.items():  # Itera sulle coppie category_subject\n",
    "            \n",
    "            if category_subject not in sweep_ids[condition][data_type]:\n",
    "                sweep_ids[condition][data_type][category_subject] = {}\n",
    "                \n",
    "                '''sweep_ids_for_models'''\n",
    "                sweep_ids_for_models[condition][data_type][category_subject] = {}\n",
    "                \n",
    "            print(f\"\\n\\n\\033[1mEstrazione Dati\\033[0m della Chiave \\033[1m{condition}_{data_type}_{category_subject}\\033[0m\")\n",
    "            \n",
    "            # Controlla se il dataset è già stato elaborato (se la chiave è già nel set)\n",
    "            if (condition, data_type, category_subject) in processed_datasets:\n",
    "                print(f\"⚠️ ATTENZIONE: Il dataset {condition} - {data_type} - {category_subject} è già stato elaborato! Salto iterazione...\")\n",
    "                continue  # Salta se il dataset è già stato processato\n",
    "\n",
    "            # Aggiungi il dataset al set\n",
    "            processed_datasets.add((condition, data_type, category_subject))\n",
    "\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "            \n",
    "            data_dict_preprocessed[condition][data_type][category_subject] = (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "            \n",
    "            # Puoi anche aggiungere altri print per verificare la dimensione dei set\n",
    "            print(f\"\\033[1mDataset Splitting\\033[0m: Train Set Shape: {X_train.shape}, Validation Set Shape: {X_val.shape}, Test set Shape: {X_test.shape}\")\n",
    "\n",
    "            \n",
    "print(f\"\\nCreato \\033[1mdata_dict_preprocessed\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbf8d0-9004-488c-83dd-b83f68b8f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4e8e4-82a6-4ade-9e6a-c65fc3900056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_pt_resp'].keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms'].keys())\n",
    "print(type(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms'].keys()))\n",
    "\n",
    "#All'interno, c'è una tupla, di 6 elementi!\n",
    "print(type(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms']['familiar_th']))\n",
    "\n",
    "#I 6 elementi della tupla sono X_train, X_val, X_test, y_train, y_val, y_test !\n",
    "print(len(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms']['familiar_th']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d383a52-cc28-4d82-9cbf-08dbf3099658",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                             PRE-LUGLIO 2025\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "        \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 10},\n",
    "        \"model_name\":{\"values\": ['CNN2D', 'BiLSTM', 'Transformer']},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\":{\"values\": [True, False]},\n",
    "        \n",
    "    }\n",
    "}\n",
    " \n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb07ffa6-8eb1-4b78-af13-7287dc4177b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                POST 12 LUGLIO 2025\n",
    "                                                                \n",
    "                                                                \n",
    "Adesso, nello sweep config ho esteso per estendere il valore degli iper-parametri\n",
    "\n",
    "1) “classici” (lr, weight_decay, beta1, beta2, eps, ecc.) \n",
    "2) \"architetturali\", ossia specifici per ogni specifico modello scelto\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ad esempio, per la mia rete CNN1D, potrei impostare un set di valori per:\n",
    "\n",
    "\n",
    "^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^\n",
    "\n",
    "Ad esempio, rispetto alla mia rete CNN1D, potrei far variare\n",
    "\n",
    "\n",
    "1) All'interno di ogni layer convolutivo \n",
    "\n",
    "(https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels \n",
    "\n",
    "(ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente,se capisco bene) \n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 12 con step di 2)\n",
    "c) la grandezza del passo, ossia dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    " \n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo \n",
    "\n",
    "(https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "\n",
    "^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^\n",
    "Invece, per la mia BiLSTM vorrei far variare\n",
    "\n",
    "\n",
    "1) il valore di hidden_sizes ossia dello spazio di embedding multidimensionale dei miei punti temporali \n",
    "del dato EEG (tutti i valori tra 16 e 32 con step di 2, ossia 16, 18, 20.. e così via)\n",
    "\n",
    "2) il valore di dropout (tra 0.0 e 0.5)\n",
    "\n",
    "3) la scelta sulla bidirezionalità o meno (True o False)\n",
    "\n",
    "^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^\n",
    "\n",
    "\n",
    "\n",
    "Invece, per il mio Transformer vorrei far variare\n",
    "\n",
    "\n",
    "1) il valore dell'embedding, ossia \"d_model\" (con valori tra 8 e 64 con step di 8)\n",
    "2) il valore di head attenzionali, ossia \"num_heads\" (con valori tra 2 e 12 con step di 2) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "N.B. \n",
    "\n",
    "In questo modo WandB proverà tutte le 27 triplette possibili di funzioni di attivazione sui tre blocchi della CNN 1D\n",
    "\n",
    "\n",
    "Sostanzialmente vorrei replicare gli stessi valori presenti nell'approccio CNN1D ma nel CNN2D ossia\n",
    "\n",
    "\n",
    "1) per kernel sizes dovrebbe esser gli stessi di \n",
    "kernel_combinations = list(product([2, 4, 6, 8], repeat=3))\n",
    "\n",
    "nel CNN1D, ogni valore veniva associato ad ogni kernel convolutivo di ogni layer (perché si creava una tripletta sostanzialmente...\n",
    "Ora invece, per ogni layer convolutivo, avro una tripletta di tuple, ciascuna da associare ad ogni layer convolutivo,\n",
    "e dovrebbe estrarre se capisco bene da questi set di coppie di valori, \n",
    "dove ciascuna coppia rappresenta la grandezza del kernel size per ogni layer convolutivo nella CNN2D..\n",
    "\n",
    "E che dovrebbero essere (2,2), (4,4), (6,6), (8,8), per cui si potrebbero creare triplette di o stesse coppie di valori,\n",
    "o di triplette di coppie di valori diversi tra i layer convolutivi... \n",
    "\n",
    "\n",
    "2)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#Genera tutte le 27 combinazioni di 3 funzioni di attivazione\n",
    "activation_combinations = list(product(['relu','selu','elu'], repeat=3))\n",
    "\n",
    "\n",
    "# I valori base “CNN1D” che prima erano [2,4,6,8], ora diventano coppie (h,w)\n",
    "\n",
    "kernel2d_sizes = [(2,2), (4,4), (6,6), (8,8)]\n",
    "\n",
    "# tutte le possibili triplette di coppie, una per ciascun conv‐layer\n",
    "conv2d_kernel_combinations = list(product(kernel2d_sizes, repeat=3))\n",
    "\n",
    "# per lo stride, usi (1,1) e (2,2) invece di 1 e 2\n",
    "stride2d_sizes = [(1,1), (2,2)]\n",
    "\n",
    "conv2d_stride_combinations = list(product(stride2d_sizes, repeat=3))\n",
    "\n",
    "# per il pooling, coppie (1,1) e (2,2)\n",
    "pool2d_sizes = [(1,1), (2,2)]\n",
    "\n",
    "pool2d_kernel_combinations = list(product(pool2d_sizes, repeat=3))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Ecco un esempio di sweep_config pensato solo per le tre architetture che lavorano su input tempo × frequenza \n",
    "(CNN2D, BiLSTM, Transformer). Ho rimosso CNN1D e ho parametrizzato:\n",
    "\n",
    "CNN2D\n",
    "\n",
    "    conv_out_channels\n",
    "    conv2d_kernel_size (tuple h×w)\n",
    "    conv2d_stride (tuple h×w)\n",
    "    pool_type\n",
    "    pool2d_kernel_size (tuple h×w)\n",
    "    fc1_units\n",
    "    activations (tuple di 3 funzioni)\n",
    "    dropout\n",
    "\n",
    "BiLSTM\n",
    "\n",
    "    hidden_size\n",
    "    bidirectional\n",
    "    dropout\n",
    "\n",
    "Transformer\n",
    "    d_model\n",
    "    num_heads\n",
    "\n",
    "Con le dipendenze condizionali via conditional.\n",
    "\n",
    "In questo modo, quando scegli CNN2D, i parametri architetturali 2D (kernel, stride, pool, ecc.) verranno pescati dalla griglia; \n",
    "Quando scegli BiLSTM o Transformer, verranno usati solo i loro iper-parametri specifici.\n",
    "\n",
    "'''\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        # Ottimizzatore\n",
    "        \"lr\":            {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":         {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "        \"beta2\":         {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "        \"eps\":           {\"values\": [1e-8, 1e-7, 1e-6, 1e-5]},\n",
    "\n",
    "        # Training\n",
    "        \"n_epochs\":      {\"value\": 100},\n",
    "        \"patience\":      {\"value\": 12},\n",
    "\n",
    "        # Scelta del modello\n",
    "        \"model_name\":    {\"values\": [\"CNN2D\", \"BiLSTM\", \"Transformer\"]},\n",
    "\n",
    "        # Dati e regolarizzazione generale\n",
    "        \"batch_size\":    {\"values\": [16, 24, 32, 48, 52, 64, 72, 84, 96]},\n",
    "        \"standardization\":{\"values\":[True, False]},\n",
    "        \n",
    "        # ================================================\n",
    "        # SOLO per CNN2D\n",
    "        # ================================================\n",
    "        \"conv_out_channels\": {\n",
    "            \"values\": list(range(16,33,4)),  # 16,20,24,28,32\n",
    "            \"conditional\": {\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"conv_kernel_size_2d\": {\n",
    "            \"values\": conv2d_kernel_combinations,\n",
    "            \"conditional\": {\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"conv_stride_2d\": {\n",
    "            \"values\": conv2d_stride_combinations,\n",
    "            \"conditional\": {\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"pool_type\": {\n",
    "            \"values\":[\"max\",\"avg\"],\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"pool_kernel_size_2d\": {\n",
    "            \"values\": pool2d_kernel_combinations,\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"fc1_units\": {\n",
    "            \"values\": list(range(8,18,2)),  # 8,10,...,16\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"activations\": {\n",
    "            \"values\": activation_combinations,\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \n",
    "        # Il dropout in CNN2D e BiLSTM\n",
    "        \"dropout\": {\n",
    "            \"values\":[0.0,0.1,0.2,0.3,0.4,0.5],\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\", \"BiLSTM\"}\n",
    "        },\n",
    "        \n",
    "        # ================================================\n",
    "        # SOLO per BiLSTM\n",
    "        # ================================================\n",
    "        \"hidden_size\": {\n",
    "            \"values\": list(range(16,34,2)),  # 16,18,...,32\n",
    "            \"conditional\":{\"model_name\":\"BiLSTM\"}\n",
    "        },\n",
    "        \"bidirectional\": {\n",
    "            \"values\":[True,False],\n",
    "            \"conditional\":{\"model_name\":\"BiLSTM\"}\n",
    "        },\n",
    "        \n",
    "        # ================================================\n",
    "        # SOLO per Transformer\n",
    "        # ================================================\n",
    "        \"d_model\": {\n",
    "            \"values\": list(range(8,65,8)),  # 8,16,...,64\n",
    "            \"conditional\":{\"model_name\":\"Transformer\"}\n",
    "        },\n",
    "        \"num_heads\": {\n",
    "            \"values\": list(range(2,14,2)),  # 2,4,...,12\n",
    "            \"conditional\":{\"model_name\":\"Transformer\"}\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b831e46-2d34-4ea2-839a-e835b2a9962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''OGNI IPER-PARAMETRO DI OGNI RETE\n",
    "\n",
    "\n",
    "ALLO STESSO LIVELLO DI PARAMETERS!\n",
    "\n",
    "\n",
    "                                                                POST 14 LUGLIO 2025\n",
    "                                                                \n",
    "                                                                \n",
    "                                                                \n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN1D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***BILSTM NEW*** \n",
    "\n",
    "\n",
    "Per la rete BiLSTM, bisogna configurare hidden_size e dropout come indicato nel sweep_config, \n",
    "insieme alla possibilità di scegliere se utilizzare o meno la bidirezionalità.\n",
    "\n",
    "\n",
    "1) il valore di hidden_sizes ossia dello spazio di embedding multidimensionale dei miei punti temporali \n",
    "del dato EEG (tutti i valori tra 16 e 32 con step di 2, ossia 16, 18, 20.. e così via)\n",
    "\n",
    "2) la scelta sulla bidirezionalità o meno (True o False)\n",
    "\n",
    "3) il valore di dropout (tra 0.0 e 0.5)\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - BILSTM\n",
    "\n",
    "\n",
    "| Iper-parametro  | Descrizione                                       | Valori possibili                 |\n",
    "| --------------- | ------------------------------------------------- | -------------------------------- |\n",
    "| `hidden_size`   | Dimensione dello stato nascosto per layer LSTM    | `[16, 18, …, 32]` (passo 2)      |\n",
    "| `bidirectional` | Se usare LSTM bidirezionale (0 → False, 1 → True) | `[0, 1]`                         |\n",
    "| **+ comune**    | `dropout`                                         | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***TRANSFORMER NEW***\n",
    "                                                                \n",
    "Per il Transformer varieremo                                                        \n",
    "\n",
    "1) il valore dell'embedding, ossia \"d_model\" (con valori tra 8 e 64 con step di 8)\n",
    "2) il valore di head attenzionali, ossia \"num_heads\" (con valori tra 2 e 12 con step di 2) \n",
    "3) il valore di fully connected layers (con valori tra 1 e 3) dell\n",
    "\n",
    "4) il valore del feed_forward multiplier: descrive esattamente il suo ruolo, ossia è un moltiplicatore (mult) applicato\n",
    "alla dimensione del modello (d_model)per fissare l’ampiezza dell’FFN\n",
    "Ossia, il fattore con cui moltiplichi il tuo d_model per ottenere la dimensione interna del blocco feed-forward nel Transformer!\n",
    "\n",
    "In pratica, lo sweep esplora solo due moltiplicatori [2,4] invece di decine di valori hard-coded.\n",
    "Il modello transformer calcola internamente ogni run il corretto dim_feedforward = ff_mult * d_model.\n",
    "        \n",
    "5) il valore (stringa) della funzione di attivazione del layer fully connected (tra relu e gelu)\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "TABELLA FINALE RIASSUNTIVA - TRANSFORMER\n",
    "\n",
    "\n",
    "| Iper-parametro            | Descrizione                                                     | Valori possibili                 |\n",
    "| ------------------------- | --------------------------------------------------------------- | -------------------------------- |\n",
    "| `d_model`                 | Dimensione dell’embedding (modello)                             | `[8, 16, 24, …, 64]` (step 8)    |\n",
    "| `num_heads`               | Numero di teste di attenzione                                   | `[2, 4, 6, 8, 10, 12]`           |\n",
    "| `num_layers`              | Numero di blocchi encoder                                       | `[1, 2, 3]`                      |\n",
    "| `ff_mult`                 | Moltiplicatore per la dimensione interna del feed-forward (FFN) | `[2, 4]`                         |\n",
    "| `transformer_activations` | Funzione di attivazione nel layer FFN                           | `[\"relu\", \"gelu\"]`               |\n",
    "| **+ comune**              | `dropout`                                                       | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - IPER-PARAMETRI COMUNI PER IL LEARNING \n",
    "\n",
    "\n",
    "| Iper-parametro                                | Descrizione                                               | Valori                                 |\n",
    "| --------------------------------------------- | --------------------------------------------------------- | -------------------------------------- |\n",
    "| `lr`, `weight_decay`, `beta1`, `beta2`, `eps` | Ottimizzatore Adam (learning rate, decay, betas, epsilon) | come da sweep: varie decadi            |\n",
    "| `n_epochs`                                    | Epoche di training                                        | `100` (fisso)                          |\n",
    "| `patience`                                    | Pazienza per early-stopping                               | `12` (fisso)                           |\n",
    "| `batch_size`                                  | Dimensione del batch                                      | `[16, 24, 32, 48, 52, 64, 72, 84, 96]` |\n",
    "| `standardization`                             | Se standardizzare i dati prima del training               | `[True, False]`                        |\n",
    "| `dropout`                                     | Tasso di dropout                                          | `[0.0 … 0.5]`                          |\n",
    "| `model_name`                                  | Scelta del modello in sweep                               | `[\"CNN1D\", \"BiLSTM\", \"Transformer\"]`   |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        # Ottimizzatore\n",
    "        \"lr\":            {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":         {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "        \"beta2\":         {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "        \"eps\":           {\"values\": [1e-8, 1e-7, 1e-6, 1e-5]},\n",
    "\n",
    "        # Training\n",
    "        \"n_epochs\":      {\"value\": 100},\n",
    "        \"patience\":      {\"value\": 12},\n",
    "\n",
    "        # Scelta del modello\n",
    "        \"model_name\":    {\"values\": [\"CNN2D\", \"BiLSTM\", \"Transformer\"]},\n",
    "\n",
    "        # Dati e regolarizzazione generale\n",
    "        \"batch_size\":    {\"values\": [16, 24, 32, 48, 52, 64, 72, 84, 96]},\n",
    "        \"standardization\":{\"values\":[True, False]},\n",
    "        \n",
    "        # --- CNN1D solo quando model_name==\"CNN2D\" ---\n",
    "        \"conv_out_channels\":{\"values\":[16,20,24,28,32]},\n",
    "\n",
    "        \"conv_k1_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k1_w\":{\"values\":[3,5,7,9]},\n",
    "        \n",
    "        \"conv_k2_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k2_w\":{\"values\":[3,5,7,9]},\n",
    "        \n",
    "        \"conv_k3_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k3_w\":{\"values\":[3,5,7,9]},\n",
    "\n",
    "        \"conv_s1_h\":{\"values\":[1,2]},\n",
    "        \"conv_s1_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"conv_s2_h\":{\"values\":[1,2]},\n",
    "        \"conv_s2_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"conv_s3_h\":{\"values\":[1,2]},\n",
    "        \"conv_s3_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p1_h\":{\"values\":[1,2]},\n",
    "        \"pool_p1_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p2_h\":{\"values\":[1,2]},\n",
    "        \"pool_p2_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        #\"pool_p3_h\":{\"values\":[1,2]},\n",
    "        #\"pool_p3_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p3_h\":{\"values\":[1]},\n",
    "        \"pool_p3_w\":{\"values\":[1]},\n",
    "\n",
    "        \"pool_type\":{\"values\":[\"max\",\"avg\"]},\n",
    "        \"fc1_units\":{\"values\":[8,10,12,14,16]},\n",
    "\n",
    "        \"cnn_act1\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \"cnn_act2\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \"cnn_act3\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \n",
    "        \n",
    "        # --- BiLSTM solo quando model_name==\"BiLSTM\" ---\n",
    "        \"hidden_size\":{\"values\":list(range(16,34,2))},\n",
    "        \"bidirectional\":{\"values\":[0,1]},\n",
    "\n",
    "        # --- Transformer solo quando model_name==\"Transformer\" ---\n",
    "        \"d_model\":{\"values\":list(range(8,65,8))},\n",
    "        #\"num_heads\":{\"values\":[2,4,6,8,10,12]},\n",
    "        \n",
    "        \"num_heads\":{\"values\":[2,4,8]}, # solo divisori di tutti i d_model\n",
    "        \n",
    "        \"num_layers\":{\"values\":[1,2,3]},\n",
    "        \"ff_mult\":{\"values\":[2,4]},\n",
    "        \"transformer_activations\":{\"values\":[\"relu\",\"gelu\"]},\n",
    "\n",
    "        # comune\n",
    "        \"dropout\":{\"values\":[0.0,0.1,0.2,0.3,0.4,0.5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8f1d0-24c4-45e4-96ee-f3f78b1634b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e62a6c-06d5-4e12-bccb-85ceed892cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(best_models)\n",
    "#print(sweep_ids_for_models)\n",
    "#print(sweep_ids)\n",
    "#print(data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86af231-818b-4d13-9aa4-fa99d643955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][5].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8733403-5308-4567-be74-466700156cf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "Come output, io otterrò **quando crei gli sweeps** una cosa come questa, ad esempio:\n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw\n",
    "        Create sweep with ID: 3b6o28jt\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/3b6o28jt\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - BiLSTM: n° sweep 3b6o28jt\n",
    "        Create sweep with ID: q6yp4fas\n",
    "\n",
    "        .....\n",
    "\n",
    "Vedendole bene, per **ogni condizione sperimentale (3)**, **per ogni dato EEG (3)** e **per ogni provenienza del dato EEG (4)**, \n",
    "Io **DOVREI OTTENERE** in totale = **3x3x4 = 36 sweeps** per **OGNI CONDIZIONE SPERIMENTALE**\n",
    "\n",
    "\n",
    "Per **ognuna di queste sweeps**, io se ho capito bene creerò **15 esperimenti** (le mie runs), che corrispondo alle **diverse configurazioni di iper-parametri testati per lo stesso specifico sweep**!\n",
    "\n",
    "(ad esempio, solo questo \n",
    "\n",
    "<br> \n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw)\n",
    "\n",
    "Dove, le diverse configurazioni, son determinate randomicamente a partire dai valori dentro la variabile \"**sweep_config**\"  che è questa \n",
    "\n",
    "\n",
    "    #Creo la configurazione dello sweep e la eseguo\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "            \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "            \"n_epochs\": {\"value\": 100},\n",
    "            \"patience\": {\"value\": 10},\n",
    "            \"model_name\":{\"values\": ['CNN1D', 'BiLSTM', 'Transformer']},\n",
    "            \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "            \"standardization\":{\"values\": [True, False]},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bee43524-8598-4987-8711-6042f9f4843c",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Popolamento di sweep_ids e lancio degli agenti:\n",
    "\n",
    "Obiettivo: \n",
    "\n",
    "Per ogni combinazione (condition, data_type, category_subject, model_name), \n",
    "Se la lista è vuota, crei uno sweep usando wandb.sweep(sweep_config, project=condition) e lo inserisci nella lista. \n",
    "In seguito, iteri su quella lista (che ora contiene IL TUO SPECIFICO sweep_id) e lanci wandb.agent() per eseguire il training.\n",
    "\n",
    "\n",
    "\n",
    "Nota importante:\n",
    "L'ID restituito da wandb.sweep() è una STRINGA UNIVOCA generata automaticamente da WandB.\n",
    "Non puoi assegnargli direttamente una stringa personalizzata, ma puoi comunque usarlo per mappare nel tuo dizionario la combinazione di fattori! \n",
    "\n",
    "In questo ciclo, il fatto che la lista parta vuota è normale: il codice la popola se necessario e poi lancia l'agente per ogni sweep_id presente.\n",
    "\n",
    "\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "INOLTRE, BISOGNA CONTROLLARE CHE SI STIA ITERANDO CORRETTAMENTE SOLO SULLA COMBINAZIONE CORRENTE DI \n",
    "\n",
    "                CONDITION, DATA_TYPE, CATEGORY_SUBJECT E MODEL_NAME\n",
    "                \n",
    "QUESTO PERCHÉ SE UN CICLO SI RIPETE PER UNA CONDIZIONE IN PIÙ UNA COMBINAZIONE, POTREBBE GENERARE PIÙ  SWEEP IDS DI QUELLI CHE TI ASPETTI!\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "SOLUZIONE:\n",
    "\n",
    "Un buon approccio per evitare la creazione ripetuta di Sweep ID \n",
    "per la stessa combinazione di condition, data_type, category_subject e model_name \n",
    "è quello di utilizzare un SET per tenere traccia delle combinazioni già processate.\n",
    "Se una combinazione è già presente nel set, non dovresti creare un nuovo Sweep ID, ma semplicemente saltare quella parte del codice\n",
    "\n",
    "\n",
    "Inoltre, ho avuto una idea ad un certo punto! \n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "\n",
    "Quando creo ogni sweep singolarmente, si genera una stringa univoca di quello sweep, che si riferisce ad un dataset che è il prodotto di diversi fattori:\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Di conseguenza, iterando su ogni sweep_ids (che ho fatto in modo avesse la STESSA struttura dei miei dati già splittati i.e, data_dict_preprocessed\n",
    "io posso, \n",
    "\n",
    "1) da un lato eseguire la creazione della stringa univoca associata a quello sweep,\n",
    "2) crearmi una 'combination_key', che sarebbe l'insieme delle stringhe che descrivono quel dataset specifico di data_dict_preprocessed\n",
    "\n",
    "che sarà costituito da\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Poiché quindi so già la corrispondenza tra ogni Sweep ID e la sua combinazione di fattori (condition, data_type, category_subject), \n",
    "posso creare un MAPPING, che associ, ad certo Sweep ID e la stringa che descrive i suoi fattori associati!\n",
    "\n",
    "\n",
    "In questo modo, forse, si riesce a risolvere il PROBLEMA 2 NELLA CELLA DI CREAZIONE DELLA FUNZIONE DI TRAINING (VEDI SOTTO!)\n",
    "\n",
    "'''\n",
    "\n",
    "#Inizializza un set per tenere traccia delle combinazioni già elaborate\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "\n",
    "                    '''QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                     CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA '''\n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3ef4f-9776-4c90-8c4a-b27932437597",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ATTENZIONE: A DIFFERENZA DI PRIMA, DOVE GLI SWEEPS ERANO CREATI SOLO PER OGNI CONDIZIONE SPERIMENTALE,\n",
    "ADESSO INVECE VENGONO CREATI PER OGNI COMBINAZIONI DI FATTORI, CHE INCLUDONO:\n",
    "\n",
    "1) DATI DI COPPIE DI CONDIZIONI SPERIMENTALI\n",
    "2) PROVEVIENZA DEI DATI (IN QUESTO SPETTOGRAMMI TIME-FREQUENCY\n",
    "3) PROVENIENZA DEI DATI STESSI (FAMILIAR VS UNFAMILIAR; THERAPIST VS PATIENT)\n",
    "\n",
    "'''\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    \n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_{data_type}_time_frequency_{category_subject}\")\n",
    "\n",
    "                    '''QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                     CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA '''\n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea3b04-3f84-4493-b478-e569100edb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola e stampa il numero totale di combinazioni uniche (e quindi di sweep creati)\n",
    "\n",
    "total_sweeps = len(created_combinations)\n",
    "total_runs = total_sweeps * 200\n",
    "\n",
    "print(f\"Numero totale di sweep creati: {total_sweeps}\")\n",
    "print(f\"Numero totale di runs da eseguire: {total_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057f2f8-76bf-4136-823e-81a6d69f88ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''ESEGUI QUI QUESTA CELLA PER VEDERE COME SI STRUTTURA SWEEP_IDS'''\n",
    "\n",
    "#sweep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b5da7-a6cb-46c6-85d7-575ff04dd02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_ids.keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ecef46-3765-4f5b-9959-b163d99f6931",
   "metadata": {},
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "\n",
    "I **numeri degli sweeps** tornano e son corretti! \n",
    "Tuttavia, avendo solo preparato l'inizializzazione degli sweeps dentro 'sweep_ids', \n",
    "Sul sito di weight and biases, io vedo le tre condizioni sperimentali, create ciascuna come un progetto separato, che è corretto, ma ancora le runs di ciascuna le vedo a 0\n",
    "\n",
    "Deduco che questo comportamento, dovrebbe esser normale, dato che ancora non ho avviato l'agente appunto wandb.agent(), con cui gli fornisco lo sweep_id generato adesso in questo loop precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efc2d6-f876-449c-b641-019586a48d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(sweep_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649d299-2369-4b69-b0af-ad9e1bdb0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84809db0-8975-4ce2-b71d-6b40705eb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed54c1a-327f-42ed-a143-85a1a5020729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fae5d7b-daca-4d22-810f-38b39f43ac7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Sweep Configuration - EEG Spectrograms - Time x Frequencies ONLY HYPER-PARAMS**\n",
    "\n",
    "#### **Sweep separati per ciascuno dei modelli CNN2D_LSTM_TF, BiLSTM e Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea054fe-af1a-48fd-849b-534a33096336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "N.B. \n",
    "\n",
    "PER SAPERE A QUALE COMBINAZIONE DI FATTORI CORRISPONDONO I DATI (i.e, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "MI CREO UN DIZIONARIO ULTERIORE, 'DATA_DICT_PREPROCESSED' CHE CONTIENE PER OGNI COMBINAZIONE DI FATTORI I DATI SPLITTATI\n",
    "\n",
    "IN QUESTO MODO, QUANDO FORNISCO ALLA FUNZIONE 'TRAINING_SWEEP' LA TUPLA CON I VARI DATI ((TRAIN, VAL E TEST))\n",
    "IO POSSO CAPIRE A QUALE COMBINAZIONI DI FATTORI CORRISPONDE QUELLA TUPLA DI DATI (TRAIN, VAL E TEST)\n",
    "\n",
    "\n",
    "INOLTRE,\n",
    "MI CREO ANCHE UNA LISTA DI TUPLE DI STRINGHE, DOVE OGNI TUPLA CONTIENE LE STRINGHE DELLE CHIAVI USATE \n",
    "PER LA GENERAZIONE DI DATA_DICT_PREPROCESSED.\n",
    "\n",
    "IN QUESTO MODO, MI ASSICURO CHE SIA UNA COERENZA TRA LA CREAZIONE DEI 'NAME' E 'TAG' DELLA RUN\n",
    "E\n",
    "LA CORRETTA ESTRAZIONE DEI DATI (OSSIA I DATI DI QUALE CONDIZIONE SPERIMENTALE, QUALI EEG INPUT, E DA CHI PROVENGONO!)  \n",
    "\n",
    "\n",
    "Questo approccio permette di garantire la corrispondenza tra \n",
    "\n",
    "1) le chiavi dei dati pre‐processati e \n",
    "2) la configurazione delle runs su W&B\n",
    "\n",
    "andando a creare due strutture in parallelo:\n",
    "\n",
    "- data_dict_preprocessed – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "                            la tupla dei dati già suddivisi (X_train, X_val, X_test, y_train, y_val, y_test);\n",
    "                            \n",
    "- sweeps_id – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "              sia la stringa univoca dello sweep ID, che l'insieme delle stringhe che formano la combinazione (condition, data_type, category_subject)\n",
    "\n",
    "\n",
    "\n",
    "LOOP DI PREPARAZIONE DATI (FINO A DATASET SPLITTING)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#A QUESTO PUNTO PER OGNI DATASET, FACCIO STEP PRIMA DELLO SWEEP\n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Seleziona il dispositivo (GPU o CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Modelli che useremo nei sweep\n",
    "MODEL_LIST = [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"] #ReadMEndYou and ReadMYMind\n",
    "\n",
    "\n",
    "\n",
    "# Dizionario per salvare gli sweep ID associati a ogni condizione sperimentale\n",
    "\n",
    "'''sweep_ids_for_models contiene la struttura che mi serve da copiare per best_models''' \n",
    "sweep_ids_for_models = {}\n",
    "\n",
    "'''sweep_ids contiene la struttura che mi serve da copiare per iterare sui singoli swweps di ogni combinazione di fattori'''\n",
    "sweep_ids = {}  \n",
    "\n",
    "'''DIZIONARIO CHE VIENE FORNITO IN INGRESSO A TRAINING_SWEEP'''\n",
    "# Dizionario per salvare la tupla di dati già preprocessati\n",
    "data_dict_preprocessed = {}\n",
    "\n",
    "\n",
    "# Loop di addestramento e test per ogni condizione sperimentale\n",
    "for condition, data_types in data_dict.items():  # Itera sulle condizioni sperimentali\n",
    "    \n",
    "    data_dict_preprocessed[condition] = {}\n",
    "    \n",
    "    # Aggiungi al dizionario sweep_ids\n",
    "    if condition not in sweep_ids:\n",
    "        sweep_ids[condition] = {}\n",
    "        \n",
    "        '''sweep_ids_for_models'''\n",
    "        sweep_ids_for_models[condition] = {}\n",
    "        \n",
    "    for data_type, categories in data_types.items():  # Itera sui tipi di dati (1_20, 1_45, wavelet)\n",
    "        \n",
    "        data_dict_preprocessed[condition][data_type] = {}\n",
    "        \n",
    "        if data_type not in sweep_ids[condition]:\n",
    "            sweep_ids[condition][data_type] = {}\n",
    "            \n",
    "            '''sweep_ids_for_models'''\n",
    "            sweep_ids_for_models[condition][data_type] = {}\n",
    "            \n",
    "        for category_subject, (X_data, y_data) in categories.items():  # Itera sulle coppie category_subject\n",
    "            \n",
    "            # 1. Prepara spazio nei dizionari: sotto category_subject, un dict per ogni modello\n",
    "            \n",
    "            data_dict_preprocessed[condition][data_type][category_subject] = None\n",
    "            \n",
    "            if category_subject not in sweep_ids[condition][data_type]:\n",
    "                \n",
    "                sweep_ids[condition][data_type][category_subject] = {}\n",
    "                \n",
    "                '''NUOVA MODIFICA'''\n",
    "                sweep_ids[condition][data_type][category_subject] = {\n",
    "                model: [] for model in MODEL_LIST\n",
    "                }\n",
    "\n",
    "                '''sweep_ids_for_models'''\n",
    "                sweep_ids_for_models[condition][data_type][category_subject] = {}\n",
    "                \n",
    "                '''NUOVA MODIFICA'''\n",
    "                sweep_ids_for_models[condition][data_type][category_subject] = {\n",
    "                model: [] for model in MODEL_LIST\n",
    "                }\n",
    "                \n",
    "            print(f\"\\n\\n\\033[1mEstrazione Dati\\033[0m della Chiave \\033[1m{condition}_{data_type}_{category_subject}\\033[0m\")\n",
    "            \n",
    "            # Controlla se il dataset è già stato elaborato (se la chiave è già nel set)\n",
    "            if (condition, data_type, category_subject) in processed_datasets:\n",
    "                print(f\"⚠️ ATTENZIONE: Il dataset {condition} - {data_type} - {category_subject} è già stato elaborato! Salto iterazione...\")\n",
    "                continue  # Salta se il dataset è già stato processato\n",
    "\n",
    "            # Aggiungi il dataset al set\n",
    "            processed_datasets.add((condition, data_type, category_subject))\n",
    "\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "            \n",
    "            data_dict_preprocessed[condition][data_type][category_subject] = (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "            \n",
    "            # Puoi anche aggiungere altri print per verificare la dimensione dei set\n",
    "            print(f\"\\033[1mDataset Splitting\\033[0m: Train Set Shape: {X_train.shape}, Validation Set Shape: {X_val.shape}, Test Set Shape: {X_test.shape}\")\n",
    "\n",
    "            \n",
    "print(f\"\\nCreato \\033[1mdata_dict_preprocessed\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a08ca3-10c8-4632-af5c-c53d088191c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf116f23-8ba2-4406-b398-cd3d47ccbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_pt_resp'].keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms'].keys())\n",
    "print(type(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms'].keys()))\n",
    "\n",
    "#All'interno, c'è una tupla, di 6 elementi!\n",
    "print(type(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms']['familiar_th']))\n",
    "\n",
    "#I 6 elementi della tupla sono X_train, X_val, X_test, y_train, y_val, y_test !\n",
    "print(len(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms']['familiar_th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50819c-a351-4a6c-89a3-15e2de12de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7506ac2-a33e-4e02-9152-906033b915a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edc613-0a27-4c95-91c7-999fd85ce350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids_for_models)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73e32d2f-9096-4bcf-84ae-1a185b0003a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        #\"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "        \n",
    "        \"lr\": {\"values\": [1e-3]}, # fissato al valore di default del paper\n",
    "        \n",
    "        #\"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "        \n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \n",
    "        #\"model_name\":{\"values\": ['CNN2D', 'BiLSTM', 'Transformer']},\n",
    "        \n",
    "        #\"model_name\":{\"values\": ['CNN2D_LSTM_FC', 'BiLSTM', 'Transformer']},\n",
    "        \"model_name\":{\"values\": ['CNN2D_LSTM_FC']},\n",
    "        \n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 72, 84, 96]},\n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 72, 84]},\n",
    "        \n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 72, 84, 96]},\n",
    "        #\"batch_size\": {\"values\": [16, 24, 32, 48, 64]},\n",
    "        \"standardization\":{\"values\": [True, False]},\n",
    "        \n",
    "        \"beta1\": {\"value\": 0.9},\n",
    "        \"beta2\": {\"value\": 0.999},\n",
    "        \"eps\": {\"value\": 1e-8},\n",
    "        \n",
    "    }\n",
    "}\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c49b01-f5cb-461d-ba8b-345a10b9be75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "                                                                    AGGIORNATA AL 19 LUGLIO\n",
    "                                                                    \n",
    "                                                                    \n",
    "#\"learning rate : {\"value\"[1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\"}\n",
    "#\"n_epochs\": {\"value\": 100},\n",
    "# \"patience\": {\"value\": 12},\n",
    "#\"batch_size\": {\"values\": [16, 24, 32, 48, 64, 72, 84, 96]}\n",
    "#\"standardization\": {\"values\": [True, False]}, \n",
    "# \"beta1\": {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "#  \"beta2\": {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "#  \"eps\": {\"value\": [1e-8, 1e-7, 1e-6, 1e-5]}                                                                                                                            \n",
    "\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]}, # fissato al valore di default del paper\n",
    "\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \n",
    "        \n",
    "        \"model_name\":{\"values\": ['CNN3D_LSTM_FC']},\n",
    "\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "\n",
    "        \"standardization\":{\"values\": [True, False]},\n",
    "\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        #In questo modo:\n",
    "        \n",
    "        \"use_lstm\":      {\"values\":[True, False]},\n",
    "        \"lstm_hidden\":   {\"values\":[32]},\n",
    "        \"dropout\":       {\"values\":[0.5]},\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo:\n",
    "\n",
    "#uno per il modello CNN3D_LSTM_FC, originariamente era così\n",
    "\n",
    "\n",
    "#sweep_config_cnn3d= {\n",
    "#    \"method\": \"random\",\n",
    "#    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "#    \"parameters\": {\n",
    "#        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "#        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "#        \"n_epochs\": {\"value\": 100},\n",
    "#       \"patience\": {\"value\": 12},\n",
    "#        \"model_name\": {\"values\": [\"CNN2D_LSTM_TF\"]},\n",
    "#        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "#        \"standardization\": {\"values\": [True,False]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "#        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "#        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "#        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "#        \"use_lstm\": {\"values\": [True, False]},\n",
    "#        \"lstm_hidden\": {\"values\": [32]},\n",
    "#        \"dropout\": {\"values\": [0.5]},\n",
    "#    }\n",
    "#}\n",
    "\n",
    "\n",
    "#sweep_config_cnn_sep = {\n",
    "#    \"method\": \"random\",\n",
    "#    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "#    \"parameters\": {\n",
    "#        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "#        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "#        \"n_epochs\": {\"value\": 100},\n",
    "#        \"patience\": {\"value\": 12},\n",
    "#        \"model_name\": {\"values\": [\"SeparableCNN2D_LSTM_FC\"]},\n",
    "#        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "#        \"standardization\": {\"values\": [True,False]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "#        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "#        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "#        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "#        \"use_lstm\": {\"values\": [True, False]},\n",
    "#        \"lstm_hidden\": {\"values\": [32]},\n",
    "#        \"dropout\": {\"values\": [0.5]},\n",
    "#    }\n",
    "#}\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Comodo mapper per il tuo loop\n",
    "sweep_config_dict_stft = {\n",
    "    \"CNN2D_LSTM_TF\": sweep_config_cnn2d_lstm_tf,\n",
    "    \"ReadMEndYou\": sweep_config_bilstm,\n",
    "    \"ReadMYMind\": sweep_config_transformer,\n",
    "}\n",
    "Nota pratica (per l’integrazione nel training)\n",
    "CNN2D_LSTM_TF: passa dropout=config.dropout.\n",
    "\n",
    "ReadMEndYou: costruisci con\n",
    "ReadMEndYou(input_size=channels*freqs, hidden_sizes=[config.hidden1, config.hidden2, config.hidden3],\n",
    "output_size=2, dropout=config.dropout, bidirectional=config.bidirectional).\n",
    "\n",
    "ReadMYMind: costruisci con\n",
    "ReadMYMind(d_model=config.d_model, num_heads=config.num_heads, num_layers=config.num_layers, num_classes=2, channels=config.channels, freqs=config.freqs).\n",
    "\n",
    "'''\n",
    "# 2.1 – Sweep config per ciascun modello\n",
    "\n",
    "#CNN2D_LSTM_TF\n",
    "sweep_config_cnn2d_lstm_tf = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN2D_LSTM_TF\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        # --- specifici del modello ---\n",
    "    \n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config_bilstm = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"BiLSTM\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        # --- specifici del modello ---\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "        \"bidirectional\": {\"values\": [False, True]},\n",
    "        \n",
    "        #Soluzione 1 per mettere valori agli hidden sizes\n",
    "        #\"hidden1\": {\"values\": [24, 32, 48, 64]},\n",
    "        #\"hidden2\": {\"values\": [48, 64, 96, 128]},\n",
    "        #\"hidden3\": {\"values\": [62, 96, 128, 160]}\n",
    "        # in build del modello: hidden_sizes=[hidden1, hidden2, hidden3]\n",
    "        \n",
    "        #Soluzione 2 per mettere valori agli hidden sizes\n",
    "        \n",
    "        #hidden_sizes = [24, 48, 62]\n",
    "        #lstm_model = ReadMEndYou(input_size=input_channels * num_freqs, hidden_sizes=hidden_sizes, output_size=num_classes)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config_transformer = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"Transformer\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        # --- specifici del modello ---\n",
    "        \"d_model\": {\"values\": [32]},\n",
    "        \"num_heads\": {\"values\": [2]},\n",
    "        \"num_layers\": {\"values\": [2]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS\n",
    "\n",
    "# 2) Popolo sweep_ids_for_models in base a MODEL_LIST (già inizializzato nella prima cella)\n",
    "'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "#for condition in sweep_ids_for_models:\n",
    "    #for data_type in sweep_ids_for_models[condition]:\n",
    "        #for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            #for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                #if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    #sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''BEST_MODELS\n",
    "\n",
    "# 3) Creo best_models da sweep_ids_for_models\n",
    "'''\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "#'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "#for condition in sweep_ids:\n",
    "    #for data_type in sweep_ids[condition]:\n",
    "        #for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            #if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                #sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "#print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac943546-dad8-4e06-b930-3e5a57e29599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(best_models)\n",
    "#print(sweep_ids_for_models)\n",
    "#print(sweep_ids)\n",
    "#print(data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd6a05-9e7a-4a81-a531-e60e556a4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f63022-8593-4d8a-b530-45ae594617d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4ee29-740c-4b7d-881b-2fd5462f5840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e59c2a-5262-46a2-92dd-e848b40c80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][5].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e721446-b0b0-4143-8526-f972a27a935d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "Come output, io otterrò **quando crei gli sweeps** una cosa come questa, ad esempio:\n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw\n",
    "        Create sweep with ID: 3b6o28jt\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/3b6o28jt\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - BiLSTM: n° sweep 3b6o28jt\n",
    "        Create sweep with ID: q6yp4fas\n",
    "\n",
    "        .....\n",
    "\n",
    "Vedendole bene, per **ogni condizione sperimentale (3)**, **per ogni dato EEG (3)** e **per ogni provenienza del dato EEG (4)**, \n",
    "Io **DOVREI OTTENERE** in totale = **3x3x4 = 36 sweeps** per **OGNI CONDIZIONE SPERIMENTALE**\n",
    "\n",
    "\n",
    "Per **ognuna di queste sweeps**, io se ho capito bene creerò **15 esperimenti** (le mie runs), che corrispondo alle **diverse configurazioni di iper-parametri testati per lo stesso specifico sweep**!\n",
    "\n",
    "(ad esempio, solo questo \n",
    "\n",
    "<br> \n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw)\n",
    "\n",
    "Dove, le diverse configurazioni, son determinate randomicamente a partire dai valori dentro la variabile \"**sweep_config**\"  che è questa \n",
    "\n",
    "\n",
    "    #Creo la configurazione dello sweep e la eseguo\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "            \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "            \"n_epochs\": {\"value\": 100},\n",
    "            \"patience\": {\"value\": 10},\n",
    "            \"model_name\":{\"values\": ['CNN1D', 'BiLSTM', 'Transformer']},\n",
    "            \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "            \"standardization\":{\"values\": [True, False]},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc50b6-44da-4401-80f8-5a40ad40d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ATTENZIONE CHE A QUESTO PUNTO\n",
    "\n",
    "\n",
    "1) sweep_ids[cond][dtype][cat][model_name] contiene le tuple (sweep_id, combo_key) per ciascun modello, che ancora non esistono perché devo esser create durante la creazione degli sweeps, ma ho solo una lista\n",
    "\n",
    "{'rest_vs_left_fist': {'spectrograms': {'familiar_th': []}}, \n",
    "'rest_vs_right_fist': {'spectrograms': {'familiar_th': []}}, \n",
    "'left_fist_vs_right_fist': {'spectrograms': {'familiar_th': []}}}\n",
    "\n",
    "\n",
    "2) sweep_ids_for_models e best_models sono paralleli a sweep_ids con lo stesso livello model_name\n",
    "\n",
    "ossia \n",
    "\n",
    "sweep_ids_for_models come\n",
    "\n",
    "{'rest_vs_left_fist': {'spectrograms': {'familiar_th': {'CNN2D_LSTM_FC': [], 'ReadMEndYou': [], 'ReadMYMind': []}}},\n",
    "'rest_vs_right_fist': {'spectrograms': {'familiar_th': {'CNN2D_LSTM_FC': [], 'ReadMEndYou': [], 'ReadMYMind': []}}},\n",
    "'left_fist_vs_right_fist': {'spectrograms': {'familiar_th': {'CNN2D_LSTM_FC': [], 'ReadMEndYou': [], 'ReadMYMind': []}}}}\n",
    "\n",
    "best_models come\n",
    "\n",
    "{'rest_vs_left_fist': {'spectrograms': {'familiar_th': \n",
    "{'CNN2D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}, \n",
    "'ReadMEndYou': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None},\n",
    "'ReadMYMind': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}}}},\n",
    "\n",
    "'rest_vs_right_fist': {'spectrograms': {'familiar_th': \n",
    "{'CNN2D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}, \n",
    "'ReadMEndYou': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None},\n",
    "'ReadMYMind': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}}}},\n",
    "\n",
    "'left_fist_vs_right_fist': {'spectrograms': {'familiar_th': \n",
    "{'CNN2D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}, \n",
    "'ReadMEndYou': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None},\n",
    "'ReadMYMind': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}}}},\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ea618-45ec-47e8-831a-6b8139394446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Popolamento di sweep_ids e lancio degli agenti:\n",
    "\n",
    "Obiettivo: \n",
    "\n",
    "Per ogni combinazione (condition, data_type, category_subject, model_name), \n",
    "Se la lista è vuota, crei uno sweep usando wandb.sweep(sweep_config, project=condition) e lo inserisci nella lista. \n",
    "In seguito, iteri su quella lista (che ora contiene IL TUO SPECIFICO sweep_id) e lanci wandb.agent() per eseguire il training.\n",
    "\n",
    "\n",
    "\n",
    "Nota importante:\n",
    "L'ID restituito da wandb.sweep() è una STRINGA UNIVOCA generata automaticamente da WandB.\n",
    "Non puoi assegnargli direttamente una stringa personalizzata, ma puoi comunque usarlo per mappare nel tuo dizionario la combinazione di fattori! \n",
    "\n",
    "In questo ciclo, il fatto che la lista parta vuota è normale: il codice la popola se necessario e poi lancia l'agente per ogni sweep_id presente.\n",
    "\n",
    "\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "INOLTRE, BISOGNA CONTROLLARE CHE SI STIA ITERANDO CORRETTAMENTE SOLO SULLA COMBINAZIONE CORRENTE DI \n",
    "\n",
    "                CONDITION, DATA_TYPE, CATEGORY_SUBJECT E MODEL_NAME\n",
    "                \n",
    "QUESTO PERCHÉ SE UN CICLO SI RIPETE PER UNA CONDIZIONE IN PIÙ UNA COMBINAZIONE, POTREBBE GENERARE PIÙ  SWEEP IDS DI QUELLI CHE TI ASPETTI!\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "SOLUZIONE:\n",
    "\n",
    "Un buon approccio per evitare la creazione ripetuta di Sweep ID \n",
    "per la stessa combinazione di condition, data_type, category_subject e model_name \n",
    "è quello di utilizzare un SET per tenere traccia delle combinazioni già processate.\n",
    "Se una combinazione è già presente nel set, non dovresti creare un nuovo Sweep ID, ma semplicemente saltare quella parte del codice\n",
    "\n",
    "\n",
    "Inoltre, ho avuto una idea ad un certo punto! \n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "\n",
    "Quando creo ogni sweep singolarmente, si genera una stringa univoca di quello sweep, che si riferisce ad un dataset che è il prodotto di diversi fattori:\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Di conseguenza, iterando su ogni sweep_ids (che ho fatto in modo avesse la STESSA struttura dei miei dati già splittati i.e, data_dict_preprocessed\n",
    "io posso, \n",
    "\n",
    "1) da un lato eseguire la creazione della stringa univoca associata a quello sweep,\n",
    "2) crearmi una 'combination_key', che sarebbe l'insieme delle stringhe che descrivono quel dataset specifico di data_dict_preprocessed\n",
    "\n",
    "che sarà costituito da\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Poiché quindi so già la corrispondenza tra ogni Sweep ID e la sua combinazione di fattori (condition, data_type, category_subject), \n",
    "posso creare un MAPPING, che associ, ad certo Sweep ID e la stringa che descrive i suoi fattori associati!\n",
    "\n",
    "\n",
    "In questo modo, forse, si riesce a risolvere il PROBLEMA 2 NELLA CELLA DI CREAZIONE DELLA FUNZIONE DI TRAINING (VEDI SOTTO!)\n",
    "\n",
    "\n",
    "\n",
    "                                                        ******IMPORTANTE MODIFICA*****\n",
    "                                                        \n",
    "Ora lo sweep_ids non si deve sdoppiare ora, perché sostanzialmente, \n",
    "per ogni modello si creano gli sweeps ids corrispondenti e salvati come valore\n",
    "dentro la chiave del modello corrispondente, sotto forma di tupla...\n",
    "\n",
    "cioè non più così\n",
    "\n",
    "\"sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\"\n",
    "\n",
    "ma una cosa del genere\n",
    "\n",
    "\"sweep_ids[condition][data_type][category_subject][model_name].append((new_sweep_id, combination_key))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COME ERA PRIMA\n",
    "\n",
    "#Inizializza un set per tenere traccia delle combinazioni già elaborate\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\")\n",
    "\n",
    "                    #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                    #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA \n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n",
    "                \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "ADESSO\n",
    "\n",
    "\n",
    "Cosa fa questo snippet\n",
    "\n",
    "Cicla su ogni (condition, data_type, category_subject) una volta sola grazie a created_combinations.\n",
    "\n",
    "All’interno, fa un sottoloop su MODEL_LIST (i tuoi due modelli).\n",
    "\n",
    "In base a model_name, sceglie sweep_config_cnn3d o sweep_config_cnn_sep.\n",
    "\n",
    "Chiama wandb.sweep(...) con il config giusto e salva il risultato in\n",
    "\n",
    "\n",
    "sweep_ids[condition][data_type][category_subject][model_name]\n",
    "anziché nella lista “piatta” che avevi prima.\n",
    "\n",
    "\n",
    "In questo modo:\n",
    "\n",
    "sweep_ids[cond][dtype][cat] resta un dict con due chiavi (\"CNN3D_LSTM_FC\" e \"SeparableCNN2D_LSTM_FC\")\n",
    "\n",
    "Ognuna di quelle chiavi punta a una propria lista di tuple (sweep_id, combo_key)\n",
    "\n",
    "Non serve sdoppiare l’intero sweep_ids, perché tiene già separati gli sweep di ciascun modello\n",
    "\n",
    "Più tardi, quando lancerai gli agent, ti basterà:\n",
    "\n",
    "\n",
    "for model_name, sweeps in sweep_ids[cond][dtype][cat].items():\n",
    "    for sweep_id, combo_key in sweeps:\n",
    "        # qui scegli il train_fn in base a model_name\n",
    "        wandb.agent(sweep_id, function=train_fn_map[model_name], count=200)\n",
    "e ogni modello girerà solo i suoi sweep.\n",
    "\n",
    "\n",
    "\n",
    "Alla fine, sweep_ids avrà la forma:\n",
    "\n",
    "{\n",
    "  'rest_vs_left_fist': {\n",
    "    'spectrograms': {\n",
    "      'familiar_th': {\n",
    "         'CNN3D_LSTM_FC':       [(sweep_id_1, 'rest_vs_left_fist_spectrograms_familiar_th')],\n",
    "         'SeparableCNN2D_LSTM_FC': [(sweep_id_2, 'rest_vs_left_fist_spectrograms_familiar_th')]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  …\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "#Ecco come puoi riscrivere solo la TERZA CELLA (quella in cui crei effettivamente gli sweep) \n",
    "#mantenendo la tua struttura “a celle” e usando per ognuno il sweep_config giusto in base al model_name.\n",
    "\n",
    "#Creazione degli sweep (Terza cella)\n",
    "#Ecco il solo snippet che devi usare per creare gli sweep ripartiti per modello, usando i due sweep_config_*:\n",
    "\n",
    "\n",
    "'''\n",
    "Per mantenere la stessa logica di prima ma tenendo conto che ora stai lavorando con modelli separati, \n",
    "dovresti modificare il controllo in modo che verifichi se una combinazione di condition, data_type, category_subject\n",
    "è già stata processata per ciascun modello.\n",
    "\n",
    "Quindi, il controllo dovrebbe essere fatto separatamente per ogni modello dentro il loop che itera sui modelli (MODEL_LIST).\n",
    "Di seguito ti mostro la versione modificata che tiene conto di questo:\n",
    "\n",
    "\n",
    "\n",
    "#Inizializza un set per tenere traccia delle combinazioni già elaborate\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\")\n",
    "\n",
    "                    #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                    #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA \n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n",
    "                \n",
    "                \n",
    "                \n",
    "'''\n",
    "\n",
    "                    \n",
    "'''\n",
    "\n",
    "Cosa è stato cambiato rispetto alla versione precedente?\n",
    "Controllo della combinazione di modello:\n",
    "La logica del controllo della combinazione (combination_key, model_name) nel set created_combinations è corretta, \n",
    "perché vogliamo evitare di creare più volte lo stesso sweep per una combinazione di condition, data_type, category_subject, e model_name.\n",
    "\n",
    "Controllo e creazione dello sweep:\n",
    "Il codice controlla prima se la combinazione con il modello non è stata già processata \n",
    "con il controllo if (combination_key, model_name) not in created_combinations. \n",
    "\n",
    "Se non è stata processata, procede a creare lo sweep corrispondente. \n",
    "Se la combinazione esiste già, salta la creazione dello sweep per quel modello.\n",
    "\n",
    "Aggiunta del nuovo sweep ID:\n",
    "Una volta creato il nuovo sweep per il modello, viene aggiunto correttamente \n",
    "alla lista del modello specifico sotto sweep_ids[condition][data_type][category_subject][model_name].\n",
    "\n",
    "Aggiunta al set delle combinazioni:\n",
    "Dopo aver creato lo sweep, aggiungiamo (combination_key, model_name) al set created_combinations\n",
    "per tenere traccia delle combinazioni già elaborate.\n",
    "\n",
    "Verifica della logica:\n",
    "La combinazione (combination_key, model_name) deve essere unica per ciascun modello, \n",
    "e quindi il controllo che evita duplicazioni nel set è corretto.\n",
    "\n",
    "La creazione dello sweep per ciascun modello separato è mantenuta, \n",
    "e viene applicata solo quando la combinazione specifica non è già stata elaborata per quel modello.\n",
    "\n",
    "In questo modo, la logica funziona come nel codice precedente, ma ora si tiene conto anche dei modelli separati, \n",
    "creando un sweep per ciascuno di essi e mantenendo la traccia delle combinazioni in modo appropriato.\n",
    "\n",
    "'''\n",
    "created_combinations = set()\n",
    "\n",
    "\n",
    "\n",
    "#CNN2D_LSTM_TF\n",
    "#sweep_config_cnn2d_lstm_tf\n",
    "\n",
    "#sweep_config_bilstm\n",
    "#sweep_config_transformer\n",
    "\n",
    "# Per semplicità, tieni MODEL_LIST a portata di mano\n",
    "MODEL_LIST = [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"]\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "            \n",
    "            # per ciascun modello, creo uno sweep separato\n",
    "            for model_name in MODEL_LIST:\n",
    "\n",
    "                # Controlla se la combinazione di condition, data_type, category_subject + modello è già stata elaborata\n",
    "                if (combination_key, model_name) not in created_combinations:\n",
    "\n",
    "                    # Scegli il config in base al model_name\n",
    "                    if model_name == \"CNN2D_LSTM_TF\":\n",
    "                        sweep_conf = sweep_config_cnn2d_lstm_tf\n",
    "                        \n",
    "                    elif model_name == \"BiLSTM\":  # ReadMEndYou\n",
    "                        sweep_conf = sweep_config_bilstm\n",
    "                        \n",
    "                    elif model_name == \"Transformer\":  # ReadMYMind\n",
    "                        sweep_conf = sweep_config_transformer\n",
    "                    else:\n",
    "                        raise ValueError(f\"Modello non riconosciuto: {model_name}\")\n",
    "                    \n",
    "                    # Controllo se la lista per il modello specifico è vuota\n",
    "                    if not sweep_ids[condition][data_type][category_subject][model_name]:\n",
    "\n",
    "                        # Crea lo sweep e lo appendo nella lista dedicata a quel modello\n",
    "                        \n",
    "                        '''\n",
    "                        QUESTO COME ERA IMPOSTATO PRIMA NELLA VERSIONE SENZA SWEEP CONFIG MODEL-SPECIFIC\n",
    "                        \n",
    "                        #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms_new\")\n",
    "                        '''\n",
    "                        \n",
    "                        #new_sweep_id = wandb.sweep(sweep_conf, project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\")\n",
    "                        \n",
    "                        '''\n",
    "                        QUESTO SAREBBE COME ORA LO IMPOSTO PER LA VERSIONE CON SWEEP CONFIG MODEL-SPECIFIC\n",
    "                        \n",
    "                        '''\n",
    "                        \n",
    "                        #new_sweep_id = wandb.sweep(sweep_conf, project=f\"{condition}_spectrograms_time_freqs_new_3d_grid_multiband\")\n",
    "                        \n",
    "                        new_sweep_id = wandb.sweep(sweep_conf, project=f\"{condition}_{data_type}_time_frequency_{category_subject}\")\n",
    "                        \n",
    "                        \n",
    "                        #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                        #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA \n",
    "                        \n",
    "                        sweep_ids[condition][data_type][category_subject][model_name].append((new_sweep_id, combination_key))\n",
    "\n",
    "                    print(f\"▶ Sweep \\033[1m{new_sweep_id}\\033[0m creato per \\033[1m{combination_key}\\033[0m, modello \\033[1m{model_name}\\033[0m\")\n",
    "                    \n",
    "                    # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                    created_combinations.add((combination_key, model_name))  # Aggiungi la combinazione con il modello\n",
    "                else:\n",
    "                    # Se la combinazione è già stata creata, salta\n",
    "                    print(f\"⚠️ {combination_key} già processato per il modello {model_name}, skip.\")\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252578bd-70f3-44f0-841b-4eb630d1b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola e stampa il numero totale di combinazioni uniche (e quindi di sweep creati)\n",
    "\n",
    "total_sweeps = len(created_combinations)\n",
    "total_runs = total_sweeps * 200\n",
    "\n",
    "print(f\"Numero totale di sweep creati: {total_sweeps}\")\n",
    "print(f\"Numero totale di runs da eseguire: {total_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04502bfa-2971-4e7e-a669-89aa3bdc279f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''ESEGUI QUI QUESTA CELLA PER VEDERE COME SI STRUTTURA SWEEP_IDS'''\n",
    "\n",
    "#sweep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276b84f-ae96-42a9-bcdb-824ed0da90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_ids.keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68463064-e4f7-47e2-aead-2d0eb9aa1651",
   "metadata": {},
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "\n",
    "I **numeri degli sweeps** tornano e son corretti! \n",
    "Tuttavia, avendo solo preparato l'inizializzazione degli sweeps dentro 'sweep_ids', \n",
    "Sul sito di weight and biases, io vedo le tre condizioni sperimentali, create ciascuna come un progetto separato, che è corretto, ma ancora le runs di ciascuna le vedo a 0\n",
    "\n",
    "Deduco che questo comportamento, dovrebbe esser normale, dato che ancora non ho avviato l'agente appunto wandb.agent(), con cui gli fornisco lo sweep_id generato adesso in questo loop precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260dee4-ee39-40b7-ad94-b1b62282f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(sweep_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06c575-f9af-42f5-a772-4f6f293b8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09939de-1da0-4084-b53d-cf9785867368",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c091704-0c17-4e9f-af70-79ca8f4ffa48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VERSIONE DEL 6 MARZO (RISOLUZIONE DEFINITIVA)**\n",
    "\n",
    "##### **Training Function Edits - EEG Spectrograms - Time x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9249b-38fe-4094-97b0-47bee31c6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "# Test\n",
    "combination_key = \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "condition_experiment, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "print(\"Condizione:\", condition_experiment)\n",
    "print(\"Soggetto:\", subject_key)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "229b2acf-bafd-48e7-ac00-7b50a0c8f6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                    **** SALVATAGGIO SOLO PESI E BIAS DI UN CERTO MODELLO ****      \n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "\n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "\n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "    #    model = CNN1D(input_channels=3, num_classes=2)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1m{config.model_name}\\033[0m\")\n",
    "    #elif config.model_name == \"BiLSTM\":\n",
    "    #    model = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1m{config.model_name}\\033[0m\")\n",
    "    #elif config.model_name == \"Transformer\":\n",
    "    #    model = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1m{config.model_name}\\033[0m\")\n",
    "        \n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    if config.model_name == \"CNN2D\":\n",
    "        model = CNN2D(input_channels=3, num_classes=2)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "        \n",
    "    elif config.model_name == \"BiLSTM\":\n",
    "        # Qui, input_size = canali * frequenze = 3 * 38 = 114\n",
    "        model = ReadMEndYou(input_size= 3 * 38, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mBiLSTM\\033[0m\")\n",
    "        \n",
    "    elif config.model_name == \"Transformer\":\n",
    "        # Per il Transformer, passiamo anche i parametri channels e freqs per adattare l'embedding\n",
    "        model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=38)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mTransformer\\033[0m\")\n",
    "\n",
    "        \n",
    "    #ORIGINAL VERSION OF TIME SERIES EEG DATA REPRESENTATION  \n",
    "    #def initialize_models():\n",
    "        #model_CNN = CNN1D(input_channels=3, num_classes=2)\n",
    "        #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        \n",
    "        #return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "\n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val\n",
    "        })\n",
    "\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "\n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        '''\n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "            model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "\n",
    "            torch.save(best_model.state_dict(), model_file)\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    torch.save(best_model.state_dict(), model_file)\n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "        \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "145e625c-d797-49f1-9db5-b7703db27fcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Testing shapes\n",
    "batch, channels, frequency, time, num_classes = 44, 61, 26, 11,2\n",
    "\n",
    "x = torch.randn(batch, channels, frequency, time)\n",
    "print(\"Input:\", x.shape)\n",
    "\n",
    "cnn = CNN2D(channels, num_classes,\n",
    "            conv_out_channels=16,\n",
    "            conv_k1_h=3,conv_k1_w=5,\n",
    "            conv_k2_h=3,conv_k2_w=5,\n",
    "            conv_k3_h=3,conv_k3_w=5,\n",
    "            conv_s1_h=1,conv_s1_w=2,\n",
    "            conv_s2_h=1,conv_s2_w=2,\n",
    "            conv_s3_h=1,conv_s3_w=2,\n",
    "            pool_p1_h=1,pool_p1_w=2,\n",
    "            pool_p2_h=1,pool_p2_w=2,\n",
    "            pool_p3_h=1,pool_p3_w=1,\n",
    "            pool_type='max',fc1_units=10,dropout=0.1,\n",
    "            cnn_act1='relu',cnn_act2='relu',cnn_act3='elu')\n",
    "\n",
    "out_cnn = cnn(x)\n",
    "print(\"CNN2D output:\", out_cnn.shape)\n",
    "\n",
    "\n",
    "lstm = ReadMEndYou(input_size=channels*frequency, hidden_size=24, output_size=num_classes,\n",
    "                   num_layers=3, dropout=0.1, bidirectional=1)\n",
    "out_lstm = lstm(x)\n",
    "print(\"LSTM output:\", out_lstm.shape)\n",
    "\n",
    "trans = ReadMYMind(num_channels=channels, num_freqs= frequency, seq_length=time,\n",
    "                   d_model=64, num_heads=8, num_layers=2, num_classes=num_classes,\n",
    "                   ff_mult=2, dropout=0.1, transformer_activations='relu')\n",
    "out_trans = trans(x)\n",
    "print(\"Transformer output:\", out_trans.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#wandb.init(project = f\"{condition}_{data_type}_time_frequency_{category_subject}\", name = run_name, tags = tags)\n",
    "# Appena caricato X_train, X_val, X_test, etc.\n",
    "    # X_train.shape == (N, channels, freq_bins, time_steps)\n",
    "    \n",
    "    #_, channels, freq_bins, time_steps = X_train.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f289d-88d2-44e8-bfd3-6aa536cf2812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                                    **** SALVATAGGIO DI **** \n",
    "                                                        \n",
    "                                                        1) PESI E BIAS DI UN CERTO MODELLO \n",
    "                                                        2) CONFIGURAZIONE IPER-PARAMETRI DI UN CERTO MODELLO\n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''NEW VERSION\n",
    "    \n",
    "    Questo assicura la coerenza tra la creazione degli sweep e le run che li eseguono,\n",
    "    e permette di tracciare meglio ogni combinazione anche su W&B.\n",
    "    '''\n",
    "    wandb.init(project = f\"{condition}_{data_type}_time_frequency_{category_subject}\", name = run_name, tags = tags)\n",
    "    \n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "\n",
    "        \n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "        #model = CNN2D(input_channels=61, num_classes=2)\n",
    "    \n",
    "    #Caricamento dati per: th_resp_vs_pt_resp - spectrograms - familiar_th\n",
    "    #Dataset caricato: th_resp_vs_pt_resp_spectrograms_familiar_th - Shape X: (1586, 3, 26, 11), Shape y: 1586\n",
    " \n",
    "    '''PRENDO LA SHAPE DEI DATI PER FORNIRE VALORI GIUSTI PER OGNI INPUt DI CIASCUNA RETE'''\n",
    "    \n",
    "    # Appena caricato X_train, X_val, X_test, etc.\n",
    "    # X_train.shape == (N, channels, freq_bins, time_steps)\n",
    "    \n",
    "    _, channels, freq_bins, time_steps = X_train.shape\n",
    "        \n",
    "\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "        \n",
    "        # Canali EEG  \n",
    "        #input_channels = 3\n",
    "        \n",
    "        #input_channels = channels * freq_bins\n",
    "        \n",
    "        #Classi fissi\n",
    "        #num_classes = 2\n",
    "        \n",
    "        # conv_kernel_size_2d è una tupla di 3 coppie: [ ((h1,w1),(h2,w2),(h3,w3)) ]\n",
    "        #k1, k2, k3 = config.conv_kernel_size_2d\n",
    "        \n",
    "        # conv_stride_2d è una tupla di 3 coppie: [ ((s1,s1),(s2,s2),(s3,s3)) ]\n",
    "        #s1, s2, s3 = config.conv_stride_2d\n",
    "        \n",
    "        # pool_kernel_size_2d è una tupla di 3 coppie: [ ((p1,p1),(p2,p2),(p3,p3)) ]\n",
    "        #p1, p2, p3 = config.pool_kernel_size_2d\n",
    "        \n",
    "        #model = CNN2D(\n",
    "            #input_channels   = input_channels,\n",
    "            #num_classes      = num_classes,\n",
    "            #conv_out_channels= config.conv_out_channels,\n",
    "            \n",
    "            #conv_kernel_size = (k1, k2, k3),\n",
    "            #conv_stride      = (s1, s2, s3),\n",
    "            \n",
    "            #pool_type        = config.pool_type,\n",
    "            \n",
    "            #pool_kernel_size = (p1, p2, p3),\n",
    "            \n",
    "            #fc1_units        = config.fc1_units,\n",
    "            #dropout          = config.dropout,\n",
    "            \n",
    "            #activations      = tuple(config.activations)\n",
    "        #)\n",
    "        \n",
    "    '''NEW VERSION'''\n",
    "    \n",
    "    if config.model_name ==\"CNN2D\":\n",
    "        \n",
    "        # Canali EEG  \n",
    "        #input_channels = 3\n",
    "        \n",
    "        input_channels = channels \n",
    "        \n",
    "        #Classi da riconoscere\n",
    "        num_classes = 2\n",
    "        \n",
    "        model = CNN2D(\n",
    "            input_channels   = channels,\n",
    "            num_classes      = num_classes,\n",
    "            conv_out_channels= config.conv_out_channels,\n",
    "\n",
    "            conv_k1_h = config.conv_k1_h, \n",
    "            conv_k1_w = config.conv_k1_w,\n",
    "            \n",
    "            conv_k2_h = config.conv_k2_h, \n",
    "            conv_k2_w = config.conv_k2_w,\n",
    "            \n",
    "            conv_k3_h = config.conv_k3_h,\n",
    "            conv_k3_w = config.conv_k3_w,\n",
    "\n",
    "            conv_s1_h = config.conv_s1_h, \n",
    "            conv_s1_w = config.conv_s1_w,\n",
    "            \n",
    "            conv_s2_h = config.conv_s2_h,\n",
    "            conv_s2_w = config.conv_s2_w,\n",
    "            \n",
    "            conv_s3_h = config.conv_s3_h,\n",
    "            conv_s3_w = config.conv_s3_w,\n",
    "\n",
    "            pool_p1_h = config.pool_p1_h,\n",
    "            pool_p1_w = config.pool_p1_w,\n",
    "            \n",
    "            pool_p2_h = config.pool_p2_h,\n",
    "            pool_p2_w = config.pool_p2_w,\n",
    "            \n",
    "            pool_p3_h = config.pool_p3_h,\n",
    "            pool_p3_w = config.pool_p3_w,\n",
    "            \n",
    "            pool_type = config.pool_type,\n",
    "\n",
    "            fc1_units = config.fc1_units,\n",
    "            dropout   = config.dropout,\n",
    "\n",
    "            cnn_act1  = config.cnn_act1,\n",
    "            cnn_act2  = config.cnn_act2,\n",
    "            cnn_act3  = config.cnn_act3,\n",
    "        )\n",
    "    \n",
    "        \n",
    "        print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "    \n",
    "    \n",
    "    #'''NEW VERSION'''\n",
    "    elif config.model_name == \"BiLSTM\":\n",
    "        \n",
    "        # Input Size = channels * freq_bins = 3 * 26 = 78\n",
    "        input_size = channels * freq_bins\n",
    "        \n",
    "        # Classi \n",
    "        num_classes = 2 \n",
    "        \n",
    "        model = ReadMEndYou(\n",
    "            input_size   = input_size,\n",
    "            hidden_size  = config.hidden_size,\n",
    "            output_size  = num_classes,\n",
    "            num_layers   = 3,\n",
    "            dropout      = config.dropout,\n",
    "            bidirectional= config.bidirectional\n",
    "        )\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mBiLSTM\\033[0m\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #'''OLD VERSION'''\n",
    "    #elif config.model_name == \"Transformer\":\n",
    "        # Per il Transformer, passiamo anche i parametri channels e freqs per adattare l'embedding\n",
    "        #model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "\n",
    "    #else:  # Transformer\n",
    "        #num_classes = 2 \n",
    "        #num_channels = channels\n",
    "        \n",
    "        #model = ReadMYMind(\n",
    "            #num_channels= num_channels,\n",
    "            #freq_bins   = freq_bins,\n",
    "            #d_model     = config.d_model,\n",
    "            #num_heads   = config.num_heads,\n",
    "            #num_layers  = 3,\n",
    "            #num_classes = num_classes\n",
    "        #)\n",
    "    \n",
    "    \n",
    "    #''NEW VERSION'''\n",
    "    elif config.model_name == \"Transformer\":\n",
    "        \n",
    "        num_classes = 2      # o il numero di classi che hai\n",
    "        \n",
    "        model = ReadMYMind(\n",
    "            num_channels            = channels,\n",
    "            num_freqs               = freq_bins,\n",
    "            \n",
    "            seq_length              = time_steps,\n",
    "            \n",
    "            d_model                 = config.d_model,\n",
    "            num_heads               = config.num_heads,\n",
    "            \n",
    "            num_layers              = config.num_layers,\n",
    "            num_classes             = num_classes,\n",
    "            \n",
    "            ff_mult                 = config.ff_mult,\n",
    "            dropout                 = config.dropout,\n",
    "            transformer_activations = config.transformer_activations,\n",
    "        )\n",
    "\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mTransformer\\033[0m\")\n",
    "\n",
    "        \n",
    "    #ORIGINAL VERSION OF TIME SERIES EEG DATA REPRESENTATION  \n",
    "    #def initialize_models():\n",
    "        #model_CNN = CNN1D(input_channels=3, num_classes=2)\n",
    "        #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        \n",
    "        #return model_CNN, model_LSTM, model_Transformer\n",
    "        \n",
    "    '''OLD VERSION'''\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    # 1) Optimizer con betas, eps, weight_decay\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        eps=config.eps,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode ='min',      # monitoriamo val_loss\n",
    "        factor = 0.1,      # dimezza lr\n",
    "        patience = 8,      # 4 epoche di plateau\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    \n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val\n",
    "        })\n",
    "\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "            \n",
    "        '''OLD VERSION'''\n",
    "        #early_stopping(accuracy_val)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"🛑 Early stopping attivato!\")\n",
    "            #break\n",
    "        \n",
    "        '''NEW VERSION'''\n",
    "        scheduler.step(loss_val)\n",
    "        early_stopping(loss_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "    \n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        \n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        IMPORTANTISSIMO: COME SALVARSI LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI UN CERTO MODELLO, DI UN DATO DI UNA CERTA COMBINAZIONE DI FATTORI\n",
    "        (CONDIZIONE SPERIMENTALE, TIPO DI DATO, PROVENIENZA DEL DATO!)\n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        \n",
    "        CHATGPT:\n",
    "        \n",
    "        Nei run eseguiti con W&B ogni esecuzione registra automaticamente la configurazione degli iper-parametri (tramite wandb.config) \n",
    "        insieme alle metriche e ai log. \n",
    "        Quindi, a meno che tu non abbia modificato il comportamento predefinito, \n",
    "        ogni run con il tuo sweep ha già la configurazione associata registrata nei run logs di W&B.\n",
    "\n",
    "        Tuttavia, per associare in modo “automatico” e diretto la migliore configurazione agli specifici modelli salvati in .pth, \n",
    "        potresti considerare di fare uno o più di questi aggiustamenti:\n",
    "\n",
    "        Salvare la configurazione nel dizionario dei best_models:\n",
    "        Quando aggiorni il dizionario best_models (cioè quando salvi il miglior modello per una determinata combinazione), \n",
    "        puoi salvare anche una copia della configurazione corrente. \n",
    "        \n",
    "        Ad esempio, potresti modificare il blocco in cui aggiorni best_models in questo modo:\n",
    "        \n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": dict(config)  # Salva la configurazione degli iper-parametri\n",
    "        }\n",
    "        \n",
    "        In questo modo, ogni volta che un modello viene considerato il migliore per quella combinazione,\n",
    "        la sua configurazione sarà salvata insieme ai pesi.\n",
    "        Questo ti permetterà, in seguito, di sapere esattamente quali iper-parametri sono stati usati per ottenere quel modello.\n",
    "        \n",
    "        \n",
    "        In sintesi, se hai già usato wandb.config e hai loggato le configurazioni durante le run,\n",
    "        W&B le ha automaticamente salvate nei run logs. \n",
    "        \n",
    "        Se vuoi rendere più esplicita l'associazione tra il modello salvato (.pth) e la sua configurazione, \n",
    "        è utile modificare il tuo codice di TRAINING per salvare ANCHE \n",
    "        \n",
    "        1) il dizionario di configurazione insieme a \n",
    "        2) i pesi nel dizionario best_models oppure nei metadati del file salvato.\n",
    "        \n",
    "        Questo piccolo accorgimento ti consentirà di recuperare facilmente la configurazione ottimale per ogni modello salvato.\n",
    "        \n",
    "        OSSIA\n",
    "        Aggiungendo la chiave \"config\": dict(config) nel dizionario che memorizza il best model,\n",
    "        salvi anche la configurazione degli iper-parametri utilizzata in quella run.\n",
    "        \n",
    "        In questo modo, per ogni modello salvato (.pth) potrai recuperare facilmente sia i pesi che la configurazione ottimale che li ha generati.\n",
    "        \n",
    "        Questo approccio garantisce che ogni modello sia associato in modo esplicito al set di iper-parametri che ha prodotto le migliori performance, \n",
    "        rendendo più semplice il successivo confronto o la replica degli esperimenti.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # ***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "        #1)Al posto di salvarmi solo i migliori pesi (i.e.,  model_file = f\"{model_path}/{best_model_name}.pth\")\n",
    "        #  ora mi salvo anche la MIGLIORE configurazione di iper-parametri trovata rispetto alle 15 RUNS di un certo SWEEP\n",
    "        #  di un certo MODELLO, applicato su un DATASET con una SPECIFICA COMBINAZIONE DI FATTORI\n",
    "        #  condizione sperimentale, tipo di dato e provenienza del dato!\n",
    "        \n",
    "    \n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \n",
    "                #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "                #***** AGGIUNTA DELLA CHIAVE CONFIG CHE PRELEVA AUTOMATICAMENTE LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DENTRO 'BEST_MODELS'\n",
    "                \n",
    "                # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                # una certa combinazione di fattori: \n",
    "                # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                \"config\": dict(config)  \n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "            #***** SALVATAGGIO DI UN FILE .PKL, CHE CONTIENE \n",
    "            \n",
    "            # I PESI E BIAS DEL MODELLO DERIVATO DALLA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI OTTENUTA DALLA MIGLIORE RUN DI UN CERTO SWEEP\n",
    "            # IN RELAZIONE AD UN CERTO DATASET COSTITUITO DA UNA CERTA COMBINAZIONE DI FATTORI\n",
    "            \n",
    "            '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "            #model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "            \n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            \n",
    "            '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "            #torch.save(best_model.state_dict(), model_file)\n",
    "            \n",
    "            # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": dict(config)\n",
    "            }, model_file)\n",
    "\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                    \n",
    "                    # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                    # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                    # una certa combinazione di fattori: \n",
    "                    # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                    \"config\": dict(config)  \n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "                #model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    \n",
    "                    '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "                    #torch.save(best_model.state_dict(), model_file)\n",
    "\n",
    "                    # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                    torch.save({\n",
    "                        \"state_dict\": best_model.state_dict(),\n",
    "                        \"config\": dict(config)\n",
    "                    }, model_file)\n",
    "                    \n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59e9e5-bece-438f-9a93-dbabbe622662",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure Final Edits - EEG Spectrograms - Time x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e810df78-0abf-4d6d-93a4-03fc3004b4fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "''' \n",
    "\n",
    "                                    QUI IL LOOP LO ESEGUO SU UN SINGOLO SWEEP DI UNA SPECIFICA COMBINAZIONE DI FATTORI\n",
    "\n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "'''\n",
    "\n",
    "#Allora la mia idea è far così\n",
    "#1) questo loop rimane così, con la differenza che, oltre al singolo sweep ID, mi carico in input alla funzione 'training_sweep' anche sweep_tuple\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    \n",
    "    #eseguo l'unpacking della tupla per prendermi solo il primo elmento della tupla (sweep_id, ...) dove ... sarebbe il combination_key\n",
    "    sweep_id, _ =  sweep_tuple\n",
    "    '''\n",
    "    Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "    per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "    In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "    '''\n",
    "    \n",
    "    # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "    \n",
    "    def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "        def train_wrapper():\n",
    "            \n",
    "            training_sweep(\n",
    "                data_dict_preprocessed, \n",
    "                sweep_config,\n",
    "                sweep_ids,\n",
    "                sweep_id,\n",
    "                sweep_tuple,\n",
    "                best_models # Viene aggiornato dentro la funzione training_sweep\n",
    "            )\n",
    "        return train_wrapper\n",
    "\n",
    "    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "    \n",
    "    # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "    wandb.agent(sweep_id, function=agent_function, count=15)\n",
    "\n",
    "    print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione: \\033[1m{condition} - {data_type} - {category_subject}\\033[0m\\n\")\n",
    "\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "578b47e8-ac1a-4587-bea7-42140b329a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE A \n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "        \n",
    "'''\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "# Itera attraverso il primo livello (tipo di combinazione)\n",
    "for condition, sub_dict in sweep_ids.items():\n",
    "    #print(f\"Processing sweep_id: {condition}\")\n",
    "    \n",
    "    # Itera attraverso il secondo livello (e.g., '1_20', '1_45', 'wavelet')\n",
    "    for data_type, condition_data in sub_dict.items():\n",
    "        #print(f\"  Processing condition: {data_type}\")\n",
    "        \n",
    "        # Itera attraverso il terzo livello (e.g., 'familiar_th', 'unfamiliar_pt', ...)\n",
    "        for category_subject, data_tuples in condition_data.items():\n",
    "            #print(f\"    Processing data type: {category_subject}\")\n",
    "            \n",
    "            # Itera sulle tuple (sweep_id, sweep_name)\n",
    "            for sweep_tuple in data_tuples:\n",
    "                #print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, _ = sweep_tuple\n",
    "                \n",
    "                #Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                #per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                #In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    def train_wrapper():\n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        print(f\"Eseguendo il training per {condition} - {data_type} - {category_subject} con sweep_id {sweep_id}\")\n",
    "                        training_sweep(\n",
    "                            data_dict_preprocessed, \n",
    "                            sweep_config,\n",
    "                            sweep_ids,\n",
    "                            sweep_id,\n",
    "                            sweep_tuple,\n",
    "                            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        )\n",
    "                    return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_id}\\033[0m\")\n",
    "                wandb.agent(sweep_id, function=agent_function, count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione: \\033[1m{condition} - {data_type} - {category_subject}\\033[0m\\n\")\n",
    "                \n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9531e61e-d1c8-491d-9fa6-2c918c2a22c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE B\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE B (SEMPLIFICATA!)\n",
    "\n",
    "'''\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, combination_key = sweep_tuple\n",
    "                \n",
    "                # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    def train_wrapper():\n",
    "                        \n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "                        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "                        training_sweep(\n",
    "                            data_dict_preprocessed, \n",
    "                            sweep_config,\n",
    "                            sweep_ids,\n",
    "                            sweep_id,\n",
    "                            sweep_tuple,\n",
    "                            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        )\n",
    "                    return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                   ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "                \n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms\", count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e96ce-6671-4c58-8b01-987d11dceabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE C \n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                W&B SWEEPS AND TRAING LAUNCH WITH MULTIPLE GPUs MANAGEMENT\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE C (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "SPIEGAZIONE\n",
    "\n",
    "GPU counter: Ho aggiunto un contatore (gpu_counter) che cicla tra le GPU disponibili. \n",
    "\n",
    "In questo modo, il primo sweep sarà eseguito sulla GPU 0, il secondo sulla GPU 1, e così via. \n",
    "Quando il contatore raggiunge il numero di GPU disponibili, torna a 0 per riusare la prima GPU.\n",
    "\n",
    "Rotazione delle GPU: All'interno del loop, per ogni sweep, viene assegnata una GPU diversa. \n",
    "Se ci sono più di 1 GPU, il contatore incrementa, e la variabile CUDA_VISIBLE_DEVICES cambia automaticamente per assegnare la GPU corretta.\n",
    "\n",
    "Esecuzione parallela: Ogni sweep viene eseguito su una GPU separata. Se ci sono 2 GPU, il primo sweep va su GPU 0, il secondo su GPU 1, il terzo su GPU 0, e così via.\n",
    "\n",
    "Risposta alla tua domanda:\n",
    "In questo modo, ogni sweep_id viene eseguito una sola volta, ma su GPU diverse (se disponibili). Non ci sono duplicati dello stesso sweep su entrambe le GPU.\n",
    "\n",
    "\n",
    "DOMANDE SUL NUOVO CODICE\n",
    "\n",
    "1) Gli sweep sono eseguiti già in parallelo giusto?\n",
    "No, in questo caso gli sweep non sono eseguiti in parallelo in modo esplicito tramite il codice che hai scritto.\n",
    "\n",
    "Anche se hai assegnato ciascun sweep a una GPU diversa, il codice esegue sequenzialmente ogni sweep, solo che li distribuisce su GPU differenti in modo rotazionale.\n",
    "Ogni volta che il ciclo passa ad un nuovo sweep, assegna un ID GPU e poi esegue l'agent su quella GPU. Non vengono eseguiti in parallelo a livello di codice.\n",
    "\n",
    "2) O semplicemente in questo modo faccio in modo di distribuire ogni sweep sull'altra GPU rispetto a quella usata dallo sweep precedente\n",
    "per \"ottimizzare\" il carico computazionale di ogni GPU?\n",
    "\n",
    "Esatto! Quello che stai facendo è distribuire i vari sweep su GPU diverse, assicurandoti che ogni sweep venga eseguito su una GPU separata (se ne hai di disponibili).\n",
    "Questo permette di ottimizzare l'uso delle risorse, evitando che una GPU venga sovraccaricata da più sweep. Se il numero di GPU disponibili è maggiore di 1, \n",
    "allora i sweep sono distribuiti sulle diverse GPU, ma ogni sweep sarà ancora eseguito singolarmente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sì, con il codice che hai fornito, stai distribuendo gli sweep tra le diverse GPU, in modo da ottimizzare il carico computazionale e non sovraccaricare una sola GPU.\n",
    "\n",
    "Dettaglio del funzionamento:\n",
    "Distribuzione delle GPU (rotazionale):\n",
    "\n",
    "Quando ci sono più di una GPU, il codice assegna a ciascun sweep una GPU diversa in modo rotazionale.\n",
    "\n",
    "Per ogni ciclo del loop, la variabile gpu_counter determina a quale GPU assegnare il prossimo sweep.\n",
    "\n",
    "Se ci sono 2 GPU, il primo sweep viene eseguito sulla GPU 0, il secondo sulla GPU 1, il terzo di nuovo sulla GPU 0, e così via.\n",
    "\n",
    "Gestione della GPU:\n",
    "\n",
    "Se hai più di una GPU, os.environ[\"CUDA_VISIBLE_DEVICES\"] imposta il dispositivo GPU corrente su cui il codice deve girare (GPU 0 o GPU 1). Questo permette di gestire quale GPU eseguirà l'addestramento per ciascun sweep.\n",
    "\n",
    "Quando num_gpus > 1, il codice alterna l'assegnazione della GPU per ogni sweep, evitando di sovraccaricare una singola GPU con troppe operazioni contemporaneamente.\n",
    "\n",
    "Ottimizzazione del carico computazionale:\n",
    "\n",
    "L'alternanza tra GPU successive per ciascun sweep aiuta a distribuire il carico in modo equilibrato, specialmente se il numero di sweep è alto.\n",
    "\n",
    "In pratica, se hai 2 GPU e 10 sweep da eseguire, ogni GPU eseguirà 5 sweep, evitando di saturare una singola GPU.\n",
    "\n",
    "Parallelizzazione effettiva:\n",
    "Nel tuo codice non c'è parallelismo vero e proprio tra le GPU (come quello che otterresti con DataParallel o DistributedDataParallel), ma piuttosto distribuisci gli sweep su GPU diverse. Ogni sweep è eseguito sequenzialmente su una GPU diversa, ma non stai parallelizzando il training dello stesso sweep su più GPU.\n",
    "\n",
    "Se volessi fare in modo che un singolo sweep fosse parallelizzato su più GPU (in modo che il lavoro venga diviso tra le GPU per uno stesso sweep), dovresti usare DataParallel o DistributedDataParallel, come descritto in precedenza. Ma in questo caso, ogni sweep è isolato e viene eseguito su una sola GPU, quindi non c'è un vero parallelismo dentro ogni singolo sweep.\n",
    "\n",
    "Domande che potrebbero sorgere:\n",
    "Gli sweep vengono eseguiti in parallelo?\n",
    "\n",
    "No, i sweep sono eseguiti uno alla volta, ma su GPU diverse. Quindi, mentre il primo sweep usa la GPU 0, il secondo usa la GPU 1 e così via. Ogni sweep viene gestito separatamente, ma sfrutti più GPU per parallelizzare l'esecuzione di più sweep contemporaneamente.\n",
    "\n",
    "La distribuzione delle GPU tra gli sweep è ottimizzata?\n",
    "\n",
    "Sì, stai bilanciando il carico computazionale tra le GPU, assegnando a ogni GPU uno sweep alternato. Se hai molte GPU, puoi ottimizzare ulteriormente distribuendo i sweep su più dispositivi.\n",
    "\n",
    "Se volessi parallelizzare più agenti W&B su diverse GPU, il codice che stai utilizzando sarebbe corretto, ma per ottimizzare ulteriormente i tempi di esecuzione, potresti prendere in considerazione anche l'utilizzo di tecniche come DataParallel o DistributedDataParallel per far sì che più GPU lavorino contemporaneamente sullo stesso sweep. Ma la logica che hai già implementato va bene per distribuire più sweep tra le GPU.\n",
    "\n",
    "Se hai bisogno di ulteriori dettagli su come implementare il parallelismo vero e proprio (inclusi DataParallel o DistributedDataParallel), fammi sapere!\n",
    "\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "\n",
    "# Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "    def train_wrapper():\n",
    "\n",
    "        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "        training_sweep(\n",
    "            data_dict_preprocessed, \n",
    "            sweep_config,\n",
    "            sweep_ids,\n",
    "            sweep_id,\n",
    "            sweep_tuple,\n",
    "            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "        )\n",
    "    return train_wrapper\n",
    "                        \n",
    "                \n",
    "# Verifica quante GPU sono disponibili\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "# Crea un contatore per assegnare un GPU diversa a ciascun sweep\n",
    "gpu_counter = 0\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, combination_key = sweep_tuple\n",
    "                \n",
    "                # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                \n",
    "                # Se ci sono più di 1 GPU, assegna a ciascuna GPU uno sweep diverso\n",
    "                if num_gpus > 1:\n",
    "                    \n",
    "                    # Assegna la GPU in modo rotazionale\n",
    "                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_counter)\n",
    "                    \n",
    "                    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                    \n",
    "                    '''OLD VERSION'''\n",
    "                    #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project=f\"{condition}\", count=100)\n",
    "                    \n",
    "                    '''NEW VERSION'''\n",
    "                    #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project=f\"{combination_key}\", count=200)\n",
    "                    \n",
    "                    wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project =f\"{condition}_{data_type}_time_frequency_{category_subject}\", count=200)\n",
    "                    \n",
    "                    \n",
    "                    # Passa alla prossima GPU per il prossimo sweep\n",
    "                    gpu_counter = (gpu_counter + 1) % num_gpus\n",
    "\n",
    "                else:\n",
    "                    # Se c'è una sola GPU, esegui il sweep sulla GPU 0\n",
    "                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "                    \n",
    "                    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                    \n",
    "                    '''OLD VERSION'''\n",
    "                    #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project=f\"{condition}\", count=100)\n",
    "                    \n",
    "                    '''NEW VERSION'''\n",
    "                    #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project=f\"{combination_key}\", count=200)\n",
    "                    wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project =f\"{condition}_{data_type}_time_frequency_{category_subject}\", count=200)\n",
    "                    \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                #def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    #def train_wrapper():\n",
    "                        \n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "                        #print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "                        #training_sweep(\n",
    "                            #data_dict_preprocessed, \n",
    "                            #sweep_config,\n",
    "                            #sweep_ids,\n",
    "                            #sweep_id,\n",
    "                            #sweep_tuple,\n",
    "                            #best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        #)\n",
    "                    #return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                '''COMMENTATO'''\n",
    "                #agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                   ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "                \n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                \n",
    "                '''COMMENTATO'''\n",
    "                #wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs_new_2d_grid_multiband_topomap\", count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ea818-1ebf-49c2-ac9c-f38ad32d0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finito Training su W&B !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bc6f6-66b5-400b-b6eb-4391238def19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa il numero totale di sweeps\n",
    "#print(f\"Numero totale di sweeps che verranno eseguiti: {total_sweeps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146a46f-a475-476d-8229-319d61e88d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_ids.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33d23f44-0def-4e5b-9251-3b93aa3b90ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "''CODICE DI ULTERIORE MODIFICA, PER VERIFICARE VISIVAMENTE LA COERENZA TRA \n",
    "\n",
    "LE SHAPE DEI DATI DEL RELATIVO SOTTO-DIZIONARIO ESTRATTO DA DATA_DICT_PREPROCESSED,\n",
    "A SEGUITO DELLA VERIFICA DI COERENZA TRA \n",
    "\n",
    "1) LA COMBINATION KEY DELLO SWEEP ID CORRENTE E \n",
    "2) LA COMBINATION KEY TROVATA NEL SOTTO-DIZIONARIO DI DATA_DICT_PREPROCESSED\n",
    "'''\n",
    "\n",
    "#Questo codice estrae ogni elemento della tupla data_tuple e,\n",
    "#se l'elemento è un array numpy, \n",
    "#stampa la sua forma. \n",
    "#Ogni print è separato da una linea di trattini per una visualizzazione chiara.\n",
    "\n",
    "#Assicurati che data_dict_preprocessed contenga effettivamente i dati nella struttura indicata \n",
    "#e che ogni elemento della tupla sia un array numpy o un oggetto che possa essere controllato tramite .shape.\n",
    "\n",
    "# Estrai la tupla dalla struttura data_dict_preprocessed\n",
    "data_tuple = data_dict_preprocessed['pt_resp_vs_shared_resp']['wavelet']['unfamiliar_pt']\n",
    "\n",
    "# Itera sulla tupla e stampa la forma di ciascun elemento\n",
    "for idx, data in enumerate(data_tuple):\n",
    "    print(f\"Shape of element {idx + 1}:\")\n",
    "\n",
    "    # Verifica se l'elemento è un array numpy, e se lo è, stampane la shape\n",
    "    if isinstance(data, np.ndarray):\n",
    "        print(data.shape)\n",
    "    else:\n",
    "        print(\"Element is not a numpy array\")\n",
    "    \n",
    "    print(\"-\" * 50)  # Separatore per chiarezza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d652dc-2c90-4e15-82eb-6dcb2f7a792f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VERSIONE DEL 6 MARZO (RISOLUZIONE DEFINITIVA)**\n",
    "\n",
    "##### **Training Function Edits - EEG Spectrograms - Time x Frequencies ONLY HYPER-PARAMS**\n",
    "\n",
    "#### **Sweep separati per ciascuno dei modelli CNN2D_LSTM_TF, BiLSTM e Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd179af1-82de-4429-adc4-073796e8d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                                    **** SALVATAGGIO DI **** \n",
    "                                                        \n",
    "                                                        1) PESI E BIAS DI UN CERTO MODELLO \n",
    "                                                        2) CONFIGURAZIONE IPER-PARAMETRI DI UN CERTO MODELLO\n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|th_resp_vs_shared_resp|pt_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "    \n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    \n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    \n",
    "    '''OCCHIO DA CAMBIARE CAMBIATO'''\n",
    "        \n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms_channels_freqs_new_3d_grid_multiband\", name=run_name, tags=tags)\n",
    "    \n",
    "    #PER TASK 1/3\n",
    "    wandb.init(project=f\"{exp_cond}_{data_type}_time_frequency_{category_subject}\", name=run_name, tags=tags)\n",
    "    \n",
    "    #PER TASK 2/4\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms_time_freqs_new_imagery_3d_grid_multiband\", name=run_name, tags=tags)\n",
    "\n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "\n",
    "        \n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "    #    model = CNN2D(input_channels=64, num_classes=2)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "    \n",
    "    '''# ====== MODELLI TIME×FREQ ======\n",
    "    \n",
    "    cnn_model = CNN2D_LSTM_TF(input_channels = input_channels, num_classes =num_classes, dropout = dropout)\n",
    "    hidden_sizes = [24, 48, 62]\n",
    "    lstm_model = ReadMEndYou(input_size=input_channels * num_freqs, hidden_sizes=hidden_sizes, output_size=num_classes)\n",
    "    transformer_model = ReadMYMind(d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes)\n",
    "    \n",
    "    # Creazione di dati fittizi per il test\n",
    "    x = torch.randn(batch_size, input_channels, num_freqs, num_timepoints)  # (batch, canali, frequenze, tempo)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # ricavo channels e freqs dai dati (shape attesa: N, C, F, T)\n",
    "    channels, freqs = int(X_train.shape[1]), int(X_train.shape[2])\n",
    "    \n",
    "    if config.model_name == \"CNN2D_LSTM_TF\":\n",
    "        \n",
    "        '''OCCHIO QUI ADESSO SAREBBE TEMPO x FREQUENZA'''\n",
    "    \n",
    "        model = CNN2D_LSTM_TF(\n",
    "            input_channels = channels, # qui 61\n",
    "            num_classes = 2,\n",
    "            dropout=config.dropout,\n",
    "        )\n",
    "\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mCNN2D_LSTM_TF\\033[0m\")\n",
    "    \n",
    "    \n",
    "    elif config.model_name == \"BiLSTM\":\n",
    "        hidden_sizes = [24, 48, 62]\n",
    "        model = ReadMEndYou(\n",
    "            input_size = channels * freqs, #  qui 61*26\n",
    "            hidden_sizes = hidden_sizes,\n",
    "            output_size = 2,\n",
    "            dropout = config.dropout,\n",
    "            bidirectional = config.bidirectional\n",
    "        )\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mReadMEndYou (BiLSTM)\\033[0m\")\n",
    "    \n",
    "    \n",
    "    elif config.model_name == \"Transformer\":\n",
    "        model = ReadMYMind(\n",
    "            d_model=config.d_model,\n",
    "            num_heads=config.num_heads,\n",
    "            num_layers=config.num_layers,\n",
    "            num_classes=2,\n",
    "            channels = channels,\n",
    "            freqs = freqs \n",
    "            \n",
    "        )\n",
    "        \n",
    "        print(f\"\\nInizializzazione Modello \\033[1mReadMYMind (Transformer)\\033[0m\")\n",
    "    else:\n",
    "        raise ValueError(f\"Modello sconosciuto: {config.model_name}\")\n",
    "        \n",
    "    #elif config.model_name == \"BiLSTM\":\n",
    "        # Qui, input_size = canali * frequenze = 3 * 38 = 78\n",
    "        #model = ReadMEndYou(input_size= 64 * 45, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mBiLSTM\\033[0m\")\n",
    "        \n",
    "    #elif config.model_name == \"Transformer\":\n",
    "        # Per il Transformer, passiamo anche i parametri channels e freqs per adattare l'embedding\n",
    "        #model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=64, freqs=45)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mTransformer\\033[0m\")\n",
    "\n",
    "        \n",
    "    #ORIGINAL VERSION OF TIME SERIES EEG DATA REPRESENTATION  \n",
    "    #def initialize_models():\n",
    "        #model_CNN = CNN1D(input_channels=3, num_classes=2)\n",
    "        #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        \n",
    "        #return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "    '''\n",
    "    Cosa è cambiato rispetto alla tua versione\n",
    "    Optimizer Adam ora prende betas=(0.9,0.999) e eps=1e-8.\n",
    "\n",
    "    ReduceLROnPlateau posizionato subito dopo l’optimizer, chiamato su val_loss ogni epoca.\n",
    "\n",
    "    EarlyStopping con patience=12, mode='min' su val_loss.\n",
    "\n",
    "    Loop sulle epoche fino a config.n_epochs (100), senza limitare a 60.\n",
    "\n",
    "    Tutti i parametri di sweep_config (lr, weight_decay, n_epochs, patience, batch_size, standardization…) rimangono esposti e loggati.\n",
    "\n",
    "    In questo modo riproduci fedelmente il training descritto nel paper, senza stravolgere la tua pipeline di sweep.\n",
    "    '''\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr = config.lr,               # da sweep: es. [0.01,0.001,...]\n",
    "        betas = (config.beta1, config.beta2),          # paper\n",
    "        eps = config.eps,                    # paper\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode ='min',      # monitoriamo val_loss\n",
    "        factor = 0.1,      # dimezza lr\n",
    "        patience = 8,      # 4 epoche di plateau\n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    \n",
    "    #early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    #'''AGGIORNAMENTI FINALI'''\n",
    "    #from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        # ---------------------- TRAIN ----------------------\n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #model.train()  \n",
    "        \n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "            \n",
    "            #'''AGGIORNAMENTI FINALI'''\n",
    "            \n",
    "            # 👇 NOVITÀ: SCORE CONTINUO PER AUC TRAIN (usa la Softmax):\n",
    "            # OPZIONE A: puoi usare la Softmax per avere le probabilità,\n",
    "            # OPZIONE B: oppure direttamente CrossEntropy y_pred[:,1] (logit della classe 1).\n",
    "            \n",
    "            # Opzione A: usare le probabilità (softmax) \n",
    "            \n",
    "            #DECOMMENTA QUESTE 2 RIGHE PER USARE SOFTMAX\n",
    "            \n",
    "            #probs_train = torch.softmax(y_pred, dim=1)\n",
    "            #y_score_train_list.extend(probs_train[:, 1].detach().cpu().numpy())\n",
    "            \n",
    "            # Opzione B: usare direttamente i logits della classe 1 (consigliata, compatibile con CrossEntropy)\n",
    "            \n",
    "            #DECOMMENTA QUESTA RIGA PER USARE CROSSENTROPY\n",
    "            \n",
    "            # y_score_train_list.extend(y_pred[:, 1].detach().cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        '''come dovrebbe essere calcolato se non si dovesse passare al load_best_run_results'''\n",
    "        #auc_train = roc_auc_score(y_true_train_list, y_pred_train_list)\n",
    "        \n",
    "        '''come è stato calcolato se si dovesse passare al load_best_run_results'''\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #try:\n",
    "            #auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        #except ValueError:\n",
    "            #print(\"⚠️ AUC non calcolabile: nel train set c'è una sola classe.\")\n",
    "            #auc_val = np.nan\n",
    "        \n",
    "        # ---------------------- VALIDATION ----------------------\n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #model.eval()\n",
    "        \n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "        \n",
    "                \n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #y_score_val_list = []  # per AUC valida\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "                \n",
    "                #'''AGGIORNAMENTI FINALI'''\n",
    "                \n",
    "                # 👇 NOVITÀ: SCORE CONTINUO PER AUC TRAIN (usa la Softmax):\n",
    "                \n",
    "                # OPZIONE A: puoi usare la Softmax per avere le probabilità,\n",
    "                # OPZIONE B: oppure direttamente CrossEntropy y_pred[:,1] (logit della classe 1).\n",
    "                \n",
    "                # Opzione A: usare le probabilità (softmax) \n",
    "                \n",
    "                #DECOMMENTA QUESTE 2 RIGHE PER USARE SOFTMAX\n",
    "                \n",
    "                #probs_val = torch.softmax(y_pred, dim=1)\n",
    "                #y_score_val_list.extend(probs_val[:, 1].detach().cpu().numpy())\n",
    "                \n",
    "                # Opzione B: usare direttamente i logits della classe 1 (consigliata, compatibile con CrossEntropy)\n",
    "                \n",
    "                #DECOMMENTA QUESTA RIGA PER USARE CROSSENTROPY\n",
    "                # y_score_val_list.extend(y_pred[:, 1].detach().cpu().numpy())\n",
    "                \n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "        \n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #precision_val = precision_score(y_true_val_list, y_pred_val_list, average='weighted')\n",
    "        #recall_val    = recall_score(y_true_val_list, y_pred_val_list, average='weighted')\n",
    "        #f1_val        = f1_score(y_true_val_list, y_pred_val_list, average='weighted')\n",
    "        \n",
    "        #try:\n",
    "            # ATTENZIONE: qui usiamo gli score continui, NON le etichette\n",
    "            #auc_val = roc_auc_score(y_true_val_list, y_score_val_list, average='weighted')\n",
    "        #except ValueError:\n",
    "            #print(\"⚠️ AUC non calcolabile: nel validation set c'è una sola classe.\")\n",
    "            #auc_val = np.nan\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \n",
    "            # TRAIN\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \n",
    "            # VALIDATION\n",
    "            \n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val,\n",
    "            \n",
    "            # se vuoi loggare anche queste (consigliato):\n",
    "            \n",
    "            #\"val_precision\": precision_val,\n",
    "            #\"val_recall\": recall_val,\n",
    "            #\"val_f1\": f1_val,\n",
    "            #\"val_auc\": auc_val,\n",
    "        })\n",
    "        \n",
    "        #Nota: questa patch qua sopra (correzione su train e validation) rende corretto anche train_auc per le run future, \n",
    "        #quindi non avrai più bisogno della “correzione a posteriori” in load_best_run_results \n",
    "        #per i nuovi esperimenti (ma la puoi lasciare per compatibilità coi vecchi run).\n",
    "\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "            \n",
    "        '''OLD VERSION'''\n",
    "        #early_stopping(accuracy_val)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"🛑 Early stopping attivato!\")\n",
    "            #break\n",
    "\n",
    "        '''NEW VERSION'''\n",
    "        scheduler.step(loss_val)\n",
    "        early_stopping(loss_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        \n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        IMPORTANTISSIMO: COME SALVARSI LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI UN CERTO MODELLO, DI UN DATO DI UNA CERTA COMBINAZIONE DI FATTORI\n",
    "        (CONDIZIONE SPERIMENTALE, TIPO DI DATO, PROVENIENZA DEL DATO!)\n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        \n",
    "        CHATGPT:\n",
    "        \n",
    "        Nei run eseguiti con W&B ogni esecuzione registra automaticamente la configurazione degli iper-parametri (tramite wandb.config) \n",
    "        insieme alle metriche e ai log. \n",
    "        Quindi, a meno che tu non abbia modificato il comportamento predefinito, \n",
    "        ogni run con il tuo sweep ha già la configurazione associata registrata nei run logs di W&B.\n",
    "\n",
    "        Tuttavia, per associare in modo “automatico” e diretto la migliore configurazione agli specifici modelli salvati in .pth, \n",
    "        potresti considerare di fare uno o più di questi aggiustamenti:\n",
    "\n",
    "        Salvare la configurazione nel dizionario dei best_models:\n",
    "        Quando aggiorni il dizionario best_models (cioè quando salvi il miglior modello per una determinata combinazione), \n",
    "        puoi salvare anche una copia della configurazione corrente. \n",
    "        \n",
    "        Ad esempio, potresti modificare il blocco in cui aggiorni best_models in questo modo:\n",
    "        \n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": dict(config)  # Salva la configurazione degli iper-parametri\n",
    "        }\n",
    "        \n",
    "        In questo modo, ogni volta che un modello viene considerato il migliore per quella combinazione,\n",
    "        la sua configurazione sarà salvata insieme ai pesi.\n",
    "        Questo ti permetterà, in seguito, di sapere esattamente quali iper-parametri sono stati usati per ottenere quel modello.\n",
    "        \n",
    "        \n",
    "        In sintesi, se hai già usato wandb.config e hai loggato le configurazioni durante le run,\n",
    "        W&B le ha automaticamente salvate nei run logs. \n",
    "        \n",
    "        Se vuoi rendere più esplicita l'associazione tra il modello salvato (.pth) e la sua configurazione, \n",
    "        è utile modificare il tuo codice di TRAINING per salvare ANCHE \n",
    "        \n",
    "        1) il dizionario di configurazione insieme a \n",
    "        2) i pesi nel dizionario best_models oppure nei metadati del file salvato.\n",
    "        \n",
    "        Questo piccolo accorgimento ti consentirà di recuperare facilmente la configurazione ottimale per ogni modello salvato.\n",
    "        \n",
    "        OSSIA\n",
    "        Aggiungendo la chiave \"config\": dict(config) nel dizionario che memorizza il best model,\n",
    "        salvi anche la configurazione degli iper-parametri utilizzata in quella run.\n",
    "        \n",
    "        In questo modo, per ogni modello salvato (.pth) potrai recuperare facilmente sia i pesi che la configurazione ottimale che li ha generati.\n",
    "        \n",
    "        Questo approccio garantisce che ogni modello sia associato in modo esplicito al set di iper-parametri che ha prodotto le migliori performance, \n",
    "        rendendo più semplice il successivo confronto o la replica degli esperimenti.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # ***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "        #1)Al posto di salvarmi solo i migliori pesi (i.e.,  model_file = f\"{model_path}/{best_model_name}.pth\")\n",
    "        #  ora mi salvo anche la MIGLIORE configurazione di iper-parametri trovata rispetto alle 15 RUNS di un certo SWEEP\n",
    "        #  di un certo MODELLO, applicato su un DATASET con una SPECIFICA COMBINAZIONE DI FATTORI\n",
    "        #  condizione sperimentale, tipo di dato e provenienza del dato!\n",
    "        \n",
    "    \n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \n",
    "                #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "                #***** AGGIUNTA DELLA CHIAVE CONFIG CHE PRELEVA AUTOMATICAMENTE LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DENTRO 'BEST_MODELS'\n",
    "                \n",
    "                # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                # una certa combinazione di fattori: \n",
    "                # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                \"config\": dict(config)  \n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "            #***** SALVATAGGIO DI UN FILE .PKL, CHE CONTIENE \n",
    "            \n",
    "            # I PESI E BIAS DEL MODELLO DERIVATO DALLA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI OTTENUTA DALLA MIGLIORE RUN DI UN CERTO SWEEP\n",
    "            # IN RELAZIONE AD UN CERTO DATASET COSTITUITO DA UNA CERTA COMBINAZIONE DI FATTORI\n",
    "            \n",
    "            '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "            #model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "            \n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            \n",
    "            '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "            #torch.save(best_model.state_dict(), model_file)\n",
    "            \n",
    "            # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": dict(config)\n",
    "            }, model_file)\n",
    "\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                    \n",
    "                    # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                    # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                    # una certa combinazione di fattori: \n",
    "                    # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                    \"config\": dict(config)  \n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "                #model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    \n",
    "                    '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "                    #torch.save(best_model.state_dict(), model_file)\n",
    "\n",
    "                    # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                    torch.save({\n",
    "                        \"state_dict\": best_model.state_dict(),\n",
    "                        \"config\": dict(config)\n",
    "                    }, model_file)\n",
    "                    \n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d43c8-41b6-4150-9dfb-0be8036310a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure Final Edits - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS**\n",
    "#### **Sweep separati per ciascuno dei modelli CNN2D_LSTM_TF, BiLSTM e Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bf1c8-8abf-4515-8e93-e54befafbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ciao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be63000-2a49-4752-85bd-fe30c79d6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE C \n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                W&B SWEEPS AND TRAING LAUNCH WITH MULTIPLE GPUs MANAGEMENT\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE C (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "SPIEGAZIONE\n",
    "\n",
    "GPU counter: Ho aggiunto un contatore (gpu_counter) che cicla tra le GPU disponibili. \n",
    "\n",
    "In questo modo, il primo sweep sarà eseguito sulla GPU 0, il secondo sulla GPU 1, e così via. \n",
    "Quando il contatore raggiunge il numero di GPU disponibili, torna a 0 per riusare la prima GPU.\n",
    "\n",
    "Rotazione delle GPU: All'interno del loop, per ogni sweep, viene assegnata una GPU diversa. \n",
    "Se ci sono più di 1 GPU, il contatore incrementa, e la variabile CUDA_VISIBLE_DEVICES cambia automaticamente per assegnare la GPU corretta.\n",
    "\n",
    "Esecuzione parallela: Ogni sweep viene eseguito su una GPU separata. Se ci sono 2 GPU, il primo sweep va su GPU 0, il secondo su GPU 1, il terzo su GPU 0, e così via.\n",
    "\n",
    "Risposta alla tua domanda:\n",
    "In questo modo, ogni sweep_id viene eseguito una sola volta, ma su GPU diverse (se disponibili). Non ci sono duplicati dello stesso sweep su entrambe le GPU.\n",
    "\n",
    "\n",
    "DOMANDE SUL NUOVO CODICE\n",
    "\n",
    "1) Gli sweep sono eseguiti già in parallelo giusto?\n",
    "No, in questo caso gli sweep non sono eseguiti in parallelo in modo esplicito tramite il codice che hai scritto.\n",
    "\n",
    "Anche se hai assegnato ciascun sweep a una GPU diversa, il codice esegue sequenzialmente ogni sweep, solo che li distribuisce su GPU differenti in modo rotazionale.\n",
    "Ogni volta che il ciclo passa ad un nuovo sweep, assegna un ID GPU e poi esegue l'agent su quella GPU. Non vengono eseguiti in parallelo a livello di codice.\n",
    "\n",
    "2) O semplicemente in questo modo faccio in modo di distribuire ogni sweep sull'altra GPU rispetto a quella usata dallo sweep precedente\n",
    "per \"ottimizzare\" il carico computazionale di ogni GPU?\n",
    "\n",
    "Esatto! Quello che stai facendo è distribuire i vari sweep su GPU diverse, assicurandoti che ogni sweep venga eseguito su una GPU separata (se ne hai di disponibili).\n",
    "Questo permette di ottimizzare l'uso delle risorse, evitando che una GPU venga sovraccaricata da più sweep. Se il numero di GPU disponibili è maggiore di 1, \n",
    "allora i sweep sono distribuiti sulle diverse GPU, ma ogni sweep sarà ancora eseguito singolarmente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sì, con il codice che hai fornito, stai distribuendo gli sweep tra le diverse GPU, in modo da ottimizzare il carico computazionale e non sovraccaricare una sola GPU.\n",
    "\n",
    "Dettaglio del funzionamento:\n",
    "Distribuzione delle GPU (rotazionale):\n",
    "\n",
    "Quando ci sono più di una GPU, il codice assegna a ciascun sweep una GPU diversa in modo rotazionale.\n",
    "\n",
    "Per ogni ciclo del loop, la variabile gpu_counter determina a quale GPU assegnare il prossimo sweep.\n",
    "\n",
    "Se ci sono 2 GPU, il primo sweep viene eseguito sulla GPU 0, il secondo sulla GPU 1, il terzo di nuovo sulla GPU 0, e così via.\n",
    "\n",
    "Gestione della GPU:\n",
    "\n",
    "Se hai più di una GPU, os.environ[\"CUDA_VISIBLE_DEVICES\"] imposta il dispositivo GPU corrente su cui il codice deve girare (GPU 0 o GPU 1). Questo permette di gestire quale GPU eseguirà l'addestramento per ciascun sweep.\n",
    "\n",
    "Quando num_gpus > 1, il codice alterna l'assegnazione della GPU per ogni sweep, evitando di sovraccaricare una singola GPU con troppe operazioni contemporaneamente.\n",
    "\n",
    "Ottimizzazione del carico computazionale:\n",
    "\n",
    "L'alternanza tra GPU successive per ciascun sweep aiuta a distribuire il carico in modo equilibrato, specialmente se il numero di sweep è alto.\n",
    "\n",
    "In pratica, se hai 2 GPU e 10 sweep da eseguire, ogni GPU eseguirà 5 sweep, evitando di saturare una singola GPU.\n",
    "\n",
    "Parallelizzazione effettiva:\n",
    "Nel tuo codice non c'è parallelismo vero e proprio tra le GPU (come quello che otterresti con DataParallel o DistributedDataParallel), ma piuttosto distribuisci gli sweep su GPU diverse. Ogni sweep è eseguito sequenzialmente su una GPU diversa, ma non stai parallelizzando il training dello stesso sweep su più GPU.\n",
    "\n",
    "Se volessi fare in modo che un singolo sweep fosse parallelizzato su più GPU (in modo che il lavoro venga diviso tra le GPU per uno stesso sweep), dovresti usare DataParallel o DistributedDataParallel, come descritto in precedenza. Ma in questo caso, ogni sweep è isolato e viene eseguito su una sola GPU, quindi non c'è un vero parallelismo dentro ogni singolo sweep.\n",
    "\n",
    "Domande che potrebbero sorgere:\n",
    "Gli sweep vengono eseguiti in parallelo?\n",
    "\n",
    "No, i sweep sono eseguiti uno alla volta, ma su GPU diverse. Quindi, mentre il primo sweep usa la GPU 0, il secondo usa la GPU 1 e così via. Ogni sweep viene gestito separatamente, ma sfrutti più GPU per parallelizzare l'esecuzione di più sweep contemporaneamente.\n",
    "\n",
    "La distribuzione delle GPU tra gli sweep è ottimizzata?\n",
    "\n",
    "Sì, stai bilanciando il carico computazionale tra le GPU, assegnando a ogni GPU uno sweep alternato. Se hai molte GPU, puoi ottimizzare ulteriormente distribuendo i sweep su più dispositivi.\n",
    "\n",
    "Se volessi parallelizzare più agenti W&B su diverse GPU, il codice che stai utilizzando sarebbe corretto, ma per ottimizzare ulteriormente i tempi di esecuzione, potresti prendere in considerazione anche l'utilizzo di tecniche come DataParallel o DistributedDataParallel per far sì che più GPU lavorino contemporaneamente sullo stesso sweep. Ma la logica che hai già implementato va bene per distribuire più sweep tra le GPU.\n",
    "\n",
    "Se hai bisogno di ulteriori dettagli su come implementare il parallelismo vero e proprio (inclusi DataParallel o DistributedDataParallel), fammi sapere!\n",
    "\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Per modificare il loop in modo che accetti i sweeps per ogni modello e gestisca correttamente\n",
    "l'esecuzione del training per ciascun modello con il relativo sweep, dobbiamo fare alcune modifiche.\n",
    "\n",
    "\n",
    "Modifiche principali:\n",
    "\n",
    "1) Funzione make_train_wrapper:\n",
    "La funzione dovrà essere adattata per passare correttamente la configurazione di sweep per ogni modello, \n",
    "invece di passare un'unica configurazione generica (sweep_config).\n",
    "\n",
    "2) Identificazione corretta del modello: \n",
    "Nel loop, per ogni combinazione (condition, data_type, category_subject)\n",
    "e per ogni modello (ad esempio, CNN3D_LSTM_FC e SeparableCNN2D_LSTM_FC), \n",
    "\n",
    "dobbiamo passare al wandb.agent il relativo sweep ID per il modello e la sua configurazione.\n",
    "\n",
    "3) Modifica della funzione make_train_wrapper per gestire ogni modello separatamente: \n",
    "Ogni modello avrà il proprio sweep e la propria configurazione.\n",
    "\n",
    "\n",
    "Spiegazione delle modifiche:\n",
    "\n",
    "1) Funzione make_train_wrapper:\n",
    "\n",
    "Adesso prende anche model_name per passare il relativo sweep_config dal dizionario sweep_config_dict.\n",
    "Passa il sweep_config corretto per ogni modello, a seconda del model_name passato nel ciclo.\n",
    "\n",
    "2) Dizionario sweep_config_dict:\n",
    "\n",
    "Ho creato un dizionario sweep_config_dict che associa ciascun modello (\"CNN3D_LSTM_FC\" e \"SeparableCNN2D_LSTM_FC\")\n",
    "alla sua configurazione di sweep (sweep_config_cnn3d e sweep_config_cnn_sep).\n",
    "Questo permette di usare la corretta configurazione per ogni modello.\n",
    "\n",
    "3) Modifica nel ciclo:\n",
    "\n",
    "Il ciclo ora scorre su model_name (i.e., i modelli CNN3D_LSTM_FC e SeparableCNN2D_LSTM_FC) \n",
    "per ogni combinazione di condition, data_type, category_subject.\n",
    "\n",
    "Per ogni modello, il relativo sweep viene creato ed eseguito.\n",
    "\n",
    "\n",
    "Risultato:\n",
    "Ora, per ogni combinazione di condition, data_type, e category_subject, \n",
    "il codice creerà e gestirà separatamente gli sweeps per ciascun modello,\n",
    "e li eseguirà utilizzando la funzione training_sweep con la relativa configurazione specifica per ogni modello.\n",
    "\n",
    "Questa modifica ti consente di avere il corretto flusso di lavoro per eseguire\n",
    "il training separato per ogni modello con la sua configurazione.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "\n",
    "# Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "\n",
    "'''ATTENZIONE AGGIUNTO model_name tra i parametri di --> make_train_wrapper'''\n",
    "\n",
    "def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name):\n",
    "    def train_wrapper():\n",
    "\n",
    "        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "        #print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "        \n",
    "        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m, modello \\033[1m{model_name}\\033[0m\")\n",
    "        training_sweep(\n",
    "            data_dict_preprocessed, \n",
    "            sweep_config_dict[model_name], # Prendi la configurazione per il modello specifico\n",
    "            sweep_ids,\n",
    "            sweep_id,\n",
    "            sweep_tuple,\n",
    "            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "        )\n",
    "    return train_wrapper\n",
    "                        \n",
    "\n",
    "# Dizionari di configurazione per ogni modello\n",
    "\n",
    "# Comodo mapper per il tuo loop\n",
    "\n",
    "#sweep_config_dict = {\n",
    "#    \"CNN2D_LSTM_TF\": sweep_config_cnn2d_lstm_tf,\n",
    "#    \"BiLSTM\": sweep_config_bilstm,\n",
    "#    \"Transformer\": sweep_config_transformer,\n",
    "#}\n",
    "\n",
    "\n",
    "'''AL PRIMO GIRO ABILITA SOLO QUESTO'''\n",
    "#sweep_config_dict = {\n",
    "#    \"CNN2D_LSTM_TF\": sweep_config_cnn2d_lstm_tf,\n",
    "#    \"BiLSTM\": sweep_config_bilstm\n",
    "#}\n",
    "\n",
    "'''AL SECONDO GIRO ABILITA QUESTO'''\n",
    "sweep_config_dict = {\n",
    "    \"Transformer\": sweep_config_transformer\n",
    "}\n",
    "\n",
    "'''AL SECONDO GIRO ABILITA QUESTO'''\n",
    "enabled_models = set(sweep_config_dict.keys())  # {'Transformer'}\n",
    "\n",
    "\n",
    "# Verifica quante GPU sono disponibili\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "# Crea un contatore per assegnare un GPU diversa a ciascun sweep\n",
    "gpu_counter = 0\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for model_name in sweep_ids[condition][data_type][category_subject]:  # Aggiunto loop per il modello\n",
    "                \n",
    "                '''AL SECONDO GIRO ABILITA QUESTO'''\n",
    "                # ⬇️ SKIPPA tutti i modelli non abilitati (CNN2D_LSTM_TF, BiLSTM, ...)\n",
    "                if model_name not in enabled_models:\n",
    "                    print(f\"Skip {model_name}: non abilitato in questo giro\")\n",
    "                    continue\n",
    "                #for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                for sweep_tuple in sweep_ids[condition][data_type][category_subject][model_name]:  # Itera sugli sweep per ciascun modello\n",
    "\n",
    "                    # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                    sweep_id, combination_key = sweep_tuple\n",
    "                    \n",
    "                    \n",
    "                    combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "                    \n",
    "                    # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                    # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                    # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "\n",
    "\n",
    "                    # Se ci sono più di 1 GPU, assegna a ciascuna GPU uno sweep diverso\n",
    "                    if num_gpus > 1:\n",
    "\n",
    "                        '''ATTENZIONE AGGIUNTO model_name tra i parametri di --> make_train_wrapper''' \n",
    "                        \n",
    "                        # Assegna la GPU in modo rotazionale\n",
    "                        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_counter)\n",
    "                        \n",
    "                        agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name)\n",
    "                    \n",
    "                        #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\", count=200)\n",
    "                        \n",
    "                        #PER TASK 1/3\n",
    "                        wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_time_frequency_{category_subject}\", count=200)\n",
    "                        \n",
    "                        #PER TASK 2/4\n",
    "                        #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_time_freqs_new_imagery_3d_grid_multiband\", count=200)\n",
    "                        \n",
    "                        # Passa alla prossima GPU per il prossimo sweep\n",
    "                        gpu_counter = (gpu_counter + 1) % num_gpus\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        # Se c'è una sola GPU, esegui il sweep sulla GPU 0\n",
    "                        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "                        \n",
    "                        agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name)\n",
    "                        \n",
    "                        #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\", count=200)\n",
    "                        \n",
    "                        #PER TASK 1/3\n",
    "                        wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_time_frequency_{category_subject}\", count=200)\n",
    "                        \n",
    "                        #PER TASK 2/4\n",
    "                        #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_time_freqs_new_imagery_3d_grid_multiband\", count=200)\n",
    "\n",
    "\n",
    "                    # Crea la funzione wrapper per l'agent\n",
    "                    '''COMMENTATO'''\n",
    "                    #agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "\n",
    "\n",
    "                    # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                    '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                       ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "\n",
    "                    print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "\n",
    "                    '''COMMENTATO'''\n",
    "                    #wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs_new_2d_grid_multiband_topomap\", count=15)\n",
    "\n",
    "                    print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722ec39-579c-44f4-beb9-58cb450a8a20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Impostazione **Recupero DL Optimized Models** - EEG Spectrograms - **Frequency x Time (2D)**\n",
    "\n",
    "### IMPLEMENTAZIONE DEI BEST MODELS DOPO W&B - EEG SPECTROGRAMS **+ GRADCAM FREQUENCY x TIME (ALL SUBJECTS)**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21048fd9-10c8-4a4b-b44a-cb1822bab02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Importing \n",
    "    \n",
    "import os\n",
    "import math\n",
    "import copy as cp \n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random \n",
    "\n",
    "#import mne \n",
    "import scipy\n",
    "\n",
    "import numpy as np  # NumPy per operazioni numeriche\n",
    "import matplotlib.pyplot as plt  # Matplotlib per la visualizzazione dei dati\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ddb39-1dcf-432a-a3fd-a1c5c1470bab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **NUOVE MODIFICHE SPECIFICHE PER I DATI NON HYPER POST W&B CON GRADCAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1126ae-d78e-4002-a5f6-dcb142f29952",
   "metadata": {},
   "source": [
    "Allora le modifche che ho ultimato quindi sono:\n",
    "\n",
    "- **1)Creazione della classe GradCAM**\n",
    "\n",
    "\n",
    "    **GRADCAM CLASS**\n",
    "\n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        class GradCAM:\n",
    "            def __init__(self, model, target_layer):\n",
    "                self.model = model\n",
    "                self.target_layer = target_layer\n",
    "                self.activations = None\n",
    "                self.gradients = None\n",
    "                # Registra hook per catturare attivazioni e gradienti\n",
    "                self.target_layer.register_forward_hook(self.save_activation)\n",
    "                self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "            def save_activation(self, module, input, output):\n",
    "                self.activations = output.detach()\n",
    "\n",
    "            def save_gradient(self, module, grad_input, grad_output):\n",
    "                self.gradients = grad_output[0].detach()\n",
    "\n",
    "\n",
    "- **2)** Creazione della funzione per generare delle immagini associate alla GradCAM compution**\n",
    "\n",
    "    \n",
    "    **FUNCTION FOR CREATING GRAD-CAM MAPS & FIGURES ASSOCIATED TO GRADCAM COMPUTATION**\n",
    "\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import io\n",
    "\n",
    "        def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "\n",
    "            \"\"\"\n",
    "            Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "            calcola la GradCAM e costruisce una figura con:\n",
    "              - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "              - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "            I titoli della figura vengono personalizzati con exp_cond, data_type, category_subject.\n",
    "            \"\"\"\n",
    "\n",
    "            # Assumiamo che il modello sia CNN2D e che il layer target sia model.conv3\n",
    "            target_layer = model.conv3\n",
    "            gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "            # Dizionari per salvare il campione per ogni classe\n",
    "            samples = {}      # Salveremo il sample input per ogni classe\n",
    "            labels_found = {} # Per tenere traccia delle etichette già trovate\n",
    "\n",
    "            # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                for i, label in enumerate(labels):\n",
    "                    label_int = int(label.item())\n",
    "                    if label_int not in labels_found:\n",
    "                        samples[label_int] = inputs[i].unsqueeze(0)  # salva come tensore 4D\n",
    "                        labels_found[label_int] = True\n",
    "                    if 0 in labels_found and 1 in labels_found:\n",
    "                        break\n",
    "                if 0 in labels_found and 1 in labels_found:\n",
    "                    break\n",
    "\n",
    "            # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "            if 0 not in samples or 1 not in samples:\n",
    "                print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "                return None\n",
    "\n",
    "            # Per ciascun campione, calcola GradCAM\n",
    "            cams = {}\n",
    "            overlays = {}\n",
    "            for cls in [0, 1]:\n",
    "                sample_input = samples[cls]\n",
    "                sample_input.requires_grad = True  # Abilita gradiente per il campione\n",
    "                cam = gradcam.generate_cam(sample_input)\n",
    "                cams[cls] = cam\n",
    "\n",
    "                # Converti il sample in immagine numpy per la visualizzazione\n",
    "                img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "                # Normalizza l'immagine in scala 0-255\n",
    "                img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "                # Applica la heatmap\n",
    "                heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "                heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "                # Sovrapponi la heatmap all'immagine originale\n",
    "                overlay = cv2.addWeighted(img_norm, 0.6, heatmap, 0.4, 0)\n",
    "                overlays[cls] = overlay\n",
    "\n",
    "            # Crea la figura con due righe e due colonne\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "            # Titolo per la prima riga\n",
    "            title_row1 = f\"Grad-CAM mapping of experimental condition {exp_cond}, EEG {data_type}, Subject {category_subject}\"\n",
    "            # Titolo per la seconda riga\n",
    "            title_row2 = f\"Grad-CAM mapping superimposition over EEG Spectrogram of experimental condition {exp_cond}, Subject {category_subject}\"\n",
    "\n",
    "            # Prima riga: solo le heatmap\n",
    "            for j, cls in enumerate([0, 1]):\n",
    "                axs[0, j].imshow(cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB))\n",
    "                axs[0, j].set_title(f\"Class {cls} Heatmap\")\n",
    "                axs[0, j].axis('off')\n",
    "            axs[0, 0].set_ylabel(title_row1, fontsize=10)\n",
    "\n",
    "            # Seconda riga: overlay della heatmap sullo spettrogramma originale\n",
    "            for j, cls in enumerate([0, 1]):\n",
    "                axs[1, j].imshow(overlays[cls])\n",
    "                axs[1, j].set_title(f\"Class {cls} Overlay\")\n",
    "                axs[1, j].axis('off')\n",
    "            axs[1, 0].set_ylabel(title_row2, fontsize=10)\n",
    "\n",
    "            # Ottimizza la disposizione della figura\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            fig_image = buf.getvalue()\n",
    "            buf.close()\n",
    "            plt.close(fig)\n",
    "\n",
    "            return fig_image\n",
    "\n",
    "\n",
    "- **3) Modifica delle funzioni per il salvataggio delle immagini create tramite la GradCAM compution**\n",
    "\n",
    "    **FUNCTIONS FOR GRADCAM COMPUTATION & SAVING**\n",
    "    \n",
    "    Questa modifica consente di creare ed adattare le path di salvataggio ANCHE delle immagini calcolate dalla classe customizzata di GradCAM, \n",
    "    delle mappe di attivazione prodotte dalle feature maps e della sovrapposizione delle stesse aree decisionali\n",
    "    rilevanti per la migliore classificazione dei dati di esempio di una certa classe,\n",
    "    a partire da un certo dataset composto da una certa combinazione di fattori\n",
    "    (i.e., exp_cond, data_type, category_subject)\n",
    "\n",
    "\n",
    "#NEW VERSIONS FOR SPECTROGRAMS WITH GRADCAM COMPUTATION ON CNN2D!\n",
    "\n",
    "    **Funzione per determinare a quale subfolder appartiene la chiave**\n",
    "    def get_subfolder_from_key(key, model_standardization):\n",
    "\n",
    "        #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "        if '_familiar_th' in key:\n",
    "            return 'th_fam'\n",
    "        elif '_unfamiliar_th' in key:\n",
    "            return 'th_unfam'\n",
    "        elif '_familiar_pt' in key:\n",
    "            return 'pt_fam'\n",
    "        elif '_unfamiliar_pt' in key:\n",
    "            return 'pt_unfam'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    **Funzione per salvare i risultati**\n",
    "    def save_performance_results(model_name, \n",
    "                                 my_train_results,\n",
    "                                 my_test_results, \n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder,\n",
    "                                 gradcam_image = None):\n",
    "        \"\"\"\n",
    "        Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "        Se gradcam_image è fornita, la salva anche in formato PNG con un nome che inizia con 'GradCAM_results'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Identificazione del subfolder in base alla chiave\n",
    "        subfolder = get_subfolder_from_key(key, model_standardization)\n",
    "\n",
    "        # Debug: controllo sulla subfolder\n",
    "        print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "\n",
    "        if subfolder is None:\n",
    "            print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "            return\n",
    "\n",
    "        # Determinazione del tipo di dato direttamente dalla chiave\n",
    "        if \"spectrograms\" in key:\n",
    "            data_type_str = \"spectrograms\"\n",
    "        else:\n",
    "            print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "            return\n",
    "\n",
    "        # Creazione del nome del file pickle con l'inclusione della combinazione key + model_name\n",
    "        if model_standardization:\n",
    "            file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}_std.pkl\"\n",
    "            folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "        else:\n",
    "            file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}.pkl\"\n",
    "            folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "\n",
    "        # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Creazione del dizionario con i risultati\n",
    "        results_dict = {\n",
    "            'my_train_results': my_train_results,\n",
    "            'my_test_results': my_test_results\n",
    "        }\n",
    "\n",
    "        # Salvataggio del dizionario con i risultati\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(results_dict, f)\n",
    "            print(f\"\\n🔬Risultati salvati con successo 👍 in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌Errore durante il salvataggio dei risultati: {e}\")\n",
    "\n",
    "        # Se è stata fornita l'immagine GradCAM, salvala come file PNG\n",
    "        if gradcam_image is not None:\n",
    "            if model_standardization:\n",
    "                gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}_std.png\"\n",
    "            else:\n",
    "                gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}.png\"\n",
    "\n",
    "            gradcam_file_path = os.path.join(folder_path, gradcam_file_name)\n",
    "\n",
    "            #try:\n",
    "            #    with open(gradcam_file_path, \"wb\") as f_img:\n",
    "            #        f_img.write(gradcam_image)\n",
    "            #    print(f\"\\n📸Immagine GradCAM salvata con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                '''\n",
    "                Se gradcam_image è un oggetto BytesIO, allora rappresenta un flusso di dati binari in memoria.\n",
    "                Quando si leggono dati da un BytesIO, il cursore interno avanza come in un file normale. \n",
    "                Se il cursore non è all'inizio, Image.open() potrebbe non leggere correttamente l'immagine.\n",
    "                👉 seek(0) riporta il cursore all'inizio del buffer prima di leggerlo con Image.open()\n",
    "\n",
    "                Per maggior info leggi cella successiva!\n",
    "                '''\n",
    "\n",
    "                # 🔄 Se gradcam_image è un buffer, convertirlo in immagine PIL\n",
    "                if isinstance(gradcam_image, io.BytesIO):\n",
    "                    gradcam_image.seek(0)  # 🔄 Reset puntatore del buffer\n",
    "                    gradcam_image = Image.open(gradcam_image)\n",
    "\n",
    "                print(f\"\\n📸Immagine GradCAM salvata con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "                # 🔄 Salvare l'immagine nel percorso specificato\n",
    "                gradcam_image.save(gradcam_file_path, format = \"PNG\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌Errore durante il salvataggio dell'immagine GradCAM: {e}\")\n",
    "\n",
    "\n",
    "- **4) Integrazione nel loop di training e test dei punti 1), 2) e 3)**\n",
    "\n",
    "    **INTEGRATION OF GRADCAM COMPUTATION IN THE TRAINING E FOR LOOP**\n",
    "\n",
    "    Nel loop che esegue il training ed il testing, integrazione della parte di inizializzazione della classe custom di GradCAM, con cui si esegue \n",
    "\n",
    "    il calcolo delle mappe di attivazione e della sovrapposizione delle mappe di attivazione stesse sullo spettogramma originale, \n",
    "    riportate poi in due immagini distinte create nella stessa figura che vengono salvate correttamente nella stessa directory path. \n",
    "\n",
    "    Le due immagini dovrebbero rappresentare l'heatmap activation e la sovrapposizione della mappa di attivazione sullo spettogramma originale,\n",
    "    relativo ad un esempio rappresentativo per ciascuna delle due classi possibili presenti nello stesso dataset correntemente iterato.\n",
    "\n",
    "    Il loro contributo è di descrivere se la CNN2D abbia identificato delle (possibili) differenti aree decisionali delle feature maps \n",
    "    (e dunque dello spettrogramma) maggiormente utili ai fini della discriminazione delle due condizioni sperimentali inserite all'interno del dataset correntemente iterato.\n",
    "\n",
    "\n",
    "        ** Dizionario per tracciare la standardizzazione usata per ogni combinazione di dati**\n",
    "        ** Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)**\n",
    "\n",
    "        models_info = {}\n",
    "\n",
    "        ** Set per tenere traccia dei dataset già elaborati**\n",
    "        processed_datasets = set()\n",
    "\n",
    "        ** Set per tenere traccia delle combinazioni già elaborate**\n",
    "        processed_models = set()\n",
    "\n",
    "        ** Path delle performance dei modelli ottimizzati con weight and biases**\n",
    "        ** Path per trovare le best performances di ogni modello per ogni combinazione dei dati**\n",
    "        base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results\"\n",
    "\n",
    "        ** Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder**\n",
    "        save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_post_WB\"\n",
    "\n",
    "\n",
    "        ** --- LOOP PRINCIPALE (con minime modifiche) ---**\n",
    "        for key, (X_data, y_data) in data_dict.items():\n",
    "\n",
    "            print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "\n",
    "            if key in processed_datasets:\n",
    "                print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "                continue\n",
    "\n",
    "            processed_datasets.add(key)\n",
    "\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "            print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "\n",
    "            for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "\n",
    "                model_key = f\"{model_name}_{key}\"\n",
    "                if model_key in processed_models:\n",
    "                    print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "                    continue\n",
    "                processed_models.add(model_key)\n",
    "\n",
    "                print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "\n",
    "                # Prova a caricare la configurazione e i pesi ottimali dal file .pkl\n",
    "\n",
    "                '''\n",
    "                load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "                parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "\n",
    "                exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "                pesi del modello e i suoi iper-parametri\n",
    "\n",
    "                Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "                '''\n",
    "\n",
    "                config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "\n",
    "                if config is None:\n",
    "                    raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "\n",
    "                '''\n",
    "                Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "                MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "                '''\n",
    "\n",
    "                # Parsifica la chiave una volta sola e memorizza i valori\n",
    "                exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "\n",
    "                '''\n",
    "                Dpodiché, \n",
    "\n",
    "                1) si carica i vari valori degli iper-parametri,\n",
    "                2) si esegue la standardizzazione se servisse,\n",
    "                3) prepara il modello per la divisione in train_loader etc.,\n",
    "                4) si carica la configurazione dei pesi del modello, \n",
    "                5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "\n",
    "                6) esegue il training e il test e poi\n",
    "\n",
    "                7) si salva il tutto nella path corrispondente...\n",
    "\n",
    "                '''\n",
    "\n",
    "                '''\n",
    "                PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "                DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "\n",
    "                model_batch_size = config[\"batch_size\"]\n",
    "                model_n_epochs = config[\"n_epochs\"]\n",
    "                model_patience = config[\"patience\"]\n",
    "                model_lr = config[\"lr\"]\n",
    "                model_weight_decay = config[\"weight_decay\"]\n",
    "                model_standardization = config[\"standardization\"]\n",
    "\n",
    "                print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, weight_decay= \\033[1m{model_weight_decay}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "\n",
    "                # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "                models_info[model_key] = {\"standardization\": model_standardization}\n",
    "\n",
    "\n",
    "                '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "                IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "                '''\n",
    "\n",
    "                if model_standardization:\n",
    "                    X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "                    print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "                else:\n",
    "                    print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "\n",
    "                # Sposta il modello sulla GPU (se disponibile)\n",
    "                device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "                # Preparazione dei dataloaders\n",
    "                train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "                    X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "\n",
    "                # Inizializzazione del modello\n",
    "                if model_name == \"CNN2D\":\n",
    "                    model = CNN2D(input_channels=3, num_classes=2)\n",
    "                elif model_name == \"BiLSTM\":\n",
    "                    model = ReadMEndYou(input_size= 3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "                elif model_name == \"Transformer\":\n",
    "                    model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "                else:\n",
    "                    raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "\n",
    "                # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "                if best_weights is not None:\n",
    "                    try:\n",
    "                        model.load_state_dict(best_weights)\n",
    "                        print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "                        continue\n",
    "\n",
    "\n",
    "                # Definizione del criterio di perdita\n",
    "                criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "\n",
    "                # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "\n",
    "                print(f\"🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "                my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "\n",
    "                print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "                my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "\n",
    "                '''\n",
    "                GRADCAM COMPUTATION PER IL MODELLO CNN2D\n",
    "\n",
    "                La funzione compute_gradcam_figure estrae due campioni (uno per ogni classe) e crea una figura con le due righe richieste.\n",
    "\n",
    "                Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "                'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "\n",
    "                La funzione 'save_performance_results' è stata modificata \n",
    "                per gestire ANCHE questo nuovo input dell'immagine \n",
    "\n",
    "                (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "                seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "\n",
    "                - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "                - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "                - provenienza del dato stesso (i.e., familiar_th)\n",
    "                )\n",
    "\n",
    "                '''\n",
    "\n",
    "                # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "                gradcam_image = None\n",
    "\n",
    "                if model_name == \"CNN2D\":\n",
    "                    gradcam_image = compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device)\n",
    "                    if gradcam_image is not None:\n",
    "                        print(f\"GradCAM image computed successfully for {model_name}.\")\n",
    "\n",
    "                print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "                save_performance_results(model_name,\n",
    "                                         my_train_results,\n",
    "                                         my_test_results,\n",
    "                                         key,\n",
    "                                         exp_cond,\n",
    "                                         model_standardization,\n",
    "                                         base_folder = save_path_folder,\n",
    "                                         gradcam_image = gradcam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f9eda-46f6-4de8-b4af-3d3624cc74ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **UTILS DATI NON HYPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ff930-b8c2-4f69-9638-9d5d86de9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(data_type, category, subject_type, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Carica i dati EEG dalla directory appropriata, già salvati con la finestra temporale (50°-300° punto)\n",
    "\n",
    "    Parameters:\n",
    "    - data_type: str, \"spectrograms\",\n",
    "    - category: str, \"familiar\" o \"unfamiliar\"\n",
    "    - subject_type: str, \"th\" (terapisti) o \"pt\" (pazienti)\n",
    "    - condition: str, condizione sperimentale da selezionare\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - X: Dati EEG sotto-selezionati (50°-300° punto e canali selezionati se applicabile)\n",
    "    - y: Etichette corrispondenti\n",
    "    \"\"\"\n",
    "\n",
    "    # Definizione dei percorsi base\n",
    "    base_paths = {\n",
    "        \"spectrograms\": {\n",
    "            \"familiar\": \"/home/stefano/Interrogait/all_datas/Familiar_Spectrograms/\",\n",
    "            \"unfamiliar\": \"/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms/\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Seleziona il path corretto\n",
    "    base_path = base_paths[data_type][category]\n",
    "\n",
    "    # Determina il nome del file corretto\n",
    "    if data_type in [\"spectrograms\"]:\n",
    "        filename = f\"new_all_{subject_type}_concat_spectrograms_coupled_exp.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"data_type non valido!\")\n",
    "        \n",
    "    # Caricamento del file\n",
    "    filepath = base_path + filename\n",
    "    \n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    '''\n",
    "    Per i dati spectrogram, la funzione seleziona la condizione desiderata (i.e., condition = \"th_resp_vs_pt_resp\") \n",
    "    e preleva i dati e le etichette associati a quella condizione.\n",
    "    '''\n",
    "    \n",
    "    # Selezione della finestra temporale e delle etichette\n",
    "    X = data[condition][\"data\"]\n",
    "    y = data[condition][\"labels\"]\n",
    "\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def select_channels(data, channels=[12, 30, 48]):\n",
    "    \"\"\"\n",
    "    Seleziona i canali EEG specificati SOLO per i dati 1-20 e 1-45.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array NumPy, dati EEG con shape (n_trials, n_channels, n_timepoints)\n",
    "    - channels: list, indici dei canali da selezionare\n",
    "\n",
    "    Returns:\n",
    "    - data filtrato sui canali specificati\n",
    "    \"\"\"\n",
    "    return data[:, channels, :]\n",
    "\n",
    "\n",
    "# Funzione per train-test split\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''ATTENZIONE CAMBIATA'''\n",
    "def standardize_data(X_train, X_val, X_test, eps = 1e-8):\n",
    "    \n",
    "    mean = X_train.mean(axis=0, keepdims=True)\n",
    "    std = X_train.std(axis=0, keepdims=True)\n",
    "    \n",
    "    #aggiungo eps per evitare divisione per zero\n",
    "    X_train = (X_train - mean) / (std + eps)\n",
    "    X_val = (X_val - mean) / (std + eps)\n",
    "    X_test = (X_test - mean) / (std + eps)\n",
    "    \n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "# Import modelli (definisci le classi CNN1D, ReadMEndYou, ReadMYMind)\n",
    "#from models import CNN1D, ReadMEndYou, ReadMYMind  # Assicurati di avere i modelli definiti in 'models.py'\n",
    "\n",
    "# Funzione per inizializzare i modelli\n",
    "def initialize_models():\n",
    "    #model = CNN1D(input_channels=3, num_classes=2)\n",
    "    model_CNN = CNN2D(input_channels=3, num_classes=2)\n",
    "    #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    model_LSTM = ReadMEndYou(input_size=3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "    model_Transformer = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "    \n",
    "    return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "'''\n",
    "Questa funzione prende in input i dati di training, validation e test, \n",
    "il tipo di modello scelto e la dimensione del batch. Si occupa di:\n",
    "\n",
    "Calcolare i pesi delle classi.\n",
    "Convertire i dati in tensori PyTorch, con le opportune trasformazioni per CNN, LSTM o Transformer.\n",
    "Creare i dataset e i dataloader per il training.\n",
    "'''\n",
    "\n",
    "\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48):\n",
    "    \n",
    "    # Calcolo dei pesi delle classi\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                         classes=np.unique(y_train), \n",
    "                                         y=y_train)\n",
    "    \n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    class_weights_tensor = class_weights_tensor.to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch con permutazione se necessario\n",
    "    if model_type == \"CNN2D_LSTM_TF\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #BiLSTM (ReadMEndYou):\n",
    "    #Ora il modello si aspetta l’input con shape (batch, canali, frequenze, tempo) \n",
    "    #e, al suo interno, \n",
    "    #esegue la permutazione per avere il tempo come dimensione sequenziale. \n",
    "    #Non serve quindi applicare una permutazione anche qui.\n",
    "    \n",
    "    elif model_type == \"BiLSTM\":\n",
    "            \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #Transformer (ReadMYMind):\n",
    "    #Analogamente, il modello gestisce internamente la riorganizzazione dell’input, quindi lasciamo i dati nella loro forma originale.\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Modello non riconosciuto. Scegli tra 'CNN', 'LSTM' o 'Transformer'.\")\n",
    "    \n",
    "    # Conversione delle etichette in tensori\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Creazione dei dataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Creazione dei dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "OLD VERSIONS BEFORE GRADCAM COMPUTATION ON CNN2D\n",
    "\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, model_standardization):\n",
    "    \n",
    "    #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "    if '_familiar_th' in key:\n",
    "        return 'th_fam'\n",
    "    elif '_unfamiliar_th' in key:\n",
    "        return 'th_unfam'\n",
    "    elif '_familiar_pt' in key:\n",
    "        return 'pt_fam'\n",
    "    elif '_unfamiliar_pt' in key:\n",
    "        return 'pt_unfam'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "     \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, my_train_results, my_test_results, key, exp_cond, model_standardization, base_folder):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, model_standardization)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"spectrograms\" in key:\n",
    "        data_type_str = \"spectrograms\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    if model_standardization:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}_std.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "        \n",
    "    else:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        \n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo 👍 in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌Errore durante il salvataggio dei risultati: {e}\")\n",
    "'''\n",
    "\n",
    "\n",
    "#NEW VERSIONS FOR SPECTROGRAMS WITH GRADCAM COMPUTATION ON CNN2D!\n",
    "\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, model_standardization):\n",
    "    \n",
    "    #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "    if '_familiar_th' in key:\n",
    "        return 'th_fam'\n",
    "    elif '_unfamiliar_th' in key:\n",
    "        return 'th_unfam'\n",
    "    elif '_familiar_pt' in key:\n",
    "        return 'pt_fam'\n",
    "    elif '_unfamiliar_pt' in key:\n",
    "        return 'pt_unfam'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "     \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, \n",
    "                             my_train_results,\n",
    "                             my_test_results, \n",
    "                             key,\n",
    "                             exp_cond,\n",
    "                             model_standardization,\n",
    "                             base_folder,\n",
    "                             gradcam_image = None):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    Se gradcam_image è fornita, la salva anche in formato PNG con un nome che inizia con 'GradCAM_results'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, model_standardization)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"spectrograms\" in key:\n",
    "        data_type_str = \"spectrograms\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file pickle con l'inclusione della combinazione key + model_name\n",
    "    if model_standardization:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}_std.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "    else:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\n🔬Risultati salvati con successo 👍 in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌Errore durante il salvataggio dei risultati: {e}\")\n",
    "    \n",
    "    # Se è stata fornita l'immagine GradCAM, salvala come file PNG\n",
    "    if gradcam_image is not None:\n",
    "        if model_standardization:\n",
    "            gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}_std.png\"\n",
    "        else:\n",
    "            gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}.png\"\n",
    "        \n",
    "        gradcam_file_path = os.path.join(folder_path, gradcam_file_name)\n",
    "        \n",
    "        #try:\n",
    "        #    with open(gradcam_file_path, \"wb\") as f_img:\n",
    "        #        f_img.write(gradcam_image)\n",
    "        #    print(f\"\\n📸Immagine GradCAM salvata con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            '''\n",
    "            Se gradcam_image è un oggetto BytesIO, allora rappresenta un flusso di dati binari in memoria.\n",
    "            Quando si leggono dati da un BytesIO, il cursore interno avanza come in un file normale. \n",
    "            Se il cursore non è all'inizio, Image.open() potrebbe non leggere correttamente l'immagine.\n",
    "            👉 seek(0) riporta il cursore all'inizio del buffer prima di leggerlo con Image.open()\n",
    "            \n",
    "            Per maggior info leggi cella successiva!\n",
    "            '''\n",
    "            \n",
    "            # 🔄 Se gradcam_image è un buffer, convertirlo in immagine PIL\n",
    "            if isinstance(gradcam_image, io.BytesIO):\n",
    "                gradcam_image.seek(0)  # 🔄 Reset puntatore del buffer\n",
    "                gradcam_image = Image.open(gradcam_image)\n",
    "            \n",
    "            '''\n",
    "            Il messaggio di errore indica che il tuo oggetto gradcam_image è di tipo bytes e non ha il metodo save(), \n",
    "            che è tipico di un oggetto PIL. \n",
    "            \n",
    "            Per risolvere questo, devi convertire i byte in un'immagine PIL. \n",
    "            Per farlo, controlla se gradcam_image sia un oggetto di tipo bytes e,\n",
    "            in tal caso, usa io.BytesIO per creare un buffer da passare a Image.open(). \n",
    "            \n",
    "            Inserisci questa conversione all'interno del blocco che salva l'immagine, così da assicurarti che,\n",
    "            indipendentemente dal tipo, gradcam_image diventi un oggetto PIL e possa chiamare il metodo save().\n",
    "            '''\n",
    "            \n",
    "            if isinstance(gradcam_image, bytes):\n",
    "                gradcam_image = io.BytesIO(gradcam_image)\n",
    "                gradcam_image.seek(0)\n",
    "                gradcam_image = Image.open(gradcam_image)\n",
    "            \n",
    "            \n",
    "            print(f\"\\n📸Immagine \\033[1mGradCAM salvata\\033[0m con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "            # 🔄 Salvare l'immagine nel percorso specificato\n",
    "            gradcam_image.save(gradcam_file_path, format = \"PNG\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌Errore durante il salvataggio dell'immagine GradCAM: {e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6948653-166b-4032-ab04-ebb6075ef096",
   "metadata": {
    "tags": []
   },
   "source": [
    "Quando si parla di buffer o file, immagina che ci sia un piccolo cursore invisibile che tiene traccia di dove ci troviamo nella lettura/scrittura.\n",
    "\n",
    "📌 Cos'è il cursore di un file o buffer?\n",
    "Il cursore interno è un puntatore che indica la posizione attuale nel file (o buffer).\n",
    "\n",
    "Quando scrivi dati, il cursore avanza alla fine di ciò che hai scritto.\n",
    "Quando leggi, il cursore avanza man mano che scorri i dati.\n",
    "Se tenti di leggere senza riportare il cursore all'inizio, potresti ottenere dati incompleti o un errore.\n",
    "📌 Esempio con un file\n",
    "Immagina un file di testo chiamato esempio.txt con questo contenuto:\n",
    "\n",
    "Copia\n",
    "Modifica\n",
    "Ciao, come stai?\n",
    "Ora vediamo cosa succede quando lo leggiamo:\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "# Apriamo il file in modalità lettura\n",
    "with open(\"esempio.txt\", \"r\") as f:\n",
    "    print(f.read())  # ✅ Leggiamo tutto -> \"Ciao, come stai?\"\n",
    "    \n",
    "    print(f.read())  # ❌ Ora il cursore è alla fine -> \"\" (stringa vuota!)\n",
    "Il secondo read() non restituisce nulla perché il cursore è già alla fine del file.\n",
    "Per rileggere il file dobbiamo spostare il cursore all'inizio con seek(0):\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "with open(\"esempio.txt\", \"r\") as f:\n",
    "    print(f.read())  # ✅ Legge tutto -> \"Ciao, come stai?\"\n",
    "    \n",
    "    f.seek(0)  # 🔄 Riporta il cursore all'inizio\n",
    "    \n",
    "    print(f.read())  # ✅ Ora rilegge tutto -> \"Ciao, come stai?\"\n",
    "📌 Esempio con BytesIO (buffer in memoria)\n",
    "Un BytesIO funziona come un file, ma è in RAM. Vediamo cosa succede senza seek(0):\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "import io\n",
    "\n",
    "# Creiamo un buffer\n",
    "buffer = io.BytesIO()\n",
    "buffer.write(b\"Ciao, come stai?\")  # ✍️ Scriviamo qualcosa\n",
    "\n",
    "print(buffer.read())  # ❌ \"\" perché il cursore è alla fine!\n",
    "\n",
    "buffer.seek(0)  # 🔄 Riportiamo il cursore all'inizio\n",
    "print(buffer.read())  # ✅ \"Ciao, come stai?\"\n",
    "📌 Applicazione al tuo codice GradCAM\n",
    "Nel tuo caso, la sequenza è questa:\n",
    "\n",
    "1️⃣ Crei un BytesIO()\n",
    "2️⃣ Salvi l'immagine nel buffer → il cursore ora è alla fine\n",
    "3️⃣ Per poterla leggere con Image.open(), devi riportarlo all'inizio con seek(0)\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')  # ✍️ Salvataggio avanza il cursore\n",
    "buf.seek(0)  # 🔄 Riporta il cursore all'inizio per poterlo leggere\n",
    "fig_image = buf.getvalue()  # Ora possiamo leggere i byte correttamente!\n",
    "buf.close()\n",
    "E quando passi il buffer a save_performance_results, devi ripetere il seek(0) prima di aprirlo con Image.open().\n",
    "\n",
    "💡 Conclusione:\n",
    "Il cursore è come un segnalibro in un file o buffer. Se non lo riporti all'inizio, leggere i dati successivamente potrebbe fallire! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec309554-91c1-48b2-bb3a-6787058142e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **NUOVE UTILS DATI NON HYPER POST W&B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806a807-76d1-4adc-b860-c158b128fe14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **SUGGERIMENTI DI MODIFICA CHATGPT DELLE UTILS DATI NON HYPER POST W&B**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5efcd69e-9dbf-4174-89e7-1bad6aa25c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### COME ANDREBBE CAMBIATA 'PREPARE_DATA_FOR_MODEL' IN CASO VOLESSI INTEGRARE LA STANDARDIZATION AL SUO INTERNO\n",
    "\n",
    "'''\n",
    "# Esempio di funzione di data preparation (OPZIONE 1)\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48, standardize=True):\n",
    "    # Calcolo dei pesi delle classi\n",
    "    # (Si assume che compute_class_weight sia già definita altrove)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(torch.float32)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Se la standardizzazione è richiesta, la eseguo qui (funzione ipotetica standardize_data)\n",
    "    if standardize:\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "    else:\n",
    "        print(\"Standardizzazione NON eseguita!\")\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch\n",
    "    if model_type == \"CNN1D\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        # Permutazione se necessario\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Modello {model_type} non riconosciuto nella preparazione dei dati.\")\n",
    "    \n",
    "    # Creazione dei DataLoader (si assume che DataLoader sia importato)\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, torch.tensor(y_train))\n",
    "    val_dataset   = TensorDataset(X_val_tensor, torch.tensor(y_val))\n",
    "    test_dataset  = TensorDataset(X_test_tensor, torch.tensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a310a8b-adca-4c38-afb9-3221d4715cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### COME ANDREBBE CAMBIATA LA LOGICA DEL LOOP PER CHATGTP\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"1_20|1_45|wavelet\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(1_20|1_45|wavelet)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\", \n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "# Funzione per caricare il file .pkl con la configurazione e i pesi ottimali\n",
    "def load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path):\n",
    "    \"\"\"\n",
    "    Costruisce il path usando:\n",
    "        base_path / exp_cond / data_type / category_subject\n",
    "    e il nome del file:\n",
    "        {model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\n",
    "    Se il file esiste, lo carica e restituisce (config, state_dict).\n",
    "    \"\"\"\n",
    "    file_name = f\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "    file_path = os.path.join(base_path, exp_cond, data_type, category_subject, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Caricamento file pkl: {file_path}\")\n",
    "        # Si assume che il file .pkl sia salvato con torch.save() e contenga un dizionario con chiavi \"config\" e \"state_dict\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = torch.load(f)\n",
    "        return data[\"config\"], data[\"state_dict\"]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file_path} non trovato.\")\n",
    "\n",
    "# Esempio di funzione di data preparation (OPZIONE 1)\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48, standardize=True):\n",
    "    # Calcolo dei pesi delle classi\n",
    "    # (Si assume che compute_class_weight sia già definita altrove)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(torch.float32)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Se la standardizzazione è richiesta, la eseguo qui (funzione ipotetica standardize_data)\n",
    "    if standardize:\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "    else:\n",
    "        print(\"Standardizzazione NON eseguita!\")\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch\n",
    "    if model_type == \"CNN1D\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        # Permutazione se necessario\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Modello {model_type} non riconosciuto nella preparazione dei dati.\")\n",
    "    \n",
    "    # Creazione dei DataLoader (si assume che DataLoader sia importato)\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, torch.tensor(y_train))\n",
    "    val_dataset   = TensorDataset(X_val_tensor, torch.tensor(y_val))\n",
    "    test_dataset  = TensorDataset(X_test_tensor, torch.tensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "\n",
    "# Funzione di training (come da te definita)\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs=100, patience=10):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loss_train_history = []\n",
    "    loss_val_history = []\n",
    "    accuracy_train_history = []\n",
    "    accuracy_val_history = []\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list = []\n",
    "        y_pred_train_list = []\n",
    "        \n",
    "        # Fase di training\n",
    "        for x, y in dataset_train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_tmp.append(train_loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Fase di validazione\n",
    "        loss_tmp_val = []\n",
    "        correct_val = 0\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataset_val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "                loss_tmp_val.append(val_loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val))\n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "    \n",
    "    print(f\"Miglior epoca: {best_epoch} con Val Acc: {max_val_acc:.4f}\")\n",
    "    # Restituisco un dizionario con i risultati (semplificato)\n",
    "    train_results = {\n",
    "        \"best_model\": best_model,\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history\n",
    "    }\n",
    "    return train_results\n",
    "\n",
    "# --- LOOP PRINCIPALE INTEGRAZIONE CONFIG DA .pkl ---\n",
    "\n",
    "# Set per tenere traccia dei dataset e dei modelli già elaborati\n",
    "processed_datasets = set()\n",
    "processed_models = set()\n",
    "\n",
    "# Supponiamo che data_dict sia già stato creato con chiavi nel formato:\n",
    "# \"th_resp_vs_pt_resp_wavelet_familiar_th\" (che contiene anche exp_cond)\n",
    "# e con valori (X_data, y_data)\n",
    "# Ad esempio:\n",
    "# data_dict = {\"th_resp_vs_pt_resp_wavelet_familiar_th\": (X, y), ...}\n",
    "\n",
    "# Definisci il base_path in cui sono salvati i file .pkl\n",
    "base_path = \"/home/stefano/Interrogait/WB_time_domain_best_results/\"\n",
    "\n",
    "# Seleziona il dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loop sui dataset\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: {key}\")\n",
    "    \n",
    "    # Controlla se il dataset è già stato elaborato\n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    # (Qui va la suddivisione in train, validation, test)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Iterazione sui modelli\n",
    "    for model_name in [\"CNN1D\", \"BiLSTM\", \"Transformer\"]:\n",
    "        \n",
    "        model_key = f\"{key}_{model_name}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        # Imposta il seme per la riproducibilità\n",
    "        torch.manual_seed(32)\n",
    "        np.random.seed(32)\n",
    "        random.seed(32)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(32)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset {key} e il modello {model_name}...\")\n",
    "        \n",
    "        # --- Estrazione della configurazione ottimale dal file .pkl ---\n",
    "        try:\n",
    "            # La chiave key contiene già l'experimental condition; per sicurezza, la parsifichiamo:\n",
    "            exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "            config, best_weights = load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento della configurazione per {model_name} su {key}: {e}\")\n",
    "            continue  # Passa al prossimo modello se non troviamo il file\n",
    "        \n",
    "        # Estrai i parametri di interesse dal dizionario di configurazione\n",
    "        batch_size      = config.get(\"batch_size\", 48)\n",
    "        n_epochs        = config.get(\"n_epochs\", 40)\n",
    "        patience        = config.get(\"patience\", 10)\n",
    "        lr              = config.get(\"lr\", 1e-4)\n",
    "        weight_decay    = config.get(\"weight_decay\", 0)\n",
    "        standardization = config.get(\"standardization\", False)\n",
    "        \n",
    "        print(f\"Parametri estratti dal file pkl per {model_name}: batch_size={batch_size}, n_epochs={n_epochs}, patience={patience}, lr={lr}, weight_decay={weight_decay}, standardization={standardization}\")\n",
    "        \n",
    "        # --- Preparazione dei dataloaders usando i parametri estratti ---\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type=model_name, batch_size=batch_size, standardize=standardization\n",
    "        )\n",
    "        \n",
    "        # --- Inizializzazione del modello (OPZIONE 2) ---\n",
    "        if model_name == \"CNN1D\":\n",
    "            model = CNN1D(input_channels=3, num_classes=2)\n",
    "        elif model_name == \"BiLSTM\":\n",
    "            model = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        elif model_name == \"Transformer\":\n",
    "            model = ReadMYMind(num_channels=3, seq_length=75, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Carica lo stato dei pesi ottimali nel modello\n",
    "        try:\n",
    "            model.load_state_dict(best_weights)\n",
    "            print(f\"Modello {model_name} inizializzato con i pesi ottimali dal file .pkl.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Definizione del criterio di perdita con i class weights\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        \n",
    "        # Definizione dell'ottimizzatore usando lr e weight_decay dal file di config\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        print(f\"Avvio del training per il modello {model_name} sul dataset {key}...\")\n",
    "        my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs=n_epochs, patience=patience)\n",
    "        \n",
    "        print(f\"Avvio del testing per il modello {model_name} sul dataset {key}...\")\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        print(f\"Salvataggio dei risultati per il modello {model_name} sul dataset {key}...\")\n",
    "        save_performance_results(model_name, my_train_results, my_test_results, key, data_type, condition=exp_cond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61525179-01e7-4c2f-ae08-3f8a45002ee4",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### **IMPLEMENTAZIONE ADOTTATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90d5d5-e268-40dd-8404-baf9563e2489",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parsing della chiave e costruzione del path:\n",
    "Usando la funzione parse_combination_key si estraggono \n",
    "\n",
    "exp_cond, data_type e category_subject dalla chiave del dataset. \n",
    "\n",
    "Questi vengono usati per costruire il percorso in cui cercare i file .pkl.\n",
    "'''\n",
    "import re \n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"1_20|1_45|wavelet\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\", \n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "\n",
    "'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "# Test\n",
    "combination_key = \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "condition_experiment, data_type, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "print(\"Condizione:\", condition_experiment)\n",
    "print(\"Data Type:\", data_type)\n",
    "print(\"Soggetto:\", subject_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f688bfc-6685-4194-bb17-261143a91fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Verifica del file .pkl:\n",
    "La funzione load_config_if_available cerca, per ogni modello, il file con nome del tipo\n",
    "\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "all’interno della struttura di cartelle basata su base_path. \n",
    "\n",
    "Se il file esiste, allora viene passata poi a load_model_config_and_weights, \n",
    "che carica il dizionario di partenza \n",
    "e da questo estrae i 2 sotto-dizionari 'config' e 'state_dict'.\n",
    "'''\n",
    "\n",
    "def load_config_if_available(dataset_key, model_name, base_path):\n",
    "    \"\"\"\n",
    "    Data una chiave (es. \"th_resp_vs_pt_resp_wavelet_familiar_th\") e il nome del modello,\n",
    "    cerca il file .pkl corrispondente e ritorna (config, state_dict).\n",
    "    Se non esiste, restituisce (None, None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(dataset_key)\n",
    "        config, state_dict = load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path)\n",
    "        print(f\"✅ File .pkl trovato per \\033[1m{model_name}\\033[0m su \\033[1m{dataset_key}\\033[0m\")\n",
    "        \n",
    "        return config, state_dict\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Nessun file .pkl per {model_name} su {dataset_key} - uso parametri di default. ({e})\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21081daa-dd47-4c94-bc3e-f930082747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Caricamento del file .pkl:\n",
    "La funzione load_model_config_and_weights cerca, per ogni modello, il file con nome del tipo\n",
    "\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "all’interno della struttura di cartelle basata su base_path. Se il file esiste, vengono restituiti config e state_dict.\n",
    "'''\n",
    "\n",
    "# Funzione per caricare il file .pkl con la configurazione e i pesi ottimali\n",
    "def load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path):\n",
    "    \"\"\"\n",
    "    Costruisce il path usando:\n",
    "        base_path / exp_cond / data_type / category_subject\n",
    "    e il nome del file:\n",
    "        {model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\n",
    "    Se il file esiste, lo carica e restituisce (config, state_dict).\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = f\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "    file_path = os.path.join(base_path, exp_cond, data_type, category_subject, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"🕵️‍♂️🔍Caricamento file .pkl: \\033[1m{file_path}\\033[0m\")\n",
    "        \n",
    "        # Il file .pkl è stato salvato con torch.save() e contiene un dizionario con chiavi al suo interno che sono: \"config\" e \"state_dict\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = torch.load(f)\n",
    "        return data[\"config\"], data[\"state_dict\"]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file_path} non trovato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e10d11-089f-4a9d-9803-bc3916336ecc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Early Stopping - EEG Spectrograms - Time x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e277e69-530b-44d1-a891-c714ee9cbf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE EARLY STOPPING\n",
    "'''\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf06724-426d-432d-87f2-439992795db2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **TRAINING (NON DEVI ESEGUIRLA VAI DIRETTAMENTE AL TESTING!)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3614802-476c-4b37-965a-1e7de89b9ae9",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                            DESCRIZIONE\n",
    "\n",
    "La funzione pbar.set_description() che stai utilizzando all'interno del ciclo for epoch in pbar:\n",
    "gestisce già il printing delle metriche di loss e accuracy sia per il training che per la validazione.\n",
    "\n",
    "Il metodo set_description() aggiorna dinamicamente la barra di avanzamento di tqdm con il testo specificato,\n",
    "quindi non è strettamente necessario stampare MANUALMENTE i risultati con print() subito dopo. \n",
    "\n",
    "Tuttavia, se desideri avere un log persistente nella console, i print() possono ancora essere utili.\n",
    "\n",
    "\n",
    "Cosa succede nella tua implementazione:\n",
    "\n",
    "1) Barra di progresso (tqdm)\n",
    "\n",
    "La barra viene aggiornata a ogni epoca con le seguenti metriche:\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "\n",
    "    Questo aggiorna dinamicamente la barra di progresso, mostrando le informazioni in tempo reale.\n",
    "\n",
    "2) Output dettagliato (print)\n",
    "\n",
    "Dopo ogni epoca stampi i risultati in formato leggibile, con caratteri in grassetto per evidenziare l'epoca corrente:\n",
    "\n",
    "\n",
    "    print(f\"\\033[1mEpoch {epoch+1}/{n_epochs}\\033[0m \")\n",
    "    print(f\"Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}\")\n",
    "    print(f\"Train Accuracy: {accuracy_train:.4f}, Val Accuracy: {accuracy_val:.4f}\\n\")\n",
    "    \n",
    "    Questo serve a mantenere un log più dettagliato e leggibile.\n",
    "\n",
    "\n",
    "Cosa potresti fare:\n",
    "\n",
    "- Se preferisci evitare stampe duplicate e affidarti solo alla barra di avanzamento, \n",
    "puoi semplicemente rimuovere i print() finali e lasciare solo pbar.set_description().\n",
    "\n",
    "- Se invece vuoi mantenere i print(), potresti usarli solo per il salvataggio in un file di log, per esempio:\n",
    "\n",
    "\n",
    "    with open(\"training_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\\n\")\n",
    "    \n",
    "\n",
    "Considerazioni generali:\n",
    "\n",
    "Efficienza: La barra di progresso è utile per un'osservazione veloce senza affollare l'output della console.\n",
    "Tracciabilità: I print() aiutano per tenere traccia di eventuali problemi e risultati storici.\n",
    "Leggibilità: Se il tuo ambiente di esecuzione è un notebook Jupyter, la barra di progresso di tqdm è spesso sufficiente e più pulita.\n",
    "\n",
    "\n",
    "N.B. SCHEDULER\n",
    "\n",
    "    Scheduler per il Learning Rate:\n",
    "    \n",
    "    torch.optim.lr_scheduler.LRScheduler provides several methods to adjust the learning rate based on the number of epochs.\n",
    "    \n",
    "    Among the strategies of learning rate scheduling there is one:\n",
    "    \n",
    "    1) torch.optim.lr_scheduler.ReduceLROnPlateau which allows dynamic learning rate reducing based on some validation measurements.\n",
    "    \n",
    "    Nel caso tu stia usando Adam, che è già un ottimizzatore con un adattamento automatico \n",
    "    dei tassi di apprendimento per i vari parametri (grazie al termine \"momentum\"), \n",
    "    \n",
    "    l'uso di uno scheduler può comunque essere utile per adattare ulteriormente il learning rate durante l'allenamento,\n",
    "    migliorando le performance finali o il comportamento di convergenza.\n",
    "    \n",
    "    ReduceLROnPlateau non esclude Adam. In effetti, puoi usare Adam come ottimizzatore insieme a ReduceLROnPlateau \n",
    "    per modificare dinamicamente il learning rate durante l'allenamento\n",
    "    \n",
    "    ReduceLROnPlateau non dipende dal tipo di ottimizzatore che stai utilizzando;\n",
    "    piuttosto, si basa sulla metriche di valutazione (ad esempio la loss di validazione) \n",
    "    per decidere quando e quanto ridurre il learning rate (funziona con qualsiasi ottimizzatore, inclusi Adam, SGD, RMSprop, e altri).\n",
    "\n",
    "    L'idea di ReduceLROnPlateau è che se la metrica che monitori non sta migliorando \n",
    "    (o non migliora abbastanza), \n",
    "    allora il learning rate viene ridotto per tentare di fare progressi con un passo più piccolo. \n",
    "    Questo approccio è utile, specialmente quando il modello non sta migliorando \n",
    "    e si potrebbe beneficiare di un apprendimento più fine\n",
    "\n",
    "\n",
    "\n",
    "IO:\n",
    "\n",
    "\n",
    "Ok voglio ragionare meglio su 'factor', ossia, hai detto che se factor=0.1, \n",
    "il learning rate verrà ridotto del 90% ogni volta che il scheduler interviene.\n",
    "Quindi, se il learning rate iniziale è 0.001, dopo un intervento sarà 0.0001.\n",
    "\n",
    "ma se volessi invece fargli fare dei passi più piccoli, \n",
    "ossia della metà di quelli che mi hai descritto tu? dovrei imporre factor a 0.05..? quindi il passaggio sarebbe da 0.001 a quanto?\n",
    "\n",
    "La mia idea è che, quando nota che la val loss non scende, \n",
    "il learning rate viene sì abbassato magari, ma in maniera più graduale, \n",
    "perché l'idea è che solo in una certa fase la val loss si sia incastrata in minimo locale, è giusto come ragionamento?\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Per includere il plot dei risultati di training (come un'immagine) nel dizionario, \n",
    "possiamo utilizzare un buffer in memoria (invece di salvare direttamente il file su disco)\n",
    "per ottenere un'immagine in formato PNG. Questo ti consente di salvare il grafico come dati binari e\n",
    "successivamente accedere all'immagine in memoria per visualizzarla quando ne hai bisogno.\n",
    "\n",
    "Passi per implementarlo:\n",
    "Creare il plot e salvarlo in un buffer di memoria (utilizzando BytesIO di io).\n",
    "Salvare i dati dell'immagine nel dizionario.\n",
    "Quando ne hai bisogno, recuperare i dati dall'immagine dal dizionario e visualizzarla.\n",
    "\n",
    "1️⃣ Modifica della funzione plot_training_results per salvare il plot in memoria:\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "2️⃣ Salvare il plot nei risultati del training (dizionario results):\n",
    "Nel momento in cui vuoi salvare il plot dei risultati di training, puoi farlo come segue:\n",
    "    \n",
    "# Esegui il training\n",
    "plot_data = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "# Salviamo i dati binari dell'immagine nel dizionario\n",
    "results = {\n",
    "    'training_results_plot': plot_data,\n",
    "    # altri risultati del training\n",
    "}\n",
    "\n",
    "3️⃣ Recuperare e visualizzare il plot dal dizionario:\n",
    "Quando vuoi visualizzare l'immagine dal dizionario, puoi farlo recuperando \n",
    "i dati binari dell'immagine e usando PIL per convertirla in un'immagine visualizzabile con matplotlib\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Recuperiamo i dati dell'immagine dal dizionario\n",
    "plot_data = results['training_results_plot']\n",
    "\n",
    "# Convertire i dati binari in immagine PIL\n",
    "img = Image.open(io.BytesIO(plot_data))\n",
    "\n",
    "# Visualizzare l'immagine\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Nascondere gli assi\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "#def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2):\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "   \n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    \n",
    "    # https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "    \n",
    "    '''Inizializza il scheduler ReduceLROnPlateau'''\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "    \n",
    "    \n",
    "    # Dizionario per memorizzare le performance di training\n",
    "    training_performances = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_precision\": [],\n",
    "        \"train_recall\": [],\n",
    "        \"train_f1_score\": [],\n",
    "        \"train_auc\": []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "\n",
    "            #Calculate the Validation Loss\n",
    "\n",
    "            #remember: since we use CrossEntropyLoss (IN PYTORCH) we DO NOT need\n",
    "            #to do any ONE HOT ENCODING between y_pred and y_train (AS IN TENSORFLOW!) \n",
    "            \n",
    "            #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "            \n",
    "             # Qui inserisci i print di controllo\n",
    "            #print(\"y_pred shape:\", y_pred.shape)\n",
    "            #print(\"y shape:\", y.view(-1).shape)\n",
    "            #print(\"y dtype:\", y.dtype)\n",
    "        \n",
    "        \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            \n",
    "            #Perform Backpropagation\n",
    "\n",
    "            #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "            #Well, at every step the gradients will accumulate with every backprop,\n",
    "            #so to prevent 'compounding', we need to reset the stored gradient for EACH NEW EPOCH!\n",
    "\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "    \n",
    "        # Alla fine del ciclo di training (oppure!) quando si attiva l'early stopping, memorizza solo l'ultimo valore\n",
    "        if epoch == n_epochs - 1 or early_stopping.early_stop:\n",
    "            training_performances = {\n",
    "                \"train_loss\": [round(loss_train_history[-1], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[-1], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        '''\n",
    "        L'early stopping interrompe il training in due possibilità, \n",
    "        a seconda della metrica monitorata \n",
    "        \n",
    "        1) Se la loss di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "        Se vuoi monitorare la loss (val_loss), allora:\n",
    "\n",
    "        a) Devi impostare mode='min' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando la loss di validazione diminuisce.\n",
    "        c) Controlli l'early stopping usando val_loss.\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(val_loss)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - -         \n",
    "        Quindi, se imposto a 'min' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore loss \n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        2) Se l' accuracy di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "         Se vuoi monitorare l'accuracy (accuracy_val)\n",
    "        \n",
    "        a) Devi impostare mode='max' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando l'accuratezza aumenta\n",
    "        c) Controlli l'early stopping usando accuracy_val.\n",
    "    \n",
    "        if accuracy_val > max_val_acc:  # Oppure usa val_loss < min_val_loss per monitorare la loss\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(accuracy_val)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping att\n",
    "        \n",
    "        \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        Quindi, se imposto a 'max' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore accuracy\n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        Quale scegliere?\n",
    "\n",
    "        Se il tuo obiettivo è minimizzare l'errore, usa la loss (opzione 1).\n",
    "        Se vuoi massimizzare la performance del modello, usa l'accuracy (opzione 2).\n",
    "        Visto che vuoi salvare il modello in base alla best accuracy, la scelta corretta è opzione 2 con mode='max'. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima.\n",
    "        \n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "        \n",
    "        \n",
    "        # Controllo per l'early stopping basato sulla loss di validazione\n",
    "        early_stopping(accuracy_val)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            #print(\"Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "            \n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "        \n",
    "        #print(f\"\\033[1mEpoch {epoch+1}/{n_epochs}\\033[0m \")\n",
    "        #print(f\"Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}\")\n",
    "        #print(f\"Train Accuracy: {accuracy_train:.4f}, Val Accuracy: {accuracy_val:.4f}\\n\")\n",
    "        \n",
    "        '''Chiamata al scheduler per ridurre il learning rate'''\n",
    "        #scheduler.step(np.mean(loss_tmp_val))  # Aggiorna il learning rate basato sulla validation loss\n",
    "        \n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": training_performances,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b2d79-ec42-49c9-87de-40efb74e2539",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### **VERSIONE PRE- WEIGHT AND BIASES (W&B)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22691457-533a-4eb2-9737-1e943e0b0bc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''UFFICIALE - VERSIONE PRE- WEIGHT AND BIASES'''\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    #Questa sarebbe la migliore accuratezza ottenuta sul validation set\n",
    "    #in base alla quale viene preso il modello migliore!\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    best_epoch = 0  # Epoca con la migliore validazione\n",
    "    \n",
    "    best_metrics = {} # Dizionario con le metriche del migliore modello nel set di validazione\n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L'early stopping interrompe il training in due possibilità, \n",
    "        a seconda della metrica monitorata \n",
    "        \n",
    "        1) Se la loss di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "        Se vuoi monitorare la loss (val_loss), allora:\n",
    "\n",
    "        a) Devi impostare mode='min' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando la loss di validazione diminuisce.\n",
    "        c) Controlli l'early stopping usando val_loss.\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(val_loss)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - -         \n",
    "        Quindi, se imposto a 'min' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore loss \n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        2) Se l' accuracy di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "         Se vuoi monitorare l'accuracy (accuracy_val)\n",
    "        \n",
    "        a) Devi impostare mode='max' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando l'accuratezza aumenta\n",
    "        c) Controlli l'early stopping usando accuracy_val.\n",
    "    \n",
    "        if accuracy_val > max_val_acc:  # Oppure usa val_loss < min_val_loss per monitorare la loss\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(accuracy_val)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping att\n",
    "        \n",
    "        \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        Quindi, se imposto a 'max' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore accuracy\n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        Quale scegliere?\n",
    "\n",
    "        Se il tuo obiettivo è minimizzare l'errore, usa la loss (opzione 1).\n",
    "        Se vuoi massimizzare la performance del modello, usa l'accuracy (opzione 2).\n",
    "        Visto che vuoi salvare il modello in base alla best accuracy, la scelta corretta è opzione 2 con mode='max'. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima\n",
    "        \n",
    "        # Controllo della miglior validazione\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_metrics = {\n",
    "                \"train_loss\": [round(loss_train_history[best_epoch], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[best_epoch], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "            best_model = cp.deepcopy(model)  # Salvo il miglior modello\n",
    "\n",
    "        # Controllo Early Stopping\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch}, recupero il modello dell'epoca {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "        \n",
    "        #print(f\"\\033[1mEpoch {epoch+1}/{n_epochs}\\033[0m \")\n",
    "        #print(f\"Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}\")\n",
    "        #print(f\"Train Accuracy: {accuracy_train:.4f}, Val Accuracy: {accuracy_val:.4f}\\n\")\n",
    "        \n",
    "        '''Chiamata al scheduler per ridurre il learning rate'''\n",
    "        #scheduler.step(np.mean(loss_tmp_val))  # Aggiorna il learning rate basato sulla validation loss\n",
    "        \n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bee50-6a68-4438-9494-cb6f7aafaec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### **VERSIONE POST- WEIGHT AND BIASES (W&B)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb4e4b20-c810-465b-897f-6c98da62ad32",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''UFFICIALE - VERSIONE POST- WEIGHT AND BIASES CON COMMENTI'''\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    #Questa sarebbe la migliore accuratezza ottenuta sul validation set\n",
    "    #in base alla quale viene preso il modello migliore!\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    best_epoch = 0  # Epoca con la migliore validazione\n",
    "    \n",
    "    best_metrics = {} # Dizionario con le metriche del migliore modello nel set di validazione\n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L'early stopping interrompe il training in due possibilità, \n",
    "        a seconda della metrica monitorata \n",
    "        \n",
    "        1) Se la loss di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "        Se vuoi monitorare la loss (val_loss), allora:\n",
    "\n",
    "        a) Devi impostare mode='min' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando la loss di validazione diminuisce.\n",
    "        c) Controlli l'early stopping usando val_loss.\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(val_loss)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - -         \n",
    "        Quindi, se imposto a 'min' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore loss \n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        2) Se l' accuracy di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "         Se vuoi monitorare l'accuracy (accuracy_val)\n",
    "        \n",
    "        a) Devi impostare mode='max' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando l'accuratezza aumenta\n",
    "        c) Controlli l'early stopping usando accuracy_val.\n",
    "    \n",
    "        if accuracy_val > max_val_acc:  # Oppure usa val_loss < min_val_loss per monitorare la loss\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(accuracy_val)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping att\n",
    "        \n",
    "        \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        Quindi, se imposto a 'max' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore accuracy\n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        Quale scegliere?\n",
    "\n",
    "        Se il tuo obiettivo è minimizzare l'errore, usa la loss (opzione 1).\n",
    "        Se vuoi massimizzare la performance del modello, usa l'accuracy (opzione 2).\n",
    "        Visto che vuoi salvare il modello in base alla best accuracy, la scelta corretta è opzione 2 con mode='max'. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima\n",
    "        \n",
    "        # Controllo della miglior validazione\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_metrics = {\n",
    "                \"train_loss\": [round(loss_train_history[best_epoch], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[best_epoch], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "            best_model = cp.deepcopy(model)  # Salvo il miglior modello\n",
    "\n",
    "        # Controllo Early Stopping\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch}, recupero il modello dell'epoca {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "        \n",
    "\n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784f61d-1179-4a14-aa0f-2368e33efab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''UFFICIALE - VERSIONE POST- WEIGHT AND BIASES SENZA COMMENTI'''\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    #Questa sarebbe la migliore accuratezza ottenuta sul validation set\n",
    "    #in base alla quale viene preso il modello migliore!\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    best_epoch = 0  # Epoca con la migliore validazione\n",
    "    \n",
    "    best_metrics = {} # Dizionario con le metriche del migliore modello nel set di validazione\n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima\n",
    "        \n",
    "        # Controllo della miglior validazione\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_metrics = {\n",
    "                \"train_loss\": [round(loss_train_history[best_epoch], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[best_epoch], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "            best_model = cp.deepcopy(model)  # Salvo il miglior modello\n",
    "\n",
    "        # Controllo Early Stopping\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"⚠️ Early stopping attivato all'epoca \\033[1m{epoch}\\033[0m, recupero il modello dell'epoca \\033[1m{best_epoch}\\033[0m\")\n",
    "            break\n",
    "\n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "\n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f42fc-70f3-4909-bf3f-e6cc40d34c7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **TESTING**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ade29ad-aa33-42b8-8bec-2f6b096c7656",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "TESTING FUNCTION: CORRETTA ANCHE PER IL GRAD-CAM\n",
    "\n",
    "SUCCESSIVAMENTE, DENTRO AL FOR LOOP DEL TRAINING E TESTING, \n",
    "SI RICHIAMA LA FUNZIONE DIRETTAMENTE DI \n",
    "\n",
    "1) compute_gradcam_figure, LA QUALE AL SUO INTERNO PRESENTA GIÀ \n",
    "TUTTO QUELLO CHE SERVE PER CALCOLARE IL GRADCAM, DI MODO CHE VADA A \n",
    "\n",
    "Selezionare esempi rappresentativi per ciascuna classe.\n",
    "Calcolare le mappe GradCAM e gli overlay.\n",
    "Creare una figura con le heatmap e le sovrapposizioni, completa di titoli esplicativi.\n",
    "Restituire un'immagine (buffer) pronta per essere salvata\n",
    "\n",
    "SUCCESSIVAMENTE, QUINDI, IL PROCEDIMENTO DIVENTA COME SEGUE:\n",
    "\n",
    "1) Si esegue il TESTING, per ottenere le metriche e salvare i risultati (senza GradCAM)\n",
    "\n",
    "2) Nel loop principale di TRAINING & TESTING, se il modello è CNN2D, allora \n",
    "\n",
    " - richiama la funzione 'compute_gradcam_figure', la quale va a\n",
    "    - calcolare le mappe di attivazione e successivamente creo le immagini che gli ho chiesto\n",
    "    - passa l'immagine ottenuta da GradCAM alla funzione 'save_performance_results', la quale va a \n",
    "        - salvare i risultati di test ottenuti dalla funzione di 'testing'\n",
    "        - salvare l'immagine risultatante del GradCAM e la sovrapposizione del GradCAM sullo spettrogramma originale della classe risultante\n",
    "        \n",
    "        \n",
    "Questo approccio garantisce chiarezza e separa la parte di performance (testing) dalla parte di explainability (GradCAM).\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def testing(results, test_loader, criterion):\n",
    "    \n",
    "    # Recupera il miglior modello ottenuto durante la validazione\n",
    "    model = results['best_model']\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()  # Imposta il modello in modalità valutazione\n",
    "\n",
    "    y_true_list = []  # Lista per salvare le etichette reali\n",
    "    y_pred_list = []  # Lista per salvare le previsioni del modello\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    test_performances = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"test_precision\": [],\n",
    "        \"test_recall\": [],\n",
    "        \"test_f1_score\": [],\n",
    "        \"test_auc\": []\n",
    "    }\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "        \n",
    "        for inputs, labels in pbar:\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Ottenere le predizioni del modello\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calcolare la loss\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "            # Memorizzare predizioni ed etichette vere\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred_list.extend(predicted.cpu().numpy())\n",
    "            y_true_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Aggiornare il numero di predizioni corrette\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            pbar.set_description(f\"Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "    # Calcolare l'accuratezza complessiva\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    # Calcolare precision, recall, F1-score, AUC durante il testing\n",
    "    precision_test = precision_score(y_true_list, y_pred_list, average='weighted')\n",
    "    recall_test = recall_score(y_true_list, y_pred_list, average='weighted')\n",
    "    f1_test = f1_score(y_true_list, y_pred_list, average='weighted')\n",
    "    auc_test = roc_auc_score(y_true_list, y_pred_list, average='weighted')  # Assicurati che il problema sia binario o multi-class\n",
    "\n",
    "    # Aggiungere questi valori nel dizionario delle performance (arrotondando a 4 decimali)\n",
    "    test_performances[\"test_loss\"].append(round(total_loss / len(test_loader), 4))  # Media della loss\n",
    "    test_performances[\"test_accuracy\"].append(round(accuracy, 4))\n",
    "    test_performances[\"test_precision\"].append(round(precision_test, 4))\n",
    "    test_performances[\"test_recall\"].append(round(recall_test, 4))\n",
    "    test_performances[\"test_f1_score\"].append(round(f1_test, 4))\n",
    "    test_performances[\"test_auc\"].append(round(auc_test, 4))\n",
    "    \n",
    "    # Creare la confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "    \n",
    "    # Stampare classification report\n",
    "    class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "    # Visualizzare la confusion matrix\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    #sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    #plt.title(\"Confusion Matrix\")\n",
    "    #plt.xlabel(\"Predicted\")\n",
    "    #plt.ylabel(\"True\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    #buf = io.BytesIO()\n",
    "    #plt.savefig(buf, format='png')\n",
    "    #buf.seek(0)\n",
    "    #conf_matrix_image_data = buf.getvalue()\n",
    "    #buf.close()\n",
    "    \n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    buf = io.BytesIO()\n",
    "    #plt.figure(figsize=(8, 6))  # Nuova figura per evitare sovrapposizioni\n",
    "    #sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    #plt.title(\"Confusion Matrix\")\n",
    "    #plt.xlabel(\"Predicted\")\n",
    "    #plt.ylabel(\"True\")\n",
    "    #plt.savefig(buf, format='png')  # Salva l'immagine nel buffer\n",
    "    buf.seek(0)  # Torna all'inizio del buffer\n",
    "    conf_matrix_image_data = buf.getvalue()  # Ottieni l'immagine in formato binario\n",
    "    buf.close()  # Chiudi il buffer\n",
    "\n",
    "    # Mostra la confusion matrix (opzionale)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "    }\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "        \"optimizer\": str(optimizer),\n",
    "        \"loss_function\": str(criterion),\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Restituisci i risultati come dizionario\n",
    "    test_results = {\n",
    "        \"test_performances\": test_performances,  # Aggiungi il dizionario delle performance\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,  # Aggiunti i due nuovi dizionari\n",
    "        \"confusion_matrix_image\": conf_matrix_image_data,  # Aggiunta l'immagine della confusion matrix\n",
    "    }\n",
    "        \n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09109209-5a03-4fce-8dcc-bd1fd584fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TESTING FUNCTION: CORRETTA ANCHE PER IL GRAD-CAM\n",
    "\n",
    "SUCCESSIVAMENTE, DENTRO AL FOR LOOP DEL TRAINING E TESTING, \n",
    "SI RICHIAMA LA FUNZIONE DIRETTAMENTE DI \n",
    "\n",
    "1) compute_gradcam_figure, LA QUALE AL SUO INTERNO PRESENTA GIÀ \n",
    "TUTTO QUELLO CHE SERVE PER CALCOLARE IL GRADCAM, DI MODO CHE VADA A \n",
    "\n",
    "Selezionare esempi rappresentativi per ciascuna classe.\n",
    "Calcolare le mappe GradCAM e gli overlay.\n",
    "Creare una figura con le heatmap e le sovrapposizioni, completa di titoli esplicativi.\n",
    "Restituire un'immagine (buffer) pronta per essere salvata\n",
    "\n",
    "SUCCESSIVAMENTE, QUINDI, IL PROCEDIMENTO DIVENTA COME SEGUE:\n",
    "\n",
    "1) Si esegue il TESTING, per ottenere le metriche e salvare i risultati (senza GradCAM)\n",
    "\n",
    "2) Nel loop principale di TRAINING & TESTING, se il modello è CNN2D, allora \n",
    "\n",
    " - richiama la funzione 'compute_gradcam_figure', la quale va a\n",
    "    - calcolare le mappe di attivazione e successivamente creo le immagini che gli ho chiesto\n",
    "    - passa l'immagine ottenuta da GradCAM alla funzione 'save_performance_results', la quale va a \n",
    "        - salvare i risultati di test ottenuti dalla funzione di 'testing'\n",
    "        - salvare l'immagine risultatante del GradCAM e la sovrapposizione del GradCAM sullo spettrogramma originale della classe risultante\n",
    "        \n",
    "        \n",
    "Questo approccio garantisce chiarezza e separa la parte di performance (testing) dalla parte di explainability (GradCAM).\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def testing(results, test_loader, criterion):\n",
    "    \n",
    "    # Recupera il miglior modello ottenuto durante la validazione\n",
    "    model = results['best_model']\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()  # Imposta il modello in modalità valutazione\n",
    "\n",
    "    y_true_list = []  # Lista per salvare le etichette reali\n",
    "    y_pred_list = []  # Lista per salvare le previsioni del modello\n",
    "    \n",
    "    '''AGGIUNTA NUOVA PER CALCOLO AUC-ROC'''\n",
    "    y_score_list = []   # <— Lista per salvare gli score per le probabilità della classe positiva (per auc-roc!)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    test_performances = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"test_precision\": [],\n",
    "        \"test_recall\": [],\n",
    "        \"test_f1_score\": [],\n",
    "        \"test_auc\": []\n",
    "    }\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "        \n",
    "        for inputs, labels in pbar:\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Ottenere le predizioni del modello\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            '''AGGIUNTA NUOVA PER CALCOLO AUC-ROC'''\n",
    "            # aggiungi queste due righe\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            y_score_list.extend(probs[:,1].cpu().numpy())\n",
    "\n",
    "            # Calcolare la loss\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "            # Memorizzare predizioni ed etichette vere\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred_list.extend(predicted.cpu().numpy())\n",
    "            y_true_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Aggiornare il numero di predizioni corrette\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            pbar.set_description(f\"Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "    # Calcolare l'accuratezza complessiva\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    # Calcolare precision, recall, F1-score, AUC durante il testing\n",
    "    precision_test = precision_score(y_true_list, y_pred_list, average='weighted')\n",
    "    recall_test = recall_score(y_true_list, y_pred_list, average='weighted')\n",
    "    f1_test = f1_score(y_true_list, y_pred_list, average='weighted')\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #auc_test = roc_auc_score(y_true_list, y_pred_list, average='weighted')  # Assicurati che il problema sia binario o multi-class\n",
    "    \n",
    "    '''AGGIUNTA NUOVA PER CALCOLO AUC-ROC\n",
    "    \n",
    "    In questo modo l’roc_auc_score calcola l’area sotto tutta la curva ROC (tutte le soglie), invece di valutare un solo punto corrispondente alla soglia 0.5\n",
    "    '''\n",
    "    \n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "    auc_test = roc_auc_score(y_true_list, y_score_list)\n",
    "\n",
    "\n",
    "    # Aggiungere questi valori nel dizionario delle performance (arrotondando a 4 decimali)\n",
    "    test_performances[\"test_loss\"].append(round(total_loss / len(test_loader), 4))  # Media della loss\n",
    "    test_performances[\"test_accuracy\"].append(round(accuracy, 4))\n",
    "    test_performances[\"test_precision\"].append(round(precision_test, 4))\n",
    "    test_performances[\"test_recall\"].append(round(recall_test, 4))\n",
    "    test_performances[\"test_f1_score\"].append(round(f1_test, 4))\n",
    "    test_performances[\"test_auc\"].append(round(auc_test, 4))\n",
    "    \n",
    "    # Creare la confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "    \n",
    "    # Stampare classification report\n",
    "    class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "    # Visualizzare la confusion matrix\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    #sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    #plt.title(\"Confusion Matrix\")\n",
    "    #plt.xlabel(\"Predicted\")\n",
    "    #plt.ylabel(\"True\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    #buf = io.BytesIO()\n",
    "    #plt.savefig(buf, format='png')\n",
    "    #buf.seek(0)\n",
    "    #conf_matrix_image_data = buf.getvalue()\n",
    "    #buf.close()\n",
    "    \n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.figure(figsize=(8, 6))  # Nuova figura per evitare sovrapposizioni\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(buf, format='png')  # Salva l'immagine nel buffer\n",
    "    buf.seek(0)  # Torna all'inizio del buffer\n",
    "    conf_matrix_image_data = buf.getvalue()  # Ottieni l'immagine in formato binario\n",
    "    buf.close()  # Chiudi il buffer\n",
    "\n",
    "    # Mostra la confusion matrix (opzionale)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    '''COMMENTATO'''\n",
    "    #model_config = {\n",
    "        #\"model_architecture\": str(model),\n",
    "        #\"batch_size_test\": test_loader.batch_size,\n",
    "    #}\n",
    "    \n",
    "    '''COMMENTATO'''\n",
    "    # Dizionario degli iper-parametri\n",
    "    #hyperparams = {\n",
    "        #\"optimizer\": str(optimizer),\n",
    "        #\"loss_function\": str(criterion),\n",
    "        #\"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    #}\n",
    "\n",
    "    \n",
    "    '''COMMENTATO'''\n",
    "    # Restituisci i risultati come dizionario\n",
    "    #test_results = {\n",
    "        #\"test_performances\": test_performances,  # Aggiungi il dizionario delle performance\n",
    "        #\"confusion_matrix\": conf_matrix,\n",
    "        #\"classification_report\": class_report,\n",
    "        #\"model_configuration\": model_config,\n",
    "        #\"hyperparameters\": hyperparams,  # Aggiunti i due nuovi dizionari\n",
    "        #\"confusion_matrix_image\": conf_matrix_image_data,  # Aggiunta l'immagine della confusion matrix\n",
    "    #}\n",
    "    \n",
    "    \n",
    "    # Restituisci i risultati come dizionario\n",
    "    test_results = {\n",
    "        \"test_performances\": test_performances,  # Aggiungi il dizionario delle performance\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"confusion_matrix_image\": conf_matrix_image_data,  # Aggiunta l'immagine della confusion matrix\n",
    "    }\n",
    "    \n",
    "        \n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615fefd-19dd-4e5c-b380-5992c03f90de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **CREAZIONE CLASSE GRADCAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa8733-f814-4fb1-8474-0a02439a944b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### **CREAZIONE CLASSE GRADCAM**\n",
    "\n",
    "'''\n",
    "Creazione della classe GradCAM\n",
    "\n",
    "-----1. Costruttore (init)-----\n",
    "\n",
    "Cosa fa:\n",
    "\n",
    "Salva il modello e il layer target (ad esempio, l'ultimo strato convoluzionale) su cui calcolare le mappe di attivazione.\n",
    "\n",
    "A) Inizializza due variabili, \n",
    "\n",
    "1) self.activations e 2) self.gradients, che verranno usate per memorizzare rispettivamente \n",
    "1) le attivazioni (feature maps) e 2) i gradienti di quel layer\n",
    "\n",
    "B) Registra due hook sul target_layer:\n",
    "\n",
    "1) Forward Hook: Quando il modello effettua la forward pass, viene eseguito save_activation per salvare le attivazioni\n",
    "2) Backward Hook: Durante la backward pass, save_gradient viene chiamato per salvare i gradienti\n",
    "\n",
    "\n",
    "-----2. Hook per Salvare Attivazioni e Gradienti-----\n",
    "\n",
    "B) Save Activation\n",
    "\n",
    "def save_activation(self, module, input, output):\n",
    "    self.activations = output.detach()\n",
    "\n",
    "Cosa fa:\n",
    "\n",
    "Quando viene eseguita la forward pass sul target_layer, questo hook cattura l'output (le attivazioni) del layer.\n",
    "Usa detach() per ottenere una copia dei dati senza il tracking dei gradienti, in modo da non interferire con la retropropagazione.\n",
    "\n",
    "C) Save Gradient\n",
    "\n",
    "def save_gradient(self, module, grad_input, grad_output):\n",
    "    self.gradients = grad_output[0].detach()\n",
    "\n",
    "\n",
    "Cosa fa:\n",
    "\n",
    "Durante la backward pass, questo hook cattura i gradienti che fluiscono attraverso il target_layer.\n",
    "grad_output è una tupla; solitamente il primo elemento contiene i gradienti utili. \n",
    "\n",
    "Anche qui si usa detach() per isolare i dati dai grafi di calcolo.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Registra hook per catturare attivazioni e gradienti\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97dfb1-13a9-43ac-aef5-02eb5e3c73bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **DRAFT IMPLEMENTATIONS OF GRADCAM COMPUTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427a270-5e36-44b1-a96d-c4ebd59f1c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### INITIAL IMPLEMENTATIONS OF GRADCAM\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Registra hook per catturare attivazioni e gradienti\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        \n",
    "1) Funzione generate_cam (interna alla classe GradCAM)\n",
    "\n",
    "    La differenza chiave tra i due approcci è proprio la selezione dell'input su cui viene calcolata la Grad-CAM. Ti riassumo le due opzioni:\n",
    "\n",
    "    1️⃣ Approccio attuale (generate_cam)\n",
    "    Viene passato un singolo input_tensor, e il Grad-CAM viene calcolato su di esso.\n",
    "    Se target_class non è specificata, viene selezionata la classe predetta dal modello per quell'input.\n",
    "    Il calcolo del Grad-CAM si basa su una backward pass del gradiente rispetto alla classe target.\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        # Effettua la forward pass\n",
    "        output = self.model(input_tensor)\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        # Azzeramento dei gradienti\n",
    "        self.model.zero_grad()\n",
    "        # Calcola il gradiente per la classe target\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "\n",
    "        # Calcola i pesi come media dei gradienti su width e height\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        # Somma pesata delle attivazioni\n",
    "        cam = torch.sum(weights * self.activations, dim=1)\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalizza la mappa\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Upsample alla dimensione dell'immagine di input\n",
    "        cam = F.interpolate(cam.unsqueeze(1), size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        return cam\n",
    "    \n",
    "    \n",
    "2) Funzione compute_gradcam_figure (esterna alla classe GradCAM)   \n",
    "    \n",
    "2️⃣ Alternativa proposta (compute_gradcam_figure)\n",
    "Seleziona esplicitamente un esempio per ciascuna classe (0 e 1) iterando sul test_loader.\n",
    "Questo garantisce che il Grad-CAM sia calcolato su esempi rappresentativi di entrambe le classi.\n",
    "La visualizzazione finale confronta le heatmap delle due classi, sovrapponendole agli spettrogrammi.\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "    I titoli della figura vengono personalizzati con exp_cond, data_type, category_subject.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assumiamo che il modello sia CNN2D e che il layer target sia model.conv3\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "    # Dizionari per salvare il campione per ogni classe\n",
    "    samples = {}      # Salveremo il sample input per ogni classe\n",
    "    labels_found = {} # Per tenere traccia delle etichette già trovate\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int not in labels_found:\n",
    "                samples[label_int] = inputs[i].unsqueeze(0)  # salva come tensore 4D\n",
    "                labels_found[label_int] = True\n",
    "            if 0 in labels_found and 1 in labels_found:\n",
    "                break\n",
    "        if 0 in labels_found and 1 in labels_found:\n",
    "            break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    if 0 not in samples or 1 not in samples:\n",
    "        print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "        return None\n",
    "\n",
    "    # Per ciascun campione, calcola GradCAM\n",
    "    cams = {}\n",
    "    overlays = {}\n",
    "    for cls in [0, 1]:\n",
    "        sample_input = samples[cls]\n",
    "        sample_input.requires_grad = True  # Abilita gradiente per il campione\n",
    "        cam = gradcam.generate_cam(sample_input)\n",
    "        cams[cls] = cam\n",
    "\n",
    "        # Converti il sample in immagine numpy per la visualizzazione\n",
    "        img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        # Normalizza l'immagine in scala 0-255\n",
    "        img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "        # Applica la heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        # Sovrapponi la heatmap all'immagine originale\n",
    "        overlay = cv2.addWeighted(img_norm, 0.6, heatmap, 0.4, 0)\n",
    "        overlays[cls] = overlay\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Titolo per la prima riga\n",
    "    title_row1 = f\"Grad-CAM mapping of experimental condition {exp_cond}, EEG {data_type}, Subject {category_subject}\"\n",
    "    # Titolo per la seconda riga\n",
    "    title_row2 = f\"Grad-CAM mapping superimposition over EEG Spectrogram of experimental condition {exp_cond}, Subject {category_subject}\"\n",
    "    \n",
    "    # Prima riga: solo le heatmap\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        axs[0, j].imshow(cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB))\n",
    "        axs[0, j].set_title(f\"Class {cls} Heatmap\")\n",
    "        axs[0, j].axis('off')\n",
    "    axs[0, 0].set_ylabel(title_row1, fontsize=10)\n",
    "    \n",
    "    # Seconda riga: overlay della heatmap sullo spettrogramma originale\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        axs[1, j].imshow(overlays[cls])\n",
    "        axs[1, j].set_title(f\"Class {cls} Overlay\")\n",
    "        axs[1, j].axis('off')\n",
    "    axs[1, 0].set_ylabel(title_row2, fontsize=10)\n",
    "    \n",
    "    # Ottimizza la disposizione della figura\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaabfeb-2702-4a6e-98cf-966cfc7fa52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''SOLUZIONE ? INTEGRARE LA 2) IN 1)\n",
    "\n",
    "\n",
    "🛠️ Cosa conviene fare?\n",
    "Se il tuo obiettivo è sempre confrontare le attivazioni per entrambe le classi, \n",
    "allora conviene integrare compute_gradcam_figure dentro la classe GradCAM e rimuovere generate_cam come metodo separato.\n",
    "\n",
    "📌 Quindi suggerirei di fare così:\n",
    "\n",
    "Rendere compute_gradcam_figure un metodo della classe GradCAM.\n",
    "Rimuovere generate_cam, perché il calcolo della CAM viene già eseguito all'interno del loop che seleziona i campioni.\n",
    "Mantenere la logica che seleziona i campioni da entrambe le classi, perché è più robusta rispetto a calcolare la CAM su un singolo input arbitrario.\n",
    "⚠️ Attenzione a una cosa però!\n",
    "Il metodo generate_cam fa un passaggio importante che non è presente in compute_gradcam_figure:\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "cam = torch.sum(weights * self.activations, dim=1)\n",
    "cam = F.relu(cam)\n",
    "🔹 Bisogna assicurarsi che questa logica venga mantenuta nel nuovo metodo!\n",
    "Attualmente compute_gradcam_figure chiama gradcam.generate_cam(sample_input), quindi se generate_cam viene eliminato, questa parte va spostata nel nuovo metodo.\n",
    "\n",
    "📌 In sintesi, cosa farei\n",
    "✅ Modificare la classe GradCAM e aggiungere direttamente compute_gradcam_figure.\n",
    "✅ Eliminare generate_cam, ma mantenere la sua logica di calcolo della CAM.\n",
    "✅ Garantire che il calcolo dei pesi e della CAM sia integrato nel nuovo metodo.\n",
    "✅ Mantenere la selezione di un campione per ciascuna classe, per una migliore interpretabilità.\n",
    "\n",
    "\n",
    "\n",
    "Ha senso integrare compute_gradcam_figure direttamente come metodo della classe GradCAM ed eliminare generate_cam, perché:\n",
    "\n",
    "Selezione più rappresentativa dei campioni\n",
    "\n",
    "Il metodo compute_gradcam_figure assicura che vengano selezionati esempi di entrambe le classi (0 e 1), cosa che generate_cam non fa.\n",
    "Questo approccio fornisce una migliore interpretabilità della Grad-CAM confrontando diverse classi.\n",
    "Chiarezza e modularità\n",
    "\n",
    "generate_cam è attualmente chiamato da compute_gradcam_figure, ma possiamo integrare direttamente la logica dentro GradCAM.\n",
    "Questo evita la duplicazione del codice e rende più chiaro il flusso.\n",
    "Ottimizzazione del calcolo\n",
    "\n",
    "La pipeline di compute_gradcam_figure gestisce direttamente la forward pass e il calcolo del gradiente per entrambi i campioni in un'unica operazione, evitando di dover chiamare generate_cam separatamente.\n",
    "Prossimi passi:\n",
    "Spostiamo compute_gradcam_figure dentro GradCAM come metodo della classe.\n",
    "Eliminiamo generate_cam e integriamo direttamente la logica di forward pass e backward pass dentro compute_gradcam_figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8ce7f-3636-45cd-a1fd-543b6b159d70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **FINAL IMPLEMENTATION OF GRADCAM COMPUTATION**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8683f6b5-abf0-4c04-b112-1e12710ef333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "PRE- FINAL VERSION WITH STILL SOME EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int not in labels_found:\n",
    "                samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                labels_found[label_int] = True\n",
    "            if 0 in labels_found and 1 in labels_found:\n",
    "                break\n",
    "        if 0 in labels_found and 1 in labels_found:\n",
    "            break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    if 0 not in samples or 1 not in samples:\n",
    "        print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "        return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        sample_input = samples[cls]\n",
    "        sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "        # Esegui forward pass per ottenere l'output del modello\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        # Se non viene specificata una classe target, seleziona quella predetta\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        # Azzeramento dei gradienti e backward pass per la classe target\n",
    "        # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "        model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        \n",
    "        #Qui si calcola la mappa CAM:\n",
    "\n",
    "        #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "        #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "        #Si applica ReLU per eliminare i valori negativi.\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "        \n",
    "        #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "        #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "        weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "        cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "        \n",
    "        # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        #Passaggio 5: Normalizzazione e upsampling\n",
    "        \n",
    "        #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "        #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "        # -------------------------------\n",
    "        # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Normalizza la mappa\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Upsample alla dimensione dell'immagine di input\n",
    "        cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cams[cls] = cam\n",
    "        \n",
    "        \n",
    "        #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "        \n",
    "        #L'immagine originale viene convertita in un array numpy.\n",
    "        #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "        #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "        #🔹 Esempio pratico:\n",
    "        #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 6: Creazione dell'Overlay\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "        # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "        \n",
    "        # Prepara l'immagine originale per la visualizzazione\n",
    "        img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Normalizza l'immagine in scala 0-255\n",
    "        img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "        \n",
    "        # Applica la heatmap usando OpenCV\n",
    "        #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "        # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "        \n",
    "        '''\n",
    "        OLD VERSIONS\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        '''\n",
    "        \n",
    "        '''COMMENTATO'''\n",
    "        #heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "        \n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Sovrapponi la heatmap all'immagine originale\n",
    "        # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "        \n",
    "        '''\n",
    "        OLD VERSIONS\n",
    "        overlay = cv2.addWeighted(img_norm, 0.6, heatmap, 0.4, 0)\n",
    "        overlays[cls] = overlay\n",
    "        '''\n",
    "        \n",
    "        overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "        overlays[cls] = overlay\n",
    "    \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    '''\n",
    "    OLD VERSION\n",
    "    # Titoli personalizzati per le righe\n",
    "    title_row1 = f\"Grad-CAM mapping of experimental condition {exp_cond}, EEG {data_type}, Subject {category_subject}\"\n",
    "    title_row2 = f\"Grad-CAM mapping superimposition over EEG Spectrogram of experimental condition {exp_cond}, Subject {category_subject}\"\n",
    "    \n",
    "    # Prima riga: visualizza solo le heatmap\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        # Visualizza la heatmap applicata sul colore (questo step può essere ripetuto, oppure si visualizza la mappa grezza)\\n\n",
    "        axs[0, j].imshow(cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB))\n",
    "        axs[0, j].set_title(f\"Class {cls} Heatmap\")\n",
    "        axs[0, j].axis('off')\n",
    "    axs[0, 0].set_ylabel(title_row1, fontsize=10)\n",
    "    \n",
    "    # Seconda riga: visualizza l'overlay della heatmap sullo spettrogramma originale\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        axs[1, j].imshow(overlays[cls])\n",
    "        axs[1, j].set_title(f\"Class {cls} Overlay\")\n",
    "        axs[1, j].axis('off')\n",
    "    axs[1, 0].set_ylabel(title_row2, fontsize=10)\n",
    "    \n",
    "    # Ottimizza la disposizione della figura\n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    plt.suptitle(f\"Grad-CAM Mapping for {exp_cond} - EEG {data_type} - Subject {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 25], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"{condition_names[cls]} Heatmap\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 25], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"{condition_names[cls]} Overlay\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be68b7-c5d7-45b4-86fa-ce68bdead9b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "FINAL VERSION WITH ULTIMATED EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "\n",
    "\n",
    "Ciao ho bisogno che in questo calcolo del gradcam, tu qui nella funzione di calcolo del gradcam...\n",
    "tu mi presenti poi le immagini (sia gradcam che per l'overlay) non solo un esempio, ma il valore medio di attivazione per la relativa condizione sperimentale...\n",
    "ossia, anziché prendere un solo esempio e presentare il gradcam e l'overlay del solo esempio della classe 0, dovresti mostrarmi il valore medio, \n",
    "e allo stesso modo della classe 1.\n",
    "\n",
    "quindi sia nel gradcam che nell'overlay dovrei avere una rappresentazione media sia delle regioni delle feature maps per la relativa condizione sperimentale,\n",
    "sia dell'overlay risultante....\n",
    "\n",
    "Ti è chiaro che cosa voglio?\n",
    "\n",
    "\n",
    "\n",
    "Esatto, ho capito. Invece di estrarre un singolo campione per ciascuna classe, la funzione dovrebbe:\n",
    "Accumulare le mappe GradCAM e gli overlay per tutti i campioni appartenenti a ciascuna classe\n",
    "Calcolare la media delle mappe GradCAM ottenute per ciascuna condizione sperimentale.\n",
    "Calcolare la media degli overlay corrispondenti, in modo da ottenere una rappresentazione aggregata della risposta del modello.\n",
    "Quindi, anziché mostrare l'attivazione per un solo esempio, la figura finale presenterà:\n",
    "Una heatmap media della GradCAM per la classe 0 e per la classe 1.\n",
    "Un overlay medio (CAM + spettrogramma) per la classe 0 e per la classe 1.\n",
    "\n",
    "\n",
    "\n",
    "Puoi implementare la media delle mappe GradCAM e degli overlay per ogni classe modificando la funzione nel modo seguente. \n",
    "In sostanza, invece di fermarti al primo campione trovato per ciascuna classe, raccogli tutti (o un sottoinsieme rappresentativo) e poi calcola la media.\n",
    "Ecco i passaggi principali con qualche suggerimento su dove inserire il nuovo codice:\n",
    "\n",
    "\n",
    "1. Raccogliere le mappe e gli overlay per ogni classe\n",
    "Dove modificare:\n",
    "\n",
    "Dopo il passaggio 3 (calcolo della GradCAM per ciascun campione), invece di elaborare un solo esempio per ciascuna classe, \n",
    "crea due liste (una per la classe 0 e una per la classe 1) in cui accumulare le mappe CAM e gli overlay per ogni campione processato.\n",
    "\n",
    "Cosa fare:\n",
    "\n",
    "Invece di usare un dizionario samples che salva solo il primo esempio trovato, modifica il ciclo sul test_loader per salvare tutti gli esempi \n",
    "(o un numero sufficiente di campioni) per ciascuna classe.\n",
    "\n",
    "Per ogni campione, calcola la GradCAM e l'overlay e aggiungili alle rispettive liste, per esempio:\n",
    "\n",
    "    cams_list[cls].append(cam)\n",
    "    overlays_list[cls].append(overlay)\n",
    "\n",
    "\n",
    "2. Calcolare la media delle mappe\n",
    "Dove modificare:\n",
    "\n",
    "Dopo aver processato tutti i campioni (o quelli desiderati) per ciascuna classe, \n",
    "aggiungi un nuovo passaggio per calcolare la media lungo l'asse delle mappe accumulate.\n",
    "\n",
    "Cosa fare:\n",
    "\n",
    "Per ogni classe, converti la lista in un array NumPy e calcola la media. Ad esempio:\n",
    "\n",
    "\n",
    "    mean_cam = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "    mean_overlay = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "    \n",
    "Salva questi valori medi in due nuovi dizionari (ad es. mean_cams e mean_overlays).\n",
    "\n",
    "\n",
    "3. Utilizzare le mappe medie per la visualizzazione\n",
    "Dove modificare:\n",
    "\n",
    "Nel passaggio 7, durante la creazione della figura finale, utilizza mean_cams e mean_overlays anziché le mappe di un singolo esempio.\n",
    "\n",
    "Cosa fare:\n",
    "\n",
    "Nel ciclo che disegna le immagini, usa ad esempio:\n",
    "\n",
    "    cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "\n",
    "E per l'overlay:\n",
    "\n",
    "    overlay_img = np.flipud(mean_overlays[cls])\n",
    "\n",
    "In questo modo la figura finale rappresenterà la media delle attivazioni per ogni condizione sperimentale.\n",
    "\n",
    "Considerazioni finali\n",
    "Selezione del campione: Se il dataset è molto grande, potresti voler limitare il numero di campioni per evitare calcoli troppo onerosi.\n",
    "Ad esempio, puoi considerare i primi N campioni per ogni classe oppure usare un campionamento casuale.\n",
    "\n",
    "Prestazioni: Calcolare la media su molti campioni potrebbe essere intensivo, quindi valuta se sia il caso di eseguire questo calcolo\n",
    "su un sottoinsieme rappresentativo.\n",
    "\n",
    "Visualizzazione: Assicurati che la normalizzazione sia coerente per la media.\n",
    "                A volte, potrebbe essere utile normalizzare ogni mappa individualmente prima di calcolare la media \n",
    "                oppure normalizzare l'immagine media risultante.\n",
    "\n",
    "Questi passaggi ti permetteranno di ottenere una rappresentazione aggregata (media) sia della GradCAM che dell'overlay, \n",
    "fornendo una visione più robusta della risposta del modello per ogni condizione sperimentale.\n",
    "\n",
    "---- ---- ---- ---- ---- ----  ---- ---- ----  ---- ---- ----  ---- ---- ---- ---- ---- ----  ---- ---- ---- ---- ---- ----  ---- ---- ---- ---- ---- ----  ---- ---- ---- \n",
    "Riepilogo delle parti modificate\n",
    "Selezione dei campioni:\n",
    "\n",
    "Sostituisci la logica che si ferma al primo campione per ogni classe con la raccolta di tutti i campioni (o un campione rappresentativo) \n",
    "per ciascuna classe, salvandoli in un dizionario di liste.\n",
    "\n",
    "Calcolo della Grad-CAM e Overlay:\n",
    "\n",
    "Per ogni campione in ciascuna classe, calcola la mappa CAM e l'overlay e aggiungili a liste (cams_list e overlays_list).\n",
    "\n",
    "Calcolo della media:\n",
    "\n",
    "Dopo il ciclo, calcola la media per ogni classe usando np.mean.\n",
    "\n",
    "Visualizzazione:\n",
    "\n",
    "Nel passaggio di creazione della figura, utilizza le mappe medie (mean_cams e mean_overlays) al posto dei singoli campioni.\n",
    "\n",
    "Queste modifiche ti permetteranno di ottenere, per ciascuna classe (0 e 1), una rappresentazione media sia della mappa Grad-CAM che dell'overlay, come richiesto.\n",
    "---- ---- ---- ---- ---- ----  ---- ---- ----  ---- ---- ----  ---- ---- ---- ---- ---- ----  ---- ---- ---- ---- ---- ----  ---- ---- ---- ---- ---- ----  ---- ---- ---- \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    #samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    #labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            \n",
    "            # Aggiungi il campione alla lista della classe corrispondente\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #if label_int not in labels_found:\n",
    "            #    samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                \n",
    "                \n",
    "                #labels_found[label_int] = True\n",
    "            #if 0 in labels_found and 1 in labels_found:\n",
    "            #    break\n",
    "        #if 0 in labels_found and 1 in labels_found:\n",
    "        #    break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    #if 0 not in samples or 1 not in samples:\n",
    "    #    print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "    #    return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    #cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    #overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        for sample_input in samples[cls]:\n",
    "        \n",
    "        #sample_input = samples[cls]\n",
    "        \n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "\n",
    "            #Passaggio 5: Normalizzazione e upsampling\n",
    "\n",
    "            #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "            #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "            # ---------------------------\n",
    "\n",
    "            # Normalizza la mappa\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "            # Upsample alla dimensione dell'immagine di input\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "            cam = cam.squeeze().cpu().numpy()\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #cams[cls] = cam\n",
    "\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa alla lista per la classe\n",
    "            cams_list[cls].append(cam)\n",
    "\n",
    "\n",
    "            #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "\n",
    "            #L'immagine originale viene convertita in un array numpy.\n",
    "            #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "            #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 6: Creazione dell'Overlay\n",
    "            # -------------------------------\n",
    "\n",
    "            # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "            # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "\n",
    "            # Prepara l'immagine originale per la visualizzazione\n",
    "            img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            # Normalizza l'immagine in scala 0-255\n",
    "            img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "            # Applica la heatmap usando OpenCV\n",
    "            #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "            # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "\n",
    "            '''\n",
    "            Il processo è lo stesso di quello descritto per le cam:\n",
    "\n",
    "            I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "            Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "            La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "            '''\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Sovrapponi la heatmap all'immagine originale\n",
    "            # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "\n",
    "\n",
    "            '''\n",
    "            Overlay troppo sfocato e colori discordanti\n",
    "            Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "            Differenza di colormap e blending:\n",
    "            L'overlay viene creato con una combinazione di due immagini: \n",
    "                1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "                2) la heatmap\n",
    "\n",
    "            Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "\n",
    "            Suggerimenti:\n",
    "\n",
    "            a) Modifica i pesi in cv2.addWeighted:\n",
    "            Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "\n",
    "            b) Uniforma il formato dell'immagine originale:\n",
    "            Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "            considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "\n",
    "            c) Usa lo stesso colormap: \n",
    "            Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "            usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "\n",
    "            '''\n",
    "            overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "            #overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #overlays[cls] = overlay\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi l'overlay alla lista per la classe\n",
    "            overlays_list[cls].append(overlay)\n",
    "    \n",
    "    \n",
    "    mean_cams = {}\n",
    "    mean_overlays = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "        mean_overlays[cls] = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "        \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping - Experimental Condition: {exp_cond} - Subject: {category_subject}\", fontsize=12)\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG trial Spectrogram\\nExperimental Condition: {exp_cond} - Subject: {category_subject}\",\n",
    "    #fontsize=10,\n",
    "    #y=0.95  # Puoi regolare la posizione verticale se necessario\n",
    "    #)\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''SOLO UN ESEMPIO'''\n",
    "        #cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Mean Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        '''SOLO UN ESEMPIO'''\n",
    "        #overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        overlay_img = np.flipud(mean_overlays[cls])\n",
    "        \n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e94b4-a3cc-40ae-8664-f2fc87d75e33",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **FINAL IMPLEMENTATION OF GRADCAM COMPUTATION PER EEG STATS**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b512b372-7a7c-4c89-a4b9-d28f488fd106",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "ORIGINAL VERSION\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            \n",
    "            # Aggiungi il campione alla lista della classe corrispondente\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "\n",
    "    \n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        for sample_input in samples[cls]:\n",
    "        \n",
    "        #sample_input = samples[cls]\n",
    "        \n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "\n",
    "            #Passaggio 5: Normalizzazione e upsampling\n",
    "\n",
    "            #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "            #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "            # ---------------------------\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            Sì, la normalizzazione viene eseguita nel Passaggio 5: Normalizzazione e upsampling della CAM, con questa parte di codice:\n",
    "\n",
    "                # Normalizza la mappa\n",
    "                cam = cam - cam.min()\n",
    "                cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            Cosa fa questa normalizzazione?\n",
    "            \n",
    "            1) Sottrazione del minimo:\n",
    "\n",
    "                Porta i valori minimi della CAM a zero.\n",
    "                Se cam.min() fosse -0.3, sottraendo cam.min() tutti i valori si traslano in modo che il minimo sia 0.\n",
    "\n",
    "            2) Divisione per il massimo:\n",
    "\n",
    "                Scala i valori in modo che il massimo diventi 1.\n",
    "                Aggiungere 1e-8 nel denominatore previene problemi di divisione per zero.\n",
    "\n",
    "            💡 Risultato → Dopo questa operazione, cam avrà valori normalizzati tra 0 e 1.\n",
    "            '''\n",
    "            \n",
    "            # Normalizza la mappa\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            print(f\"Valori minimi e massimi dopo la normalizzazione: min={cam.min()}, max={cam.max()}\")\n",
    "            assert 0 <= cam.min() and cam.max() <= 1, \"Errore: i valori della CAM non sono nel range [0,1]!\"\n",
    "\n",
    "            # Upsample alla dimensione dell'immagine di input\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "            cam = cam.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa alla lista per la classe\n",
    "            cams_list[cls].append(cam)\n",
    "\n",
    "\n",
    "            #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "\n",
    "            #L'immagine originale viene convertita in un array numpy.\n",
    "            #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "            #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 6: Creazione dell'Overlay\n",
    "            # -------------------------------\n",
    "\n",
    "            # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "            # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "\n",
    "            # Prepara l'immagine originale per la visualizzazione\n",
    "            img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            # Normalizza l'immagine in scala 0-255\n",
    "            img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "            # Applica la heatmap usando OpenCV\n",
    "            #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "            # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "\n",
    "            '''\n",
    "            Il processo è lo stesso di quello descritto per le cam:\n",
    "\n",
    "            I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "            Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "            La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "            '''\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Sovrapponi la heatmap all'immagine originale\n",
    "            # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "\n",
    "\n",
    "            '''\n",
    "            Overlay troppo sfocato e colori discordanti\n",
    "            Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "            Differenza di colormap e blending:\n",
    "            L'overlay viene creato con una combinazione di due immagini: \n",
    "                1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "                2) la heatmap\n",
    "\n",
    "            Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "\n",
    "            Suggerimenti:\n",
    "\n",
    "            a) Modifica i pesi in cv2.addWeighted:\n",
    "            Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "\n",
    "            b) Uniforma il formato dell'immagine originale:\n",
    "            Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "            considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "\n",
    "            c) Usa lo stesso colormap: \n",
    "            Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "            usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "\n",
    "            '''\n",
    "            overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi l'overlay alla lista per la classe\n",
    "            overlays_list[cls].append(overlay)\n",
    "    \n",
    "    \n",
    "    mean_cams = {}\n",
    "    mean_overlays = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "        mean_overlays[cls] = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "        \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Mean Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        overlay_img = np.flipud(mean_overlays[cls])\n",
    "        \n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "463c009e-0f3a-46a5-9100-916556e08199",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "NEW VERSION - CON NUOVO CONTRASTO?\n",
    "\n",
    "Riassunto delle modifiche e dove sono state inserite:\n",
    "\n",
    "1) Normalizzazione robusta basata sui percentili (nuovo Passaggio 7.1):\n",
    "\n",
    "    Inserita subito dopo il ciclo che calcola mean_cams e mean_overlays.\n",
    "\n",
    "    Per ciascuna classe, viene calcolato il vettore dei pixel (flatten), vengono ricavati i quantili\n",
    "    (5° e 95° percentile), viene eseguito il clipping e poi la normalizzazione.\n",
    "\n",
    "\n",
    "2) Istogramma della distribuzione dei valori (nuovo Passaggio 7.2):\n",
    "\n",
    "    Nella figura finale, ho creato una terza riga di subplot in cui viene plottato l'istogramma dei valori della heatmap media\n",
    "    (prima della normalizzazione robusta) per ciascuna classe.\n",
    "\n",
    "\n",
    "3) Figura finale:\n",
    "\n",
    "    La figura finale ora ha 3 righe e 2 colonne:\n",
    "\n",
    "        Riga 1: Heatmap CAM normalizzate.\n",
    "        Riga 2: Overlay (CAM sovrapposta allo spettrogramma).\n",
    "        Riga 3: Istogramma della distribuzione dei valori della CAM.\n",
    "\n",
    "Questo codice aggiornato implementa tutti i passaggi che hai descritto. Fammi sapere se hai ulteriori domande o se necessiti di altre modifiche!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "      \n",
    "      - Riga 3: Istogramma della distribuzione dei valori della heatmap media.\n",
    "      - Riga 4: Spettrogramma medio (raw) per i trial di ciascuna classe.\n",
    "      \n",
    "      \n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            \n",
    "            # Aggiungi il campione alla lista della classe corrispondente\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "\n",
    "    \n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        for sample_input in samples[cls]:\n",
    "        \n",
    "        #sample_input = samples[cls]\n",
    "        \n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "\n",
    "            #Passaggio 5: Normalizzazione e upsampling\n",
    "\n",
    "            #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "            #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "            # ---------------------------\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            Sì, la normalizzazione viene eseguita nel Passaggio 5: Normalizzazione e upsampling della CAM, con questa parte di codice:\n",
    "\n",
    "                # Normalizza la mappa\n",
    "                cam = cam - cam.min()\n",
    "                cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            Cosa fa questa normalizzazione?\n",
    "            \n",
    "            1) Sottrazione del minimo:\n",
    "\n",
    "                Porta i valori minimi della CAM a zero.\n",
    "                Se cam.min() fosse -0.3, sottraendo cam.min() tutti i valori si traslano in modo che il minimo sia 0.\n",
    "\n",
    "            2) Divisione per il massimo:\n",
    "\n",
    "                Scala i valori in modo che il massimo diventi 1.\n",
    "                Aggiungere 1e-8 nel denominatore previene problemi di divisione per zero.\n",
    "\n",
    "            💡 Risultato → Dopo questa operazione, cam avrà valori normalizzati tra 0 e 1.\n",
    "            '''\n",
    "            \n",
    "            # Normalizza la mappa\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            #print(f\"Valori minimi e massimi dopo la normalizzazione: min={cam.min()}, max={cam.max()}\")\n",
    "            assert 0 <= cam.min() and cam.max() <= 1, \"Errore: i valori della CAM non sono nel range [0,1]!\"\n",
    "\n",
    "            # Upsample alla dimensione dell'immagine di input\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "            cam = cam.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa alla lista per la classe\n",
    "            cams_list[cls].append(cam)\n",
    "\n",
    "\n",
    "            #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "\n",
    "            #L'immagine originale viene convertita in un array numpy.\n",
    "            #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "            #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 6: Creazione dell'Overlay\n",
    "            # -------------------------------\n",
    "\n",
    "            # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "            # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "\n",
    "            # Prepara l'immagine originale per la visualizzazione\n",
    "            img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            # Normalizza l'immagine in scala 0-255\n",
    "            img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "            # Applica la heatmap usando OpenCV\n",
    "            #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "            # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "\n",
    "            '''\n",
    "            Il processo è lo stesso di quello descritto per le cam:\n",
    "\n",
    "            I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "            Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "            La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "            '''\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Sovrapponi la heatmap all'immagine originale\n",
    "            # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "\n",
    "\n",
    "            '''\n",
    "            Overlay troppo sfocato e colori discordanti\n",
    "            Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "            Differenza di colormap e blending:\n",
    "            L'overlay viene creato con una combinazione di due immagini: \n",
    "                1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "                2) la heatmap\n",
    "\n",
    "            Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "\n",
    "            Suggerimenti:\n",
    "\n",
    "            a) Modifica i pesi in cv2.addWeighted:\n",
    "            Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "\n",
    "            b) Uniforma il formato dell'immagine originale:\n",
    "            Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "            considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "\n",
    "            c) Usa lo stesso colormap: \n",
    "            Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "            usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "\n",
    "            '''\n",
    "            overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi l'overlay alla lista per la classe\n",
    "            overlays_list[cls].append(overlay)\n",
    "    \n",
    "    \n",
    "    mean_cams = {}\n",
    "    mean_overlays = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "        mean_overlays[cls] = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    NEW VERSION\n",
    "    '''\n",
    "    \n",
    "    # =======================================================\n",
    "    # Nuovo Passaggio 7.1: Normalizzazione basata sui percentili\n",
    "    # Per ogni classe, restringiamo il range della heatmap media\n",
    "    # per enfatizzare il contrasto nelle regioni più frequenti.\n",
    "    # =======================================================\n",
    "    normalized_mean_cams = {}\n",
    "    hist_data = {}  # Per salvare i dati dell'istogramma per ciascuna classe\n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        # Estrai tutti i pixel in un vettore\n",
    "        heatmap_flat = mean_cams[cls].flatten()\n",
    "        \n",
    "        '''\n",
    "        L'idea è enfatizzare le regioni in cui i valori della heatmap sono più frequenti, \n",
    "        riducendo l'influenza degli outlier, e normalizzare in modo adattivo alla forma della distribuzione\n",
    "        \n",
    "        1. Mediana ± margine percentile (centrato sulla mediana)\n",
    "        \n",
    "        Calcoli la mediana e costruisci un range percentile attorno a essa (es. ±20%)\n",
    "        \n",
    "        Pro: Robusto, adatto a distribuzioni skewed o simmetriche\t\n",
    "        Contro: Se la mediana non coincide con la modalità (valore più frequente), potresti perdere le aree più “dense”\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Calcola la mediana\n",
    "        median = np.median(heatmap_flat)\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Percentile della mediana\n",
    "        median_percentile = np.sum(heatmap_flat < median) / len(heatmap_flat) * 100\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Margine personalizzabile\n",
    "        margin = 20  \n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Definisci range percentile dinamico\n",
    "        lower_p = max(0, median_percentile - margin)\n",
    "        upper_p = min(100, median_percentile + margin)\n",
    "    \n",
    "        # Calcola i quantili (ad esempio 30° e 70° percentile)\n",
    "        #vmin = np.percentile(heatmap_flat, 30)\n",
    "        #vmax = np.percentile(heatmap_flat, 70)\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Calcola limiti reali\n",
    "        vmin = np.percentile(heatmap_flat, lower_p)\n",
    "        vmax = np.percentile(heatmap_flat, upper_p)\n",
    "\n",
    "        # Clipping dei valori per limitare gli outlier\n",
    "        heatmap_clipped = np.clip(mean_cams[cls], vmin, vmax)\n",
    "\n",
    "        \n",
    "        # Normalizza tra 0 e 1 usando i valori clipped\n",
    "        heatmap_normalized = (heatmap_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "        normalized_mean_cams[cls] = heatmap_normalized\n",
    "        \n",
    "        # Salva i dati per l'istogramma (per visualizzare la distribuzione)\n",
    "        hist_data[cls] = heatmap_flat\n",
    "        \n",
    "        #print(f\"\\n[Classe \\033[1m{cls}\\033[0m] Mediana: \\033[1m{median:.4f}\\033[0m | vmin ({lower_p:.1f}°): {vmin:.4f} | vmax ({upper_p:.1f}°): {vmax:.4f}\\n\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    OLD VERSION\n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "    #Istogramma della distribuzione dei valori della heatmap media per ciascuna classe.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    # - Terza Riga : L'istogramma della distribuzione dei valori della heatmap media per ciascuna classe.\n",
    "    \n",
    "    #fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    NEW VERSION\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # =======================================================\n",
    "    # Nuovo Passaggio 7.2: Calcolo dello spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Cosa fa questo codice?\n",
    "    ✅ Calcola lo spettrogramma medio per ogni classe (0 e 1) prendendo i trials da samples e facendo la media sulla prima dimensione (batch).\n",
    "    ✅ Plotta lo spettrogramma medio nella quarta riga del grafico finale, con una colonna per ogni classe.\n",
    "    ✅ Usa una colormap jet per una migliore visualizzazione.\n",
    "    ✅ Evita errori: Se una classe non ha dati, non plotta nulla per quella colonna.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Sì, l'errore indica che stai cercando di convertire un tensore PyTorch che richiede il calcolo del gradiente\n",
    "    #in un array NumPy direttamente con .numpy(), cosa che non è permessa.\n",
    "    \n",
    "    \n",
    "    mean_raw_spectrograms = {}\n",
    "    for cls in [0, 1]:\n",
    "        if len(samples[cls]) > 0:\n",
    "            # Stacka tutti i trials per la classe e calcola la media sul batch (dimensione 0)\n",
    "            \n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().cpu().numpy()\n",
    "            '''\n",
    "            🔍 Perché dovrebbe funzionare?\n",
    "            .detach(): Disattiva il tracciamento del gradiente (rende il tensore statico, senza dipendenze dalla computational graph di PyTorch).\n",
    "            .cpu(): Porta il tensore sulla CPU (necessario per numpy()).\n",
    "            .numpy(): Converte il tensore in un array NumPy.\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            1) Calcolo della media degli spettrogrammi (rimozione dei canali)\n",
    "            L'errore \"Invalid shape (3, 26, 11)\" con questa riga commentata sopra ☝️\n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "             \n",
    "            indica che l'array finale ha 3 canali in più (la prima dimensione) che non ti aspetti. \n",
    "            I tuoi dati originali hanno la forma:\n",
    "\n",
    "            (trials, canali, frequenze, tempo)\n",
    "\n",
    "            Se vuoi ottenere una rappresentazione media dello spettrogramma per tutti i trial di una classe, mediando anche sui canali, \n",
    "            allora devi calcolare la media lungo la dimensione dei trial e quella dei canali.\n",
    "            \n",
    "            Dovresti fare:\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0, 1)).detach().cpu().numpy()\n",
    "            \n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "            SPIEGAZIONE:\n",
    "\n",
    "            torch.cat(samples[cls], dim=0)\n",
    "            => Concatena tutti i trial per quella classe lungo la dimensione 0, ottenendo un tensore con forma:\n",
    "            (num_trials, canali, frequenze, tempo).\n",
    "\n",
    "            .mean(dim=(0, 1))\n",
    "            => Calcola la media prima lungo la dimensione dei trial (dim=0) e poi lungo quella dei canali (dim=1) in un'unica operazione, \n",
    "            ottenendo un tensore di forma (frequenze, tempo).\n",
    "\n",
    "            .detach().cpu().numpy()\n",
    "            => Rimuove il tracking del gradiente, sposta il tensore sulla CPU e lo converte in un array NumPy, pronto per imshow.\n",
    "\n",
    "            Questo ti darà l'array 2D (frequenze × tempo) che imshow si aspetta.\n",
    "            \n",
    "            \n",
    "            CHAGPT:\n",
    "            \n",
    "            Nel contesto del tuo esempio:\n",
    "\n",
    "            La forma iniziale dei dati EEG in un formato tempo-frequenza era (batch, canali, frequenze, tempo), che è una matrice 4D. \n",
    "            \n",
    "            Qui, hai un batch di dati, dove ogni dato ha la dimensione dei canali, frequenze, e tempo.\n",
    "\n",
    "            Usando il codice:\n",
    "\n",
    "                mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0,1)).detach().cpu().numpy()\n",
    "                \n",
    "            Stai concatenando lungo la dimensione del batch (dim=0), quindi ottieni una matrice che somma tutte le informazioni sul batch. \n",
    "            Successivamente, con .mean(dim=(0,1)) stai calcolando la media lungo le dimensioni dei canali (0) e del batch (1), \n",
    "            riducendo il risultato a una matrice 2D con la forma (frequenze, tempo), che è quella che desideri, ovvero \n",
    "            \n",
    "            --> la media delle frequenze e del tempo su tutto il batch e i canali.\n",
    "\n",
    "            Quindi sì, la forma risultante di mean_raw_spectrograms[cls] sarà una matrice 2D che rappresenta \n",
    "                1) le frequenze sulle righe e \n",
    "                2) il tempo sulle colonne\n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "\n",
    "            '''\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim =(0, 1)).detach().cpu().numpy()\n",
    "        else:\n",
    "            mean_raw_spectrograms[cls] = None\n",
    "    \n",
    "    \n",
    "    # =======================================================\n",
    "    # Passaggio 7.3: Creazione della figura finale\n",
    "    # Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Heatmap CAM normalizzata (basata sui percentili) per ciascuna classe\n",
    "    #  - Riga 2: Overlay (CAM + spettrogramma) per ciascuna classe\n",
    "    #  - Riga 3: Istogramma della distribuzione dei valori della heatmap media per ciascuna classe\n",
    "    #  - Riga 4: Spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Output atteso\n",
    "    \n",
    "    Ora avrai una figura con 4 righe di subplot: \n",
    "    1️⃣ Heatmap Grad-CAM media per classe\n",
    "    2️⃣ Grad-CAM normalizzato\n",
    "    3️⃣ Istogramma dei valori della heatmap (pre-normalizzazione)\n",
    "    4️⃣ Spettrogramma medio per ogni classe (la nuova riga che hai chiesto!)\n",
    "    '''\n",
    "    \n",
    "    # Creiamo una figura con 4 righe e 2 colonne\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Overlay over EEG Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()  # Regola automaticamente la spaziatura globale\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM) normalizzate\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        #cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * normalized_mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Mean Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "        \n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi (CAM + spettrogramma)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        overlay_img = np.flipud(mean_overlays[cls])\n",
    "        \n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "        \n",
    "    \n",
    "        '''\n",
    "\n",
    "        Istogramma della distribuzione dei valori della heatmap media (prima della normalizzazione robusta) per ciascuna classe\n",
    "\n",
    "        1) Cosa rappresenta l'istogramma?\n",
    "\n",
    "        L'istogramma mostra la distribuzione dei valori della heatmap media (Grad-CAM) per ogni classe.\n",
    "        In altre parole, stai visualizzando quante volte certi valori della heatmap compaiono nella distribuzione per quella classe.\n",
    "\n",
    "        2) Cosa intendo con \"prima della normalizzazione robusta\"?\n",
    "\n",
    "        Nel tuo codice, la heatmap media è stata calcolata per ciascuna classe.\n",
    "        Successivamente, hai applicato una normalizzazione robusta per rendere i valori più comparabili tra classi diverse.\n",
    "        Tuttavia, l'istogramma che stai plottando rappresenta i valori della heatmap prima che questa normalizzazione venga applicata.\n",
    "        Questo ti permette di vedere la distribuzione originale dei valori senza alterazioni dovute alla normalizzazione.\n",
    "\n",
    "\n",
    "        3) Come si collega alla figura finale?\n",
    "\n",
    "        Hai creato una terza riga di subplot (axs[2, j]) in cui plotti questo istogramma per ogni classe.\n",
    "        Il titolo che hai scelto chiarisce bene il significato della visualizzazione, specificando che si tratta dell'istogramma dei valori Grad-CAM \n",
    "        PRIMA della normalizzazione, per ogni classe.\n",
    "        La tua modifica al titolo è chiara e aiuta a evitare ambiguità.\n",
    "        \n",
    "        Per enfatizzare ancora di più il concetto, si potrebbe aggiungere \"(Raw Values)\" nel titolo, ad esempio:\n",
    "        \n",
    "        axs[2, j].set_title(f\"Histogram of Mean Grad-CAM Values (Raw) for Class {condition_names[cls]}\", fontsize=12)\n",
    "        oppure\n",
    "        axs[2, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) Before Normalization\\nClass{condition_names[cls]}\", fontsize=12)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "    # Terza 3: Visualizzazione degli istogrammi della distribuzione dei valori\n",
    "        \n",
    "    '''\n",
    "    Questi valori rappresentano la distribuzione delle attivazioni prima della procedura di normalizzazione che viene applicata per amplificare il contrasto, \n",
    "    quindi sì, si tratta di valori di attivazione della mappa Grad-CAM.\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Istogramma dei valori prima della normalizzazione: Stai visualizzando un istogramma di questi valori medi, \n",
    "                                                           prima che vengano normalizzati o clippati \n",
    "                                                           (secondo il processo di normalizzazione basato sui percentili che hai descritto). \n",
    "                                                           Questo ti dà una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti prima che tu applichi il filtro per migliorare\n",
    "                                                           il contrasto nelle aree di interesse.\n",
    "    \n",
    "    \n",
    "    Perché \"Grad-CAM value\" può creare confusione:\n",
    "    Il termine \"Grad-CAM value\" potrebbe sembrare che faccia riferimento direttamente ai valori generati dalla mappa Grad-CAM finale. \n",
    "    Ma in realtà, i valori che stai trattando sono le attivazioni mediate e clippate, che formano la heatmap. \n",
    "    L'istogramma che stai tracciando rappresenta la distribuzione delle attivazioni prima della normalizzazione.\n",
    "\n",
    "    Riepilogo\n",
    "    Quindi, questi valori sono attivazioni pesate per ciascun pixel della mappa Grad-CAM, e mediati per classe. \n",
    "    Il processo di normalizzazione che segue (basato sui percentili) serve a enfatizzare il contrasto in modo da focalizzarsi sulle aree più significative \n",
    "    per la previsione.\n",
    "\n",
    "    Per rispondere alla tua domanda: sì, è corretto dire che stai visualizzando la distribuzione delle attivazioni pesate prima della normalizzazione\n",
    "    per migliorare il contrasto, ma è meglio riferirsi a questi valori come valori di attivazione della mappa Grad-CAM o valori della heatmap Grad-CAM, \n",
    "    piuttosto che \"Grad-CAM value\" che potrebbe risultare ambiguo.\n",
    "\n",
    "    Se vuoi, puoi anche aggiungere una nota nella visualizzazione dell'istogramma che chiarisca il processo:\n",
    "    \n",
    "    axs[2, j].set_title(f\"Histogram of Heatmap Activation Values (Raw, before Normalization) Class {condition_names[cls]}\", fontsize=12)\n",
    "    oppure\n",
    "    axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "    \n",
    "    è molto chiara e corretta!\n",
    "    Indica perfettamente che stai visualizzando l'istogramma dei valori di attivazione medi della heatmap, \n",
    "    senza fare confusione sul fatto che si tratti di valori medi per ciascuna classe.\n",
    "    \n",
    "    In sintesi, questa frase comunica in modo preciso che stai mostrando la distribuzione delle attivazioni mediate dalla mappa Grad-CAM\n",
    "    per una specifica classe. Quindi sì, va benissimo!\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        axs[2, j].hist(hist_data[cls], bins= 'auto', color='blue', edgecolor='black')\n",
    "        #axs[2, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[2, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        axs[2, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "    \n",
    "        '''\n",
    "\n",
    "        Spiegazione delle modifiche aggiunte:\n",
    "\n",
    "        1) Calcolo dello spettrogramma medio raw:\n",
    "        \n",
    "        Dopo aver raccolto i campioni nel dizionario samples, viene creato il dizionario mean_raw_spectrograms.\n",
    "        Per ogni classe, i tensori vengono concatenati lungo la dimensione batch e si calcola la media sul batch (dim=0).\n",
    "        Il risultato viene convertito in un array NumPy per il plotting.\n",
    "\n",
    "        2) Aggiornamento della figura finale:\n",
    "        La figura viene creata con 4 righe e 2 colonne.\n",
    "        La quarta riga (axs[3, j]) visualizza lo spettrogramma medio per ogni classe usando imshow, con la colormap 'jet' (puoi modificarla se preferisci).\n",
    "        Vengono aggiunti titoli, etichette e una barra dei colori.\n",
    "\n",
    "        '''\n",
    "    \n",
    "    # Quarta 4: Spettrogramma medio (raw) per ciascuna classe\n",
    "    \n",
    "    # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        if mean_raw_spectrograms[cls] is not None:\n",
    "            im = axs[3, j].imshow(mean_raw_spectrograms[cls], extent=[0, 1000, 0, 26],aspect='auto', cmap='jet', origin='lower')\n",
    "            axs[3, j].set_title(f\"Mean Raw Spectrogram for Class {condition_names[cls]}\", fontsize=12)\n",
    "            axs[3, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "            axs[3, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "            fig.colorbar(im, ax=axs[3, j])\n",
    "            axs[3, j].axis('on')\n",
    "        else:\n",
    "            axs[3, j].axis(\"off\")\n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65640b3a-5bd8-4fcb-9742-7e8498020ed1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                                                     NEW VERSION 27/06/2025\n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                                    VERSION TIME x FREQUENCY\n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                                            VERSIONE C\n",
    "                                                                \n",
    "                                                                NORMALIZZAZIONE SOLO SU SCALA DI COLORI \n",
    "                                                                PER HEATMAP MEDIA DEL GRADCAM \n",
    "                                                                \n",
    "                                                                SCALA LOGARITMICA PER SPETTOGRAMMA\n",
    "                                                                    CONGIUNTA PER DUE CLASSI\n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                                            UFFICIALE\n",
    "                                                                \n",
    "\n",
    "# Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Istogramma della distribuzione dei valori della heatmap media RAW per ciascuna classe \n",
    "    #            rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 2: GradCAM medio della distribuzione dei valori della heatmap media per ogni classe, \n",
    "    #            a seguito della NORMALIZZAZIONE rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 3: Spettrogramma medio (RAW) rispetto ai Trial della Stessa Classe, su range logaritmico \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "      \n",
    "      - Riga 3: Istogramma della distribuzione dei valori della heatmap media.\n",
    "      - Riga 4: Spettrogramma medio (raw) per i trial di ciascuna classe.\n",
    "      \n",
    "      \n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            \n",
    "            # Aggiungi il campione alla lista della classe corrispondente\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "\n",
    "    \n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        for sample_input in samples[cls]:\n",
    "        \n",
    "        #sample_input = samples[cls]\n",
    "        \n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            TUTTO IL PASSAGGIO DELLO STEP 5 \n",
    "            \n",
    "            OSSIA NORMALIZZAZIONE i.e.,  NEL SENSO DI RISCALATURA NEL RANGE 0-1 + UPSAMPLING \n",
    "            \n",
    "            (CHE SERVIVA PER UNIFORMARE I VALORI E ADATTARSI ALLA DIMENSIONE DELLA IMMAGINE ORIGINALE \n",
    "            PER VEDERE UN SOLO ESEMPIO DELLA CLASSE RISPETTO ALLA MAPPA DI ATTIVAZIONE E ALL'OVERLAY\n",
    "            DEL GRADCAM RISPETTO ALLA IMMAGINE ORIGINALE)\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            \n",
    "            NON SERVE PIU', AD ECCEZIONE DI QUESTE RIGHE CHE ORA TI RIMETTO QUI SOTTO!'''\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            ✅ Cosa fa correttamente questo codice:\n",
    "            \n",
    "            Estrae i campioni da test_loader separandoli in samples[0] e samples[1].\n",
    "            \n",
    "            Per ogni campione di ogni classe:\n",
    "            \n",
    "            Calcola la Grad-CAM raw (senza riscaling),\n",
    "            La interpola per adattarla alla dimensione originale (n_freq, n_time)\n",
    "            Applica ReLU per tenere solo le attivazioni positive (come da standard Grad-CAM)\n",
    "            La converte in NumPy e la salva in cams_list[cls].\n",
    "            \n",
    "            Alla fine, fa la media delle CAM raw per ciascuna classe:\n",
    "            \n",
    "            mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "            \n",
    "            🔍 Stato attuale del dato\n",
    "            \n",
    "            cams_list[cls] → lista di array cam 2D non normalizzati, uno per ogni trial.\n",
    "            mean_cams[cls] → array 2D (frequenza × tempo), media dei trial per ciascuna classe.\n",
    "            \n",
    "            La normalizzazione Z-score congiunta la farai dopo, sulla base di mean_cams.\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            target_size = (sample_input.shape[2:]) # -> (n_freq, n_time)\n",
    "            \n",
    "            cam = F.interpolate(cam.unsqueeze(1), size = target_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # squeeze\n",
    "            cam = cam.squeeze()                 # tensor 2D\n",
    "            \n",
    "            # Infine sposti su CPU e passi a numpy\n",
    "            cam = cam.cpu().numpy()\n",
    "\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa del singolo esempio alla lista per la classe (per poi dopo farci la media dentro mean_cams!)\n",
    "            cams_list[cls].append(cam)\n",
    "            \n",
    "    \n",
    "    # ============================================================\n",
    "    # Calcolo dello heatmap media dei valori (raw) per ciascuna classe\n",
    "    # ============================================================\n",
    "    \n",
    "    mean_cams = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    # Calcolo dello spettrogramma medio (raw) per ciascuna classe\n",
    "    # ============================================================\n",
    "    \n",
    "    mean_raw_spectrograms = {}\n",
    "    for cls in [0, 1]:\n",
    "        if len(samples[cls]) > 0:\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim =(0, 1)).detach().cpu().numpy()\n",
    "        else:\n",
    "            mean_raw_spectrograms[cls] = None\n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().cpu().numpy()\n",
    "            '''\n",
    "            🔍 Perché dovrebbe funzionare?\n",
    "            .detach(): Disattiva il tracciamento del gradiente (rende il tensore statico, senza dipendenze dalla computational graph di PyTorch).\n",
    "            .cpu(): Porta il tensore sulla CPU (necessario per numpy()).\n",
    "            .numpy(): Converte il tensore in un array NumPy.\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            1) Calcolo della media degli spettrogrammi (rimozione dei canali)\n",
    "            L'errore \"Invalid shape (3, 26, 11)\" con questa riga commentata sopra ☝️\n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "             \n",
    "            indica che l'array finale ha 3 canali in più (la prima dimensione) che non ti aspetti. \n",
    "            I tuoi dati originali hanno la forma:\n",
    "\n",
    "            (trials, canali, frequenze, tempo)\n",
    "\n",
    "            Se vuoi ottenere una rappresentazione media dello spettrogramma per tutti i trial di una classe, mediando anche sui canali, \n",
    "            allora devi calcolare la media lungo la dimensione dei trial e quella dei canali.\n",
    "            \n",
    "            Dovresti fare:\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0, 1)).detach().cpu().numpy()\n",
    "            \n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "            SPIEGAZIONE:\n",
    "\n",
    "            torch.cat(samples[cls], dim=0)\n",
    "            => Concatena tutti i trial per quella classe lungo la dimensione 0, ottenendo un tensore con forma:\n",
    "            (num_trials, canali, frequenze, tempo).\n",
    "\n",
    "            .mean(dim=(0, 1))\n",
    "            => Calcola la media prima lungo la dimensione dei trial (dim=0) e poi lungo quella dei canali (dim=1) in un'unica operazione, \n",
    "            ottenendo un tensore di forma (frequenze, tempo).\n",
    "\n",
    "            .detach().cpu().numpy()\n",
    "            => Rimuove il tracking del gradiente, sposta il tensore sulla CPU e lo converte in un array NumPy, pronto per imshow.\n",
    "\n",
    "            Questo ti darà l'array 2D (frequenze × tempo) che imshow si aspetta.\n",
    "            \n",
    "            \n",
    "            CHAGPT:\n",
    "            \n",
    "            Nel contesto del tuo esempio:\n",
    "\n",
    "            La forma iniziale dei dati EEG in un formato tempo-frequenza era (batch, canali, frequenze, tempo), che è una matrice 4D. \n",
    "            \n",
    "            Qui, hai un batch di dati, dove ogni dato ha la dimensione dei canali, frequenze, e tempo.\n",
    "\n",
    "            Usando il codice:\n",
    "\n",
    "                mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0,1)).detach().cpu().numpy()\n",
    "                \n",
    "            Stai concatenando lungo la dimensione del batch (dim=0), quindi ottieni una matrice che somma tutte le informazioni sul batch. \n",
    "            Successivamente, con .mean(dim=(0,1)) stai calcolando la media lungo le dimensioni dei canali (0) e del batch (1), \n",
    "            riducendo il risultato a una matrice 2D con la forma (frequenze, tempo), che è quella che desideri, ovvero \n",
    "            \n",
    "            --> la media delle frequenze e del tempo su tutto il batch e i canali.\n",
    "\n",
    "            Quindi sì, la forma risultante di mean_raw_spectrograms[cls] sarà una matrice 2D che rappresenta \n",
    "                1) le frequenze sulle righe e \n",
    "                2) il tempo sulle colonne\n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "\n",
    "            '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # =======================================================\n",
    "    # Passaggio Finale: Creazione della figura finale\n",
    "    # Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Istogramma della distribuzione dei valori della heatmap media per ciascuna classe \n",
    "    #            normalizzata rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 2: GradCAM medio della distribuzione dei valori della heatmap media per ogni classe, \n",
    "    #            a seguito della normalizzazione rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 3: Spettrogramma medio (raw) rispetto ai Trial della Stessa Classe, su range logaritmico \n",
    "    # =======================================================\n",
    "    \n",
    "    \n",
    "    Quando devo plottare l'istogramma dei valori di ogni heatmap media solamente (riga 3), \n",
    "    devo plottarli in base alla normalizzazione rispetto alla distribuzione congiunta.\n",
    "    \n",
    "    Quindi, devo plottarli in base al range minimo e massimo della intera distribuzione congiunta, quando è stata normalizzata!\n",
    "    Di conseguenza devo fare\n",
    "    \n",
    "    1) Prendere la Media delle CAM per ogni classe (già fatto)\n",
    "    2) Costruzione distribuzione congiunta raw\n",
    "    3) Calcolo Media e Deviazione Standard della Distribuzione Congiunta\n",
    "    4) Normalizzazione Z-score della Distribuzione Congiunta\n",
    "    \n",
    "    5) Prendo il range minimo e massimo della Distribuzione Congiunta Normalizzata\n",
    "    \n",
    "    Ossia, il range minimo e massimo su cui plottare entrambe le heatmap medie normalizzate in base alla distribuzione congiunta,\n",
    "    dovrà essere rispetto alla distribuzione congiunta a seguito della normalizzazione.\n",
    "    \n",
    "    Quindi, dovrei ricreare un'altra variabile che contiene i valori normalizzati di entrambe le distribuzioni assieme,\n",
    "    ossia una cosa del tipo\n",
    "    \n",
    "    normalized_all_vals = np.concatenate([normalized_mean_cams[0].flatten(), normalized_mean_cams[1].flatten()])\n",
    "    \n",
    "    e da questa prendere il minimo ed il massimo!\n",
    "    \n",
    "    \n",
    "    '''\n",
    "   \n",
    "\n",
    "    # Creiamo una figura con 4 righe e 2 colonne\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Overlay over EEG Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    #fig, axs = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Creiamo una figura con 3 righe e 2 colonne\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    plt.suptitle(f\"Grad-CAM Mapping over EEG Trials\\nExperimental Conditions: {exp_cond}\", fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()  # Regola automaticamente la spaziatura globale\n",
    "    plt.subplots_adjust(hspace = 0.5, wspace = 0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    # PLOT RIGA 1: Visualizzazione degli istogrammi della distribuzione dei valori delle heatmap medie RAW\n",
    "    # RISPETTO ALLA DISTRIBUZIONE CONGIUNTA\n",
    "        \n",
    "    '''\n",
    "    Questi valori rappresentano la distribuzione delle attivazioni di entrambe le classi, \n",
    "    rispetto alla DISTRIBUZIONE CONGIUNTA\n",
    "    \n",
    "    Quindi si tratta di valori di attivazione della mappa Grad-CAM media rispetto alla distribuzione congiunta!\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Istogramma dei valori raw medi di ogni classe (su distribuzione congiunta!): Stai visualizzando un istogramma di questi valori medi, \n",
    "                                                           sulla distribuzione congiunta, ossia\n",
    "                                                           \n",
    "                                                           - prendo i valori (raw)delle heatmap media di entrambe le classi\n",
    "                                                           - calcolare la distribuzione congiunta dei valori (all_vals = ...)\n",
    "                                                           - ottengo quindi la nuova distribuzione congiunta dalle heatmap medie di entrambe le classi\n",
    "                                                          \n",
    "                                                           - calcolo minimo e massimo a seguito della normalizzazione (?) e non prima\n",
    "                                                           - faccio i plot di entrambe delle heatmap medie raw,\n",
    "                                                             ma rispetto a distribuzione congiunta\n",
    "                                                             \n",
    "                                                           \n",
    "                                                           Questo darebbe una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti tra le 2 classi (che ora son confrontabili!)\n",
    "                                                           a livello RAW!\n",
    "                                                        \n",
    "    \n",
    "    N.B. PER IL NOME DEL TITOLO DEL PLOT\n",
    "    \n",
    "    Perché \"Grad-CAM value\" può creare confusione:\n",
    "    Il termine \"Grad-CAM value\" potrebbe sembrare che faccia riferimento direttamente ai valori generati dalla mappa Grad-CAM finale. \n",
    "    Ma in realtà, i valori che stai trattando sono le attivazioni mediate e clippate, che formano la heatmap. \n",
    "    L'istogramma che stai tracciando rappresenta la distribuzione delle attivazioni prima della normalizzazione.\n",
    "\n",
    "    Riepilogo\n",
    "    Quindi, questi valori sono attivazioni pesate per ciascun pixel della mappa Grad-CAM, e mediati per classe. \n",
    "    Il processo di normalizzazione che segue (basato sui percentili) serve a enfatizzare il contrasto in modo da focalizzarsi sulle aree più significative \n",
    "    per la previsione.\n",
    "\n",
    "    Per rispondere alla tua domanda: sì, è corretto dire che stai visualizzando la distribuzione delle attivazioni pesate prima della normalizzazione\n",
    "    per migliorare il contrasto, ma è meglio riferirsi a questi valori come valori di attivazione della mappa Grad-CAM o valori della heatmap Grad-CAM, \n",
    "    piuttosto che \"Grad-CAM value\" che potrebbe risultare ambiguo.\n",
    "\n",
    "    Se vuoi, puoi anche aggiungere una nota nella visualizzazione dell'istogramma che chiarisca il processo:\n",
    "    \n",
    "    axs[2, j].set_title(f\"Histogram of Heatmap Activation Values (Raw, before Normalization) Class {condition_names[cls]}\", fontsize=12)\n",
    "    oppure\n",
    "    axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "    \n",
    "    è molto chiara e corretta!\n",
    "    Indica perfettamente che stai visualizzando l'istogramma dei valori di attivazione medi della heatmap, \n",
    "    senza fare confusione sul fatto che si tratti di valori medi per ciascuna classe.\n",
    "    \n",
    "    In sintesi, questa frase comunica in modo preciso che stai mostrando la distribuzione delle attivazioni mediate dalla mappa Grad-CAM\n",
    "    per una specifica classe. Quindi sì, va benissimo!\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #PER PLOT RIGA 1 \n",
    "    \n",
    "    # Creo la distribuzione congiunta dei valori di ogni heatmap media RAW delle due classi, srotolando i valori di entrambe\n",
    "    all_vals_raw = np.concatenate([mean_cams[0].flatten(), mean_cams[1].flatten()])\n",
    "    \n",
    "    # Il range minimo e massimo su cui plottare entrambe le heatmap medie raw in base alla distribuzione congiunta (riga 3)\n",
    "    # dovrà essere rispetto alla distribuzione congiunta raw\n",
    "    \n",
    "    vmin_raw = all_vals_raw.min()\n",
    "    vmax_raw = all_vals_raw.max()\n",
    "    \n",
    "    \n",
    "    # Prima riga: Visualizza l'istogramma della heatmap media rispetto alla distribuzione congiunta!\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        axs[0, j].hist(mean_cams[cls].flatten(), bins= 'auto', color='blue', edgecolor='black')\n",
    "        #axs[0, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].set_title(f\"Histogram of Mean Heatmap Activation Values (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        axs[0, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # PLOT RIGA 2: Visualizzazione dei valori delle heatmap medie delle due classi\n",
    "    # RISPETTO ALLA DISTRIBUZIONE CONGIUNTA, SU CUI VIENE FATTA LA NORMALIZZAZIONE\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questi valori rappresentano le heatmap medie delle attivazioni di entrambe le classi, \n",
    "    rispetto alla DISTRIBUZIONE CONGIUNTA, SU CUI VIENE FATTA LA NORMALIZZAZIONE\n",
    "    \n",
    "    Quindi si tratta di valori di attivazione della mappa Grad-CAM media rispetto alla distribuzione congiunta NORMALIZZATA\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Calcolo la distribuzione congiunta dei valori raw medi di ogni classe (su distribuzione congiunta!): \n",
    "        Stai visualizzando un istogramma di questi valori medi, sulla DISTRIBUZIONE CONGIUNTA, ossia\n",
    "                                                           \n",
    "                                                           - prendo i valori (raw)delle heatmap media di entrambe le classi\n",
    "                                                           - calcolare la distribuzione congiunta dei valori (all_vals = ...)\n",
    "                                                           - ottengo quindi la nuova distribuzione congiunta dalle heatmap medie di entrambe le classi\n",
    "                                                           \n",
    "                                                           - calcolo media e deviazione standard delle distribuzione congiunta\n",
    "                                                           - faccio la normalizzazione della distribuzione congiunta\n",
    "                                                           \n",
    "                                                           - calcolo minimo e massimo a seguito della normalizzazione e non prima\n",
    "                                                             della distribuzione congiunta normalizzata\n",
    "                                                           \n",
    "                                                           - faccio i plot di entrambe delle heatmap medie normalizzate,\n",
    "                                                             ma rispetto alla distribuzione congiunta\n",
    "                                                             \n",
    "                                                           \n",
    "                                                           Questo darebbe una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti tra le 2 classi (che ora son confrontabili!)\n",
    "                                                           a livello NORMALIZZATO!\n",
    "                                                        \n",
    "    '''\n",
    "    \n",
    "    '''SOPRA ABBIAMO CREATO --> all_vals_raw'''\n",
    "    \n",
    "    # Creo la distribuzione congiunta dei valori di ogni heatmap media RAW delle due classi, srotolando i valori di entrambe\n",
    "    #all_vals_raw = np.concatenate([mean_cams[0].flatten(), mean_cams[1].flatten()])\n",
    "    \n",
    "    #Calcolo media e deviazione standard della distribuzione congiunta dei valori (raw) delle heatmap medie di entrambe le classi \n",
    "    #joint_mean = np.mean(all_vals_raw)\n",
    "    #joint_std = np.std(all_vals_raw)\n",
    "    \n",
    "    # Normalizzazione Z-score della distribuzione congiunta\n",
    "    #normalized_mean_cams = {}\n",
    "    \n",
    "    #for cls in [0, 1]:\n",
    "        #normalized_mean_cams[cls] = (mean_cams[cls] - joint_mean) / joint_std\n",
    "\n",
    "    # Il range minimo e massimo su cui plottare entrambe le heatmap medie normalizzate in base alla distribuzione congiunta (riga 3)\n",
    "    # dovrà essere rispetto alla distribuzione congiunta a seguito della normalizzazione\n",
    "    \n",
    "    #normalized_all_vals = np.concatenate([normalized_mean_cams[0].flatten(), normalized_mean_cams[1].flatten()])\n",
    "    \n",
    "    #vmin_normalized = normalized_all_vals.min()\n",
    "    #vmax_normalized = normalized_all_vals.max()\n",
    "    \n",
    "    vmin_normalized = all_vals_raw.min()\n",
    "    vmax_normalized = all_vals_raw.max()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    # Opzione: normalizzazione robusta con percentili\n",
    "    vmin_normalized, vmax_normalized = np.percentile(all_vals_raw, [5, 95])\n",
    "    '''\n",
    "    \n",
    "    # Seconda riga: Mean heatmap di ogni classe normalizzata a partire dalla distribuzione congiunta ( = di entrambe le classi)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        \n",
    "        im = axs[1, j].imshow(\n",
    "            mean_cams[cls],\n",
    "            #normalized_mean_cams[cls], #QUI LA RENDO IN 2D, NON IN 1D COME PRIMA\n",
    "            #cmap='seismic',\n",
    "            cmap='RdYlBu_r',\n",
    "            vmin= vmin_normalized, vmax= vmax_normalized,\n",
    "            extent=[0, 1000, 0, 26],\n",
    "            aspect='auto',\n",
    "            origin='lower'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # → calcola 6 tick equi-spaziati\n",
    "        ticks = np.linspace(vmin_normalized, vmax_normalized, 6)  \n",
    "        \n",
    "        cbar = fig.colorbar(\n",
    "            im,\n",
    "            ax=axs[1, j],\n",
    "            orientation='horizontal',\n",
    "            pad=0.12,\n",
    "            ticks=ticks)\n",
    "        \n",
    "        cbar.ax.set_xticklabels([f\"{t:.4f}\" for t in ticks])\n",
    "\n",
    "        \n",
    "        axs[1, j].set_title(f\"Mean Grad-CAM Heatmap (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        \n",
    "        '''QUESTA NON CONSENTE DEFINIZIONE ASSI!'''\n",
    "        #axs[1, j].axis('off')\n",
    "        \n",
    "        axs[1, j].axis('on') \n",
    "        axs[1,j].set_xlabel(\"Time (mms)\")\n",
    "        axs[1,j].set_ylabel(\"Frequency (Hz)\")\n",
    "        \n",
    "        #fig.colorbar(im, ax=axs[3, j], orientation='horizontal', pad=0.05)\n",
    "    \n",
    "    print(f\"\\033[1mRange heatmap raw globale (vmin_raw, vmax_raw): {vmin_normalized}, {vmax_normalized}\\033[0m\")\n",
    "    \n",
    "    # PLOT RIGA 3: Spettrogramma medio (raw) per ciascuna classe log-scaled\n",
    "    \n",
    "    '''\n",
    "    Spiegazione delle modifiche aggiunte:\n",
    "\n",
    "    1) Calcolo dello spettrogramma medio raw:\n",
    "\n",
    "    Dopo aver raccolto i campioni nel dizionario samples, viene creato il dizionario mean_raw_spectrograms.\n",
    "    Per ogni classe, i tensori vengono concatenati lungo la dimensione batch e si calcola la media sul batch (dim=0).\n",
    "    \n",
    "    Poi, però, ogni spettogramma medio deve congiunto in una distribuzione in modo da plottare poi il valore dello spettrogramma  \n",
    "    rispetto al minimo ed al massimo della distribuzione congiunta dello spettrogramma medio di entrambe le classi! \n",
    "    \n",
    "    Il risultato viene convertito in un array NumPy per il plotting.\n",
    "    '''\n",
    "    \n",
    "    # Calcolo della distribuzione congiunta degli spettrogrammi medi delle due classi! \n",
    "    #all_vals_raw_samples = np.concatenate([mean_raw_spectrograms[0].flatten(), mean_raw_spectrograms[1].flatten()])\n",
    "    \n",
    "    '''SE VOLESSI RESTRINGERE TRA 5° e 95° PERCENTILE'''\n",
    "    #low_raw, high_raw = np.percentile(all_vals_raw, [5, 95])\n",
    "    #half_width_raw = max(abs(low_raw), abs(high_raw))   \n",
    "    #vmin_raw, vmax_raw = -half_width_raw, +half_width_raw\n",
    "    \n",
    "    '''ALTRIMENTI, TENGO TUTTO IL RANGE, DAL MINIMO AL MASSIMO'''\n",
    "    \n",
    "    #Ora qui prendo il miimo e massimo a partire dalla distribuzione congiunta!\n",
    "    #vmin_raw_samples, vmax_raw_samples = all_vals_raw_samples.min(), all_vals_raw_samples.max()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1) Qual è la differenza tra prima e ora?\n",
    "    \n",
    "    Prima calcolavo, dentro il for cls in [0,1], un nuovo vmin_raw_samples e vmax_raw_samples separatamente per ciascuna classe.\n",
    "    Di conseguenza ogni subplot sulla riga 3 aveva la sua scala di colori, rendendo impossibile un confronto diretto visivo \n",
    "    fra le due condizioni.\n",
    "    \n",
    "    Ora invece calcolerai una sola volta il log-power medio di entrambe le classi, ne ricavi un unico array congiunto,\n",
    "    quindi ne estrai un solo vmin e vmax. Questo ti garantisce che entrambi i subplot della riga 3 useranno la stessa scala di colori.\n",
    "\n",
    "\n",
    "    Per far sì che tutte e due le condizioni usino lo stesso minimo e massimo, sposto la raccolta dei limiti fuori dal ciclo,\n",
    "    usando la distribuzione congiunta dei log-power di entrambe le classi\n",
    "    \n",
    "    vmin_raw_samples e vmax_raw_samples li calcoli una volta sola, su tutti i valori logaritmici concatenati.\n",
    "    Entrambe le mappe usano esattamente lo stesso range, così le barre dei colori saranno allineate.\n",
    "    \n",
    "    Con questa modifica:\n",
    "\n",
    "    log_mean_power contiene già i valori in scala logaritmica.\n",
    "    vmin_raw_samples e vmax_raw_samples sono condivisi fra entrambe le colonne.\n",
    "    Ogni subplot userà la stessa “barretta” di colore, quindi potrai confrontare direttamente “deep blues” e “reds” delle due condizioni.\n",
    "    '''\n",
    "    \n",
    "    # 1. Calcola i log-power medi per ciascuna classe\n",
    "    log_mean_power = {\n",
    "        cls: np.log1p(mean_raw_spectrograms[cls])\n",
    "        for cls in [0,1]\n",
    "    }\n",
    "\n",
    "    # 2. Raccogli TUTTI i valori in un unico array\n",
    "    all_log_vals = np.concatenate([\n",
    "        log_mean_power[0].flatten(),\n",
    "        log_mean_power[1].flatten()\n",
    "    ])\n",
    "\n",
    "    # 3. Estrai un unico vmin/vmax condiviso\n",
    "    vmin_raw_samples = all_log_vals.min()\n",
    "    vmax_raw_samples = all_log_vals.max()\n",
    "\n",
    "        \n",
    "    # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        #if mean_raw_spectrograms[cls] is not None:\n",
    "        if log_mean_power[cls] is not None:    \n",
    "            \n",
    "            #Trasformo in scala logaritmica i miei dati EEG sulla spettro medio di ogni classe\n",
    "            #mean_raw_spectrograms[cls] = np.log1p(mean_raw_spectrograms[cls])\n",
    "        \n",
    "            \n",
    "            # Calcolo della distribuzione congiunta degli spettrogrammi medi delle due classi! \n",
    "            #all_vals_raw_samples = np.concatenate([mean_raw_spectrograms[0].flatten(), mean_raw_spectrograms[1].flatten()])\n",
    "            \n",
    "            #Ora qui prendo il miimo e massimo a partire dalla distribuzione congiunta!\n",
    "            #vmin_raw_samples, vmax_raw_samples = all_vals_raw_samples.min(), all_vals_raw_samples.max()\n",
    "    \n",
    "            im = axs[2, j].imshow(log_mean_power[cls], \n",
    "                                  #mean_raw_spectrograms[cls], \n",
    "                                  extent=[0, 1000, 0, 26],\n",
    "                                  aspect='auto', \n",
    "                                  cmap='jet', \n",
    "                                  vmin = vmin_raw_samples, vmax = vmax_raw_samples,\n",
    "                                  origin='lower')\n",
    "            \n",
    "            axs[2, j].set_title(f\"Log-Scaled Mean Raw Spectrogram - Class {condition_names[cls]}\", fontsize=12)\n",
    "            axs[2, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "            axs[2, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "            \n",
    "        \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            ATTENZIONE QUI CHE C'ERA UN GRAVE ERRORE\n",
    "            \n",
    "            --> fig.colorbar(im, ax=axs[3, j]) \n",
    "            \n",
    "            #Qui la Color Bar Verticale sarebbe \n",
    "            #scala dello spettrogramma raw, finita per sbaglio sul Δ-GradCAM perché hai scritto ax=axs[3,j] invece di ax=axs[4,j].\n",
    "            \n",
    "            \n",
    "            La barra VERTICALE (CHE DOVEVA STAR NELLA 5° RIGA!!!!) della color bar accanto alla heatmap ti sta mostrando\n",
    "            \n",
    "            i VALORI ASSOLUTI della Grad-CAM (nel tuo caso non normalizzati, quindi scala di milioni --> variabile hist_data\n",
    "            ossia l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "            '''\n",
    "            fig.colorbar(im, ax=axs[2, j])\n",
    "            \n",
    "            axs[2, j].axis('on')\n",
    "        else:\n",
    "            axs[2, j].axis(\"off\")\n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79c640-cd68-4338-9094-40badf3e7c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''GRADCAM ALGORITHM PER RAPPRESENTAZIONE EEG TEMPO x FREQUENZA NUOVA CON \n",
    "\n",
    "1) gestione del cudrnn per modelli con layer LSTM per far sì che il contesto imposta train() per sbloccare CuDNN-RNN, \n",
    "congela BN/Dropout in eval(), abilita i gradienti e ripristina tutto alla fine.\n",
    "\n",
    "2) integri la gestione del test loader in formato raw per i plot sullo spettogramma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                VERSIONE NUOVA PER RAPPRESENTAZIONE TEMPO x FREQUENZA\n",
    "                                                \n",
    "                                                            (POST 27/06 ULTIMA VERSIONE DATATA)\n",
    "                                                                        \n",
    "                                                                        17/09/2025\n",
    "                                                                    \n",
    "\n",
    "\n",
    "ATTENZIONE CHE QUI LE SHAPE DEI DATI SONO DIVERSE OSSIA\n",
    "\n",
    "1000 mms e 61 CANALI (PROGETTO INTERROGAIT!)\n",
    "\n",
    "quindi il parametro \"extent\" passa in riga 2 e 3 da\n",
    "\n",
    "extent=[0, 4000, 0, 81] a --> extent=[0, 1000, 0, 26]\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def model_has_cudnn_rnn(model):\n",
    "    \"\"\"Ritorna True se il modello usa LSTM/GRU/RNN supportati da CuDNN.\"\"\"\n",
    "    return any(isinstance(m, (nn.LSTM, nn.GRU, nn.RNN)) for m in model.modules())\n",
    "    \n",
    "\n",
    "'''RICORDATI: aggiunto parametro TEST_LOADER_RAW per i plots della POTENZA SPETTRALE MEDIA PER BANDA (i.e., test_loader_raw)'''\n",
    "def compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device):\n",
    "    \n",
    "    \n",
    "    '''SOLO PER I MODELLI OTTIMIZZATI CON ANCHE LA LSTM'''\n",
    "    \n",
    "    #Solo i modelli con LSTM entrano in questo giro; gli altri non cambiano di stato.\n",
    "    #Con questa sequenza:\n",
    "    #non ottieni più l’errore “cudnn RNN backward…”;\n",
    "    #la rete “si comporta” come in eval (Dropout off, BN congelato) mentre calcoli le CAM;\n",
    "    #l’ambiente di chiamata (il tuo loop di testing) riceve il modello esattamente nello stato in cui l’aveva passato alla funzione compute_gradcam_figure\n",
    "    \n",
    "\n",
    "    ### Perché serve model.train() anche se la CAM è presa prima della LSTM\n",
    "    \n",
    "    #Il backward, per arrivare dal loss (o dal logit scelto) fino al tuo layer conv3, deve comunque attraversare l’LSTM che sta più avanti nella rete.\n",
    "    #Le implementazioni CuDNN degli RNN (LSTM/GRU) alzano un’eccezione se provi a chiamare tensor.backward() mentre il modulo è in modalità eval().\n",
    "    #RuntimeError: cudnn RNN backward can only be called in training mode\n",
    "    #Quindi, anche se la CAM è calcolata su conv3, devi mettere l’intero modello in train() per il tempo del backward.\n",
    "    #condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    ### Che cos’è model.training\n",
    "    \n",
    "    #model.training è un semplice flag booleano (impostato da nn.Module.train() / nn.Module.eval()), ereditato da tutti i sotto‑moduli.\n",
    "    #Con was_training = model.training ricordi in che stato era il modello (quasi sempre False, cioè eval, nel tuo flusso)\n",
    "    #per poterlo ripristinare dopo.\n",
    "    \n",
    "    #Facendo così\n",
    "    \n",
    "    #for m in model.modules():\n",
    "    #if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n",
    "                      #nn.Dropout, nn.Dropout2d, nn.Dropout3d)):\n",
    "        #if m.training:         # cioè erano in train\n",
    "            #m.eval()\n",
    "            #frozen_layers.append(m)\n",
    "    \n",
    "    #Li sposti in eval uno per uno, senza toccare il resto della rete che deve restare in train() per far funzionare CuDNN‑RNN.\n",
    "    \n",
    "    \n",
    "    ### Perché, a fine blocco, servono due ripristini\n",
    "    \n",
    "    #1) Riattivo i BatchNorm / Dropout che avevo forzato in eval:\n",
    "    \n",
    "    #for m in frozen_layers:\n",
    "        #m.train()              # torna come prima\n",
    "    \n",
    "    #2) Riporto l’intero modello nello stato in cui si trovava prima del Grad‑CAM:\n",
    "    \n",
    "    #model.train(was_training)  # se era eval() torna eval, altrimenti resta train\n",
    "    \n",
    "    #Se non facessi il punto 1, lasceresti quei moduli permanentemente in eval anche quando, più tardi, \n",
    "    #rientri in training (per esempio in un fine‑tuning).\n",
    "    #Se non facessi il punto 2, lasceresti tutto il modello in train → dropout attivo, BN che accumula statistiche, ecc.\n",
    "\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    # ❶ — se serve, abilito temporaneamente la modalità train per il modello ottimizzato che aveva ANCHE la LSTM... \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    \n",
    "    needs_train_mode = model_has_cudnn_rnn(model)\n",
    "    \n",
    "    if needs_train_mode:\n",
    "        was_training = model.training      # salvo lo stato\n",
    "        model.train()                      # abilito backward su CuDNN‑RNN\n",
    "        \n",
    "        # ➊ salvo lo stato di OGNI BN/Dropout\n",
    "        \n",
    "        saved = [(m, m.training) for m in model.modules()\n",
    "             if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n",
    "                               nn.Dropout, nn.Dropout2d, nn.Dropout3d))]\n",
    "        \n",
    "        model.train()                              # abilita backward su CuDNN‑RNN\n",
    "        \n",
    "        # ➋ congelo in ogni layer della rete gli strati di BatchNorm e Dropout\n",
    "        for m, _ in saved:\n",
    "            m.eval()\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # ❷ — QUI sotto metti tutto il tuo codice Grad‑CAM\n",
    "    #      (forward, backward, costruzione delle mappe, plot, …)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # … il tuo lunghissimo corpo della funzione rimane invariato …\n",
    "    # → al momento di fare backward NON avrà più l’eccezione\n",
    "    #   “cudnn RNN backward can only be called in training mode”\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Determina il target layer in base al tipo di modello\n",
    "    #if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        #target_layer = model.dw_conv1  # Per il modello separabile 2D\n",
    "    #else:\n",
    "        #target_layer = model.conv3  # Per il modello CNN3D\n",
    "    \n",
    "    '''OLD APPROACH'''\n",
    "    #condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    '''NEW APPROACH'''\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Mapping etichette condizioni per i TITOLI dei plot\n",
    "    # -------------------------------\n",
    "    \n",
    "    label_map = {\n",
    "        \"th_resp\": \"obs_resp\",\n",
    "        \"pt_resp\": \"rec_resp\",\n",
    "    }\n",
    "    \n",
    "    def remap_condition_label(s: str) -> str:\n",
    "        for old, new in label_map.items():\n",
    "            s = s.replace(old, new)\n",
    "        return s\n",
    "    \n",
    "    # Rimappa i nomi delle condizioni usati nei titoli dei subplot\n",
    "    condition_names = [remap_condition_label(x) for x in condition_names]\n",
    "    \n",
    "    # Rimappa anche la stringa mostrata nel titolo principale (suptitle)\n",
    "    exp_cond_display = remap_condition_label(exp_cond)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    # ✅ Raccogli TUTTI i campioni per ciascuna classe\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    \n",
    "    '''DATI ORIGINALI DEL TEST LOADER'''\n",
    "    samples = {0: [], 1: []}\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "    \n",
    "    '''TEST_LOADER RAW (DATI NON STANDARDIZZATI)'''\n",
    "    samples_raw = {0: [], 1: []}\n",
    "    for inputs, labels in test_loader_raw:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples_raw:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                samples_raw[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "    \n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        for sample_input in samples[cls]:\n",
    "        \n",
    "        #sample_input = samples[cls]\n",
    "        \n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            TUTTO IL PASSAGGIO DELLO STEP 5 \n",
    "            \n",
    "            OSSIA NORMALIZZAZIONE i.e.,  NEL SENSO DI RISCALATURA NEL RANGE 0-1 + UPSAMPLING \n",
    "            \n",
    "            (CHE SERVIVA PER UNIFORMARE I VALORI E ADATTARSI ALLA DIMENSIONE DELLA IMMAGINE ORIGINALE \n",
    "            PER VEDERE UN SOLO ESEMPIO DELLA CLASSE RISPETTO ALLA MAPPA DI ATTIVAZIONE E ALL'OVERLAY\n",
    "            DEL GRADCAM RISPETTO ALLA IMMAGINE ORIGINALE)\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            \n",
    "            NON SERVE PIU', AD ECCEZIONE DI QUESTE RIGHE CHE ORA TI RIMETTO QUI SOTTO!'''\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            ✅ Cosa fa correttamente questo codice:\n",
    "            \n",
    "            Estrae i campioni da test_loader separandoli in samples[0] e samples[1].\n",
    "            \n",
    "            Per ogni campione di ogni classe:\n",
    "            \n",
    "            Calcola la Grad-CAM raw (senza riscaling),\n",
    "            La interpola per adattarla alla dimensione originale (n_freq, n_time)\n",
    "            Applica ReLU per tenere solo le attivazioni positive (come da standard Grad-CAM)\n",
    "            La converte in NumPy e la salva in cams_list[cls].\n",
    "            \n",
    "            Alla fine, fa la media delle CAM raw per ciascuna classe:\n",
    "            \n",
    "            mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "            \n",
    "            🔍 Stato attuale del dato\n",
    "            \n",
    "            cams_list[cls] → lista di array cam 2D non normalizzati, uno per ogni trial.\n",
    "            mean_cams[cls] → array 2D (frequenza × tempo), media dei trial per ciascuna classe.\n",
    "            \n",
    "            La normalizzazione Z-score congiunta la farai dopo, sulla base di mean_cams.\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            target_size = (sample_input.shape[2:]) # -> (n_freq, n_time)\n",
    "            \n",
    "            cam = F.interpolate(cam.unsqueeze(1), size = target_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # squeeze\n",
    "            cam = cam.squeeze()                 # tensor 2D\n",
    "            \n",
    "            # Infine sposti su CPU e passi a numpy\n",
    "            cam = cam.cpu().numpy()\n",
    "\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa del singolo esempio alla lista per la classe (per poi dopo farci la media dentro mean_cams!)\n",
    "            cams_list[cls].append(cam)\n",
    "            \n",
    "    \n",
    "    # ============================================================\n",
    "    # Calcolo dello heatmap media dei valori (raw) per ciascuna classe\n",
    "    # ============================================================\n",
    "    \n",
    "    mean_cams = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    # Calcolo dello spettrogramma medio (raw) per ciascuna classe\n",
    "    # ============================================================\n",
    "    \n",
    "    mean_raw_spectrograms = {}\n",
    "    for cls in [0, 1]:\n",
    "        if len(samples_raw[cls]) > 0:\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples_raw[cls], dim=0).mean(dim =(0, 1)).detach().cpu().numpy()\n",
    "        else:\n",
    "            mean_raw_spectrograms[cls] = None\n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().cpu().numpy()\n",
    "            '''\n",
    "            🔍 Perché dovrebbe funzionare?\n",
    "            .detach(): Disattiva il tracciamento del gradiente (rende il tensore statico, senza dipendenze dalla computational graph di PyTorch).\n",
    "            .cpu(): Porta il tensore sulla CPU (necessario per numpy()).\n",
    "            .numpy(): Converte il tensore in un array NumPy.\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            1) Calcolo della media degli spettrogrammi (rimozione dei canali)\n",
    "            L'errore \"Invalid shape (3, 26, 11)\" con questa riga commentata sopra ☝️\n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "             \n",
    "            indica che l'array finale ha 3 canali in più (la prima dimensione) che non ti aspetti. \n",
    "            I tuoi dati originali hanno la forma:\n",
    "\n",
    "            (trials, canali, frequenze, tempo)\n",
    "\n",
    "            Se vuoi ottenere una rappresentazione media dello spettrogramma per tutti i trial di una classe, mediando anche sui canali, \n",
    "            allora devi calcolare la media lungo la dimensione dei trial e quella dei canali.\n",
    "            \n",
    "            Dovresti fare:\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0, 1)).detach().cpu().numpy()\n",
    "            \n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "            SPIEGAZIONE:\n",
    "\n",
    "            torch.cat(samples[cls], dim=0)\n",
    "            => Concatena tutti i trial per quella classe lungo la dimensione 0, ottenendo un tensore con forma:\n",
    "            (num_trials, canali, frequenze, tempo).\n",
    "\n",
    "            .mean(dim=(0, 1))\n",
    "            => Calcola la media prima lungo la dimensione dei trial (dim=0) e poi lungo quella dei canali (dim=1) in un'unica operazione, \n",
    "            ottenendo un tensore di forma (frequenze, tempo).\n",
    "\n",
    "            .detach().cpu().numpy()\n",
    "            => Rimuove il tracking del gradiente, sposta il tensore sulla CPU e lo converte in un array NumPy, pronto per imshow.\n",
    "\n",
    "            Questo ti darà l'array 2D (frequenze × tempo) che imshow si aspetta.\n",
    "            \n",
    "            \n",
    "            CHAGPT:\n",
    "            \n",
    "            Nel contesto del tuo esempio:\n",
    "\n",
    "            La forma iniziale dei dati EEG in un formato tempo-frequenza era (batch, canali, frequenze, tempo), che è una matrice 4D. \n",
    "            \n",
    "            Qui, hai un batch di dati, dove ogni dato ha la dimensione dei canali, frequenze, e tempo.\n",
    "\n",
    "            Usando il codice:\n",
    "\n",
    "                mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0,1)).detach().cpu().numpy()\n",
    "                \n",
    "            Stai concatenando lungo la dimensione del batch (dim=0), quindi ottieni una matrice che somma tutte le informazioni sul batch. \n",
    "            Successivamente, con .mean(dim=(0,1)) stai calcolando la media lungo le dimensioni dei canali (0) e del batch (1), \n",
    "            riducendo il risultato a una matrice 2D con la forma (frequenze, tempo), che è quella che desideri, ovvero \n",
    "            \n",
    "            --> la media delle frequenze e del tempo su tutto il batch e i canali.\n",
    "\n",
    "            Quindi sì, la forma risultante di mean_raw_spectrograms[cls] sarà una matrice 2D che rappresenta \n",
    "                1) le frequenze sulle righe e \n",
    "                2) il tempo sulle colonne\n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "\n",
    "            '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # =======================================================\n",
    "    # Passaggio Finale: Creazione della figura finale\n",
    "    # Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Istogramma della distribuzione dei valori della heatmap media per ciascuna classe \n",
    "    #            normalizzata rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 2: GradCAM medio della distribuzione dei valori della heatmap media per ogni classe, \n",
    "    #            a seguito della normalizzazione rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 3: Spettrogramma medio (raw) rispetto ai Trial della Stessa Classe, su range logaritmico \n",
    "    # =======================================================\n",
    "    \n",
    "    \n",
    "    Quando devo plottare l'istogramma dei valori di ogni heatmap media solamente (riga 3), \n",
    "    devo plottarli in base alla normalizzazione rispetto alla distribuzione congiunta.\n",
    "    \n",
    "    Quindi, devo plottarli in base al range minimo e massimo della intera distribuzione congiunta, quando è stata normalizzata!\n",
    "    Di conseguenza devo fare\n",
    "    \n",
    "    1) Prendere la Media delle CAM per ogni classe (già fatto)\n",
    "    2) Costruzione distribuzione congiunta raw\n",
    "    3) Calcolo Media e Deviazione Standard della Distribuzione Congiunta\n",
    "    4) Normalizzazione Z-score della Distribuzione Congiunta\n",
    "    \n",
    "    5) Prendo il range minimo e massimo della Distribuzione Congiunta Normalizzata\n",
    "    \n",
    "    Ossia, il range minimo e massimo su cui plottare entrambe le heatmap medie normalizzate in base alla distribuzione congiunta,\n",
    "    dovrà essere rispetto alla distribuzione congiunta a seguito della normalizzazione.\n",
    "    \n",
    "    Quindi, dovrei ricreare un'altra variabile che contiene i valori normalizzati di entrambe le distribuzioni assieme,\n",
    "    ossia una cosa del tipo\n",
    "    \n",
    "    normalized_all_vals = np.concatenate([normalized_mean_cams[0].flatten(), normalized_mean_cams[1].flatten()])\n",
    "    \n",
    "    e da questa prendere il minimo ed il massimo!\n",
    "    \n",
    "    \n",
    "    '''\n",
    "   \n",
    "\n",
    "    # Creiamo una figura con 4 righe e 2 colonne\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Overlay over EEG Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    #fig, axs = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Creiamo una figura con 3 righe e 2 colonne\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping over EEG Trials\\nExperimental Conditions: {exp_cond}\", fontsize=15)\n",
    "    plt.suptitle(f\"Grad-CAM Mapping over EEG Trials\\nExperimental Conditions: {exp_cond_display}\", fontsize=15)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()  # Regola automaticamente la spaziatura globale\n",
    "    plt.subplots_adjust(hspace = 0.5, wspace = 0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    # PLOT RIGA 1: Visualizzazione degli istogrammi della distribuzione dei valori delle heatmap medie RAW\n",
    "    # RISPETTO ALLA DISTRIBUZIONE CONGIUNTA\n",
    "        \n",
    "    '''\n",
    "    Questi valori rappresentano la distribuzione delle attivazioni di entrambe le classi, \n",
    "    rispetto alla DISTRIBUZIONE CONGIUNTA\n",
    "    \n",
    "    Quindi si tratta di valori di attivazione della mappa Grad-CAM media rispetto alla distribuzione congiunta!\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Istogramma dei valori raw medi di ogni classe (su distribuzione congiunta!): Stai visualizzando un istogramma di questi valori medi, \n",
    "                                                           sulla distribuzione congiunta, ossia\n",
    "                                                           \n",
    "                                                           - prendo i valori (raw)delle heatmap media di entrambe le classi\n",
    "                                                           - calcolare la distribuzione congiunta dei valori (all_vals = ...)\n",
    "                                                           - ottengo quindi la nuova distribuzione congiunta dalle heatmap medie di entrambe le classi\n",
    "                                                          \n",
    "                                                           - calcolo minimo e massimo a seguito della normalizzazione (?) e non prima\n",
    "                                                           - faccio i plot di entrambe delle heatmap medie raw,\n",
    "                                                             ma rispetto a distribuzione congiunta\n",
    "                                                             \n",
    "                                                           \n",
    "                                                           Questo darebbe una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti tra le 2 classi (che ora son confrontabili!)\n",
    "                                                           a livello RAW!\n",
    "                                                        \n",
    "    \n",
    "    N.B. PER IL NOME DEL TITOLO DEL PLOT\n",
    "    \n",
    "    Perché \"Grad-CAM value\" può creare confusione:\n",
    "    Il termine \"Grad-CAM value\" potrebbe sembrare che faccia riferimento direttamente ai valori generati dalla mappa Grad-CAM finale. \n",
    "    Ma in realtà, i valori che stai trattando sono le attivazioni mediate e clippate, che formano la heatmap. \n",
    "    L'istogramma che stai tracciando rappresenta la distribuzione delle attivazioni prima della normalizzazione.\n",
    "\n",
    "    Riepilogo\n",
    "    Quindi, questi valori sono attivazioni pesate per ciascun pixel della mappa Grad-CAM, e mediati per classe. \n",
    "    Il processo di normalizzazione che segue (basato sui percentili) serve a enfatizzare il contrasto in modo da focalizzarsi sulle aree più significative \n",
    "    per la previsione.\n",
    "\n",
    "    Per rispondere alla tua domanda: sì, è corretto dire che stai visualizzando la distribuzione delle attivazioni pesate prima della normalizzazione\n",
    "    per migliorare il contrasto, ma è meglio riferirsi a questi valori come valori di attivazione della mappa Grad-CAM o valori della heatmap Grad-CAM, \n",
    "    piuttosto che \"Grad-CAM value\" che potrebbe risultare ambiguo.\n",
    "\n",
    "    Se vuoi, puoi anche aggiungere una nota nella visualizzazione dell'istogramma che chiarisca il processo:\n",
    "    \n",
    "    axs[2, j].set_title(f\"Histogram of Heatmap Activation Values (Raw, before Normalization) Class {condition_names[cls]}\", fontsize=12)\n",
    "    oppure\n",
    "    axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "    \n",
    "    è molto chiara e corretta!\n",
    "    Indica perfettamente che stai visualizzando l'istogramma dei valori di attivazione medi della heatmap, \n",
    "    senza fare confusione sul fatto che si tratti di valori medi per ciascuna classe.\n",
    "    \n",
    "    In sintesi, questa frase comunica in modo preciso che stai mostrando la distribuzione delle attivazioni mediate dalla mappa Grad-CAM\n",
    "    per una specifica classe. Quindi sì, va benissimo!\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #PER PLOT RIGA 1 \n",
    "    \n",
    "    # Creo la distribuzione congiunta dei valori di ogni heatmap media RAW delle due classi, srotolando i valori di entrambe\n",
    "    all_vals_raw = np.concatenate([mean_cams[0].flatten(), mean_cams[1].flatten()])\n",
    "    \n",
    "    # Il range minimo e massimo su cui plottare entrambe le heatmap medie raw in base alla distribuzione congiunta (riga 3)\n",
    "    # dovrà essere rispetto alla distribuzione congiunta raw\n",
    "    \n",
    "    vmin_raw = all_vals_raw.min()\n",
    "    vmax_raw = all_vals_raw.max()\n",
    "    \n",
    "    \n",
    "    # Prima riga: Visualizza l'istogramma della heatmap media rispetto alla distribuzione congiunta!\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        axs[0, j].hist(mean_cams[cls].flatten(), bins= 'auto', color='blue', edgecolor='black')\n",
    "        #axs[0, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].set_title(f\"Histogram of Mean Heatmap Activation Values (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        axs[0, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # PLOT RIGA 2: Visualizzazione dei valori delle heatmap medie delle due classi\n",
    "    # RISPETTO ALLA DISTRIBUZIONE CONGIUNTA, SU CUI VIENE FATTA LA NORMALIZZAZIONE\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questi valori rappresentano le heatmap medie delle attivazioni di entrambe le classi, \n",
    "    rispetto alla DISTRIBUZIONE CONGIUNTA, SU CUI VIENE FATTA LA NORMALIZZAZIONE\n",
    "    \n",
    "    Quindi si tratta di valori di attivazione della mappa Grad-CAM media rispetto alla distribuzione congiunta NORMALIZZATA\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Calcolo la distribuzione congiunta dei valori raw medi di ogni classe (su distribuzione congiunta!): \n",
    "        Stai visualizzando un istogramma di questi valori medi, sulla DISTRIBUZIONE CONGIUNTA, ossia\n",
    "                                                           \n",
    "                                                           - prendo i valori (raw)delle heatmap media di entrambe le classi\n",
    "                                                           - calcolare la distribuzione congiunta dei valori (all_vals = ...)\n",
    "                                                           - ottengo quindi la nuova distribuzione congiunta dalle heatmap medie di entrambe le classi\n",
    "                                                           \n",
    "                                                           - calcolo media e deviazione standard delle distribuzione congiunta\n",
    "                                                           - faccio la normalizzazione della distribuzione congiunta\n",
    "                                                           \n",
    "                                                           - calcolo minimo e massimo a seguito della normalizzazione e non prima\n",
    "                                                             della distribuzione congiunta normalizzata\n",
    "                                                           \n",
    "                                                           - faccio i plot di entrambe delle heatmap medie normalizzate,\n",
    "                                                             ma rispetto alla distribuzione congiunta\n",
    "                                                             \n",
    "                                                           \n",
    "                                                           Questo darebbe una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti tra le 2 classi (che ora son confrontabili!)\n",
    "                                                           a livello NORMALIZZATO!\n",
    "                                                        \n",
    "    '''\n",
    "    \n",
    "    '''SOPRA ABBIAMO CREATO --> all_vals_raw'''\n",
    "    \n",
    "    # Creo la distribuzione congiunta dei valori di ogni heatmap media RAW delle due classi, srotolando i valori di entrambe\n",
    "    #all_vals_raw = np.concatenate([mean_cams[0].flatten(), mean_cams[1].flatten()])\n",
    "    \n",
    "    #Calcolo media e deviazione standard della distribuzione congiunta dei valori (raw) delle heatmap medie di entrambe le classi \n",
    "    #joint_mean = np.mean(all_vals_raw)\n",
    "    #joint_std = np.std(all_vals_raw)\n",
    "    \n",
    "    # Normalizzazione Z-score della distribuzione congiunta\n",
    "    #normalized_mean_cams = {}\n",
    "    \n",
    "    #for cls in [0, 1]:\n",
    "        #normalized_mean_cams[cls] = (mean_cams[cls] - joint_mean) / joint_std\n",
    "\n",
    "    # Il range minimo e massimo su cui plottare entrambe le heatmap medie normalizzate in base alla distribuzione congiunta (riga 3)\n",
    "    # dovrà essere rispetto alla distribuzione congiunta a seguito della normalizzazione\n",
    "    \n",
    "    #normalized_all_vals = np.concatenate([normalized_mean_cams[0].flatten(), normalized_mean_cams[1].flatten()])\n",
    "    \n",
    "    #vmin_normalized = normalized_all_vals.min()\n",
    "    #vmax_normalized = normalized_all_vals.max()\n",
    "    \n",
    "    vmin_normalized = all_vals_raw.min()\n",
    "    vmax_normalized = all_vals_raw.max()\n",
    "\n",
    "    \n",
    "    '''\n",
    "    # Opzione: normalizzazione robusta con percentili\n",
    "    vmin_normalized, vmax_normalized = np.percentile(all_vals_raw, [5, 95])\n",
    "    '''\n",
    "    \n",
    "    # Seconda riga: Mean heatmap di ogni classe normalizzata a partire dalla distribuzione congiunta ( = di entrambe le classi)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        \n",
    "        im = axs[1, j].imshow(\n",
    "            mean_cams[cls],\n",
    "            #normalized_mean_cams[cls], #QUI LA RENDO IN 2D, NON IN 1D COME PRIMA\n",
    "            #cmap='seismic',\n",
    "            cmap='RdYlBu_r',\n",
    "            vmin= vmin_normalized, vmax= vmax_normalized,\n",
    "            #extent=[0, 4000, 0, 81],\n",
    "            extent=[0, 1000, 0, 26],\n",
    "            aspect='auto',\n",
    "            origin='lower'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # → calcola 6 tick equi-spaziati\n",
    "        ticks = np.linspace(vmin_normalized, vmax_normalized, 6)  \n",
    "        \n",
    "        cbar = fig.colorbar(\n",
    "            im,\n",
    "            ax=axs[1, j],\n",
    "            orientation='horizontal',\n",
    "            pad=0.12,\n",
    "            ticks=ticks)\n",
    "        \n",
    "        cbar.ax.set_xticklabels([f\"{t:.4f}\" for t in ticks])\n",
    "\n",
    "        \n",
    "        axs[1, j].set_title(f\"Mean Grad-CAM Heatmap (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        \n",
    "        '''QUESTA NON CONSENTE DEFINIZIONE ASSI!'''\n",
    "        #axs[1, j].axis('off')\n",
    "        \n",
    "        axs[1, j].axis('on') \n",
    "        axs[1,j].set_xlabel(\"Time (mms)\")\n",
    "        axs[1,j].set_ylabel(\"Frequency (Hz)\")\n",
    "        \n",
    "        #fig.colorbar(im, ax=axs[3, j], orientation='horizontal', pad=0.05)\n",
    "    \n",
    "    print(f\"\\033[1mRange heatmap raw globale (vmin_raw, vmax_raw): {vmin_normalized}, {vmax_normalized}\\033[0m\")\n",
    "    \n",
    "    # PLOT RIGA 3: Spettrogramma medio (raw) per ciascuna classe log-scaled\n",
    "    \n",
    "    '''\n",
    "    Spiegazione delle modifiche aggiunte:\n",
    "\n",
    "    1) Calcolo dello spettrogramma medio raw:\n",
    "\n",
    "    Dopo aver raccolto i campioni nel dizionario samples, viene creato il dizionario mean_raw_spectrograms.\n",
    "    Per ogni classe, i tensori vengono concatenati lungo la dimensione batch e si calcola la media sul batch (dim=0).\n",
    "    \n",
    "    Poi, però, ogni spettogramma medio deve congiunto in una distribuzione in modo da plottare poi il valore dello spettrogramma  \n",
    "    rispetto al minimo ed al massimo della distribuzione congiunta dello spettrogramma medio di entrambe le classi! \n",
    "    \n",
    "    Il risultato viene convertito in un array NumPy per il plotting.\n",
    "    '''\n",
    "    \n",
    "    # Calcolo della distribuzione congiunta degli spettrogrammi medi delle due classi! \n",
    "    #all_vals_raw_samples = np.concatenate([mean_raw_spectrograms[0].flatten(), mean_raw_spectrograms[1].flatten()])\n",
    "    \n",
    "    '''SE VOLESSI RESTRINGERE TRA 5° e 95° PERCENTILE'''\n",
    "    #low_raw, high_raw = np.percentile(all_vals_raw, [5, 95])\n",
    "    #half_width_raw = max(abs(low_raw), abs(high_raw))   \n",
    "    #vmin_raw, vmax_raw = -half_width_raw, +half_width_raw\n",
    "    \n",
    "    '''ALTRIMENTI, TENGO TUTTO IL RANGE, DAL MINIMO AL MASSIMO'''\n",
    "    \n",
    "    #Ora qui prendo il miimo e massimo a partire dalla distribuzione congiunta!\n",
    "    #vmin_raw_samples, vmax_raw_samples = all_vals_raw_samples.min(), all_vals_raw_samples.max()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1) Qual è la differenza tra prima e ora?\n",
    "    \n",
    "    Prima calcolavo, dentro il for cls in [0,1], un nuovo vmin_raw_samples e vmax_raw_samples separatamente per ciascuna classe.\n",
    "    Di conseguenza ogni subplot sulla riga 3 aveva la sua scala di colori, rendendo impossibile un confronto diretto visivo \n",
    "    fra le due condizioni.\n",
    "    \n",
    "    Ora invece calcolerai una sola volta il log-power medio di entrambe le classi, ne ricavi un unico array congiunto,\n",
    "    quindi ne estrai un solo vmin e vmax. Questo ti garantisce che entrambi i subplot della riga 3 useranno la stessa scala di colori.\n",
    "\n",
    "\n",
    "    Per far sì che tutte e due le condizioni usino lo stesso minimo e massimo, sposto la raccolta dei limiti fuori dal ciclo,\n",
    "    usando la distribuzione congiunta dei log-power di entrambe le classi\n",
    "    \n",
    "    vmin_raw_samples e vmax_raw_samples li calcoli una volta sola, su tutti i valori logaritmici concatenati.\n",
    "    Entrambe le mappe usano esattamente lo stesso range, così le barre dei colori saranno allineate.\n",
    "    \n",
    "    Con questa modifica:\n",
    "\n",
    "    log_mean_power contiene già i valori in scala logaritmica.\n",
    "    vmin_raw_samples e vmax_raw_samples sono condivisi fra entrambe le colonne.\n",
    "    Ogni subplot userà la stessa “barretta” di colore, quindi potrai confrontare direttamente “deep blues” e “reds” delle due condizioni.\n",
    "    '''\n",
    "    \n",
    "    # 1. Calcola i log-power medi per ciascuna classe\n",
    "    log_mean_power = {\n",
    "        cls: np.log1p(mean_raw_spectrograms[cls])\n",
    "        for cls in [0,1]\n",
    "    }\n",
    "\n",
    "    # 2. Raccogli TUTTI i valori in un unico array\n",
    "    all_log_vals = np.concatenate([\n",
    "        log_mean_power[0].flatten(),\n",
    "        log_mean_power[1].flatten()\n",
    "    ])\n",
    "\n",
    "    # 3. Estrai un unico vmin/vmax condiviso\n",
    "    vmin_raw_samples = all_log_vals.min()\n",
    "    vmax_raw_samples = all_log_vals.max()\n",
    "\n",
    "        \n",
    "    # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        #if mean_raw_spectrograms[cls] is not None:\n",
    "        if log_mean_power[cls] is not None:    \n",
    "            \n",
    "            #Trasformo in scala logaritmica i miei dati EEG sulla spettro medio di ogni classe\n",
    "            #mean_raw_spectrograms[cls] = np.log1p(mean_raw_spectrograms[cls])\n",
    "        \n",
    "            \n",
    "            # Calcolo della distribuzione congiunta degli spettrogrammi medi delle due classi! \n",
    "            #all_vals_raw_samples = np.concatenate([mean_raw_spectrograms[0].flatten(), mean_raw_spectrograms[1].flatten()])\n",
    "            \n",
    "            #Ora qui prendo il miimo e massimo a partire dalla distribuzione congiunta!\n",
    "            #vmin_raw_samples, vmax_raw_samples = all_vals_raw_samples.min(), all_vals_raw_samples.max()\n",
    "    \n",
    "            im = axs[2, j].imshow(log_mean_power[cls], \n",
    "                                  #mean_raw_spectrograms[cls], \n",
    "                                  #extent=[0, 4000, 0, 81],\n",
    "                                  extent=[0, 1000, 0, 26],\n",
    "                                  aspect='auto', \n",
    "                                  cmap='jet', \n",
    "                                  vmin = vmin_raw_samples, vmax = vmax_raw_samples,\n",
    "                                  origin='lower')\n",
    "            \n",
    "            axs[2, j].set_title(f\"Log-Scaled Mean Raw Spectrogram - Class {condition_names[cls]}\", fontsize=12)\n",
    "            axs[2, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "            axs[2, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "            \n",
    "        \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            ATTENZIONE QUI CHE C'ERA UN GRAVE ERRORE\n",
    "            \n",
    "            --> fig.colorbar(im, ax=axs[3, j]) \n",
    "            \n",
    "            #Qui la Color Bar Verticale sarebbe \n",
    "            #scala dello spettrogramma raw, finita per sbaglio sul Δ-GradCAM perché hai scritto ax=axs[3,j] invece di ax=axs[4,j].\n",
    "            \n",
    "            \n",
    "            La barra VERTICALE (CHE DOVEVA STAR NELLA 5° RIGA!!!!) della color bar accanto alla heatmap ti sta mostrando\n",
    "            \n",
    "            i VALORI ASSOLUTI della Grad-CAM (nel tuo caso non normalizzati, quindi scala di milioni --> variabile hist_data\n",
    "            ossia l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "            '''\n",
    "            fig.colorbar(im, ax=axs[2, j])\n",
    "            \n",
    "            axs[2, j].axis('on')\n",
    "        else:\n",
    "            axs[2, j].axis(\"off\")\n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    # ❸ — Ripristino allo stato precedente il modello ottimizzato trovato migliore, che aveva incluso anche layer LSTM\n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    \n",
    "    if needs_train_mode:\n",
    "        # ➌ ripristino layer singoli (i.e., riporto BN/Dropout dove stavano in eval mode)\n",
    "        for m, old_flag in saved:\n",
    "            m.train(old_flag)\n",
    "        # ➍ ripristino lo stato globale del modello (di nuovo ad .eval())\n",
    "        # i.e.,  come era stato passato in input alla funzione compute_gradcam_figure a partire 'load_best_run_results'!\n",
    "        \n",
    "        #Così simuli l’eval (Dropout off, BN congelato) pur essendo in train() per soddisfare CuDNN‑RNN.\n",
    "        model.train(was_training)\n",
    "        \n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375dd19-f4b1-4e8e-b468-15cde4d67e65",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **MODELLI CNN2D, BiLSTM e Transformer**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec5be0a1-bcbc-43d2-8576-52d1adb3eda0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI\n",
    "'''\n",
    "\n",
    "\n",
    "'''CNN2D\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, canali, altezza, larghezza). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (38)\n",
    "e la \"larghezza\" come le finestre temporali (6).\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, canali, frequenze, tempo)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "'''\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 3 * 26 = 78)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "'''\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "self.embedding = nn.Linear(seq_length, d_model)\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Scelta: In questa implementazione abbiamo deciso di usare il tempo come sequenza.\n",
    "#In alternativa, potresti scegliere i bin di frequenza come sequenza, ma ciò richiederebbe una diversa riorganizzazione delle dimensioni \n",
    "#(ad esempio, un permute diverso).\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=3, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0aeb1562-d77a-4367-95f0-bd59e13bd3ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NUOVI PER P300 FROM 2D TIME-FREQUENCY SIGNAL  - LUGLIO 2025\n",
    "\n",
    "\n",
    "ATTENZIONE CHE ORA RISPETTO A PRIMA (PRE-LUGLIO 2025)\n",
    "\n",
    "\n",
    "\n",
    "Ora però, ragionandoci, potrei inserire dei valori da cui pescare, \n",
    "\n",
    "durante l'ottimizzazione degli iper-parametri della mia rete, che si riferiscono \n",
    "\n",
    "1) a valori di alcuni parametri generale dell'apprendimento delle reti\n",
    "2) a valori dei parametri architetturali di ciascuna delle mie singole reti neurali testate\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN2D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***OLD CNN2D***\n",
    "\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, canali, altezza, larghezza). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (38)\n",
    "e la \"larghezza\" come le finestre temporali (6).\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, canali, frequenze, tempo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''CNN2D CON LE FUNZIONI DI ATTIVAZIONI DINAMICHE (LAYER CONVOLUTIVI + FC1) E DINAMISMO DEI KERNEL SIZE DI CONV e POOL LAYER e STRIDE'''\n",
    "\n",
    "\n",
    "#https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, freq, tempo)\n",
    "        x = self.pool1(self.act_fns[0]( self.bn1(self.conv1(x)) ))\n",
    "        x = self.pool2(self.act_fns[1]( self.bn2(self.conv2(x)) ))\n",
    "        x = self.pool3(self.act_fns[2]( self.bn3(self.conv3(x)) ))\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #x = self.dropout( random.choice(self.act_fns)( self.fc1(x) ) )\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***BILSTM NEW*** \n",
    "\n",
    "\n",
    "Per la rete BiLSTM, bisogna configurare hidden_size e dropout come indicato nel sweep_config, \n",
    "insieme alla possibilità di scegliere se utilizzare o meno la bidirezionalità.\n",
    "\n",
    "\n",
    "1) il valore di hidden_sizes ossia dello spazio di embedding multidimensionale dei miei punti temporali \n",
    "del dato EEG (tutti i valori tra 16 e 32 con step di 2, ossia 16, 18, 20.. e così via)\n",
    "\n",
    "2) la scelta sulla bidirezionalità o meno (True o False)\n",
    "\n",
    "3) il valore di dropout (tra 0.0 e 0.5)\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - BILSTM\n",
    "\n",
    "\n",
    "| Iper-parametro  | Descrizione                                       | Valori possibili                 |\n",
    "| --------------- | ------------------------------------------------- | -------------------------------- |\n",
    "| `hidden_size`   | Dimensione dello stato nascosto per layer LSTM    | `[16, 18, …, 32]` (passo 2)      |\n",
    "| `bidirectional` | Se usare LSTM bidirezionale (0 → False, 1 → True) | `[0, 1]`                         |\n",
    "| **+ comune**    | `dropout`                                         | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***BILSTM OLD***\n",
    "\n",
    "\n",
    "\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 3 * 26 = 78)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''BILSTM HIDDEN SIZE, DROPOUT e BIDIREZIONALITA' DINAMICI\n",
    "\n",
    "Ecco come potresti adattare la tua BiLSTM “1D” in modo che lavori sul “2D” \n",
    "(canali×frequenze come feature per ciascun time‑step),\n",
    "mantenendo esattamente lo stesso schema iper‑parametrico che stai usando nello sweep:\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # invece di passare input_size in unità temporali, lo passi già come canali*frequenze\n",
    "        input_size:   int,   # = num_channels * num_freqs\n",
    "        hidden_size:  int,\n",
    "        output_size:  int,\n",
    "        num_layers:   int,   # --> ricordati qui è 3!\n",
    "        dropout:      float,\n",
    "        \n",
    "        # da sweep: bidirectional come 0/1\n",
    "        bidirectional: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bool(bidirectional)\n",
    "        \n",
    "        nd = 2 if self.bidirectional else 1\n",
    "        \n",
    "        # costruisci la lista degli hidden sizes\n",
    "        #base_sizes = [hidden_size] * num_layers\n",
    "        \n",
    "        #self.hidden_sizes = [\n",
    "            #h * (2 if self.bidirectional else 1)\n",
    "            #for h in base_sizes\n",
    "        #]\n",
    "        \n",
    "        # 1) hidden_size “per direzione” (non raddoppiato qui)\n",
    "        self.hidden_sizes = [hidden_size] * num_layers  # [24,24,24]\n",
    "        \n",
    "        # primo LSTM\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_sizes[0],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        # secondo LSTM\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[0]* nd,\n",
    "            hidden_size=self.hidden_sizes[1],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        # terzo LSTM\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=self.hidden_sizes[1]* nd,\n",
    "            hidden_size=self.hidden_sizes[2],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # fully‑connected sullo stato finale\n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_sizes[2]* nd,\n",
    "            output_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x ha shape (batch, canali, frequenze, tempo)\n",
    "        # 1) permuta per avere tempo come seq‑dim\n",
    "        #    (batch, tempo, canali, freq)\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        b, t, c, f = x.shape\n",
    "        \n",
    "        # 2) appiattisci canali×freq in feature vector\n",
    "        x = x.reshape(b, t, c * f)  # -> (batch, tempo, input_size)\n",
    "        \n",
    "        # 3) scorri gli LSTM + dropout\n",
    "        out, _ = self.lstm1(x); out = self.dropout(out)\n",
    "        out, _ = self.lstm2(out); out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out); out = self.dropout(out)\n",
    "        \n",
    "        # 4) prendi l'ultimo time‑step\n",
    "        out = out[:, -1, :]        # (batch, hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)        # (batch, output_size)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***TRANSFORMER NEW***\n",
    "                                                                \n",
    "Per il Transformer varieremo                                                        \n",
    "\n",
    "1) il valore dell'embedding, ossia \"d_model\" (con valori tra 8 e 64 con step di 8)\n",
    "2) il valore di head attenzionali, ossia \"num_heads\" (con valori tra 2 e 12 con step di 2) \n",
    "3) il valore di fully connected layers (con valori tra 1 e 3) dell\n",
    "\n",
    "4) il valore del feed_forward multiplier: descrive esattamente il suo ruolo, ossia è un moltiplicatore (mult) applicato\n",
    "alla dimensione del modello (d_model)per fissare l’ampiezza dell’FFN\n",
    "Ossia, il fattore con cui moltiplichi il tuo d_model per ottenere la dimensione interna del blocco feed-forward nel Transformer!\n",
    "\n",
    "In pratica, lo sweep esplora solo due moltiplicatori [2,4] invece di decine di valori hard-coded.\n",
    "Il modello transformer calcola internamente ogni run il corretto dim_feedforward = ff_mult * d_model.\n",
    "        \n",
    "5) il valore (stringa) della funzione di attivazione del layer fully connected (tra relu e gelu)\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "TABELLA FINALE RIASSUNTIVA - TRANSFORMER\n",
    "\n",
    "\n",
    "| Iper-parametro            | Descrizione                                                     | Valori possibili                 |\n",
    "| ------------------------- | --------------------------------------------------------------- | -------------------------------- |\n",
    "| `d_model`                 | Dimensione dell’embedding (modello)                             | `[8, 16, 24, …, 64]` (step 8)    |\n",
    "| `num_heads`               | Numero di teste di attenzione                                   | `[2, 4, 6, 8]`                   |\n",
    "| `num_layers`              | Numero di blocchi encoder                                       | `[1, 2, 3]`                      |\n",
    "| `ff_mult`                 | Moltiplicatore per la dimensione interna del feed-forward (FFN) | `[2, 4]`                         |\n",
    "| `transformer_activations` | Funzione di attivazione nel layer FFN                           | `[\"relu\", \"gelu\"]`               |\n",
    "| **+ comune**              | `dropout`                                                       | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    " \n",
    "                                                                ***TRANSFORMER OLD***\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "--> self.embedding = nn.Linear(seq_length, d_model)\n",
    "\n",
    "\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "--> self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "\n",
    "- Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "- Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=3, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Ecco una versione “2D” del tuo Transformer, \n",
    "che mantiene esattamente gli stessi iper‑parametri (d_model, num_heads, num_layers, ff_mult, dropout, transformer_activations)\n",
    "\n",
    "ma lavora su input di shape (batch, canali, frequenze, tempo).\n",
    "\n",
    "\n",
    "                                                                    ***TRANSFORMER PENULTIMO OLD***\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        num_channels: int,\n",
    "        num_freqs: int,\n",
    "        \n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        \n",
    "        num_layers: int,\n",
    "        num_classes: int,\n",
    "        \n",
    "        ff_mult: int, #<–– nuovo: moltiplicatore per ottenere dimensione del feedforward layer Transformer\n",
    "        dropout: float,\n",
    "        transformer_activations: str,  # \"relu\" o \"gelu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1) layer di embedding: da (channels*freqs) a d_model\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        # self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        self.embedding = nn.Linear(num_channels * num_freqs, d_model)\n",
    "        \n",
    "        # 2) Feed‑forward interno al Fully Connected Layer dell'Encoder Layer del Transformer\n",
    "        dim_feedforward = ff_mult * d_model\n",
    "\n",
    "        # Costruiamo un singolo encoder layer riusabile\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = d_model,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout,\n",
    "            activation = transformer_activations\n",
    "        )\n",
    "        # 3) due encoder: spaziale e temporale\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer  = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 4) # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        \n",
    "        # 5) # Fusione e classificazione finale\n",
    "        self.fc_fusion   = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # → permutiamo per avere il tempo come sequenza\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2) # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "\n",
    "        # → appiattiamo canali×freqs in feature vector\n",
    "        #    (batch, tempo, c*f)\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "\n",
    "        # 1) embedding → (batch, tempo, d_model)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "    \n",
    "        # 2) Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)  # → (tempo, batch, d_model)\n",
    "\n",
    "        # 3) Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # 4) Applichiamo il Transformer per l'attenzione temporale\n",
    "        \n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # 5) Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "\n",
    "        # 6) Fusione: per esempio, facciamo una media sul tempo (dimensione 0) \n",
    "        #    (tempo, batch, d_model) → media su dim 0 → (batch, d_model)\n",
    "        \n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0)) # -> (batch, d_model)\n",
    "\n",
    "        # 7) classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Versione 2D del tuo Transformer, ReadMYMind, che tratta i dati EEG in forma di spettrogramma (batch, canali, frequenze, tempo):\n",
    "\n",
    "Ramo temporale: \n",
    "\n",
    "Token length = T (time steps).\n",
    "Feature vector per token = C × F (canali × frequenze).\n",
    "\n",
    "Ramo frequenziale: \n",
    "\n",
    "Token length = F (frequenze).\n",
    "Feature vector per token = C × T (canali × time steps).\n",
    "\n",
    "Cross‑attention fra le due viste, somma e media sui time steps, e infine due FC per fusione e classificazione.\n",
    "\n",
    "\n",
    "\n",
    "L'architettura che hai postato per ReadMYMind si adatta perfettamente a una rappresentazione 2D time–frequency dei tuoi dati EEG. In breve:\n",
    "\n",
    "Due branche di embedding e encoding\n",
    "\n",
    "Temporal branch: tratta la sequenza lungo la dimensione tempo (T token), \n",
    "mappando per ciascun time‑step l’intero vettore “canali × frequenze” in uno spazio di dimensione d_model.\n",
    "\n",
    "Frequency branch: analogamente, tratti i token lungo la dimensione frequenza (F token), \n",
    "mappando per ciascun bin spettrale l’intero vettore “canali × time‑step” in d_model.\n",
    "\n",
    "TransformerEncoder separati\n",
    "\n",
    "Entrambi i transformer encoder hanno la stessa configurazione (d_model, nhead, dim_feedforward), \n",
    "ma uno gira su sequenze lunghe T e l’altro su sequenze lunghe F. \n",
    "\n",
    "In questo modo impari pattern temporali e pattern spettrali in parallelo.\n",
    "\n",
    "Cross‑attention\n",
    "\n",
    "Usi l’output temporale (xt) come query e l’output spettrale (xf) come key/value.\n",
    "In pratica, per ogni time‑step il modello “chiede” quali frequenze (e in quale misura) sono rilevanti, fondendo le due viste.\n",
    "\n",
    "Fusione e classificazione\n",
    "\n",
    "Sommi element‑wise xt + x_cross, fai la media sui T token (riassumendo l’intera sequenza temporale) e infine\n",
    "passi attraverso due layer lineari (fc_fusion e fc_classify).\n",
    "\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "        \n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        num_channels: int,\n",
    "        num_freqs: int,\n",
    "        seq_length: int,\n",
    "        \n",
    "        # sweep params:\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        \n",
    "        num_layers: int,\n",
    "        num_classes: int,\n",
    "        \n",
    "        ff_mult: int, #<–– nuovo: moltiplicatore per ottenere dimensione del feedforward layer Transformer\n",
    "        dropout: float,\n",
    "        transformer_activations: str, # \"relu\" o \"gelu\"\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_feedforward = ff_mult * d_model\n",
    "        \n",
    "        # temporal embedding: features=C*F at each time-step\n",
    "        self.embedding_temporal = nn.Linear(num_channels * num_freqs, d_model)\n",
    "        \n",
    "        # frequency embedding: features=C*T at each frequency\n",
    "        self.embedding_frequency = nn.Linear(num_channels * seq_length, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=transformer_activations\n",
    "        )\n",
    "        # Transformer over time tokens\n",
    "        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Transformer over frequency tokens\n",
    "        self.frequency_transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, F, T)\n",
    "        B, C, F, T = x.shape\n",
    "        \n",
    "        # temporal branch: tokens over T\n",
    "        xt = x.permute(0, 3, 1, 2).reshape(B, T, C * F)  # (B, T, C*F)\n",
    "        xt = self.embedding_temporal(xt)                  # (B, T, d_model)\n",
    "        xt = xt.permute(1, 0, 2)                          # (T, B, d_model)\n",
    "        xt = self.temporal_transformer(xt)                # (T, B, d_model)\n",
    "        \n",
    "        # frequency branch: tokens over F\n",
    "        xf = x.permute(0, 2, 1, 3).reshape(B, F, C * T)   # (B, F, C*T)\n",
    "        xf = self.embedding_frequency(xf)                 # (B, F, d_model)\n",
    "        xf = xf.permute(1, 0, 2)                          # (F, B, d_model)\n",
    "        xf = self.frequency_transformer(xf)               # (F, B, d_model)\n",
    "        \n",
    "        # cross-attention: query=temporal, key/value=frequency\n",
    "        x_cross, _ = self.cross_attention(xt, xf, xf)     # (T, B, d_model)\n",
    "        \n",
    "        # fuse and classify: sum+mean over time\n",
    "        x_fused = (xt + x_cross).mean(dim=0)              # (B, d_model)\n",
    "        x_fused = self.fc_fusion(x_fused)                 # (B, d_model)\n",
    "        out = self.fc_classify(x_fused)                   # (B, num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d99d2-3abb-432d-ac96-8868822e402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NUOVI PER P300 FROM 2D TIME-FREQUENCY SIGNAL  - AGGIORNATI A SETTEMBRE 2025 COME QUELLI DEL TASK MOTORIO!\n",
    "\n",
    "\n",
    "                                                                ***CNN2D_LSTM_TF*** \n",
    "\n",
    "\n",
    "Uso la stessa rete neurale usata per Brain Decoding Task Motorio\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN2D_LSTM_TF(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels=61, num_classes=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # --- Block 1 ---\n",
    "        self.bn1   = nn.BatchNorm2d(input_channels)    # normalizza 64 canali\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # --- Block 2 (residual) ---\n",
    "        # Proiezione 1×1 per riallineare i canali di skip (32→64)\n",
    "        self.res_conv = nn.Conv2d(32, 64, kernel_size=1, bias=False)\n",
    "        self.res_bn   = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a   = nn.BatchNorm2d(32)\n",
    "        self.conv2a = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2b   = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(128)\n",
    "\n",
    "        # --- Head: Dropout + LSTM + FC finale ---\n",
    "        self.dropout     = nn.Dropout(dropout)\n",
    "        self.hidden_size = 64\n",
    "        \n",
    "        # dopo 3 pool: freq da 81→10, time da 9→1 → feature per timestep = 128×1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,64,81,9)\n",
    "\n",
    "        # --- Block 1 ---\n",
    "        x = self.bn1(x)                   # → (B,64,81,9)\n",
    "        x = F.relu(self.conv1(x))         # → (B,32,81,9)\n",
    "        x = self.pool(x)                  # → (B,32,40,4)\n",
    "\n",
    "        # --- Block 2 (residuo) ---\n",
    "        res = x                           # skip: (B,32,40,4)\n",
    "        res = self.res_conv(res)          # progetto: → (B,64,40,4)\n",
    "        res = self.res_bn(res)            # → (B,64,40,4)\n",
    "\n",
    "        # main path\n",
    "        x = self.bn2a(x)                  # → (B,32,40,4)\n",
    "        x = F.relu(self.conv2a(x))        # → (B,64,40,4)\n",
    "        x = self.bn2b(x)                  # → (B,64,40,4)\n",
    "        x = self.conv2b(x)                # → (B,64,40,4)\n",
    "\n",
    "        x = x + res                       # somma residua valida → (B,64,40,4)\n",
    "        x = F.relu(x)                     \n",
    "        x = self.pool(x)                  # → (B,64,20,2)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # → (B,128,20,2)\n",
    "        x = self.pool(x)                     # → (B,128,10,1)\n",
    "\n",
    "        # --- Prepara per LSTM ---\n",
    "        x = x.permute(0, 2, 1, 3)         # → (B,10,128,1)\n",
    "        b, seq, ch, tw = x.size()        \n",
    "        x = x.reshape(b, seq, ch * tw)    # → (B,10,128)\n",
    "\n",
    "        # --- LSTM + classificazione ---\n",
    "        out, _ = self.lstm(self.dropout(x))  # → out: (B,10,64)\n",
    "        last = out[:, -1, :]                 # prendo l’ultima uscita → (B,64)\n",
    "        logits = self.classifier(last)       # → (B,2)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "'''\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "'''\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 64 * 81 = 7471)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "'''\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "self.embedding = nn.Linear(seq_length, d_model)\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Scelta: In questa implementazione abbiamo deciso di usare il tempo come sequenza.\n",
    "#In alternativa, potresti scegliere i bin di frequenza come sequenza, ma ciò richiederebbe una diversa riorganizzazione delle dimensioni \n",
    "#(ad esempio, un permute diverso).\n",
    "\n",
    "\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=61, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5917fc-4969-4faf-b1f7-8995faaeadad",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **NUOVO LOOP PER DATI NON HYPER SU CNN2D, BiLSTM e Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6c2da-209d-4d56-a62c-1538c9e0b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import random\n",
    "#perché è importante numpy.random.seed()?\n",
    "#https://www.analyticsvidhya.com/blog/2021/12/what-does-numpy-random-seed-do/#:~:text=The%20numpy%20random%20seed%20is,displays%20the%20same%20random%20numbers.\n",
    "from numpy.random import seed\n",
    "\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "#importing librerie pytorch\n",
    "import torch \n",
    "import torch.nn as nn #neural network module\n",
    "import torch.optim as optim #ottimizzatore\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "#importing librerie numpy, pandas, scikit-learn e matplotlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e62c8-533b-471a-95a6-d98d08aa24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOP PER CARICARE I DATI NON HYPER\n",
    "data_dict = {}\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "\n",
    "    for data_type in [\"spectrograms\"]:\n",
    "        \n",
    "        for category in [\"familiar\", \"unfamiliar\"]:\n",
    "            \n",
    "            for subject_type in [\"th\", \"pt\"]:\n",
    "            \n",
    "                # Caricamento e suddivisione dei dati\n",
    "                if data_type == \"wavelet\":\n",
    "                    X, y = load_data(data_type, category, subject_type, wavelet_level=\"delta\")\n",
    "                else:\n",
    "                    X, y = load_data(data_type, category, subject_type)\n",
    "\n",
    "                #key = f\"{condition}/{data_type}_{category}_{subject_type}\"\n",
    "                key = f\"{condition}_{data_type}_{category}_{subject_type}\"\n",
    "                data_dict[key] = (X, y)\n",
    "\n",
    "                # Stampa di conferma\n",
    "                print(f\"Dataset caricato: \\033[1m{key}\\033[0m - Forma X: {X.shape}, Lunghezza y: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f5aaf-2faa-4398-a4b8-6a09eaaafc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766db1e-6b99-42f7-a960-4b37e986798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perfetto ora, siccome ho creato data_dict nel modo di cui sopra, \n",
    "ora dentro ogni chiave, \n",
    "ci sono già tutte le chiavi associate correttamente, per estrarmi i dati e labels corrispondenti di quella combinazione di fattori lì.\n",
    "\n",
    "infatti dentro ogni chiave c'è una tupla, con 2 elementi, il primo è l'array dei dati, il secondo è l'array delle labels\n",
    "'''\n",
    "\n",
    "data_dict['th_resp_vs_pt_resp_spectrograms_familiar_th'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44797dc1-087c-4a3a-92ac-99c210a6d97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''NEW VERSION'''\n",
    "\n",
    "\n",
    "'''ATTENZIONE CHE QUI HO AGGIUNTO --> \"_time_frequency_\" alla base_dir!'''\n",
    "\n",
    "\n",
    "# Percorso base per il salvataggio\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_time_frequency_best_models_post_WB_GradCAM_Checks\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Tipologie di dati\n",
    "data_types = [\"spectrograms\"]\n",
    "\n",
    "# Subfolders per tipologia di soggetto\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "# Creazione della struttura delle cartelle\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "            \n",
    "            print(f\"Cartella creata: \\033[1m{path}\\033[0m\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c903509d-0387-4e3f-9d49-046aacec1cf7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        # Ottimizzatore\n",
    "        \"lr\":            {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":         {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "        \"beta2\":         {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "        \"eps\":           {\"values\": [1e-8, 1e-7, 1e-6, 1e-5]},\n",
    "\n",
    "        # Training\n",
    "        \"n_epochs\":      {\"value\": 100},\n",
    "        \"patience\":      {\"value\": 12},\n",
    "\n",
    "        # Scelta del modello\n",
    "        \"model_name\":    {\"values\": [\"CNN2D\", \"BiLSTM\", \"Transformer\"]},\n",
    "\n",
    "        # Dati e regolarizzazione generale\n",
    "        \"batch_size\":    {\"values\": [16, 24, 32, 48, 52, 64, 72, 84, 96]},\n",
    "        \"standardization\":{\"values\":[True, False]},\n",
    "        \n",
    "        # --- CNN1D solo quando model_name==\"CNN2D\" ---\n",
    "        \"conv_out_channels\":{\"values\":[16,20,24,28,32]},\n",
    "\n",
    "        \"conv_k1_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k1_w\":{\"values\":[3,5,7,9]},\n",
    "        \n",
    "        \"conv_k2_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k2_w\":{\"values\":[3,5,7,9]},\n",
    "        \n",
    "        \"conv_k3_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k3_w\":{\"values\":[3,5,7,9]},\n",
    "\n",
    "        \"conv_s1_h\":{\"values\":[1,2]},\n",
    "        \"conv_s1_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"conv_s2_h\":{\"values\":[1,2]},\n",
    "        \"conv_s2_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"conv_s3_h\":{\"values\":[1,2]},\n",
    "        \"conv_s3_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p1_h\":{\"values\":[1,2]},\n",
    "        \"pool_p1_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p2_h\":{\"values\":[1,2]},\n",
    "        \"pool_p2_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        #\"pool_p3_h\":{\"values\":[1,2]},\n",
    "        #\"pool_p3_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p3_h\":{\"values\":[1]},\n",
    "        \"pool_p3_w\":{\"values\":[1]},\n",
    "\n",
    "        \"pool_type\":{\"values\":[\"max\",\"avg\"]},\n",
    "        \"fc1_units\":{\"values\":[8,10,12,14,16]},\n",
    "\n",
    "        \"cnn_act1\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \"cnn_act2\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \"cnn_act3\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \n",
    "        \n",
    "        # --- BiLSTM solo quando model_name==\"BiLSTM\" ---\n",
    "        \"hidden_size\":{\"values\":list(range(16,34,2))},\n",
    "        \"bidirectional\":{\"values\":[0,1]},\n",
    "\n",
    "        # --- Transformer solo quando model_name==\"Transformer\" ---\n",
    "        \"d_model\":{\"values\":list(range(8,65,8))},\n",
    "        #\"num_heads\":{\"values\":[2,4,6,8,10,12]},\n",
    "        \n",
    "        \"num_heads\":{\"values\":[2,4,8]}, # solo divisori di tutti i d_model\n",
    "        \n",
    "        \"num_layers\":{\"values\":[1,2,3]},\n",
    "        \"ff_mult\":{\"values\":[2,4]},\n",
    "        \"transformer_activations\":{\"values\":[\"relu\",\"gelu\"]},\n",
    "\n",
    "        # comune\n",
    "        \"dropout\":{\"values\":[0.0,0.1,0.2,0.3,0.4,0.5]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866445c-3553-4a91-9cf1-3b0f9faceb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VERSIONE NUOVA UFFICIALE\n",
    "\n",
    "\n",
    "Ecco come puoi correggere solo il calcolo dell’AUC–ROC sul training set a posteriori, \n",
    "lasciando invariato tutto il resto di load_best_run_results. \n",
    "\n",
    "\n",
    "L’idea è:\n",
    "\n",
    "1) Estrarre la history normale da W&B (che contiene il vecchio train_auc)\n",
    "2) Individuare best_epoch\n",
    "3) Caricare il modello migliore da disco\n",
    "4) Rifare un passaggio solo sullo train_loader per ottenere le vere probabilità e ricalcolare la ROC–AUC\n",
    "5) Sovrascrivere il vecchio valore auc_train_history[best_epoch] e aggiornare best_metrics[\"train_auc\"]\n",
    "\n",
    "\n",
    "\n",
    "Cosa è cambiato\n",
    "\n",
    "1) Ti ho inserito un passaggio 6) in cui ricalcoli l’AUC–ROC vero del train set, usando torch.softmax(…,dim=1)[:,1].\n",
    "2) Sostituisci il vecchio auc_train_history[best_epoch] col valore corretto.\n",
    "3) Ricomponi best_metrics[\"train_auc\"] con true_auc_train.\n",
    "\n",
    "Da qui in poi, puoi chiamare subito dopo la tua testing(...) per ottenere anche tutte le metriche sul test set e salvare la tabella finale in cui:\n",
    "\n",
    "“Train” = best_metrics[\"train_*\"] (ora con AUC corretta)\n",
    "\n",
    "“Test” = test_results[\"test_performances\"]\n",
    "\n",
    "Ecco fatto: nessun re‑training, solo un passaggio aggiuntivo per correggere il calcolo dell’AUC–ROC sul train set.\n",
    "\n",
    "\n",
    "\n",
    "Quindi il punto 6\n",
    "\n",
    "# --- 6) Ricalcolo vero train AUC–ROC sul train_loader ---\n",
    "\n",
    "serve per ri-calcolarsi correttamente l'auc roc al train set nell'epoca in cui sul val set ho ottenuto la migliore validation accuracy, \n",
    "che corrisponde quindi al modello salvato dentro il best_model che io ri-prelevo quando poi lo do in pasto al test set?\n",
    "\n",
    "\n",
    "Esattamente: quel passaggio 6):\n",
    "\n",
    "Riprende il modello caricato dal file .pkl (che è proprio il best_model scelto sull’epoca di miglior val_accuracy),\n",
    "\n",
    "Lo mette in eval() e senza gradienti scorre tutto il train_loader,\n",
    "\n",
    "Calcola le probabilità (softmax(:,1)) e da quelle ricava la vera ROC–AUC per il train set,\n",
    "\n",
    "Infine sovrascrive auc_train_history[best_epoch] e aggiorna best_metrics[\"train_auc\"] con questo valore corretto.\n",
    "\n",
    "In questo modo la tua colonna “Train” nella tabella conterrà davvero l’AUC–ROC calcolata sulle probabilità del modello nella stessa epoca \n",
    "in cui hai ottenuto la migliore validazione, cioè esattamente quei pesi che poi passerai al test set.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from wandb import Api\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "1) questa serve per plottare le metriche di loss e accuracy in ogni modello e condizione sperimentale\n",
    "per salvarla dentro al dizionario 'training_plot' come buffer di memoria\n",
    "'''\n",
    "\n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    '''\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    #Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    \n",
    "    #plt.close(fig)\n",
    "    \n",
    "    '''\n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    '''\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "    \n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "'''\n",
    "2) questa serve per estrarmi le stringhe per ricostruire il nome del progetto su W&B per \n",
    "poi estrarmi le metriche ottenute sul training e validation \n",
    "da salvare sempre dentro al dizionario 'training_plot' \n",
    "'''\n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"spectrograms\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "\n",
    "\n",
    "'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "# Test\n",
    "#combination_key = \"rest_vs_left_fist_spectrograms_familiar_th\"\n",
    "#condition_experiment, data_type, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "#print(\"Condizione:\", condition_experiment)\n",
    "#print(\"Data Type:\", data_type)\n",
    "#print(\"Soggetto:\", subject_key)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Ecco come puoi correggere solo il calcolo dell’AUC–ROC sul training set a posteriori, lasciando invariato tutto il resto di load_best_run_results. \n",
    "\n",
    "\n",
    "L’idea è:\n",
    "\n",
    "1) Estrarre la history normale da W&B (che contiene il vecchio train_auc)\n",
    "2) Individuare best_epoch\n",
    "3) Caricare il modello migliore da disco\n",
    "4) Rifare un passaggio solo sullo train_loader per ottenere le vere probabilità e ricalcolare la ROC–AUC\n",
    "5) Sovrascrivere il vecchio valore auc_train_history[best_epoch] e aggiornare best_metrics[\"train_auc\"]\n",
    "\n",
    "\n",
    "\n",
    "Cosa è cambiato\n",
    "\n",
    "1) Ti ho inserito un passaggio 6) in cui ricalcoli l’AUC–ROC vero del train set, usando torch.softmax(…,dim=1)[:,1].\n",
    "2) Sostituisci il vecchio auc_train_history[best_epoch] col valore corretto.\n",
    "3) Ricomponi best_metrics[\"train_auc\"] con true_auc_train.\n",
    "\n",
    "Da qui in poi, puoi chiamare subito dopo la tua testing(...) per ottenere anche tutte le metriche sul test set e salvare la tabella finale in cui:\n",
    "\n",
    "“Train” = best_metrics[\"train_*\"] (ora con AUC corretta)\n",
    "\n",
    "“Test” = test_results[\"test_performances\"]\n",
    "\n",
    "Ecco fatto: nessun re‑training, solo un passaggio aggiuntivo per correggere il calcolo dell’AUC–ROC sul train set.\n",
    "\n",
    "\n",
    "\n",
    "Quindi il punto 6\n",
    "\n",
    "# --- 6) Ricalcolo vero train AUC–ROC sul train_loader ---\n",
    "\n",
    "serve per ri-calcolarsi correttamente l'auc roc al train set nell'epoca in cui sul val set ho ottenuto la migliore validation accuracy, \n",
    "che corrisponde quindi al modello salvato dentro il best_model che io ri-prelevo quando poi lo do in pasto al test set?\n",
    "\n",
    "\n",
    "Esattamente: quel passaggio 6):\n",
    "\n",
    "Riprende il modello caricato dal file .pkl (che è proprio il best_model scelto sull’epoca di miglior val_accuracy),\n",
    "\n",
    "Lo mette in eval() e senza gradienti scorre tutto il train_loader,\n",
    "\n",
    "Calcola le probabilità (softmax(:,1)) e da quelle ricava la vera ROC–AUC per il train set,\n",
    "\n",
    "Infine sovrascrive auc_train_history[best_epoch] e aggiorna best_metrics[\"train_auc\"] con questo valore corretto.\n",
    "\n",
    "In questo modo la tua colonna “Train” nella tabella conterrà davvero l’AUC–ROC calcolata sulle probabilità del modello nella stessa epoca \n",
    "in cui hai ottenuto la migliore validazione, cioè esattamente quei pesi che poi passerai al test set.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "3) Dopodiché, comincia la funzione di load_best_run_results che, \n",
    "per ogni progetto e sweep del relativo modello,\n",
    "\n",
    "si va ad estrarre le metriche del train (corregge il calcolo del train_auc)\n",
    "e si calcola anche per il validation phase la confusion matrix e classification report\n",
    "\n",
    "\n",
    "4) dopodichè dovrebbe richiamare la funzione di \n",
    "\"plot_training_results\" in modo che poi si salvi i plot di training e validation (sia loss che accuracy)\n",
    "in modo che si salvi tutto in una immagine come buffer che viene spuntato fuori da quella funzione \n",
    "\n",
    "e poi inserito come valore dentro al dizionario training_results che sarà l'output di \"load_best_run_results\" \n",
    "\n",
    "\n",
    "quindi qui sotto mi manca richiamare la funzione \"plot_training_results\" con una variabile tipo training_plot = plot_training_results che avrà come argomenti\n",
    "\n",
    "queste liste qua salvate come colonne del df creato dentro a 'load_best_run_results!'\n",
    "\n",
    "\n",
    "loss_train_history     = df[\"train_loss\"].tolist()\n",
    "loss_val_history       = df[\"val_loss\"].tolist()\n",
    "accuracy_train_history = df[\"train_accuracy\"].tolist()\n",
    "accuracy_val_history   = df[\"val_accuracy\"].tolist()\n",
    "\n",
    "\n",
    "5) dopodiché mi serve caricare tutte queste info dentro al dizionario train_results, che sarà l'output di load_best_run_results... \n",
    "e su questo ho dei dubbi su quali chiavi del dizionario tenere separate oppure se \"unirne\" qualcuna, aggregando tutte le info del sweep_config assieme, \n",
    "sia che siano veri iper-parametri (learning rate etc) o parametri architetturali della rete (anche se avevano valori fissi) il più delle volte se vedi\n",
    "\n",
    "\n",
    "\n",
    "# --- CNN1D solo quando model_name==\"CNN1D\" ---\n",
    "\n",
    "sweep_config_cnn1d = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN1D\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "        \n",
    "        \"conv_out_channels\":{\"values\":[16]},\n",
    "\n",
    "        \"conv_k1\":{\"values\":[3]},\n",
    "        \"conv_k2\":{\"values\":[3]},\n",
    "        \"conv_k3\":{\"values\":[3]},\n",
    "\n",
    "        \"conv_s1\":{\"values\":[1]},\n",
    "        \"conv_s2\":{\"values\":[1]},\n",
    "        \"conv_s3\":{\"values\":[1]},\n",
    "\n",
    "        \"pool_p1\":{\"values\":[1]},\n",
    "        \"pool_p2\":{\"values\":[1]},\n",
    "        \"pool_p3\":{\"values\":[1]},\n",
    "\n",
    "        \"pool_type\":{\"values\":[\"avg\"]},\n",
    "        \"fc1_units\":{\"values\":[8]},\n",
    "\n",
    "        \"cnn_act1\":{\"values\":[\"relu\"]},\n",
    "        \"cnn_act2\":{\"values\":[\"relu\"]},\n",
    "        \"cnn_act3\":{\"values\":[\"relu\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# --- BiLSTM solo quando model_name==\"BiLSTM\" ---\n",
    "\n",
    "sweep_config_bilstm = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"BiLSTM\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "\n",
    "        # --- BiLSTM solo quando model_name==\"BiLSTM\" ---\n",
    "        \"hidden_size\":{\"values\":[16]},\n",
    "        \"bidirectional\":{\"values\":[0,1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# --- Transformer solo quando model_name ==\"Transformer\" ---\n",
    "\n",
    "sweep_config_transformer = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"Transformer\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "         # --- Transformer solo quando model_name==\"Transformer\" ---\n",
    "        \"d_model\":{\"values\":[8]},\n",
    "        \n",
    "        #\"num_heads\":{\"values\":[2,4,6,8,10,12]}, # 6,10,12 vanno tolti, perché non divisori di tutti i d_model!\n",
    "        \"num_heads\":{\"values\":[2]}, # solo divisori di tutti i d_model\n",
    "        \n",
    "        \"num_layers\":{\"values\":[2]},\n",
    "        \"ff_mult\":{\"values\":[2]},\n",
    "        \"transformer_activations\":{\"values\":[\"relu\",\"gelu\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def load_best_run_results(\n",
    "    key, # es. \"rest_vs_left_fist_spectrograms_familiar_th\"\n",
    "    model, # # <-- istanza PyTorch già caricata con i pesi best es. \"CNN3D_LSTM_FC\"\n",
    "    \n",
    "    sweep_config,      # <— qui richiamo lo sweep config del modello corrispondente\n",
    "    \n",
    "    data_loaders, # dict con DataLoader per \"train\" e \"val\"\n",
    "    entity= \"my_wb_entity\"): # entity = \"stefano‑bargione‑universit‑di‑roma‑tor‑vergata\"\n",
    "    \n",
    "    \n",
    "    # --- 1) Parse key e ricava project name ---\n",
    "    exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "    \n",
    "    \n",
    "    '''CAMBIATA PER DATI INTERROGAIT IN RAPPRESENTAZIONE TIME DOMAIN 1D'''\n",
    "    #project = f\"{exp_cond}_{data_type}_channels_freqs_new_3d_grid_multiband\"\n",
    "    #project = f\"{exp_cond}_{data_type}_time_freqs_new_imagery_3d_grid_multiband\"\n",
    "    \n",
    "    #project = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    \n",
    "    \n",
    "    #project=f\"{exp_cond}_{data_type}_time_freqs_{category_subject}\"\n",
    "    \n",
    "    \n",
    "    project=f\"{exp_cond}_{data_type}_time_frequency_{category_subject}\"\n",
    "    \n",
    "    '''OLD APPROACH'''\n",
    "    #model_name = type(model).__name__\n",
    "    \n",
    "\n",
    "    '''SE ESTRAGGO SWEEP ID A POSTERIORI DAL PROGETTO\n",
    "\n",
    "    1) Prendo tutte le run del progetto e modello corrispondente\n",
    "    2) Filtro solo quelle con config[\"model_name\"] == model_name.\n",
    "    3) Controllo che ce ne sia almeno una (altrimenti errore).\n",
    "    4) Costruisce un set di tutti gli r.sweep e verifica che sia esattamente uno (altrimenti errore).\n",
    "    5) Estrae quello unico (.pop()) e lo stampa insieme al numero di run.\n",
    "    6) Infine, seleziona la singola best_run sulla base di val_accuracy.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    '''NEW APPROACH'''\n",
    "    # === PATCH: accetta alias tra nome classe PyTorch e nome usato nello sweep ===\n",
    "    def _get_param_list(cfg, key):\n",
    "        p = cfg[\"parameters\"][key]\n",
    "        vals = p.get(\"values\", p.get(\"value\"))\n",
    "        if isinstance(vals, str): \n",
    "            return [vals]\n",
    "        return list(vals) if isinstance(vals, (list, tuple)) else [vals]\n",
    "\n",
    "    model_class = type(model).__name__          # es. \"ReadMEndYou\"\n",
    "    cfg_names   = _get_param_list(sweep_config, \"model_name\")   # es. [\"BiLSTM\"]\n",
    "    aliases     = set([model_class, *cfg_names])                # {\"ReadMEndYou\",\"BiLSTM\"}\n",
    "    \n",
    "    def matches_aliases(r):\n",
    "        return (\n",
    "            r.config.get(\"model_name\") in aliases or\n",
    "            r.config.get(\"model_class\") in aliases or\n",
    "            bool(set(r.tags or []) & aliases)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # 2) Recupero tutte le run del progetto\n",
    "    api  = Api()\n",
    "    runs = api.runs(f\"{entity}/{project}\")\n",
    "\n",
    "    # 3) filtro solo quelle del modello giusto\n",
    "    \n",
    "    '''OLD APPROACH'''\n",
    "    #runs_filtered = [r for r in runs if r.config.get(\"model_name\", \"\") == model_name]\n",
    "    #n_runs = len(runs_filtered)\n",
    "    \n",
    "    '''NEW APPROACH'''\n",
    "    runs_filtered = [r for r in runs if matches_aliases(r)]\n",
    "    n_runs = len(runs_filtered)\n",
    "    \n",
    "\n",
    "    if n_runs == 0:\n",
    "        raise RuntimeError(f\"Nessuna run trovata per progetto `{project}` e modello `{model_name}`\")\n",
    "\n",
    "    # 4) controllo che le run filtrate appartengano tutte allo stesso sweep\n",
    "    unique_sweeps = {r.sweep for r in runs_filtered}\n",
    "    if len(unique_sweeps) != 1:\n",
    "        raise RuntimeError(\n",
    "            f\"Trovati più sweep per progetto `{project}` e modello `{model_name}`: {unique_sweeps}\"\n",
    "        )\n",
    "\n",
    "    # 5) estraggo lo sweep_id\n",
    "    sweep_id_unico = unique_sweeps.pop()\n",
    "    #print(f\"✓ Trovate \\033[1m{n_runs}\\033[0m runs in progetto `{project}` e modello `{model_name}`, sweep: `{sweep_id_unico}`\")\n",
    "    print(f\"✓ Trovate \\033[1m{n_runs}\\033[0m runs\\n\")\n",
    "    print(f\"✓ Progetto \\033[1m`{project}`\\033[0m\\n\")\n",
    "    print(f\"✓ Modello \\033[1m`{model_name}`\\033[0m\\n\")\n",
    "    print(f\"✓ Sweep \\033[1m`{sweep_id_unico}`\\033[0m\\n\\n\")\n",
    "\n",
    "    # 6) scelgo la run con val_accuracy massima\n",
    "    best_run = max(runs_filtered, key=lambda r: r.summary.get(\"val_accuracy\", 0.0))\n",
    "\n",
    "    # --- 7) Estraggo tutta la history (compresi i train_auc sbagliati) ---\n",
    "    df = best_run.history(\n",
    "        keys=[\n",
    "          \"train_loss\",\"train_accuracy\",\"train_precision\",\n",
    "          \"train_recall\",\"train_f1\",\"train_auc\",\n",
    "          \"val_loss\",\"val_accuracy\"\n",
    "        ],\n",
    "        pandas=True\n",
    "    )\n",
    "    # converto in liste\n",
    "    loss_train_history     = df[\"train_loss\"].tolist()\n",
    "    loss_val_history       = df[\"val_loss\"].tolist()\n",
    "    accuracy_train_history = df[\"train_accuracy\"].tolist()\n",
    "    accuracy_val_history   = df[\"val_accuracy\"].tolist()\n",
    "    precision_train_history= df[\"train_precision\"].tolist()\n",
    "    recall_train_history   = df[\"train_recall\"].tolist()\n",
    "    f1_train_history       = df[\"train_f1\"].tolist()\n",
    "    auc_train_history      = df[\"train_auc\"].tolist()\n",
    "\n",
    "    # best_epoch (su val_accuracy)\n",
    "    best_epoch = int(df[\"val_accuracy\"].idxmax())\n",
    "\n",
    "    # --- 8) Prendo il modello ottimizzato .pkl corrispondente passato in input ---\n",
    "    device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- 9) Ricalcolo vero train AUC–ROC sul train_loader ---\n",
    "    y_t_train, y_s_train = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loaders[\"train\"]:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs  = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            y_s_train.extend(probs)\n",
    "            y_t_train.extend(y.numpy())\n",
    "            \n",
    "    true_auc_train = roc_auc_score(np.array(y_t_train), np.array(y_s_train))\n",
    "\n",
    "    # Sovrascrivo il vecchio valore sbagliato\n",
    "    auc_train_history[best_epoch] = true_auc_train\n",
    "\n",
    "    # Ricostruisco best_metrics\n",
    "    best_metrics = {\n",
    "      \"train_loss\":       [round(loss_train_history[best_epoch],4)],\n",
    "      \"train_accuracy\":   [round(accuracy_train_history[best_epoch],4)],\n",
    "      \"train_precision\":  [round(precision_train_history[best_epoch],4)],\n",
    "      \"train_recall\":     [round(recall_train_history[best_epoch],4)],\n",
    "      \"train_f1_score\":   [round(f1_train_history[best_epoch],4)],\n",
    "      \"train_auc\":        [round(true_auc_train,4)]\n",
    "    }\n",
    "\n",
    "    # --- 10) Ricreo confusion matrix e classification report su val set ---\n",
    "    y_t_val, y_p_val = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loaders[\"val\"]:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs  = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            preds  = (probs >= 0.5).astype(int)\n",
    "            y_p_val.extend(preds)\n",
    "            y_t_val.extend(y.numpy())\n",
    "            \n",
    "    confusion_matrix_val = confusion_matrix(y_t_val, y_p_val)\n",
    "    classification_report_val = classification_report(y_t_val, y_p_val, output_dict=False)\n",
    "\n",
    "    #Solo una nota: qui non serve per training che l'auc abbia l'average='weighted' \n",
    "    #perché è binario e stai usando score continui.\n",
    "    #anche se sopra lo avevi messo in \"training_sweep\".\n",
    "    \n",
    "    #Per le altre metriche (precision, recall, f1_score invece) l'average andava bene!\n",
    "    #Anche in binario: average='weighted' = fai la media pesata per supporto delle metriche per ciascuna classe (0 e 1). \n",
    "    #È sensato se hai sbilanciamento e vuoi che le metriche riflettano anche quanto è frequente ciascuna classe. \n",
    "    \n",
    "    #L’unica cosa da essere consapevoli è che non stai riportando “F1 della classe positiva”, \n",
    "    #ma una F1 complessiva pesata sulle due classi. \n",
    "    #Ma va bene, basta essere coerenti e chiari nel testo della tesi/paper.\"\n",
    "    \n",
    "\n",
    "    # --- 10) Ricreo confusion matrix e classification report su val set ---\n",
    "    \n",
    "    #Per il validation set, invece, rifai il calcolo:\n",
    "    \n",
    "    #y_t_val = true labels (0/1).\n",
    "    #y_p_val = predizioni binarie (0/1), usate per accuracy / precision / recall / f1.\n",
    "    #y_s_val = score continui (probabilità o logit della classe 1), usati per il calcolo dell'AUC-ROC:\n",
    "    \n",
    "    #Quindi diventerà ---> val_auc = roc_auc_score(y_t_val, y_s_val)\n",
    "    #Quindi qui \"y_s_val\" è semplicemente la lista di p(y=1) per ogni campione di validation.\n",
    "    \n",
    "    y_t_val, y_p_val, y_s_val = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loaders[\"val\"]:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs  = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            preds  = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            \n",
    "            y_p_val.extend(preds) # predizioni 0/1\n",
    "            y_s_val.extend(probs) # score continui per AUC\n",
    "            y_t_val.extend(y.numpy()) \n",
    "            \n",
    "    confusion_matrix_val = confusion_matrix(y_t_val, y_p_val)\n",
    "    classification_report_val = classification_report(y_t_val, y_p_val, output_dict=False)\n",
    "    \n",
    "    # Metriche Validation\n",
    "    #val_accuracy = accuracy_score(y_t_val, y_p_val)\n",
    "    val_precision = precision_score(y_t_val, y_p_val, average='weighted')\n",
    "    val_recall    = recall_score(y_t_val, y_p_val, average='weighted')\n",
    "    val_f1        = f1_score(y_t_val, y_p_val, average='weighted')\n",
    "    \n",
    "    try:\n",
    "        val_auc = roc_auc_score(y_t_val, y_s_val)   # <-- NOTHING average=... qui\n",
    "    except ValueError:\n",
    "        print(\"⚠️ AUC non calcolabile: nel val set c'è una sola classe.\")\n",
    "        val_auc = np.nan\n",
    "    \n",
    "    # Val performances alla best_epoch\n",
    "    \n",
    "    '''\n",
    "    Qui la cosa importante è: il modello con cui stai facendo il forward su data_loaders[\"val\"] \n",
    "    è il best model, cioè quello che hai caricato da .pkl e che dovrebbe corrispondere esattamente \n",
    "    ai pesi di best_epoch per quella run.\n",
    "    \n",
    "    Per cui, val_loss e val_accuracy che salvi nel dict sono proprio quelli loggati all’epoca best_epoch durante il training.\n",
    "    Questi sono coerenti con “la migliore epoca secondo val_accuracy”.\n",
    "    \n",
    "    Mentre le altre metriche (precision, recall, f1_score, son ricalcolate in base al best model che aveva ottenuto\n",
    "    a quella epoca specifica la migliore val_accuracy!\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    validation_performances = {\n",
    "        # dalla history di W&B (loss/acc per quella epoch)\n",
    "        \"val_loss\":       [round(loss_val_history[best_epoch],4)],\n",
    "        \"val_accuracy\":   [round(accuracy_val_history[best_epoch],4)],\n",
    "        \n",
    "        # dalle metriche ricalcolate con il best_model\n",
    "        \"val_precision\":  [round(val_precision,4)],\n",
    "        \"val_recall\":     [round(val_recall,4)],\n",
    "        \"val_f1_score\":   [round(val_f1,4)],\n",
    "        \"val_auc\":        [round(val_auc,4)],\n",
    "    }\n",
    "    \n",
    "        \n",
    "    # --- 10) Plot delle curve loss/accuracy tra train e test ---\n",
    "    training_plot = plot_training_results(\n",
    "        loss_train_history,\n",
    "        loss_val_history,\n",
    "        accuracy_train_history,\n",
    "        accuracy_val_history\n",
    "    )\n",
    "\n",
    "    # --- 11) Composizione del dict finale identico a `training()` ---\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \n",
    "        \"best_model\": model,\n",
    "        \n",
    "        # VALIDATION\n",
    "        \"validation_performances\": validation_performances,\n",
    "        \n",
    "        \"confusion_matrix\": confusion_matrix_val,\n",
    "        \"classification_report\": classification_report_val,\n",
    "    \n",
    "        \"hyperparams\" : {k: best_run.config[k] for k in best_run.config.keys() if k in sweep_config[\"parameters\"]},\n",
    "            \n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "    \n",
    "    '''\n",
    "    Ho questo errore \"Errore “cudnn RNN backward can only be called in training mode”\" solo con i dati di \n",
    "    left_fist_vs_right_fist, per il modello SeparableCNN2D_LSTM_FC, \n",
    "    mentre con i dati delle altre condizioni sperimentali, ossia:\n",
    "    \n",
    "    rest_vs_left_fist o rest_vs_right_fist, sempre per il modello SeparableCNN2D_LSTM_FC,non succede ... come mai solo con l'ultimo succede? \n",
    "    \n",
    "    cioè dove dovrei aver lasciato il modello caricato in eval.() ?\n",
    "    \n",
    "    probabilmente qui nella funzione load_best_train_results!?\n",
    "    \n",
    "    quindi qui poi alla fine dovrei rimettere il modello in un'altra modalità alla fine della funzione? \n",
    "    perché in sostanza, dovrebbe succedere che in sostanza... non succede nulla per lo stesso modello per  gli altri dati, \n",
    "    perché ogni volta che ne prendo uno lo porto in eval e vabbè.. ma poi il problema succede solo per l'ultimo caso solo, \n",
    "    perché forse l'ultimo proprio, ossia solo SeparableCNN2D_LSTM_FC usa proprio il layer LSTM e quindi da errore là,\n",
    "    perché dentro a load_best_train_results è rimasto in .eval() ed ha il layer LSTM e quindi dà errore?\n",
    "    \n",
    "    \n",
    "    \n",
    "    Perché l’errore appare “solo” con l’ultima combinazione\n",
    "\n",
    "    1. load_best_run_results() termina con:\n",
    "\n",
    "    model.to(device).eval()   # ← il modello rimane in eval()\n",
    "    \n",
    "    2. In compute_gradcam_figure() tu usi il best model che hai messo in train_results[\"best_model\"] (quello appena impostato in eval()), poi esegui:\n",
    "\n",
    "   \n",
    "    output = model(sample_input)\n",
    "    ...\n",
    "    target.backward()         # <-- gradiente attraverso l’LSTM\n",
    "    3. Il kernel CuDNN per gli RNN (LSTM/GRU) rifiuta il backward quando il modulo è in modalità inference (eval()), e solleva:\n",
    "\n",
    "    \n",
    "    RuntimeError: cudnn RNN backward can only be called in training mode\n",
    "    \n",
    "    4. Per le combinazioni precedenti con lo stesso modello “SeparableCNN2D_LSTM_FC” non è esploso perché, con ogni probabilità, \n",
    "    use_lstm=False nelle relative run migliori (quindi l’LSTM non c’è e CuDNN non interviene).\n",
    "    \n",
    "    Nell’ultima combinazione invece la best‑run ha use_lstm=True, quindi compare l’LSTM e l’errore salta fuori.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2cd87-a7bf-4ed2-85de-ca505cfc92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 – Sweep config per ciascun modello\n",
    "\n",
    "#CNN2D_LSTM_TF\n",
    "sweep_config_cnn2d_lstm_tf = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN2D_LSTM_TF\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        # --- specifici del modello ---\n",
    "    \n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config_bilstm = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"BiLSTM\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        # --- specifici del modello ---\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "        \"bidirectional\": {\"values\": [False, True]},\n",
    "        \n",
    "        #Soluzione 1 per mettere valori agli hidden sizes\n",
    "        #\"hidden1\": {\"values\": [24, 32, 48, 64]},\n",
    "        #\"hidden2\": {\"values\": [48, 64, 96, 128]},\n",
    "        #\"hidden3\": {\"values\": [62, 96, 128, 160]}\n",
    "        # in build del modello: hidden_sizes=[hidden1, hidden2, hidden3]\n",
    "        \n",
    "        #Soluzione 2 per mettere valori agli hidden sizes\n",
    "        \n",
    "        #hidden_sizes = [24, 48, 62]\n",
    "        #lstm_model = ReadMEndYou(input_size=input_channels * num_freqs, hidden_sizes=hidden_sizes, output_size=num_classes)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config_transformer = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"Transformer\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        # --- specifici del modello ---\n",
    "        \"d_model\": {\"values\": [32]},\n",
    "        \"num_heads\": {\"values\": [2]},\n",
    "        \"num_layers\": {\"values\": [2]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310363b-2ad0-49d2-a08d-61c7e9884413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "\n",
    "'''ATTENZIONE CHE QUI HO AGGIUNTO --> \"_time_frequency\" alla base_folder!'''\n",
    "\n",
    "\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_time_frequency\"\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_post_WB\"\n",
    "\n",
    "\n",
    "'''ATTENZIONE CHE QUI HO AGGIUNTO --> \"_time_frequency\" alla save_path_folder!'''\n",
    "\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_time_frequency_best_models_post_WB_GradCAM_Checks\"\n",
    "\n",
    "                                              #spectrograms_time_frequency_best_models_post_WB_GradCAM_Checks\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    CREO COPIA TEST_LOADER_RAW PER I PLOT DEL POWER RAW PER BANDA E CLASSE\n",
    "    '''\n",
    "    # 1) salva una copia RAW dei soli dati di test PRIMA di standardizzare\n",
    "    X_test_raw = X_test.copy()\n",
    "    y_test_raw = y_test.copy()\n",
    "    \n",
    "    # 2) tensori\n",
    "    X_raw_tensor = torch.tensor(X_test_raw, dtype=torch.float32)\n",
    "    y_raw_tensor = torch.tensor(y_test_raw, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    for model_name in [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"]:\n",
    "        \n",
    "            \n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        # Prova a caricare la configurazione e i pesi ottimali dal file .pkl\n",
    "        \n",
    "        '''\n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "        pesi del modello e i suoi iper-parametri\n",
    "        \n",
    "        Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "        '''\n",
    "        \n",
    "        config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "        \n",
    "        if config is None:\n",
    "            raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "        \n",
    "        '''\n",
    "        Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "        MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "        '''\n",
    "        \n",
    "        # Parsifica la chiave una volta sola e memorizza i valori\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "        \n",
    "        '''\n",
    "        Dpodiché, \n",
    "        \n",
    "        1) si carica i vari valori degli iper-parametri,\n",
    "        2) si esegue la standardizzazione se servisse,\n",
    "        3) prepara il modello per la divisione in train_loader etc.,\n",
    "        4) si carica la configurazione dei pesi del modello, \n",
    "        5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "        \n",
    "        6) esegue il training e il test e poi\n",
    "        \n",
    "        7) si salva il tutto nella path corrispondente...\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "        DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "        \n",
    "        #---\n",
    "        #model_lr = config[\"lr\"]\n",
    "        #model_weight_decay = config[\"weight_decay\"]\n",
    "        #model_n_epochs = config[\"n_epochs\"]\n",
    "        #model_patience = config[\"patience\"]\n",
    "        \n",
    "        \n",
    "        #model_batch_size = config[\"batch_size\"]\n",
    "        #model_standardization = config[\"standardization\"]\n",
    "        \n",
    "        #model_n_epochs = config[\"n_epochs\"]\n",
    "        #model_patience = config[\"patience\"]\n",
    "        \n",
    "        #model_lr = config[\"lr\"]\n",
    "        \n",
    "        #'''NUOVE MODIFICHE'''\n",
    "        #model_beta1 =  config[\"beta1\"]\n",
    "        #model_beta2 =  config[\"beta2\"]\n",
    "        #model_eps = config[\"eps\"]\n",
    "        #---\n",
    "        \n",
    "        \n",
    "        model_lr = config[\"lr\"]\n",
    "        model_weight_decay = config[\"weight_decay\"]\n",
    "        model_n_epochs = config[\"n_epochs\"]\n",
    "        model_patience = config[\"patience\"]\n",
    "        \n",
    "        model_batch_size = config[\"batch_size\"]\n",
    "        model_standardization = config[\"standardization\"]\n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        model_beta1 = config[\"beta1\"]\n",
    "        model_beta2 = config[\"beta2\"]\n",
    "        model_eps = config[\"eps\"]\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, weight_decay= \\033[1m{model_weight_decay}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "        \n",
    "        # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "        models_info[model_key] = {\"standardization\": model_standardization}\n",
    "        \n",
    "        \n",
    "        # 3) dataset & loader per test set (per plots power raw) –‑  IMPORTANTISSIMO: shuffle=False\n",
    "        raw_dataset = TensorDataset(X_raw_tensor, y_raw_tensor)\n",
    "        test_loader_raw = DataLoader(raw_dataset,\n",
    "                             batch_size=model_batch_size,\n",
    "                             shuffle=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "        IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "        '''\n",
    "\n",
    "        if model_standardization:\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "        \n",
    "        # Sposta il modello sulla GPU (se disponibile)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        \n",
    "        # Preparazione dei dataloaders\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "        \n",
    "        #sweep_config_cnn2d_lstm_tf\n",
    "        #sweep_config_bilstm\n",
    "        #sweep_config_transformer\n",
    "        \n",
    "        '''PARAMETRI MODEL-SPECIFIC DI CNN2D_LSTM_TF, BiLSTM e Transformer, ma richiamati al momento della inizializzazione dei relativi sweep_config! '''\n",
    "    \n",
    "        '''\n",
    "        # Appena caricato X_train, X_val, X_test, etc.\n",
    "        # X_train.shape == (N, channels, freq_bins, time_steps)\n",
    "\n",
    "        _, channels, freq_bins, time_steps = X_train.shape\n",
    "        \n",
    "        #Classificazione (binaria)\n",
    "        \n",
    "        num_classes = 2 \n",
    "        \n",
    "        #CNN2D_LSTM_TF\n",
    "        model_conv_out_channels = config[\"conv_out_channels\"]\n",
    "        \n",
    "        model_conv_k1_h = config[\"conv_k1_h\"]\n",
    "        model_conv_k1_w = config[\"conv_k1_w\"]\n",
    "         \n",
    "        model_conv_k2_h = config[\"conv_k2_h\"]\n",
    "        model_conv_k2_w = config[\"conv_k2_w\"]\n",
    "        \n",
    "        model_conv_k3_h = config[\"conv_k3_h\"]\n",
    "        model_conv_k3_w = config[\"conv_k3_w\"]\n",
    "        \n",
    "        model_conv_s1_h = config[\"conv_s1_h\"]\n",
    "        model_conv_s1_w = config[\"conv_s1_w\"]\n",
    "\n",
    "        model_conv_s2_h = config[\"conv_s2_h\"]\n",
    "        model_conv_s2_w = config[\"conv_s2_w\"]\n",
    "\n",
    "        model_conv_s3_h = config[\"conv_s3_h\"]\n",
    "        model_conv_s3_w = config[\"conv_s3_w\"]\n",
    "\n",
    "        model_pool_p1_h = config[\"pool_p1_h\"]\n",
    "        model_pool_p1_w = config[\"pool_p1_w\"]\n",
    "        \n",
    "        model_pool_p2_h = config[\"pool_p2_h\"]\n",
    "        model_pool_p2_w = config[\"pool_p2_w\"]\n",
    "        \n",
    "        model_pool_p3_h = config[\"pool_p3_h\"]\n",
    "        model_pool_p3_w = config[\"pool_p3_w\"]\n",
    "        \n",
    "        model_pool_type = config[\"pool_type\"]\n",
    "        model_fc1_units = config[\"fc1_units\"]\n",
    "                                 \n",
    "        model_cnn_act1 = config[\"cnn_act1\"]\n",
    "        model_cnn_act2 = config[\"cnn_act2\"]\n",
    "        model_cnn_act3 = config[\"cnn_act3\"]\n",
    "        \n",
    "        \n",
    "        #BiLSTM\n",
    "        model_hidden_size = config[\"hidden_size\"]\n",
    "        model_bidirectional = config[\"bidirectional\"]\n",
    "        \n",
    "        \n",
    "        #Transformer\n",
    "        model_d_model = config[\"d_model\"]\n",
    "        model_num_heads = config[\"num_heads\"]\n",
    "        \n",
    "        model_num_layers = config[\"num_layers\"]\n",
    "        model_ff_mult = config[\"ff_mult\"]\n",
    "        model_transformer_activations = config[\"transformer_activations\"]\n",
    "        \n",
    "        model_dropout = config[\"dropout\"]\n",
    "        \n",
    "        \n",
    "        # Inizializzazione del modello\n",
    "        if model_name == \"CNN2D\":\n",
    "            #model = CNN2D(input_channels=3, num_classes=2)\n",
    "            \n",
    "            model = CNN2D(\n",
    "                input_channels=channels,\n",
    "                num_classes=num_classes,\n",
    "                conv_out_channels = model_conv_out_channels,\n",
    "                conv_k1_h = model_conv_k1_h, conv_k1_w = model_conv_k1_w,\n",
    "                conv_k2_h = model_conv_k2_h, conv_k2_w = model_conv_k2_w,\n",
    "                conv_k3_h = model_conv_k3_h, conv_k3_w = model_conv_k3_w,\n",
    "                conv_s1_h = model_conv_s1_h, conv_s1_w = model_conv_s1_w,\n",
    "                conv_s2_h = model_conv_s2_h, conv_s2_w = model_conv_s2_w,\n",
    "                conv_s3_h = model_conv_s3_h, conv_s3_w = model_conv_s3_w,\n",
    "                pool_p1_h = model_pool_p1_h, pool_p1_w = model_pool_p1_w,\n",
    "                pool_p2_h = model_pool_p2_h, pool_p2_w = model_pool_p2_w,\n",
    "                pool_p3_h = model_pool_p3_h, pool_p3_w = model_pool_p3_w,\n",
    "                pool_type = model_pool_type,\n",
    "                fc1_units = model_fc1_units,\n",
    "                dropout = model_dropout,\n",
    "                cnn_act1 = model_cnn_act1, \n",
    "                cnn_act2 = model_cnn_act2,\n",
    "                cnn_act3 = model_cnn_act3\n",
    "            )\n",
    "            \n",
    "        elif model_name == \"BiLSTM\":\n",
    "            #model = ReadMEndYou(input_size= 3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "            \n",
    "            model = ReadMEndYou(input_size=channels*freq_bins, \n",
    "                                hidden_size= model_hidden_size,\n",
    "                                output_size=num_classes,\n",
    "                                num_layers=3, \n",
    "                                dropout= model_dropout, \n",
    "                                bidirectional= model_bidirectional)\n",
    "       \n",
    "        #trans = ReadMYMind(num_channels=channels, num_freqs= frequency, seq_length=time,\n",
    "                           #d_model=64, num_heads=8, num_layers=2, num_classes=num_classes,\n",
    "                           #ff_mult=2, dropout=0.1, transformer_activations='relu')\n",
    "        #out_trans = trans(x)\n",
    "        #print(\"Transformer output:\", out_trans.shape)\n",
    "\n",
    "        #wandb.init(project = f\"{condition}_{data_type}_time_frequency_{category_subject}\", name = run_name, tags = tags)\n",
    "            \n",
    "        elif model_name == \"Transformer\":\n",
    "            \n",
    "            model = ReadMYMind(num_channels = channels, \n",
    "                               num_freqs = freq_bins, \n",
    "                               \n",
    "                               seq_length = time_steps,\n",
    "                               d_model = model_d_model,\n",
    "                               \n",
    "                               num_heads = model_num_heads, \n",
    "                               num_layers = model_num_layers,\n",
    "                               \n",
    "                               num_classes = num_classes,\n",
    "                               \n",
    "                               ff_mult = model_ff_mult,\n",
    "                               dropout = model_dropout,\n",
    "                               transformer_activations = model_transformer_activations)\n",
    "        else:\n",
    "            raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        channels, freqs = int(X_train.shape[1]), int(X_train.shape[2])\n",
    "        \n",
    "        if model_name == \"CNN2D_LSTM_TF\":\n",
    "            \n",
    "            sweep_config = sweep_config_cnn2d_lstm_tf\n",
    "            \n",
    "            model_dropout = float(config.get('dropout', 0.5))\n",
    "            \n",
    "            \n",
    "            model = CNN2D_LSTM_TF(input_channels = channels, num_classes = 2, dropout = model_dropout) #input_channels = 61\n",
    "            print(f\"\\nInizializzazione Modello \\033[1mCNN2D_LSTM_TF\\033[0m\")\n",
    "            \n",
    "        elif model_name == \"BiLSTM\":\n",
    "            \n",
    "            sweep_config = sweep_config_bilstm\n",
    "            \n",
    "            model_dropout = float(config.get('dropout', 0.5))\n",
    "            model_bidirectional = bool(config.get('bidirectional', False))\n",
    "            \n",
    "            #input_size= 61 * 26\n",
    "            model = ReadMEndYou(input_size= channels * freqs, hidden_sizes=[24, 48, 62], output_size=2, dropout = model_dropout, bidirectional = model_bidirectional)\n",
    "            \n",
    "            print(f\"\\nInizializzazione Modello \\033[1mReadMEndYou (BiLSTM)\\033[0m\")\n",
    "            \n",
    "        elif model_name == \"Transformer\":\n",
    "            \n",
    "            sweep_config = sweep_config_transformer\n",
    "            \n",
    "            model_d_model = int(config.get('d_model', 32))\n",
    "            model_num_heads = int(config.get('num_heads', 2))\n",
    "            model_num_layers  = int(config.get('num_layers', 2))\n",
    "            \n",
    "            model = ReadMYMind(d_model = model_d_model, num_heads = model_num_heads, num_layers = model_num_layers, num_classes=2, channels = channels, freqs= freqs) #channels = 61, freqs=26\n",
    "            print(f\"\\nInizializzazione Modello \\033[1mReadMYMind (Transformer)\\033[0m\")\n",
    "        else:\n",
    "            raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "            \n",
    "        \n",
    "        # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "        if best_weights is not None:\n",
    "            try:\n",
    "                model.load_state_dict(best_weights)\n",
    "                print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # Definizione del criterio di perdita\n",
    "        criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "        \n",
    "        '''NEW VERSION'''\n",
    "        # 1) Optimizer con betas, eps, weight_decay\n",
    "    \n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr = model_lr, \n",
    "            betas=(model_beta1, model_beta2),\n",
    "            eps=model_eps,\n",
    "            weight_decay=model_weight_decay\n",
    "        )\n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        #print(f\"🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        #my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "    \n",
    "        #print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        #my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        \n",
    "        # 1) prepara i data_loaders per train/val\n",
    "        data_loaders = {\n",
    "            \"train\": train_loader,\n",
    "            \"val\":   val_loader\n",
    "        }\n",
    "        \n",
    "        print(f\"🏋️‍♂️Salvo le metriche del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m a seguito della ottimizzazione su W&B ...\")\n",
    "        \n",
    "        \n",
    "        #ATTENZIONE al potenziale problema di stringa, non di API: \n",
    "\n",
    "        #i due esempi che hai postato in realtà usano diversi caratteri “‑” (uno è il classico ASCII U+002D, l’altro è un non‑breaking hyphen U+2011 o simili), quindi quando chiami\n",
    "        \n",
    "        #entity = \"stefano‑bargione‑universit‑di‑roma‑tor‑vergata\"\n",
    "        #stai passando un nome che W&B non riconosce (e quindi api.projects(entity=…) torna vuoto), mentre con\n",
    "\n",
    "        #entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "        #funziona perché lì usi i semplici - ASCII.\n",
    "\n",
    "        my_train_results = load_best_run_results(\n",
    "            key=key,\n",
    "            model = model,\n",
    "            sweep_config = sweep_config,\n",
    "            data_loaders = data_loaders,\n",
    "            entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L’entity che passi a Api().runs(f\"{entity}/{project}\") è semplicemente il tuo account (o l’organizzazione) su W&B,\n",
    "        cioè la parte che compare subito prima del nome del progetto nell’URL.\n",
    "\n",
    "        Per esempio, se quando apri il tuo progetto su W&B vedi un indirizzo del tipo\n",
    "        \n",
    "        -> https://wandb.ai/steclab/some_project_name, allora entity = \"steclab\".\n",
    "        \n",
    "        Se invece lavori sotto un’organizzazione \n",
    "        \n",
    "        -> “cool‑team”, e l’URL è https://wandb.ai/cool-team/some_project_name, allora userai entity = \"cool-team\".\n",
    "\n",
    "        Puoi verificarlo:\n",
    "\n",
    "        Accedi a wandb.ai e vai sul progetto.\n",
    "        Leggi la prima parte dell’URL (tra wandb.ai/ e il /project_name).\n",
    "        Copiala esattamente come stringa in entity.\n",
    "\n",
    "        Così il tuo Api().runs(f\"{entity}/{project}\") andrà a pescare proprio le run che hai lanciato tu.\n",
    "\n",
    "        my_train_results = load_best_run_results(\n",
    "            key= key,\n",
    "            model = model,\n",
    "            sweep_config = sweep_config,\n",
    "            data_loaders = data_loaders,\n",
    "            entity= \"mio-entity\"\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        # 3) usa il best_model caricato dentro `train_results` e chiama il testing\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        GRADCAM COMPUTATION PER IL MODELLO CNN2D_LSTM_TF\n",
    "        \n",
    "        La funzione compute_gradcam_figure estrae due campioni (uno per ogni classe) e crea una figura con le due righe richieste.\n",
    "        \n",
    "        Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "        'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "        \n",
    "        La funzione 'save_performance_results' è stata modificata \n",
    "        per gestire ANCHE questo nuovo input dell'immagine \n",
    "        \n",
    "        (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "        seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "        \n",
    "        - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "        - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "        - provenienza del dato stesso (i.e., familiar_th)\n",
    "        )\n",
    "        \n",
    "        Spiegazione:\n",
    "        \n",
    "        La funzione compute_gradcam_figure eseguire il calcolo di GradCAM (vedi dettagli nella sua funzione)\n",
    "        e alla fine ritornerà in output una variabile \n",
    "        \n",
    "        'fig_image' che sarà poi assegnata alla variabile 'gradcam_image',\n",
    "        che è un oggetto buffer, che contiene i dati binari dell'immagine in formato PNG\n",
    "        (poiché abbiamo usato plt.savefig con format='png'). \n",
    "        \n",
    "        Quindi, quando passi gradcam_image (cioè fig_image) alla funzione 'save_performance_results',\n",
    "        viene scritto direttamente su disco come file PNG.\n",
    "        \n",
    "        Non c'è bisogno di ri-aprire o convertire ulteriormente, a meno che tu non voglia manipolare l'immagine in seguito.\n",
    "        Quindi, la soluzione è corretta così com'è:\n",
    "        il buffer viene salvato come file PNG nella directory specificata, \n",
    "        e successivamente potrai aprirlo con una libreria come cv2 o PIL se necessario.        \n",
    "        \n",
    "        Quindi, gradcam_image (i.e., fig_image) viene quindi passato correttamente dentro al loop di training e test, \n",
    "        tramite 'save_performance_results', come input, \n",
    "        che salverà quindi poi l'immagine nella path corrispondente \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "        gradcam_image = None\n",
    "        \n",
    "        #if model_name == \"CNN2D\":\n",
    "        \n",
    "        '''ATTENZIONE MODIFICA QUI'''\n",
    "        \n",
    "        if model_name == \"CNN2D_LSTM_TF\":\n",
    "            \n",
    "            '''\n",
    "            ATTENZIONE! Qui ho aggiunto alla nuova versione di \"compute_gradcam_figure\" (versione del 17/09/2025)\n",
    "            il test_loader_raw tra gli argomenti della funzione!\n",
    "            '''\n",
    "            gradcam_image = compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device)\n",
    "            if gradcam_image is not None:\n",
    "                print(f\"Creazione di \\033[1mGradCAM Image\\033[0m per il modello \\033[1m{model_name}\\033[0m.\")\n",
    "                \n",
    "        print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        save_performance_results(model_name,\n",
    "                                 my_train_results,\n",
    "                                 my_test_results,\n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder = save_path_folder,\n",
    "                                 gradcam_image = gradcam_image)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        N.B\n",
    "        \n",
    "        gradcam_image = None avverrà solo all'inizio cioè per il primo modello CNN2D, che verrà testato con una certa combinazione di dati mi sa.. \n",
    "        ma servirebbe tracciare in qualche modo \n",
    "\n",
    "        1) o che la gradcam_image di ogni combinazione venga ri-azzerata alla fine loop\n",
    "        2) o che venga monitorato che gradcam_image di una combinazione di dati già analizzata venga esclusa poi\n",
    "        (o messa in un set) in modo che rivenga per errore sovrascritta più volte.. \n",
    "        \n",
    "        Forse la strada più veloce potrebbe essere la soluzione 1)\n",
    "        \n",
    "        La soluzione più veloce e semplice è reimpostare la variabile gradcam_image a None alla fine dell'iterazione per ogni combinazione di dati\n",
    "        (cioè, all'interno del ciclo esterno che itera su key). \n",
    "         \n",
    "        In questo modo, per ogni nuovo dataset la variabile viene \"azzera\" e viene calcolata l'immagine GradCAM solo per quella combinazione, \n",
    "        evitando di sovrascrivere accidentalmente i risultati già calcolati per combinazioni precedenti.\n",
    "         \n",
    "        Un'altra possibilità sarebbe tenere traccia delle chiavi (o combinazioni) per cui hai già calcolato la GradCAM,\n",
    "        ad esempio usando un set, e saltare il calcolo se la combinazione è già presente. \n",
    "        \n",
    "        Tuttavia, se ogni combinazione deve avere la sua immagine, \n",
    "        la soluzione più semplice è quella di reimpostare gradcam_image = None alla fine dell'iterazione.\n",
    "        \n",
    "        Quindi, per esempio, alla fine del ciclo per ogni dataset (key) potresti fare:\n",
    "        (VEDI SOTTO)\n",
    "        \n",
    "        In questo modo, ti assicuri che per ogni nuova combinazione la variabile sia pulita e pronta per essere ricalcolata, \n",
    "        senza rischio di sovrascrivere o confondere i risultati\n",
    "        '''\n",
    "        \n",
    "        # Reimposta gradcam_image a None per la prossima combinazione di dati\n",
    "        gradcam_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8e522-d101-4b76-8db7-0170bb0780ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"finito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8015d7-533f-4a30-9731-20797563f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2413eb-47bf-46ed-b911-37f83f47aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/stefano/Interrogait/models_info_spectrograms_time_frequency_EEG_GradCAM_Checks.pkl', 'wb') as f:\n",
    "    pickle.dump(models_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d998dc2-64c0-4357-a02f-f6410e25a23c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **CREAZIONE DELLE TABLES CON INTEGRAZIONE DELLE PERFORMANCE TRAINING & TEST DEI MODELLI DENTRO DATAFRAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472eb5a-d1b6-4b61-9fa7-f0c2e98b9a47",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Integrazioni Performance Training e Test del Modello dentro DataFrame - OLD APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600bdf5-89e2-47d0-a47e-cd0154b54169",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **OLD BEST APPROACH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6c9fa-2509-4282-ad57-f4c62c882290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Definiamo le path\n",
    "paths = {\n",
    "    \"TH_FAM\": \"/home/stefano/Interrogait/PRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/TH_FAM_UNSCALED/\",\n",
    "    \"PT_FAM\": \"/home/stefano/Interrogait/PRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/PT_FAM_UNSCALED/\",\n",
    "    \"TH_UNFAM\": \"/home/stefano/Interrogait/PRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/TH_UNFAM_UNSCALED/\",\n",
    "    \"PT_UNFAM\": \"/home/stefano/Interrogait/PPRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/PT_UNFAM_UNSCALED/\"\n",
    "}\n",
    "\n",
    "\n",
    "# Identificatori delle triplette\n",
    "identifiers = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Dizionario per salvare i risultati\n",
    "all_models_dict = {}\n",
    "\n",
    "# Iteriamo su ogni path\n",
    "for condition, path in paths.items():\n",
    "    models_dict = {identifier: {} for identifier in identifiers}  # Dizionario per i modelli della path corrente\n",
    "    \n",
    "    # Controlliamo che la directory esista\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory non trovata: {path}\")\n",
    "        continue\n",
    "    \n",
    "    # Otteniamo la lista di file nella directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filtriamo e carichiamo i file per ciascun identificatore\n",
    "    for identifier in identifiers:\n",
    "        for file in files:\n",
    "            if file.endswith(f\"{identifier}.pkl\"):  # Controlliamo se il file termina con l'identificatore\n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        models_dict[identifier][file] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "    \n",
    "    # Salviamo il dizionario della path corrente nel dizionario principale\n",
    "    all_models_dict[condition] = models_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402923f-a254-4ffc-a89a-7c73171bcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora all_models_dict contiene i dati strutturati per ogni path e identificatore\n",
    "# Stampa i tipi di ogni sotto-dizionario\n",
    "for path_key, identifier_dict in all_models_dict.items():\n",
    "    print(f\"Path: {path_key} - Tipo: {type(identifier_dict)}\")\n",
    "    for identifier, model_dict in identifier_dict.items():\n",
    "        print(f\"  Identifier: {identifier} - Tipo: {type(model_dict)}\")\n",
    "        for model, data in model_dict.items():\n",
    "            print(f\"    Model: {model} - Tipo: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eeff8a-9128-446f-b6ee-7aef2aaefc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_dict.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac8c3a8b-327b-4ee1-aef2-96366b470883",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#METTENDO TUTTE LE FEATURE WAVELET ASSIEME NELLA STESSA TABLE\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Definiamo le path\n",
    "paths = {\n",
    "    \"TH_FAM\": \"/home/stefano/Interrogait/Model_Results/TH_FAM_UNSCALED/\",\n",
    "    \"PT_FAM\": \"/home/stefano/Interrogait/Model_Results/PT_FAM_UNSCALED/\",\n",
    "    \"TH_UNFAM\": \"/home/stefano/Interrogait/Model_Results/TH_UNFAM_UNSCALED/\",\n",
    "    \"PT_UNFAM\": \"/home/stefano/Interrogait/Model_Results/PT_UNFAM_UNSCALED/\"\n",
    "}\n",
    "\n",
    "\n",
    "# Identificatori delle triplette\n",
    "identifiers = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Dizionario per salvare i risultati\n",
    "all_models_dict = {}\n",
    "\n",
    "# Iteriamo su ogni path\n",
    "for condition, path in paths.items():\n",
    "    models_dict = {identifier: {} for identifier in identifiers}  # Dizionario per i modelli della path corrente\n",
    "    \n",
    "    # Controlliamo che la directory esista\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory non trovata: {path}\")\n",
    "        continue\n",
    "    \n",
    "    # Otteniamo la lista di file nella directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filtriamo e carichiamo i file per ciascun identificatore\n",
    "    for identifier in identifiers:\n",
    "        for file in files:\n",
    "            if file.endswith(f\"{identifier}.pkl\"):  # Controlliamo se il file termina con l'identificatore\n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        models_dict[identifier][file] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "    \n",
    "    # Salviamo il dizionario della path corrente nel dizionario principale\n",
    "    all_models_dict[condition] = models_dict\n",
    "\n",
    "    \n",
    "# Definizione delle metriche in ordine\n",
    "metrics = [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]\n",
    "\n",
    "# Creiamo una struttura dati iniziale\n",
    "df_data = {\"Metriche\": metrics}\n",
    "\n",
    "# Iteriamo su ogni condizione in all_models_dict\n",
    "for condition, models_dict in all_models_dict.items():\n",
    "    print(f\"\\nProcessing condition: {condition}\\n\")\n",
    "\n",
    "    # Iteriamo su ogni identificatore dentro il dizionario di modelli\n",
    "    for identifier, models in models_dict.items():\n",
    "        print(f\"  Processing identifier: {identifier}\")\n",
    "\n",
    "        # Iteriamo su ogni modello per questa combinazione condition-identifier\n",
    "        for model_name, model_data in models.items():\n",
    "            \n",
    "            name_model = model_name.split(\"_\")[0]  # Prende solo la parte prima del primo '_' \n",
    "            \n",
    "            print(f\"    Processing model: {name_model}\")\n",
    "\n",
    "            # Supponiamo che 'model_data' contenga i risultati del modello\n",
    "            try:\n",
    "                # Puoi accedere ai risultati di training e testing (se esistono)\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "                \n",
    "                 # Converte i valori da liste a float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training EEG {identifier})\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing EEG {identifier})\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "                \n",
    "            # Creazione del DataFrame finale\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            # Creazione del DataFrame finale\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            # Crea una figura per il plot\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))  # Imposta le dimensioni della figura\n",
    "\n",
    "            # Nasconde l'asse per non visualizzare i numeri\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Rimuovi l'indice dal DataFrame\n",
    "            df_without_index = df_performances.reset_index(drop=True)\n",
    "\n",
    "            # Usa pandas per creare una tabella nel grafico\n",
    "            tabla = table(ax, df_without_index, loc='center', colWidths=[0.2]*len(df_performances.columns))\n",
    "\n",
    "            # Personalizza l'aspetto della tabella\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(10)\n",
    "            tabla.scale(2, 2)  # Scala per una migliore visibilità\n",
    "\n",
    "            # Metti i nomi delle colonne in grassetto\n",
    "            for key, cell in tabla.get_celld().items():\n",
    "                if key[0] == 0:  # Se la riga è la prima (intestazioni delle colonne)\n",
    "                    cell.set_text_props(weight='bold')  # Metti in grassetto\n",
    "\n",
    "            # Aumenta la larghezza della colonna se c'è uno strabordamento\n",
    "            tabla.auto_set_column_width([0, 1, 2, 3, 4, 5])  # Modifica questa lista in base al numero di colonne\n",
    "\n",
    "            # Rimuovi la colonna dell'indice (indice di riga)\n",
    "            tabla.get_celld().pop((0, 0))  # Rimuove l'indice dalla tabella\n",
    "\n",
    "            # Creazione della directory se non esiste\n",
    "            output_dir = paths[condition]  # Usa la path dinamica corrispondente alla condition\n",
    "\n",
    "            file_name = f\"{condition}_{identifier}_models.png\"\n",
    "\n",
    "            img_file_path = os.path.join(output_dir, file_name)  # Nome della path del \n",
    "\n",
    "            # Salva la figura\n",
    "            fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "            print(f\"Tabella salvata in: {img_file_path}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64209c3d-b0a3-4fad-b3f0-dd0d4a6b8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "# Definiamo le path\n",
    "paths = {\n",
    "    \"TH_FAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/TH_FAM_UNSCALED/\",\n",
    "    \"PT_FAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/PT_FAM_UNSCALED/\",\n",
    "    \"TH_UNFAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/TH_UNFAM_UNSCALED/\",\n",
    "    \"PT_UNFAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/PT_UNFAM_UNSCALED/\"\n",
    "}\n",
    "\n",
    "# Identificatori delle triplette\n",
    "identifiers = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Iteriamo su ogni path\n",
    "for condition, path in paths.items():\n",
    "    \n",
    "    # Dizionario per i modelli della path corrente\n",
    "    models_dict = {identifier: {} for identifier in identifiers}\n",
    "    \n",
    "    # Controlliamo che la directory esista\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory non trovata: {path}\")\n",
    "        continue\n",
    "    \n",
    "    # Otteniamo la lista di file nella directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filtriamo e carichiamo i file per ciascun identificatore\n",
    "    for identifier in identifiers:\n",
    "        for file in files:\n",
    "            if file.endswith(f\"{identifier}.pkl\"):  # Controlliamo se il file termina con l'identificatore\n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        models_dict[identifier][file] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "    # Ora creiamo un file separato per ogni identificatore\n",
    "    for identifier in identifiers:\n",
    "        df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "        \n",
    "        print(f\"\\nProcessing condition: {condition}, identifier: {identifier}\\n\")\n",
    "\n",
    "        # Iteriamo sui modelli relativi a questo identificatore\n",
    "        for model_name, model_data in models_dict[identifier].items():\n",
    "            name_model = model_name.split(\"_\")[0]  # Prende solo la parte prima del primo '_'\n",
    "            print(f\"    Processing model: {name_model}\")\n",
    "\n",
    "            try:\n",
    "                # Recupera i risultati di training e testing\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "\n",
    "                # Converti i valori in float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training)\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing)\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "        # Creazione del DataFrame per l'identificatore specifico\n",
    "        df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "        # Crea un'immagine della tabella\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Usa pandas per creare una tabella nel grafico\n",
    "        tabla = table(ax, df_performances, loc='center', colWidths=[0.2]*len(df_performances.columns))\n",
    "\n",
    "        # Personalizza la tabella\n",
    "        tabla.auto_set_font_size(True)\n",
    "        tabla.set_fontsize(10)\n",
    "        tabla.scale(2, 2)\n",
    "\n",
    "        # Evidenzia i nomi delle colonne\n",
    "        for key, cell in tabla.get_celld().items():\n",
    "            if key[0] == 0:  # Se la riga è la prima (intestazioni delle colonne)\n",
    "                cell.set_text_props(weight='bold')  # Grassetto\n",
    "\n",
    "        # Creazione della directory se non esiste\n",
    "        output_dir = paths[condition]\n",
    "        file_name = f\"{condition}_{identifier}_models.png\"\n",
    "        img_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Salva l'immagine della tabella\n",
    "        fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)  # Chiudi la figura per liberare memoria\n",
    "\n",
    "        print(f\"Tabella salvata in: {img_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec5998-f818-4b1e-9a00-5576a7d87fa1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Integrazioni Performance Training e Test del Modello dentro DataFrame - NEW APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5fc73-5cc3-4bdb-ad08-51417607749b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Spiegazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107d0c2-a132-4cf8-8826-378ac6624312",
   "metadata": {},
   "source": [
    "Ok in questo modo, model_standardization_dict dovrebbe andare a salvarsi se, i dati per quella combinazione di fattori, rispetto ad uno specifico modello, siano stati standardizzati o meno.\n",
    "\n",
    "Di conseguenza, dentro questo loop\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas.plotting import table\n",
    "\n",
    "    # Base folder\n",
    "    base_folder = \"/home/stefano/Interrogait/time_domain_best_models_post_WB\"\n",
    "\n",
    "    # Condizioni sperimentali\n",
    "    experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "    # Tipologie di dati\n",
    "    data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "    # Subfolders per tipologia di soggetto\n",
    "    subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "    # Dizionario per salvare tutti i modelli\n",
    "    all_models = {}\n",
    "\n",
    "    # Caricamento dei modelli\n",
    "    for condition in experimental_conditions:\n",
    "        for data_type in data_types:\n",
    "            for subfolder in subfolders:\n",
    "\n",
    "                path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"Directory non trovata: {path}\")\n",
    "                    continue\n",
    "\n",
    "                # Creiamo la chiave per questa combinazione\n",
    "                key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "                all_models[key] = {}\n",
    "\n",
    "                # Otteniamo la lista di file nella directory\n",
    "                files = os.listdir(path)\n",
    "\n",
    "                # Filtriamo e carichiamo i file .pkl\n",
    "                for file in files:\n",
    "                    if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                        file_path = os.path.join(path, file)\n",
    "                        try:\n",
    "                            with open(file_path, \"rb\") as f:\n",
    "                                all_models[key][file] = pickle.load(f)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "    # Creazione delle tabelle di performance\n",
    "    for key, models_dict in all_models.items():\n",
    "\n",
    "        # Otteniamo le informazioni dalla chiave\n",
    "        #condition, data_type, subfolder = key.split(\"_\", 2)\n",
    "        condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "\n",
    "        print(f\"\\nProcessing: \\033[1m{condition}\\033[0m - \\033[1m{data_type}\\033[0m - \\033[1m{subfolder}\\033[0m\\n\")\n",
    "\n",
    "        # Creazione della tabella\n",
    "        df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "        # Iteriamo sui modelli caricati\n",
    "        for model_name, model_data in models_dict.items():\n",
    "            name_model = model_name.split(\"_\")[0]  # Nome modello\n",
    "            print(f\"    Processing model: \\033[1m{name_model}\\033[0m\")\n",
    "\n",
    "            try:\n",
    "                # Recupera i risultati di training e testing\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "\n",
    "                # Converti i valori in float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training)\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing)\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "        # Creazione del DataFrame\n",
    "        #df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "        # Crea un'immagine della tabella\n",
    "        #fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        #ax.axis('off')\n",
    "        #tabla = table(ax, df_performances, loc='center', colWidths=[0.2] * len(df_performances.columns))\n",
    "        #tabla.auto_set_font_size(True)\n",
    "        #tabla.set_fontsize(10)\n",
    "        #tabla.scale(2, 2)\n",
    "\n",
    "        # Evidenzia i nomi delle colonne\n",
    "        #for key, cell in tabla.get_celld().items():\n",
    "        #    if key[0] == 0:\n",
    "        #        cell.set_text_props(weight='bold')\n",
    "\n",
    "        # Salva l'immagine della tabella\n",
    "        path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "        file_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "        img_file_path = os.path.join(path, file_name)\n",
    "        #fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "        #plt.close(fig)\n",
    "\n",
    "        print(f\"\\nTabella dei dati di \\033[1m{key}\\033[0m salvati in: \\n\\033[1m{img_file_path}\\033[0m\")\n",
    "\n",
    "\n",
    "vorrei provare ad iterare con \"zip\", sia all_models che su model_standardization_dict ...? (che forse dovrebbero avere la stessa struttura, che renderebbe possibile questa cosa...?)\n",
    "\n",
    "E, nel momento in cui si aggiungono le metriche del training e test del relativo modello, controllare rispetto a model_standardization_dict (di cui si ha la chiave per accedere all' informazione su se quel modello, per quella combinazioni di fattori che compongono quel dato) se il dato sia stato standardizzato... \n",
    "\n",
    "Se questo è VERO, allora nella colonna del dataframe che si riferisce al modello... vorrei che ci mettessi accanto, alla stringa che si riferisce al nome del modello (name_model) un asterisco, SOLO SE, per quel modello, allenato con quella combinazioni di fattori che compongono quel dato, i dati siano stati standardizzati...\n",
    "\n",
    "chiaro?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfed86e-32d9-4cdb-846f-0d79d4d9fcd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Implementazione "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa33f87-9788-4cf1-ae45-163b6b9f0da5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **CREAZIONE DELLE TABLES CON INTEGRAZIONE DELLE PERFORMANCE TRAINING & TEST DEI MODELLI DENTRO DATAFRAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a516c-9918-4601-839c-94d5cd9636fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Integrazioni in Tabella delle Performance Training e Test del Modello dentro DataFrame - NEW APPROACH**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f02a64-b50f-4356-8b4e-36927d52d145",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Spiegazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8323e-14a5-4ad0-9fa7-2dd8348c7e37",
   "metadata": {},
   "source": [
    "Ok in questo modo, model_standardization_dict dovrebbe andare a salvarsi se, i dati per quella combinazione di fattori, rispetto ad uno specifico modello, siano stati standardizzati o meno.\n",
    "\n",
    "Di conseguenza, dentro questo loop\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas.plotting import table\n",
    "\n",
    "    # Base folder\n",
    "    base_folder = \"/home/stefano/Interrogait/time_domain_best_models_post_WB\"\n",
    "\n",
    "    # Condizioni sperimentali\n",
    "    experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "    # Tipologie di dati\n",
    "    data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "    # Subfolders per tipologia di soggetto\n",
    "    subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "    # Dizionario per salvare tutti i modelli\n",
    "    all_models = {}\n",
    "\n",
    "    # Caricamento dei modelli\n",
    "    for condition in experimental_conditions:\n",
    "        for data_type in data_types:\n",
    "            for subfolder in subfolders:\n",
    "\n",
    "                path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"Directory non trovata: {path}\")\n",
    "                    continue\n",
    "\n",
    "                # Creiamo la chiave per questa combinazione\n",
    "                key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "                all_models[key] = {}\n",
    "\n",
    "                # Otteniamo la lista di file nella directory\n",
    "                files = os.listdir(path)\n",
    "\n",
    "                # Filtriamo e carichiamo i file .pkl\n",
    "                for file in files:\n",
    "                    if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                        file_path = os.path.join(path, file)\n",
    "                        try:\n",
    "                            with open(file_path, \"rb\") as f:\n",
    "                                all_models[key][file] = pickle.load(f)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "    # Creazione delle tabelle di performance\n",
    "    for key, models_dict in all_models.items():\n",
    "\n",
    "        # Otteniamo le informazioni dalla chiave\n",
    "        #condition, data_type, subfolder = key.split(\"_\", 2)\n",
    "        condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "\n",
    "        print(f\"\\nProcessing: \\033[1m{condition}\\033[0m - \\033[1m{data_type}\\033[0m - \\033[1m{subfolder}\\033[0m\\n\")\n",
    "\n",
    "        # Creazione della tabella\n",
    "        df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "        # Iteriamo sui modelli caricati\n",
    "        for model_name, model_data in models_dict.items():\n",
    "            name_model = model_name.split(\"_\")[0]  # Nome modello\n",
    "            print(f\"    Processing model: \\033[1m{name_model}\\033[0m\")\n",
    "\n",
    "            try:\n",
    "                # Recupera i risultati di training e testing\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "\n",
    "                # Converti i valori in float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training)\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing)\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "        # Creazione del DataFrame\n",
    "        #df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "        # Crea un'immagine della tabella\n",
    "        #fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        #ax.axis('off')\n",
    "        #tabla = table(ax, df_performances, loc='center', colWidths=[0.2] * len(df_performances.columns))\n",
    "        #tabla.auto_set_font_size(True)\n",
    "        #tabla.set_fontsize(10)\n",
    "        #tabla.scale(2, 2)\n",
    "\n",
    "        # Evidenzia i nomi delle colonne\n",
    "        #for key, cell in tabla.get_celld().items():\n",
    "        #    if key[0] == 0:\n",
    "        #        cell.set_text_props(weight='bold')\n",
    "\n",
    "        # Salva l'immagine della tabella\n",
    "        path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "        file_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "        img_file_path = os.path.join(path, file_name)\n",
    "        #fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "        #plt.close(fig)\n",
    "\n",
    "        print(f\"\\nTabella dei dati di \\033[1m{key}\\033[0m salvati in: \\n\\033[1m{img_file_path}\\033[0m\")\n",
    "\n",
    "\n",
    "vorrei provare ad iterare con \"zip\", sia all_models che su model_standardization_dict ...? (che forse dovrebbero avere la stessa struttura, che renderebbe possibile questa cosa...?)\n",
    "\n",
    "E, nel momento in cui si aggiungono le metriche del training e test del relativo modello, controllare rispetto a model_standardization_dict (di cui si ha la chiave per accedere all' informazione su se quel modello, per quella combinazioni di fattori che compongono quel dato) se il dato sia stato standardizzato... \n",
    "\n",
    "Se questo è VERO, allora nella colonna del dataframe che si riferisce al modello... vorrei che ci mettessi accanto, alla stringa che si riferisce al nome del modello (name_model) un asterisco, SOLO SE, per quel modello, allenato con quella combinazioni di fattori che compongono quel dato, i dati siano stati standardizzati...\n",
    "\n",
    "chiaro?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2ad31-dffa-4c59-bd43-463bbf3137b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Implementazione "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142c7b2-cf5f-4aa5-b1b4-3f04612f21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "path = '/home/stefano/Interrogait/'\n",
    "\n",
    "with open(f\"{path}models_info_spectrograms_time_frequency_EEG_GradCAM_Checks.pkl\", \"rb\") as f:\n",
    "    models_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b3894-499e-43d4-bb7a-e8323049af65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In questo codice:\n",
    "\n",
    "model_info.get('standardization', False) cerca la chiave 'standardization' all'interno di ogni sottodizionario. \n",
    "Se non esiste, restituirà False come valore di default.\n",
    "Se standardization è True, stampa la chiave associata.\n",
    "'''\n",
    "\n",
    "# Ciclo attraverso le chiavi di 'models_info'\n",
    "for key, model_info in models_info.items():\n",
    "    # Controllo se 'standardization' è True\n",
    "    if model_info.get('standardization', False):  # Default a False nel caso in cui non esista la chiave\n",
    "        print(key)  # Stampa la chiave\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15d9d5-665f-4386-91d6-954032901ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49ca7b-04c6-43ed-b732-85f8bb9ac1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, model_info in all_models.items():\n",
    "#    print(key)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa30e95e-62d8-4042-8176-1bcf9670ce8b",
   "metadata": {},
   "source": [
    "'''\n",
    "Siccome la stringa associata alla category subject è diversa tra i due.. \n",
    "\n",
    "familiar_th  familiar_pt unfamiliar_pt unfamiliar_pt  da un lato (models_info)\n",
    "th_fam, pt_fam, th_unfam, pt_unfam  dall'altro (all_models)\n",
    "\n",
    "la corrispondenza non avverrà mai... per cui, si deve fare il mapping corrispondente tra \n",
    "le stringhe di uno e dell'altro, in modo che models_info cambi come parte della stringa della sua chiave da queste \n",
    "\n",
    "familiar_th  familiar_pt unfamiliar_pt unfamiliar_pt\n",
    "a queste\n",
    "th_fam, pt_fam, th_unfam, pt_unfam \n",
    "\n",
    "'''\n",
    "\n",
    "mapping_subject = {\n",
    "    \"familiar_th\": \"th_fam\",\n",
    "    \"familiar_pt\": \"pt_fam\",\n",
    "    \"unfamiliar_th\": \"th_unfam\",\n",
    "    \"unfamiliar_pt\": \"pt_unfam\"\n",
    "}\n",
    "\n",
    "# Creiamo un nuovo dizionario con le chiavi corrette\n",
    "updated_models_info = {}\n",
    "\n",
    "for key, value in models_info.items():\n",
    "    for old_suffix, new_suffix in mapping_subject.items():\n",
    "        if key.endswith(old_suffix):\n",
    "            new_key = key.replace(old_suffix, new_suffix)\n",
    "            updated_models_info[new_key] = value\n",
    "            break  # Evita sostituzioni multiple se una è già stata fatta\n",
    "    else:\n",
    "        # Se nessuna sostituzione è stata fatta, mantieni la chiave originale\n",
    "        updated_models_info[key] = value\n",
    "\n",
    "# Sostituisci il vecchio dizionario con quello aggiornato\n",
    "models_info = updated_models_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fbdd5-4972-4492-94e4-6d9c6ffa53f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Siccome la stringa associata alla category subject è diversa tra i due.. \n",
    "\n",
    "familiar_th  familiar_pt unfamiliar_pt unfamiliar_pt  da un lato (models_info)\n",
    "th_fam, pt_fam, th_unfam, pt_unfam  dall'altro (all_models)\n",
    "\n",
    "la corrispondenza non avverrà mai... per cui, si deve fare il mapping corrispondente tra \n",
    "le stringhe di uno e dell'altro, in modo che models_info cambi come parte della stringa della sua chiave da queste \n",
    "\n",
    "familiar_th  familiar_pt unfamiliar_pt unfamiliar_pt\n",
    "a queste\n",
    "th_fam, pt_fam, th_unfam, pt_unfam \n",
    "\n",
    "'''\n",
    "\n",
    "mapping_subject = {\n",
    "    \"familiar_th\": \"th_fam\",\n",
    "    \"familiar_pt\": \"pt_fam\",\n",
    "    \"unfamiliar_th\": \"th_unfam\",\n",
    "    \"unfamiliar_pt\": \"pt_unfam\"\n",
    "}\n",
    "\n",
    "def remap_key_suffix(key: str, mapping: dict) -> str:\n",
    "    # prova i pattern più lunghi prima per evitare match parziali (es. \"unfamiliar_th\" vs \"familiar_th\")\n",
    "    for old in sorted(mapping.keys(), key=len, reverse=True):\n",
    "        if key.endswith(old):\n",
    "            return key[:-len(old)] + mapping[old]   # sostituisci SOLO il suffisso\n",
    "    return key\n",
    "\n",
    "updated_models_info = { remap_key_suffix(k, mapping_subject): v\n",
    "                        for k, v in models_info.items() }\n",
    "\n",
    "models_info = updated_models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ad592-71c5-4a05-bbbd-abeaff032445",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c6fa8-fcaf-49a8-8c17-531e82f0a8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Ciclo attraverso le chiavi di 'models_info' AGGIORNATO!'''\n",
    "\n",
    "for key, model_info in models_info.items():\n",
    "    # Controllo se 'standardization' è True\n",
    "    if model_info.get('standardization', False):  # Default a False nel caso in cui non esista la chiave\n",
    "        print(key)  # Stampa la chiavi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa2e6-e6b0-4537-a671-53eaca80d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parsing della chiave e costruzione del path:\n",
    "Usando la funzione parse_combination_key si estraggono \n",
    "\n",
    "exp_cond, data_type e category_subject dalla chiave del dataset. \n",
    "\n",
    "Questi vengono usati per costruire il percorso in cui cercare i file .pkl.\n",
    "'''\n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_models_keys(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    \n",
    "    Il formato atteso PRIMA è:\n",
    "    \n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ spectrograms\" _ \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \n",
    "    Il formato atteso ORA è:\n",
    "    \n",
    "     \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ spectrograms\" _ \"th_fam|th_unfam|pt_fam|pt_unfam\"\n",
    "     \n",
    "    \"\"\"\n",
    "    #r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\", \n",
    "    \n",
    "    match = re.match(\n",
    "        \n",
    "        #PRIMA\n",
    "        #r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(1_20|1_45|wavelet)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        \n",
    "        #DOPO\n",
    "        \n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        \n",
    "        #r\"^(rest_vs_left_fist|rest_vs_right_fist|left_fist_vs_right_fist)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        #r\"^(rest_vs_both_feet|rest_vs_both_fists|both_feet_vs_both_fists)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "    return exp_cond, data_type, category_subject"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afca6fe9-6d06-4db8-858f-855a23659126",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''CODICE DI PROVA PER POPOLAMENTO CORRETTO DEL DIZIONARIO ALL_MODELS'''\n",
    "\n",
    "# Dizionario per salvare tutti i modelli\n",
    "all_models = {}\n",
    "\n",
    "# Iteriamo su ogni combinazione\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Creiamo la chiave per questa combinazione\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            # Otteniamo la lista di file nella directory\n",
    "            files = os.listdir(path)\n",
    "            \n",
    "            # Filtriamo e carichiamo i file .pkl\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "# Controlliamo le chiavi principali del dizionario\n",
    "print(all_models.keys())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd3851fc-7418-43f9-8f7d-fb5a88a10a57",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "OLD APPROACH\n",
    "\n",
    "Adesso replichiamo l'approccio usato prima, ma stavolta integrado tutte le combinazioni di dati. \n",
    "Andiamo a\n",
    "\n",
    "1) iterare sulla struttura delle directory a partire da base_folder, \n",
    "2) caricare i modelli .pkl per ogni combinazione di fattori che compongono i dati\n",
    "3) creare un DataFrame che raccolga le metriche di tutti i modelli relativi alla stessa combinazione di dati. \n",
    "\n",
    "Infine, salviamo questa tabella come immagine all'interno della cartella corrispondente\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "# Base folder\n",
    "base_folder = \"/home/stefano/Interrogait/time_domain_best_models_post_WB\"\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Tipologie di dati\n",
    "data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Subfolders per tipologia di soggetto\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "# Iteriamo su ogni combinazione\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Dizionario per i modelli della path corrente\n",
    "            models_dict = {}\n",
    "            \n",
    "            # Otteniamo la lista di file nella directory\n",
    "            files = os.listdir(path)\n",
    "            \n",
    "            # Filtriamo e carichiamo i file .pkl\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            models_dict[file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "                        \n",
    "            '''\n",
    "            # Creazione della tabella\n",
    "            df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "            \n",
    "            print(f\"\\nProcessing: {condition} - {data_type} - {subfolder}\\n\")\n",
    "            \n",
    "            # Iteriamo sui modelli\n",
    "            for model_name, model_data in models_dict.items():\n",
    "                name_model = model_name.split(\"_\")[0]  # Nome modello\n",
    "                print(f\"    Processing model: {name_model}\")\n",
    "                \n",
    "                try:\n",
    "                    # Recupera i risultati di training e testing\n",
    "                    train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                    test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "                    \n",
    "                    # Converti i valori in float\n",
    "                    train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                    test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "                    \n",
    "                    # Aggiungi le metriche di training\n",
    "                    df_data[f\"{name_model} (Training)\"] = [\n",
    "                        train_scores.get(\"train_accuracy\", float('nan')),\n",
    "                        train_scores.get(\"train_loss\", float('nan')),\n",
    "                        train_scores.get(\"train_precision\", float('nan')),\n",
    "                        train_scores.get(\"train_recall\", float('nan')),\n",
    "                        train_scores.get(\"train_f1_score\", float('nan')),\n",
    "                        train_scores.get(\"train_auc\", float('nan')),\n",
    "                    ]\n",
    "                    \n",
    "                    # Aggiungi le metriche di test\n",
    "                    df_data[f\"{name_model} (Testing)\"] = [\n",
    "                        test_scores.get(\"test_accuracy\", float('nan')),\n",
    "                        test_scores.get(\"test_loss\", float('nan')),\n",
    "                        test_scores.get(\"test_precision\", float('nan')),\n",
    "                        test_scores.get(\"test_recall\", float('nan')),\n",
    "                        test_scores.get(\"test_f1_score\", float('nan')),\n",
    "                        test_scores.get(\"test_auc\", float('nan')),\n",
    "                    ]\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "            \n",
    "            # Creazione del DataFrame\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "            \n",
    "            # Crea un'immagine della tabella\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.axis('off')\n",
    "            tabla = table(ax, df_performances, loc='center', colWidths=[0.2]*len(df_performances.columns))\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(10)\n",
    "            tabla.scale(2, 2)\n",
    "            \n",
    "            # Evidenzia i nomi delle colonne\n",
    "            for key, cell in tabla.get_celld().items():\n",
    "                if key[0] == 0:\n",
    "                    cell.set_text_props(weight='bold')\n",
    "            \n",
    "            # Salva l'immagine della tabella\n",
    "            file_name = f\"{condition}_{data_type}_{subfolder}_models.png\"\n",
    "            img_file_path = os.path.join(path, file_name)\n",
    "            fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"Tabella salvata in: {img_file_path}\")\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67778be2-73d7-4ccc-aa11-8b1f304b8b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NEW APPROACH \n",
    "\n",
    "Adesso replichiamo l'approccio usato prima, ma stavolta integrado tutte le combinazioni di dati. \n",
    "Andiamo a\n",
    "\n",
    "1) iterare sulla struttura delle directory a partire da base_folder, \n",
    "2) caricare i modelli .pkl per ogni combinazione di fattori che compongono i dati\n",
    "3) creare un DataFrame che raccolga le metriche di tutti i modelli relativi alla stessa combinazione di dati. \n",
    "\n",
    "Infine, salviamo questa tabella come immagine all'interno della cartella corrispondente\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "\n",
    "\n",
    "def safe_load_any(path):\n",
    "    # 1) Prova come salvataggio PyTorch (mappa tutto su CPU)\n",
    "    try:\n",
    "        return torch.load(path, map_location='cpu')\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) Fallback: semplice pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "# Base folder\n",
    "#base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_post_WB\"\n",
    "\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_time_frequency_best_models_post_WB_GradCAM_Checks\"\n",
    "                                        \n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "#experimental_conditions = [\"rest_vs_left_fist\", \"rest_vs_right_fist\", \"left_fist_vs_right_fist\"]\n",
    "#experimental_conditions = [\"rest_vs_both_feet\", \"rest_vs_both_fists\", \"both_feet_vs_both_fists\"]\n",
    "\n",
    "# Tipologie di dati\n",
    "data_types = [\"spectrograms\"]\n",
    "\n",
    "#data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "\n",
    "# Subfolders per tipologia di soggetto\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "#subfolders = [\"th_fam\"]\n",
    "\n",
    "# Dizionario per salvare tutti i modelli\n",
    "all_models = {}\n",
    "\n",
    "# Caricamento dei modelli\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Creiamo la chiave per questa combinazione\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            # Otteniamo la lista di file nella directory\n",
    "            files = os.listdir(path)\n",
    "            \n",
    "            '''\n",
    "            Così:\n",
    "            se il file è stato creato con torch.save(...) (e magari contiene tensori su GPU), verrà rimappato su CPU;\n",
    "            se è un pickle “normale”, si userà pickle.load.\n",
    "            '''\n",
    "            # Filtriamo e carichiamo i file .pkl\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    \n",
    "                    try:\n",
    "                        all_models[key][file] = safe_load_any(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "                        \n",
    "                        #with open(file_path, \"rb\") as f:\n",
    "                            #all_models[key][file] = pickle.load(f)\n",
    "                    #except Exception as e:\n",
    "                        #print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "# Creazione delle tabelle di performance\n",
    "for key, models_dict in all_models.items():\n",
    "    \n",
    "    # Otteniamo le informazioni dalla chiave\n",
    "    condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "    \n",
    "    print(f\"\\nProcessing: \\033[1m{condition}\\033[0m - \\033[1m{data_type}\\033[0m - \\033[1m{subfolder}\\033[0m\\n\")\n",
    "    \n",
    "    # Creazione della tabella\n",
    "    df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "    # Iteriamo sui modelli caricati\n",
    "    for model_name, model_data in models_dict.items():\n",
    "        \n",
    "        # Estrai il nome del modello dal file (ad esempio, \"CNN1D\" da \"CNN1D_performances_...pkl\")\n",
    "        name_model = model_name.split(\"_\")[0]\n",
    "        \n",
    "        if model_name.startswith((\"CNN2D_LSTM\")):\n",
    "            \n",
    "            # es. “CNN3D_LSTM_FC”  →  split → [“CNN3D”, “LSTM”, “FC”] → prendi i primi 2 elementi\n",
    "            parts = model_name.split(\"_\")\n",
    "            name_model = \"_\".join(parts[:2])      # “CNN3D_LSTM” o “SeparableCNN2D_LSTM”\n",
    "        else:\n",
    "            name_model = model_name.split(\"_\")[0]  # Prende solo CNN1D, CNN2D\n",
    "        \n",
    "        print(f\"    Processing model: \\033[1m{name_model}\\033[0m\")\n",
    "        \n",
    "        # Costruisci la chiave utilizzata nel dizionario models_info\n",
    "        \n",
    "        '''\n",
    "        Nota: occorrerà che il formato della chiave sia consistente tra i due loop.\n",
    "        \n",
    "        Ad esempio, se nel primo loop era f\"{key}_{model_name}\", qui potresti dover fare:\n",
    "        model_key = f\"{key}_{name_model}\"\n",
    "        \n",
    "        Oppure, se nel primo loop era f\"{model_name}_{key}\", qui potresti dover fare:\n",
    "        model_key = f\"{name_model}_{key}\"\n",
    "        \n",
    "        '''\n",
    "        model_key = f\"{name_model}_{key}\"\n",
    "        \n",
    "        # Controlla se i dati sono stati standardizzati per questo modello\n",
    "        standardization_flag = models_info.get(model_key, {}).get(\"standardization\", False)\n",
    "        \n",
    "        if standardization_flag:\n",
    "            suffix = \"\" \n",
    "        else:\n",
    "            suffix = \"\" \n",
    "        \n",
    "        try:\n",
    "            # Recupera i risultati di training e testing\n",
    "            train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "            test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "            \n",
    "            # Converti i valori in float\n",
    "            train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "            test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "            \n",
    "            \n",
    "            # Aggiunge le metriche di training, modificando il nome della colonna se è vera la condizione\n",
    "            col_train = f\"{name_model} (Training){suffix}\"  # Usa suffix qui per il nome\n",
    "            \n",
    "            df_data[f\"{col_train}\"] = [\n",
    "                train_scores[\"train_accuracy\"],\n",
    "                train_scores[\"train_loss\"],\n",
    "                train_scores[\"train_precision\"],\n",
    "                train_scores[\"train_recall\"],\n",
    "                train_scores[\"train_f1_score\"],\n",
    "                train_scores[\"train_auc\"],\n",
    "            ]\n",
    "\n",
    "            # Aggiunge le metriche di training, modificando il nome della colonna se è vera la condizione\n",
    "            col_test = f\"{name_model} (Test){suffix}\"  # Usa suffix qui per il nome\n",
    "            \n",
    "            df_data[f\"{col_test}\"] = [\n",
    "                test_scores[\"test_accuracy\"],\n",
    "                test_scores[\"test_loss\"],\n",
    "                test_scores[\"test_precision\"],\n",
    "                test_scores[\"test_recall\"],\n",
    "                test_scores[\"test_f1_score\"],\n",
    "                test_scores[\"test_auc\"],\n",
    "            ]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "    # Creazione del DataFrame\n",
    "    df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "    # Crea un'immagine della tabella\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Aggiunta del titolo\n",
    "    #title = f\"DL Models performances for Exp Conditions: {condition}, EEG data: {data_type}, Subject: {subfolder}\"\n",
    "    title = f\"DL Models performances for Exp Conditions: {condition}, EEG data: {data_type}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    tabla = table(ax, df_performances, loc='center', colWidths=[0.2] * len(df_performances.columns))\n",
    "    tabla.auto_set_font_size(True)\n",
    "    tabla.set_fontsize(10)\n",
    "    tabla.scale(2, 2)\n",
    "\n",
    "    # Evidenzia i nomi delle colonne\n",
    "    for key, cell in tabla.get_celld().items():\n",
    "        if key[0] == 0:\n",
    "            cell.set_text_props(weight='bold')\n",
    "\n",
    "    # Salva l'immagine della tabella\n",
    "    path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "    file_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "    img_file_path = os.path.join(path, file_name)\n",
    "    fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"\\nTabella dei dati di \\033[1m{condition}_{data_type}_{subfolder}\\033[0m salvati in: \\n\\033[1m{img_file_path}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607a166-8864-4cc1-927e-9d76f49bb826",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Integrazioni in Tabelle AGGREGATE delle Performance Training e Test del Modello dentro DataFrame - NEW APPROACH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc6228-ae82-44ca-a03d-6cea26c3544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perfetto ora va. ma io vorrei anche rendere le tabelle ancora più informative.. ossia\n",
    "\n",
    "vorrei ricreare lo stesso codice ma questa volta anziché avere una tabella specifica SOLO\n",
    "per un certo tipo di condizione sperimentale, tipo di dato e soggetto...\n",
    "\n",
    "io vorrei provare quanto meno ad 'allargare' le tabelle, nel senso di mettere nella stessa tabella\n",
    "la stessa condizione sperimentale e tipo di dato, per tutti e 3 i modelli, \n",
    "\n",
    "ma confrontando però la performance dello STESSO MODELLO per gli STESSI TIPI DI CONDIZIONE SPERIMENTALE, TIPO DI DATO e TIPI DI SOGGETTI (ossia RUOLO nel task)\n",
    "... ossia ad esempio\n",
    "\n",
    "\n",
    "A) Ossia.. quindi, farei prima i RUOLI di th_fam e th_unfam ...ossia\n",
    "\n",
    "per 'th_resp_vs_pt_resp' (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_45\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per th_fam \n",
    "2) sia per th_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp', (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_20\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per th_fam \n",
    "2) sia per th_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp', (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati delta_wavelet\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per th_fam \n",
    "2) sia per th_unfam.. \n",
    "\n",
    "in modo da avere un confronto diretto visivo per la stessa condizione sperimentale, stesso tipo di feature dei dati EEG usata, \n",
    "rispetto allo stesso modello, ma confrontando però la performance tra i due soggetti che hanno fatto lo STESSO RUOLO nei 2 gruppi (controllo e sperimentale).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "B) Allo stesso modo.. quindi, farei la STESSA COSA anche per i RUOLI di pt_fam e pt_unfam...ossia\n",
    "\n",
    " \n",
    "per 'th_resp_vs_pt_resp' (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_45\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per pt_fam \n",
    "2) sia per pt_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp', (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_20\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per pt_fam \n",
    "2) sia per pt_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp' (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!),, per i dati delta_wavelet\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test' \n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per pt_fam \n",
    "2) sia per pt_unfam.. \n",
    "  \n",
    "magari, nella prima riga metto le performance di training e test dei modelli che son con \"_fam\" \n",
    "e invece sotto le stesse performance dello stesso modello, condizione e tipo di dato, per chi è \"_unfam\", \n",
    "\n",
    "\n",
    "in modo da distinguire in base alla riga quali sono le performance di uno rispetto a quelle dell'altro soggetto, \n",
    "che avrà svolto lo stesso ruolo ma nel gruppo o di controllo o sperimentale...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9c87b-8704-4d0b-b225-ca3207311ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Yes! idea chiarissima. Senza stravolgere il tuo codice, \n",
    "aggiungi un secondo pass che costruisce (e salva) le tabelle aggregate per ruolo per ogni (condizione, data_type). \n",
    "\n",
    "Le colonne restano i 3 modelli × (Training/Test), le righe diventano le metriche replicate per i due soggetti del ruolo (fam / unfam). \n",
    "\n",
    "Il simbolo * per la standardizzazione lo mettiamo dentro la cella (così può cambiare tra fam e unfam).\n",
    "\n",
    "Incolla questo blocco dopo aver popolato all_models (puoi tenere anche le tabelle “singole” che già fai):\n",
    "\n",
    "\n",
    "\n",
    "# ===== TABELLE AGGREGATE PER RUOLO (th_fam vs th_unfam e pt_fam vs pt_unfam) =====\n",
    "\n",
    "MODEL_ORDER = [\"CNN1D\", \"BiLSTM\", \"Transformer\"]\n",
    "METRICS = [\n",
    "    (\"Accuracy\",  \"train_accuracy\", \"test_accuracy\"),\n",
    "    (\"Loss\",      \"train_loss\",     \"test_loss\"),\n",
    "    (\"Precision\", \"train_precision\",\"test_precision\"),\n",
    "    (\"Recall\",    \"train_recall\",   \"test_recall\"),\n",
    "    (\"F1-Score\",  \"train_f1_score\", \"test_f1_score\"),\n",
    "    (\"AUC-ROC\",   \"train_auc\",      \"test_auc\"),\n",
    "]\n",
    "\n",
    "def find_model_blob(all_models, condition, data_type, subfolder, model_prefix):\n",
    "    \"\"\"Ritorna il dict salvato a disco per il modello richiesto (o None).\"\"\"\n",
    "    key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "    if key not in all_models:\n",
    "        return None\n",
    "    for fname, blob in all_models[key].items():\n",
    "        # match robusto: inizia con \"<MODEL>_\"\n",
    "        if fname.startswith(model_prefix + \"_\"):\n",
    "            return blob\n",
    "    return None\n",
    "\n",
    "def fmt(v, star=False):\n",
    "    try:\n",
    "        val = float(v)\n",
    "        s = f\"{val:.3f}\"\n",
    "    except Exception:\n",
    "        s = \"-\"\n",
    "    return s + (\"*\" if star else \"\")\n",
    "\n",
    "# Gruppi di ruolo\n",
    "ROLE_GROUPS = {\n",
    "    \"THroles\": [\"th_fam\", \"th_unfam\"],\n",
    "    \"PTroles\": [\"pt_fam\", \"pt_unfam\"],\n",
    "}\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for role_label, subs in ROLE_GROUPS.items():\n",
    "\n",
    "            # Costruisci righe: una sezione per sub='..._fam' e una per '..._unfam'\n",
    "            rows = []\n",
    "            # Colonne: 3 modelli × (Training/Test)\n",
    "            columns = [\"Metriche\"]\n",
    "            for m in MODEL_ORDER:\n",
    "                columns.append(f\"{m} (Training)\")\n",
    "                columns.append(f\"{m} (Test)\")\n",
    "\n",
    "            df_data = {c: [] for c in columns}\n",
    "\n",
    "            for subfolder in subs:\n",
    "                # intestazione “visiva” delle righe: preferisci th_fam/th_unfam ecc.\n",
    "                for label, tr_key, te_key in METRICS:\n",
    "                    df_data[\"Metriche\"].append(f\"{subfolder} — {label}\")\n",
    "\n",
    "                    for m in MODEL_ORDER:\n",
    "                        # recupero blob salvato per quel subfolder/modello\n",
    "                        blob = find_model_blob(all_models, condition, data_type, subfolder, m)\n",
    "\n",
    "                        # standardization flag (per cella)\n",
    "                        mi_key = f\"{m}_{condition}_{data_type}_{subfolder}\"\n",
    "                        std_flag = bool(models_info.get(mi_key, {}).get(\"standardization\", False))\n",
    "\n",
    "                        if blob is None:\n",
    "                            # niente file -> celle vuote\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            tr = blob.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "                            te = blob.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "                            # training_performances/test_performances hanno valori come liste [val]\n",
    "                            tr_val = tr.get(tr_key, [None])[0]\n",
    "                            te_val = te.get(te_key, [None])[0]\n",
    "\n",
    "                            df_data[f\"{m} (Training)\"].append(fmt(tr_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Test)\"].append(fmt(te_val, star=std_flag))\n",
    "                        except Exception:\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "\n",
    "            # DataFrame e salvataggio\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            ax.axis('off')\n",
    "\n",
    "            title = f\"DL Models performances — {condition} — EEG: {data_type} — {role_label}\"\n",
    "            ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "            tabla = table(ax, df_performances, loc='center',\n",
    "                          colWidths=[0.25] + [0.12]*(len(df_performances.columns)-1))\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(9)\n",
    "            tabla.scale(1.2, 1.2)\n",
    "\n",
    "            for k, cell in tabla.get_celld().items():\n",
    "                if k[0] == 0:\n",
    "                    cell.set_text_props(weight='bold')\n",
    "\n",
    "            out_dir = os.path.join(base_folder, condition, data_type)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "            out_path = os.path.join(out_dir, out_name)\n",
    "            fig.savefig(out_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Tabella aggregata salvata: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031ab16-0e49-433b-b961-b96443be1872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "perfetto — qui sotto trovi il tuo script “chiavi in mano” con il secondo pass che genera anche le tabelle aggregate per ruolo \n",
    "\n",
    "(THroles = th_fam/th_unfam, PTroles = pt_fam/pt_unfam) per ogni coppia (condizione, data_type).\n",
    "\n",
    "Ho aggiunto:\n",
    "\n",
    "parse_combination_models_keys() con wavelet_delta nel regex.\n",
    "\n",
    "Caricamento “robusto” di models_info (se non esiste, procede senza *).\n",
    "\n",
    "Funzioni di supporto find_model_blob() e fmt().\n",
    "\n",
    "Secondo pass che salva i PNG ..._{condition}_{data_type}_{THroles|PTroles}.png nella cartella di quella coppia.\n",
    "\n",
    "\n",
    "Se vuoi nascondere completamente il vecchio primo pass e tenere solo i comparativi per ruolo, basta commentare/bloccare la sezione “Pass 1”.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Puoi commentare tutto il “Pass 1” senza problemi: il “Pass 2” non dipende da quello.\n",
    "Il “Pass 2” usa solo:\n",
    "\n",
    "all_models (riempito nel blocco di caricamento iniziale, prima del Pass 1),\n",
    "\n",
    "models_info (per mettere l’asterisco * se standardizzato),\n",
    "\n",
    "le funzioni helper (find_model_blob, fmt) e le costanti (MODEL_ORDER, ROLE_GROUPS, ecc.).\n",
    "Quindi, finché lasci il caricamento di all_models e gli helper, funziona da solo.\n",
    "\n",
    "Sì, le tabelle aggregate del Pass 2 vengono salvate esattamente nel percorso costruito da queste righe:\n",
    "\n",
    "out_dir = os.path.join(base_folder, condition, data_type)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "\n",
    "\n",
    "quindi avrai file tipo:\n",
    "\n",
    "/home/stefano/Interrogait/time_domain_1D_best_models_post_WB/th_resp_vs_pt_resp/1_45/models_performances_th_resp_vs_pt_resp_1_45_THroles.png\n",
    "\n",
    "/home/stefano/Interrogait/time_domain_1D_best_models_post_WB/pt_resp_vs_shared_resp/wavelet_delta/models_performances_pt_resp_vs_shared_resp_wavelet_delta_PTroles.png\n",
    "\n",
    "Se preferisci tenerle in una sottocartella tipo aggregated/, cambia così:\n",
    "\n",
    "out_dir = os.path.join(base_folder, condition, data_type, \"aggregated\")\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def parse_combination_models_keys(combination_key: str):\n",
    "    \"\"\"\n",
    "    Ritorna (exp_cond, data_type, category_subject) da chiavi tipo:\n",
    "    th_resp_vs_pt_resp_1_45_th_fam\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        #r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(1_20|1_45|wavelet_delta)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "# Carica models_info (flag standardization per cella). Se non c'è, prosegue senza '*'\n",
    "try:\n",
    "    with open(\"/home/stefano/Interrogait/models_info_spectrograms_time_frequency_EEG_GradCAM_Checks.pkl\", \"rb\") as f:\n",
    "        models_info = pickle.load(f)\n",
    "except Exception:\n",
    "    print(\"⚠️  models_info non trovato/caricabile: le tabelle verranno create senza indicatore * di standardizzazione.\")\n",
    "    models_info = {}\n",
    "\n",
    "def find_model_blob(all_models, condition, data_type, subfolder, model_prefix):\n",
    "    \"\"\"Ritorna il dict salvato a disco per il modello richiesto (o None) cercando per filename prefix.\"\"\"\n",
    "    key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "    if key not in all_models:\n",
    "        return None\n",
    "    for fname, blob in all_models[key].items():\n",
    "        if fname.startswith(model_prefix + \"_\"):\n",
    "            return blob\n",
    "    return None\n",
    "\n",
    "def fmt(v, star=False):\n",
    "    \"\"\"Formatta un valore numerico a 3 decimali e aggiunge '*' se standardizzato.\"\"\"\n",
    "    try:\n",
    "        val = float(v)\n",
    "        s = f\"{val:.3f}\"\n",
    "    except Exception:\n",
    "        s = \"-\"\n",
    "    return s + (\"*\" if star else \"\")\n",
    "\n",
    "# ---------- Config ----------\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_time_frequency_best_models_post_WB_GradCAM_Checks\"\n",
    "\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "#data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "data_types = [\"spectrograms\"]\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "MODEL_ORDER = [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"]\n",
    "METRICS = [\n",
    "    (\"Accuracy\",  \"train_accuracy\", \"test_accuracy\"),\n",
    "    (\"Loss\",      \"train_loss\",     \"test_loss\"),\n",
    "    (\"Precision\", \"train_precision\",\"test_precision\"),\n",
    "    (\"Recall\",    \"train_recall\",   \"test_recall\"),\n",
    "    (\"F1-Score\",  \"train_f1_score\", \"test_f1_score\"),\n",
    "    (\"AUC-ROC\",   \"train_auc\",      \"test_auc\"),\n",
    "]\n",
    "\n",
    "ROLE_GROUPS = {\n",
    "    \"Observer_Role\": [\"th_fam\", \"th_unfam\"],\n",
    "    \"Receiver_Role\": [\"pt_fam\", \"pt_unfam\"],\n",
    "}\n",
    "\n",
    "\n",
    "# --- aggiungi in alto (vicino a ROLE_GROUPS / MODEL_ORDER) ---\n",
    "DISPLAY_LABELS = {\n",
    "    \"th_fam\":   \"observer_fam\",\n",
    "    \"th_unfam\": \"observer_unfam\",\n",
    "    \"pt_fam\":   \"receiver_fam\",\n",
    "    \"pt_unfam\": \"receiver_unfam\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Caricamento modelli ----------\n",
    "all_models = {}\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            for file in os.listdir(path):\n",
    "                if file.endswith(\".pkl\"):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "# ---------- Pass 1: tabella per singola combinazione (come avevi) ----------\n",
    "\n",
    "'''\n",
    "for key, models_dict in all_models.items():\n",
    "    condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "    print(f\"\\nProcessing: {condition} - {data_type} - {subfolder}\\n\")\n",
    "\n",
    "    df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "    for filename, model_data in models_dict.items():\n",
    "        name_model = filename.split(\"_\")[0]  # CNN1D / BiLSTM / Transformer\n",
    "\n",
    "        # Standardization: usa models_info[colonna] a livello di modello-subfolder\n",
    "        model_key = f\"{name_model}_{key}\"\n",
    "        standardization_flag = bool(models_info.get(model_key, {}).get(\"standardization\", False))\n",
    "        suffix = \"*\" if standardization_flag else \"\"\n",
    "\n",
    "        try:\n",
    "            train_scores = model_data.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "            test_scores  = model_data.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "            # convert list -> float\n",
    "            train_scores = {k: float(v[0]) for k, v in train_scores.items()}\n",
    "            test_scores  = {k: float(v[0]) for k, v in test_scores.items()}\n",
    "\n",
    "            col_train = f\"{name_model} (Training){suffix}\"\n",
    "            col_test  = f\"{name_model} (Test){suffix}\"\n",
    "\n",
    "            df_data[col_train] = [\n",
    "                train_scores.get(\"train_accuracy\", float(\"nan\")),\n",
    "                train_scores.get(\"train_loss\", float(\"nan\")),\n",
    "                train_scores.get(\"train_precision\", float(\"nan\")),\n",
    "                train_scores.get(\"train_recall\", float(\"nan\")),\n",
    "                train_scores.get(\"train_f1_score\", float(\"nan\")),\n",
    "                train_scores.get(\"train_auc\", float(\"nan\")),\n",
    "            ]\n",
    "            df_data[col_test] = [\n",
    "                test_scores.get(\"test_accuracy\", float(\"nan\")),\n",
    "                test_scores.get(\"test_loss\", float(\"nan\")),\n",
    "                test_scores.get(\"test_precision\", float(\"nan\")),\n",
    "                test_scores.get(\"test_recall\", float(\"nan\")),\n",
    "                test_scores.get(\"test_f1_score\", float(\"nan\")),\n",
    "                test_scores.get(\"test_auc\", float(\"nan\")),\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"    Errore nell'elaborazione di {filename}: {e}\")\n",
    "\n",
    "    df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.axis(\"off\")\n",
    "    title = f\"DL Models performances for Exp Conditions: {condition}, EEG data: {data_type}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    tabla = table(ax, df_performances, loc=\"center\", colWidths=[0.2] * len(df_performances.columns))\n",
    "    tabla.auto_set_font_size(True)\n",
    "    tabla.set_fontsize(10)\n",
    "    tabla.scale(2, 2)\n",
    "\n",
    "    for kcell, cell in tabla.get_celld().items():\n",
    "        if kcell[0] == 0:\n",
    "            cell.set_text_props(weight=\"bold\")\n",
    "\n",
    "    out_dir = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Tabella singola salvata: {out_path}\")\n",
    "'''\n",
    "\n",
    "\n",
    "# ---------- Pass 2: tabelle aggregate per ruolo ----------\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for role_label, subs in ROLE_GROUPS.items():\n",
    "\n",
    "            columns = [\"Metriche\"]\n",
    "            for m in MODEL_ORDER:\n",
    "                columns.append(f\"{m} (Training)\")\n",
    "                columns.append(f\"{m} (Test)\")\n",
    "\n",
    "            df_data = {c: [] for c in columns}\n",
    "\n",
    "            for idx_sub, subfolder in enumerate(subs):\n",
    "                \n",
    "                '''CONVERSIONE LABELS DEL RUOLO --> da th_fam a observer_fam etc'''\n",
    "                # solo per la visualizzazione converto th_fam->observer_fam, ecc.\n",
    "                subfolder_disp = DISPLAY_LABELS.get(subfolder, subfolder)\n",
    "                \n",
    "                # per ciascun subfolder (fam / unfam) aggiungo le 6 metriche\n",
    "                for label, tr_key, te_key in METRICS:\n",
    "                    \n",
    "                    #df_data[\"Metriche\"].append(f\"{subfolder} — {label}\")\n",
    "                    \n",
    "                    # usa il suffisso \"display\" SOLO per la label della riga\n",
    "                    df_data[\"Metriche\"].append(f\"{subfolder_disp} — {label}\")\n",
    "                    \n",
    "\n",
    "                    for m in MODEL_ORDER:\n",
    "                        blob = find_model_blob(all_models, condition, data_type, subfolder, m)\n",
    "\n",
    "                        mi_key = f\"{m}_{condition}_{data_type}_{subfolder}\"\n",
    "                        std_flag = bool(models_info.get(mi_key, {}).get(\"standardization\", False))\n",
    "\n",
    "                        if blob is None:\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            tr = blob.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "                            te = blob.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "                            tr_val = tr.get(tr_key, [None])[0]\n",
    "                            te_val = te.get(te_key, [None])[0]\n",
    "\n",
    "                            df_data[f\"{m} (Training)\"].append(fmt(tr_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Test)\"].append(fmt(te_val, star=std_flag))\n",
    "                        except Exception:\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "\n",
    "                # riga separatrice tra fam e unfam (opzionale ma utile visivamente)\n",
    "                if idx_sub == 0:\n",
    "                    df_data[\"Metriche\"].append(\"\")  # riga vuota\n",
    "                    for m in MODEL_ORDER:\n",
    "                        df_data[f\"{m} (Training)\"].append(\"\")\n",
    "                        df_data[f\"{m} (Test)\"].append(\"\")\n",
    "\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            SHOW_ONLY = False  # <- True per visualizzare, False per salvare\n",
    "            \n",
    "            \n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            title = f\"DL Models performances — {condition} — EEG feature: {data_type} — {role_label}\"\n",
    "            ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "            tabla = table(\n",
    "                ax,\n",
    "                df_performances,\n",
    "                loc=\"center\",\n",
    "                colWidths=[0.25] + [0.12] * (len(df_performances.columns) - 1),\n",
    "            )\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(9)\n",
    "            tabla.scale(1.2, 1.2)\n",
    "\n",
    "            for kcell, cell in tabla.get_celld().items():\n",
    "                if kcell[0] == 0:\n",
    "                    cell.set_text_props(weight=\"bold\")\n",
    "            \n",
    "            \n",
    "            '''Con \"aggregated\", io aggiungo una sotto-cartella ancora alla path di salvataggio delle tabelle'''\n",
    "            \n",
    "            if SHOW_ONLY:\n",
    "                plt.show()\n",
    "                print(f\"Tabella aggregata di: models_performances_{condition}_{data_type}_{role_label}.png\")\n",
    "            else:\n",
    "                \n",
    "                out_dir = os.path.join(base_folder, condition, data_type, \"aggregated\")\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "                out_path = os.path.join(out_dir, out_name)\n",
    "                fig.savefig(out_path, bbox_inches=\"tight\", dpi=300)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"Tabella aggregata salvata: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993d368-e93a-40b9-8760-54dcd0245f9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Implementazione : Versione dal 24 novembre 2025 - Versione Aggregata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af1a47-335f-443f-96ac-a120d26be448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''METRICHE PRIMA DI TUTTI I MODELLI SUL TRAIN ... POI SUL VALIDATION ...  E POI SUL TEST SET'''\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def find_model_blob(all_models, condition, data_type, subfolder, model_prefix):\n",
    "    \"\"\"Ritorna il dict salvato a disco per il modello richiesto (o None) cercando per filename prefix.\"\"\"\n",
    "    key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "    if key not in all_models:\n",
    "        return None\n",
    "    for fname, blob in all_models[key].items():\n",
    "        if fname.startswith(model_prefix + \"_\"):\n",
    "            return blob\n",
    "    return None\n",
    "\n",
    "\n",
    "def fmt(v, star=False):\n",
    "    \"\"\"Formatta un valore numerico a 3 decimali e aggiunge '*' se standardizzato (se vuoi).\"\"\"\n",
    "    try:\n",
    "        val = float(v)\n",
    "        s = f\"{val:.3f}\"\n",
    "    except Exception:\n",
    "        s = \"-\"\n",
    "    return s + (\"\" if star else \"\")\n",
    "\n",
    "\n",
    "def format_col_label(label: str) -> str:\n",
    "    \"\"\"\n",
    "    Converte 'CNN1D (Training)' -> 'CNN1D\\n(Training)' per header a due righe.\n",
    "    Lascia 'Metrics' invariato.\n",
    "    \"\"\"\n",
    "    if label == \"Metrics\":\n",
    "        return label\n",
    "    if \"(\" in label and label.endswith(\")\"):\n",
    "        model, phase = label.split(\"(\", 1)\n",
    "        model = model.strip()\n",
    "        phase = \"(\" + phase.strip()\n",
    "        return f\"{model}\\n{phase}\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def pretty_condition_name(cond: str) -> str:\n",
    "    \"\"\"\n",
    "    th_resp_vs_pt_resp        -> 'observer resp vs receiver resp'\n",
    "    th_resp_vs_shared_resp    -> 'observer resp vs shared resp'\n",
    "    pt_resp_vs_shared_resp    -> 'receiver resp vs shared resp'\n",
    "    \"\"\"\n",
    "    token_map = {\n",
    "        \"th_resp\": \"observer resp\",\n",
    "        \"pt_resp\": \"receiver resp\",\n",
    "        \"shared_resp\": \"shared resp\",\n",
    "    }\n",
    "    parts = cond.split(\"_vs_\")\n",
    "    pretty_parts = [token_map.get(p, p.replace(\"_\", \" \")) for p in parts]\n",
    "    return \" vs \".join(pretty_parts)\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_time_frequency_best_models_post_WB_GradCAM_Checks\"\n",
    "\n",
    "experimental_conditions = [\n",
    "    \"th_resp_vs_pt_resp\",\n",
    "    \"th_resp_vs_shared_resp\",\n",
    "    \"pt_resp_vs_shared_resp\",\n",
    "]\n",
    "\n",
    "#data_types = [\"1_20\", \"1_45\", \"wavelet\"]\n",
    "data_types = [\"spectrograms\"]\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "#MODEL_ORDER = [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"]\n",
    "\n",
    "MODEL_ORDER = [\"CNN2D_LSTM\", \"BiLSTM\", \"Transformer\"]\n",
    "PHASES = [\"Training\", \"Validation\", \"Test\"]   # <--- aggiunto per chiarezza\n",
    "\n",
    "# (label, train_key, val_key, test_key)\n",
    "METRICS = [\n",
    "    (\"Accuracy\",  \"train_accuracy\",  \"val_accuracy\",  \"test_accuracy\"),\n",
    "    (\"Loss\",      \"train_loss\",      \"val_loss\",      \"test_loss\"),\n",
    "    (\"Precision\", \"train_precision\", \"val_precision\", \"test_precision\"),\n",
    "    (\"Recall\",    \"train_recall\",    \"val_recall\",    \"test_recall\"),\n",
    "    (\"F1-Score\",  \"train_f1_score\",  \"val_f1_score\",  \"test_f1_score\"),\n",
    "    (\"AUC-ROC\",   \"train_auc\",       \"val_auc\",       \"test_auc\"),\n",
    "]\n",
    "\n",
    "ROLE_GROUPS = {\n",
    "    \"Observer_Role\": [\"th_fam\", \"th_unfam\"],\n",
    "    \"Receiver_Role\": [\"pt_fam\", \"pt_unfam\"],\n",
    "}\n",
    "\n",
    "# Etichette che appariranno nella colonna \"Metrics\"\n",
    "DISPLAY_LABELS = {\n",
    "    \"th_fam\":   \"observers familiar group\",\n",
    "    \"th_unfam\": \"observers unfamiliar group\",\n",
    "    \"pt_fam\":   \"receivers familiar group\",\n",
    "    \"pt_unfam\": \"receivers unfamiliar group\",\n",
    "}\n",
    "\n",
    "# Etichette per il tipo di dato nel titolo\n",
    "DATA_LABELS = {\n",
    "    \"spectrograms\": \"Time x Frequency\"\n",
    "}\n",
    "\n",
    "SHOW_ONLY = False  # cambia in False per salvare i PNG\n",
    "#SHOW_ONLY = True  # cambia in False per salvare i PNG\n",
    "\n",
    "# ---------- Carica models_info se esiste ----------\n",
    "try:\n",
    "    with open(\"/home/stefano/Interrogait/models_info_spectrograms_time_frequency_EEG_GradCAM_Checks.pkl\", \"rb\") as f:\n",
    "        models_info = pickle.load(f)\n",
    "except Exception:\n",
    "    print(\"⚠️  models_info non trovato/caricabile: nessun indicatore di standardizzazione (*).\")\n",
    "    models_info = {}\n",
    "\n",
    "# ---------- Caricamento modelli ----------\n",
    "all_models = {}\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            for file in os.listdir(path):\n",
    "                if file.endswith(\".pkl\"):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#  PASS 2: tabelle aggregate per ruolo (Observer / Receiver)\n",
    "# =========================================================\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for role_label, subs in ROLE_GROUPS.items():\n",
    "\n",
    "            print(f\"\\nProcessing aggregate table: {condition} - {data_type} - {role_label}\")\n",
    "\n",
    "            # ---------- COSTRUZIONE DF ----------\n",
    "            # ORA: prima tutte le colonne Train (tutti i modelli),\n",
    "            #      poi tutte le colonne Validation, poi Test.\n",
    "            columns = [\"Metrics\"]\n",
    "            for phase in PHASES:                      # <- loop su Training / Validation / Test\n",
    "                for m in MODEL_ORDER:                 #    e dentro sui modelli\n",
    "                    columns.append(f\"{m} ({phase})\")\n",
    "\n",
    "            df_data = {c: [] for c in columns}\n",
    "\n",
    "            for idx_sub, subfolder in enumerate(subs):\n",
    "\n",
    "                subfolder_disp = DISPLAY_LABELS.get(subfolder, subfolder)\n",
    "\n",
    "                for label, tr_key, val_key, te_key in METRICS:\n",
    "\n",
    "                    df_data[\"Metrics\"].append(f\"{subfolder_disp} — {label}\")\n",
    "\n",
    "                    for m in MODEL_ORDER:\n",
    "                        blob = find_model_blob(all_models, condition, data_type, subfolder, m)\n",
    "\n",
    "                        mi_key = f\"{m}_{condition}_{data_type}_{subfolder}\"\n",
    "                        std_flag = bool(models_info.get(mi_key, {}).get(\"standardization\", False))\n",
    "\n",
    "                        if blob is None:\n",
    "                            for phase in PHASES:\n",
    "                                df_data[f\"{m} ({phase})\"].append(\"-\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            tr = blob.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "                            va = blob.get(\"my_train_results\", {}).get(\"validation_performances\", {})\n",
    "                            te = blob.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "                            tr_val = tr.get(tr_key,  [None])[0]\n",
    "                            va_val = va.get(val_key, [None])[0]\n",
    "                            te_val = te.get(te_key, [None])[0]\n",
    "\n",
    "                            df_data[f\"{m} (Training)\"].append(fmt(tr_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Validation)\"].append(fmt(va_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Test)\"].append(fmt(te_val, star=std_flag))\n",
    "\n",
    "                        except Exception:\n",
    "                            for phase in PHASES:\n",
    "                                df_data[f\"{m} ({phase})\"].append(\"-\")\n",
    "\n",
    "                # riga vuota di separazione (fam / unfam)\n",
    "                if idx_sub == 0:\n",
    "                    df_data[\"Metrics\"].append(\"\")\n",
    "                    for phase in PHASES:\n",
    "                        for m in MODEL_ORDER:\n",
    "                            df_data[f\"{m} ({phase})\"].append(\"\")\n",
    "\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            # =========================\n",
    "            #  PREPARAZIONE PER PLOT\n",
    "            # =========================\n",
    "            df_display = df_performances.copy()\n",
    "\n",
    "            col_weights = []\n",
    "            for col in df_display.columns:\n",
    "                header_len = len(str(col))\n",
    "                body_max = df_display[col].astype(str).map(len).max()\n",
    "                col_weights.append(max(header_len, body_max))\n",
    "\n",
    "            col_weights = np.array(col_weights, dtype=float)\n",
    "            col_weights[0] *= 1.4  # \"Metrics\" più larga\n",
    "            col_widths = (col_weights / col_weights.sum()) * 0.98\n",
    "\n",
    "            # ---------- FIGURA & AX ----------\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            cond_pretty = pretty_condition_name(condition)\n",
    "            data_pretty = DATA_LABELS.get(data_type, data_type)\n",
    "            role_pretty = role_label.replace(\"_\", \" \")\n",
    "\n",
    "            line1 = \"Deep Learning Models performances for Brain Decoding of Sense of Responsibility\"\n",
    "            line2 = f\"Experimental Conditions: {cond_pretty} — EEG Spectrogram: {data_pretty} — Subject Cohort: {role_pretty}\"\n",
    "\n",
    "            ax.set_title(\n",
    "                f\"{line1}\\n{line2}\",\n",
    "                fontsize=11,\n",
    "                pad=6,\n",
    "            )\n",
    "\n",
    "            col_labels = [format_col_label(c) for c in df_display.columns]\n",
    "\n",
    "            tabla = ax.table(\n",
    "                cellText=df_display.values,\n",
    "                colLabels=col_labels,\n",
    "                loc=\"upper center\",\n",
    "                cellLoc=\"center\",\n",
    "                colWidths=col_widths.tolist(),\n",
    "            )\n",
    "\n",
    "            tabla.auto_set_font_size(False)\n",
    "            base_fontsize = 6\n",
    "            header_fontsize = 6\n",
    "\n",
    "            tabla.set_fontsize(base_fontsize)\n",
    "            tabla.scale(1.1, 1.1)\n",
    "\n",
    "            for (row, col), cell in tabla.get_celld().items():\n",
    "                if row == 0:\n",
    "                    cell.set_text_props(weight=\"bold\", fontsize=header_fontsize)\n",
    "\n",
    "            if SHOW_ONLY:\n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "                #print(f\"Tabella aggregata mostrata: {condition} - {data_type} - {role_label}\")\n",
    "                #out_dir = os.path.join(base_folder, condition, data_type)\n",
    "                #os.makedirs(out_dir, exist_ok=True)\n",
    "                #out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "                #out_path = os.path.join(out_dir, out_name)\n",
    "                #print(f'out_path: {out_path}') \n",
    "            else:\n",
    "                out_dir = os.path.join(base_folder, condition, data_type, \"aggregated\")\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "                out_path = os.path.join(out_dir, out_name)\n",
    "                print(f'out_path: \\033[1m{out_path}\\033[0m') \n",
    "\n",
    "                fig.savefig(out_path, bbox_inches=\"tight\", dpi=300)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"Tabella aggregata salvata: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408091b-576b-4723-b895-3d86e6f30de4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Impostazione **Weight & Biases DL Training** con **Rappresentazione Electrodes x Frequencies Signal (2D)** \n",
    "\n",
    "## Optimization Weight and Biases - EEG Spectrograms - Electrodes x Frequencies \n",
    "### (Solo Iper-parametri Dinamici)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d064b3-2897-49e3-a71f-4341efa1c73d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure FINAL SEQUENCE OF STEPS - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc31f3-39d4-4b95-b7f9-9d29f418764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Importing \n",
    "    \n",
    "    \n",
    "#import mne \n",
    "\n",
    "import os\n",
    "import math\n",
    "import copy as cp \n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import scipy\n",
    "\n",
    "import numpy as np  # NumPy per operazioni numeriche\n",
    "import matplotlib.pyplot as plt  # Matplotlib per la visualizzazione dei dati\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567ac81-539a-43e3-ac02-95013a6f6aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())  # Numero di GPU disponibili\n",
    "print(torch.cuda.current_device())  # ID della GPU in uso\n",
    "print(torch.cuda.get_device_name(0))  # Nome della GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e98c35-f528-4715-bd0a-d42ef120ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec4115-36ee-44a2-b95f-4aed3f05f8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "\n",
    "# Apri il file in modalità lettura binaria ('rb')\n",
    "\n",
    "#path = '/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms/'\n",
    "\n",
    "#path = '/home/stefano/Interrogait/all_datas/Familiar_Spectrograms_channels_frequencies/'\n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/Familiar_Spectrograms_channels_frequencies/'\n",
    "\n",
    "with open(f\"{path}new_all_th_concat_spectrograms_coupled_exp.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0910c2f-dc3d-48e4-8192-1d383c4fe548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Itera sulle chiavi del dizionario principale\n",
    "for condition, values in data.items():\n",
    "    if isinstance(values, dict) and \"data\" in values and \"labels\" in values:\n",
    "        X_shape = values[\"data\"].shape\n",
    "        y_length = len(values[\"labels\"])\n",
    "        print(f\"🔹 Condizione: {condition}\")\n",
    "        print(f\"   ➡ Shape dati: {X_shape}\")\n",
    "        print(f\"   ➡ Lunghezza labels: {y_length}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fa1a8-49b0-40b6-a690-7618fbf66988",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Utils Functions - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee2ee7-26bd-4ffc-8877-d09321004be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "QUI DENTRO HO CONFIGURATO \n",
    "LE FUNZIONI DI CONTROLLO DELLE STRINGHE \n",
    "PER IL SALVATAGGIO DELLE PERFORMANCE DEL MODELLO\n",
    "NELLE RELATIVE SUBFOLDERS\n",
    "\n",
    "(I.E., get_subfolder_from_key, get_subfolder_from_key_hyper)\n",
    "\n",
    "IN MODO CHE SI LEGHINO ALLA CHIAVE 'STANDARDIZATION' DELL'OGGETTO SWEEP_CONFIG\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data_hyper(data_type, category, wavelet_level=None, condition= \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Carica i dati EEG dalla directory appropriata, seleziona la finestra temporale (50°-300° punto, ossia 0-1000 mms).\n",
    "\n",
    "    Parameters:\n",
    "    - data_type: str, \"1_20\", \"1_45\" o \"wavelet\"\n",
    "    - category: str, \"familiar\" o \"unfamiliar\"\n",
    "    - wavelet_level: str, \"theta\", \"delta\", ecc. (solo per dati wavelet)\n",
    "    - condition: str, condizione sperimentale da selezionare\n",
    "\n",
    "    Returns:\n",
    "    - X: Dati EEG sotto-selezionati (50°-300° punto)\n",
    "    - y: Etichette corrispondenti\n",
    "    \"\"\"\n",
    "    # Definizione dei percorsi base\n",
    "    base_paths = {\n",
    "        \"1_20\": {\n",
    "            \"familiar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_20/hyper_dataset_EEG_preprocessed_1_20_familiar_{condition}.pkl\",\n",
    "            \"unfamiliar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_20/hyper_dataset_EEG_preprocessed_1_20_unfamiliar_{condition}.pkl\"\n",
    "        },\n",
    "        \"1_45\": {\n",
    "            \"familiar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_45/hyper_dataset_EEG_preprocessed_1_45_familiar_{condition}.pkl\",\n",
    "            \"unfamiliar\": f\"/home/stefano/Interrogait/all_datas/Hyper_Datasets_EEG_1_45/hyper_dataset_EEG_preprocessed_1_45_unfamiliar_{condition}.pkl\"\n",
    "        },\n",
    "        \"wavelet\": {\n",
    "            \"familiar\": \"/home/stefano/Interrogait/all_datas/Hyper_Datasets_Wavelet_Reconstructions/hyper_dataset_wavelet_familiar.pkl\",\n",
    "            \"unfamiliar\": \"/home/stefano/Interrogait/all_datas/Hyper_Datasets_Wavelet_Reconstructions/hyper_dataset_wavelet_unfamiliar.pkl\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Seleziona il path corretto\n",
    "    filepath = base_paths[data_type][category]\n",
    "    \n",
    "    # Caricamento del file\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Selezione della finestra temporale e delle etichette\n",
    "    X = data[wavelet_level][condition][\"data\"][:, :, 125:200] if data_type == \"wavelet\" else data[\"data\"][:, :, 50:300]\n",
    "    y = data[wavelet_level][condition][\"labels\"] if data_type == \"wavelet\" else data[\"labels\"]\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "\n",
    "'''\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key_hyper(key, sweep_config):\n",
    "    \n",
    "    \n",
    "    #Mi richiamo la chiave 'standardization' che ho impostato nella configurazione dell'oggetto weight and biases\n",
    "    #(i.e., sweep_config['standardization']) e eseguo una procedura condizionale \n",
    "    \n",
    "    #ossia che, se risulta o True o False, lui cambi le condizioni di gestione \n",
    "    #della costruzione delle path di salvataggio \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Controlla se i dati sono standardizzati\n",
    "    if sweep_config['standardization']:\n",
    "        \n",
    "        #PER I DATI SCALED\n",
    "            \n",
    "        if '_familiar' in key:\n",
    "            return 'HYPER_FAM'\n",
    "        elif '_unfamiliar' in key:\n",
    "            return 'HYPER_UNFAM'\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        \n",
    "        #PER I DATI UNSCALED\n",
    "        if '_familiar' in key:\n",
    "            return 'HYPER_FAM_UNSCALED'\n",
    "        elif '_unfamiliar' in key:\n",
    "            return 'HYPER_UNFAM_UNSCALED'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results_hyper(model_name, my_train_results, my_test_results, key, data_type, sweep_config, condition= \"th_resp_vs_pt_resp\"):\n",
    "    \n",
    "    #Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key_hyper(key, sweep_config)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"wavelet\" in key:\n",
    "        data_type_str = \"wavelet_delta\"\n",
    "    elif \"1_20\" in key:\n",
    "        data_type_str = \"1_20\"\n",
    "    elif \"1_45\" in key:\n",
    "        data_type_str = \"1_45\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    file_name = f\"{model_name}_performances_{condition}_{subfolder}_{data_type_str}.pkl\"\n",
    "    folder_path = os.path.join(base_folder, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo ✅ in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei risultati: {e}\")\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_type, category, subject_type, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Carica i dati EEG dalla directory appropriata, già salvati con la finestra temporale (50°-300° punto)\n",
    "\n",
    "    Parameters:\n",
    "    - data_type: str, \"spectrograms\",\n",
    "    - category: str, \"familiar\" o \"unfamiliar\"\n",
    "    - subject_type: str, \"th\" (terapisti) o \"pt\" (pazienti)\n",
    "    - condition: str, condizione sperimentale da selezionare\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - X: Dati EEG sotto-selezionati (50°-300° punto e canali selezionati se applicabile)\n",
    "    - y: Etichette corrispondenti\n",
    "    \"\"\"\n",
    "\n",
    "    # Definizione dei percorsi base\n",
    "    base_paths = {\n",
    "        \"spectrograms\": {\n",
    "            \"familiar\": \"/home/stefano/Interrogait/all_datas/Familiar_Spectrograms_channels_frequencies/\",\n",
    "            \"unfamiliar\": \"/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms_channels_frequencies/\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Seleziona il path corretto\n",
    "    base_path = base_paths[data_type][category]\n",
    "\n",
    "    # Determina il nome del file corretto\n",
    "    if data_type in [\"spectrograms\"]:\n",
    "        filename = f\"new_all_{subject_type}_concat_spectrograms_coupled_exp.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"data_type non valido!\")\n",
    "        \n",
    "    # Caricamento del file\n",
    "    filepath = base_path + filename\n",
    "    \n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    '''\n",
    "    Per i dati spectrogram, la funzione seleziona la condizione desiderata (i.e., condition = \"th_resp_vs_pt_resp\") \n",
    "    e preleva i dati e le etichette associati a quella condizione.\n",
    "    '''\n",
    "    \n",
    "    # Selezione della finestra temporale e delle etichette\n",
    "    X = data[condition][\"data\"]\n",
    "    y = data[condition][\"labels\"]\n",
    "\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def select_channels(data, channels=[12, 30, 48]):\n",
    "    \"\"\"\n",
    "    Seleziona i canali EEG specificati SOLO per i dati 1-20 e 1-45.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array NumPy, dati EEG con shape (n_trials, n_channels, n_timepoints)\n",
    "    - channels: list, indici dei canali da selezionare\n",
    "\n",
    "    Returns:\n",
    "    - data filtrato sui canali specificati\n",
    "    \"\"\"\n",
    "    return data[:, channels, :]\n",
    "\n",
    "\n",
    "# Funzione per train-test split\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "'''ATTENZIONE MODIFICATA FUNZIONE DI STANDARDIZZAZIONE'''\n",
    "# Funzione per standardizzare i dati\n",
    "# Con questa modifica eviti che std==0 produca NaN e i tuoi loss torneranno numeri sensati.\n",
    "def standardize_data(X_train, X_val, X_test, eps = 1e-8):\n",
    "    \n",
    "    mean = X_train.mean(axis=0, keepdims=True)\n",
    "    std = X_train.std(axis=0, keepdims=True)\n",
    "    \n",
    "    #aggiungo eps per evitare divisione per zero\n",
    "    X_train = (X_train - mean) / (std + eps)\n",
    "    X_val = (X_val - mean) / (std + eps)\n",
    "    X_test = (X_test - mean) / (std + eps)\n",
    "    \n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "# Import modelli (definisci le classi CNN1D, ReadMEndYou, ReadMYMind)\n",
    "#from models import CNN1D, ReadMEndYou, ReadMYMind  # Assicurati di avere i modelli definiti in 'models.py'\n",
    "\n",
    "# Funzione per inizializzare i modelli\n",
    "def initialize_models():\n",
    "    #model = CNN1D(input_channels=3, num_classes=2)\n",
    "    model_CNN = CNN2D(input_channels = 61, num_classes=2)\n",
    "    #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    model_LSTM = ReadMEndYou(input_size=61 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "    model_Transformer = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=61, freqs=26)\n",
    "    \n",
    "    return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "'''\n",
    "Questa funzione prende in input i dati di training, validation e test, \n",
    "il tipo di modello scelto e la dimensione del batch. Si occupa di:\n",
    "\n",
    "Calcolare i pesi delle classi.\n",
    "Convertire i dati in tensori PyTorch, con le opportune trasformazioni per CNN, LSTM o Transformer.\n",
    "Creare i dataset e i dataloader per il training.\n",
    "'''\n",
    "\n",
    "\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48):\n",
    "    \n",
    "    # Calcolo dei pesi delle classi\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                         classes=np.unique(y_train), \n",
    "                                         y=y_train)\n",
    "    \n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    class_weights_tensor = class_weights_tensor.to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch con permutazione se necessario\n",
    "    \n",
    "    #SeparableCNN2D_LSTM_FC\n",
    "    \n",
    "    #CNN3D_LSTM_FC\n",
    "    if model_type == \"CNN3D_LSTM_FC\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    ##SeparableCNN2D_LSTM_FC\n",
    "    elif model_type == \"SeparableCNN2D_LSTM_FC\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        \n",
    "    \n",
    "    elif model_type == \"CNN2D\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #BiLSTM (ReadMEndYou):\n",
    "    #Ora il modello si aspetta l’input con shape (batch, canali, frequenze, tempo) \n",
    "    #e, al suo interno, \n",
    "    #esegue la permutazione per avere il tempo come dimensione sequenziale. \n",
    "    #Non serve quindi applicare una permutazione anche qui.\n",
    "    \n",
    "    elif model_type == \"BiLSTM\":\n",
    "            \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #Transformer (ReadMYMind):\n",
    "    #Analogamente, il modello gestisce internamente la riorganizzazione dell’input, quindi lasciamo i dati nella loro forma originale.\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Modello non riconosciuto. Scegli tra 'CNN2D', 'LSTM' o 'Transformer'.\")\n",
    "    \n",
    "    # Conversione delle etichette in tensori\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Creazione dei dataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Creazione dei dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "QUESTE DUE FUNZIONI PRESE DA \n",
    "\n",
    "EEG Motor Movement - Imagery Dataset (EEGMMIDB) - TASK 1 - 2D GRID - ALL FREQS + 3D CONV CONV SEP.ipynb\n",
    "\n",
    "DA SEZIONE\n",
    "\n",
    "## Impostazione **Weight & Biases DL Training** con **Rappresentazione Tempo-Frequenza dei miei dati EEG** \n",
    "a seconda del Dataset del Task scelto\n",
    "'''\n",
    "\n",
    "\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, model_standardization):\n",
    "    \n",
    "    #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "    if '_familiar_th' in key:\n",
    "        return 'th_fam'\n",
    "    elif '_unfamiliar_th' in key:\n",
    "        return 'th_unfam'\n",
    "    elif '_familiar_pt' in key:\n",
    "        return 'pt_fam'\n",
    "    elif '_unfamiliar_pt' in key:\n",
    "        return 'pt_unfam'\n",
    "    else:\n",
    "        return None\n",
    "     \n",
    "   \n",
    "    \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, my_train_results, my_test_results, key, data_type, sweep_config, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, sweep_config)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "     # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"spectrograms\" in key:\n",
    "        data_type_str = \"spectrograms\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    file_name = f\"{model_name}_performances_{condition}_{subfolder}_{data_type_str}.pkl\"\n",
    "    folder_path = os.path.join(base_folder, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo ✅ in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei risultati: {e}\")\n",
    "        \n",
    "        \n",
    "'''\n",
    "\n",
    "QUESTE FUNZIONI TROVATE COME ERANO NEL NOTEBOOK\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, sweep_config):\n",
    "    \n",
    "    \n",
    "    #Mi richiamo la chiave 'standardization' che ho impostato nella configurazione dell'oggetto weight and biases\n",
    "    #(i.e., sweep_config['standardization']) e eseguo una procedura condizionale \n",
    "    \n",
    "    #ossia che, se risulta o True o False, lui cambi le condizioni di gestione \n",
    "    #della costruzione delle path di salvataggio \n",
    "    \n",
    "    \n",
    "    # Controlla se i dati sono standardizzati\n",
    "    if sweep_config['standardization']:\n",
    "    \n",
    "        #PER I DATI SCALED\n",
    "        if '_familiar_th' in key:\n",
    "            return 'TH_FAM'\n",
    "        elif '_unfamiliar_th' in key:\n",
    "            return 'TH_UNFAM'\n",
    "        elif '_familiar_pt' in key:\n",
    "            return 'PT_FAM'\n",
    "        elif '_unfamiliar_pt' in key:\n",
    "            return 'PT_UNFAM'\n",
    "        else:\n",
    "            return None\n",
    "    else: \n",
    "        #PER I DATI UNSCALED\n",
    "\n",
    "        if '_familiar_th' in key:\n",
    "            return 'TH_FAM_UNSCALED'\n",
    "        elif '_unfamiliar_th' in key:\n",
    "            return 'TH_UNFAM_UNSCALED'\n",
    "        elif '_familiar_pt' in key:\n",
    "            return 'PT_FAM_UNSCALED'\n",
    "        elif '_unfamiliar_pt' in key:\n",
    "            return 'PT_UNFAM_UNSCALED'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "     \n",
    "   \n",
    "    \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, my_train_results, my_test_results, key, data_type, sweep_config, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, sweep_config)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"wavelet\" in key:\n",
    "        data_type_str = \"wavelet_delta\"\n",
    "    elif \"1_20\" in key:\n",
    "        data_type_str = \"1_20\"\n",
    "    elif \"1_45\" in key:\n",
    "        data_type_str = \"1_45\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    file_name = f\"{model_name}_performances_{condition}_{subfolder}_{data_type_str}.pkl\"\n",
    "    folder_path = os.path.join(base_folder, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo ✅ in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio dei risultati: {e}\")\n",
    "        \n",
    "        \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f4ae2-19ad-4334-80c8-887899c9a5bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Neural Network Models - EEG Spectrograms - Electrodes x Frequencies - previous versions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d22eef-c028-4459-a3f9-935028365412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Neural Network Models - EEG Spectrograms - Electrodes x Frequencies - LSTM & Transformer (DA NON USARE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8072d0-c91b-4cfb-bc25-0a7f6644ef86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Gli LSTM si aspettano un input in forma (batch, lunghezza_sequenza, dimensione_feature). \n",
    "Dovrai quindi decidere qual è la dimensione sequenziale.\n",
    "\n",
    "Opzione comune: usare il tempo come sequenza\n",
    "Step 1: Trasponi i dati in modo da avere il tempo come dimensione sequenziale.\n",
    "\n",
    "Dalla forma (batch, canali, frequenze, tempo) puoi fare:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # Diventa (batch, tempo, canali, frequenze)\n",
    "\n",
    "Step 2: Unisci le dimensioni dei canali e dei bin di frequenza in un’unica dimensione di feature:\n",
    "\n",
    "\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # Ora: (batch, tempo, canali*frequenze)\n",
    "\n",
    "Nel tuo caso, per 3 canali e 38 bin di frequenza: input_size = 3 * 38 = 114 e lunghezza sequenza = 6.\n",
    "\n",
    "Nota: Se invece preferisci usare i bin di frequenza come sequenza, potresti fare:\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # Sequence length = 38, feature size = 3*6 = 18\n",
    "La scelta dipende dal tipo di informazione temporale o spettrale che vuoi evidenziare.\n",
    "\n",
    "'''\n",
    "\n",
    "class ReadMEndYou(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.5, bidirectional=False):\n",
    "        \"\"\"\n",
    "        input_size: dimensione delle feature per time-step (dovrà essere canali * frequenze)\n",
    "        hidden_sizes: lista con le dimensioni degli hidden state, es. [24, 48, 62]\n",
    "        output_size: numero di classi\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        super(ReadMEndYou, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional # Impostazione della bidirezionalità    \n",
    "        \n",
    "        # Adattiamo hidden_size in base alla bidirezionalità\n",
    "        self.hidden_sizes = [\n",
    "            hidden_sizes[0] * 2 if bidirectional else hidden_sizes[0],\n",
    "            hidden_sizes[1] * 2 if bidirectional else hidden_sizes[1],\n",
    "            hidden_sizes[2] * 2 if bidirectional else hidden_sizes[2]\n",
    "        ]\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0, \n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0] * 2 if bidirectional else self.hidden_sizes[0],\n",
    "                             hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1] * 2 if bidirectional else self.hidden_sizes[1],\n",
    "                             hidden_size=self.hidden_sizes[2],\n",
    "                             num_layers=1, \n",
    "                             batch_first=True, \n",
    "                             dropout=0,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_sizes[2] * 2 if bidirectional else self.hidden_sizes[2], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Trasponi per avere il tempo come dimensione sequenziale:\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        \n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        # Ora input_size deve essere channels * freqs (es. 3 * 26 = 78)\n",
    "        \n",
    "        # LSTM 1\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # LSTM 3\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Estraiamo l'output dell'ultimo time-step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout prima del layer fully connected    \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Passaggio attraverso il layer finale per la previsione\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "'''\n",
    "Il modulo Transformer in PyTorch lavora tipicamente su input di forma (seq_length, batch, embedding_dim).\n",
    "\n",
    "Nel codice attuale, si parte da una forma simile a (batch, canali, seq_length), ma dovrai adattarla alla nuova struttura.\n",
    "\n",
    "Possibili approcci:\n",
    "\n",
    "1) Approccio A: usare il tempo come sequenza\n",
    "\n",
    "Se consideri il tempo (6 time windows) come la sequenza, puoi procedere come segue:\n",
    "\n",
    "A) Unisci canali e frequenze in un’unica dimensione di feature:\n",
    "\n",
    "# Dati originali: (batch, canali, frequenze, tempo)\n",
    "x = x.permute(0, 3, 1, 2)  # (batch, tempo, canali, frequenze)\n",
    "batch, tempo, canali, frequenze = x.shape\n",
    "x = x.reshape(batch, tempo, canali * frequenze)  # (batch, tempo, 3*38 = 114)\n",
    "\n",
    "B) Modifica il layer di embedding:\n",
    "\n",
    "Nel codice attuale, l'embedding è definito come:\n",
    "\n",
    "self.embedding = nn.Linear(seq_length, d_model)\n",
    "Dovrai cambiarlo in modo che mappi le dimensioni delle feature (in questo caso 114) a uno spazio latente:\n",
    "\n",
    "self.embedding = nn.Linear(canali * frequenze, d_model)\n",
    "\n",
    "C) Permuta per il Transformer:\n",
    "\n",
    "Dopo l'embedding, passa l'input alla forma (seq_length, batch, d_model):\n",
    "\n",
    "x = x.permute(1, 0, 2)  # Ora: (tempo, batch, d_model)\n",
    "\n",
    "\n",
    "2) Approccio B: usare i bin di frequenza come sequenza\n",
    "In alternativa, se reputi più rilevante la risoluzione spettrale, puoi considerare i 38 bin come sequenza e combinare canali e tempo:\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1, 3)  # (batch, frequenze, canali, tempo)\n",
    "batch, frequenze, canali, tempo = x.shape\n",
    "x = x.reshape(batch, frequenze, canali * tempo)  # (batch, frequenze, 3*6 = 18)\n",
    "\n",
    "E poi procedere con un embedding layer che mappa da 18 a d_model e permutare in (frequenze, batch, d_model).\n",
    "\n",
    "Scelta dell'approccio:\n",
    "Se l'aspetto temporale è più critico, probabilmente è meglio usare l’Approccio A (sequenza di lunghezza 6).\n",
    "Se invece vuoi dare maggior rilievo alla struttura spettrale, l’Approccio B potrebbe essere più indicato.\n",
    "\n",
    "Ricorda che la scelta dipende dalla natura del tuo problema e dalla rilevanza delle informazioni temporali rispetto a quelle spettrali.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Scelta: In questa implementazione abbiamo deciso di usare il tempo come sequenza.\n",
    "#In alternativa, potresti scegliere i bin di frequenza come sequenza, ma ciò richiederebbe una diversa riorganizzazione delle dimensioni \n",
    "#(ad esempio, un permute diverso).\n",
    "\n",
    "class ReadMYMind(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, num_layers, num_classes, channels=61, freqs=26):\n",
    "        \n",
    "        super(ReadMYMind, self).__init__()\n",
    "\n",
    "        # Il layer di embedding mapperà la feature dimension (channels * freqs) a d_model\n",
    "        self.embedding = nn.Linear(channels * freqs, d_model)\n",
    "        \n",
    "        # Transformer per l'attenzione spaziale (qui si applica direttamente alla sequenza temporale)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Transformer per l'attenzione temporale (si potrebbe considerare un'iterazione successiva)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # Cross-attention per combinare le rappresentazioni\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Fusione e classificazione finale\n",
    "        self.fc_fusion = nn.Linear(d_model, d_model)\n",
    "        self.fc_classify = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Utilizziamo il tempo come sequenza\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (batch, tempo, canali, frequenze)\n",
    "        \n",
    "        batch, time, channels, freqs = x.shape\n",
    "        x = x.reshape(batch, time, channels * freqs)  # -> (batch, tempo, channels*frequencies)\n",
    "        \n",
    "        # Embedding: (batch, tempo, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer richiede input di forma (seq_length, batch, embedding_dim)\n",
    "        x = x.permute(1, 0, 2)  # -> (tempo, batch, d_model)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione spaziale\n",
    "        x_spatial = self.spatial_transformer(x)\n",
    "        \n",
    "        # Applichiamo il Transformer per l'attenzione temporale\n",
    "        x_temporal = self.temporal_transformer(x_spatial)\n",
    "        \n",
    "        # Cross-attention: (tempo, batch, d_model)\n",
    "        x_cross, _ = self.cross_attention(x_spatial, x_temporal, x_temporal)\n",
    "        \n",
    "        # Fusione: per esempio, facciamo una media sul tempo (dimensione 0)\n",
    "        x_fused = self.fc_fusion((x_spatial + x_temporal).mean(dim=0))  # -> (batch, d_model)\n",
    "        \n",
    "        # Classificazione finale\n",
    "        output = self.fc_classify(x_fused)  # -> (batch, num_classes)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b599f-49e1-4201-ad30-aafbf48bffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ecco un codice che fornisce dati di input fittizi a ciascuna rete neurale, \n",
    "stampa le dimensioni a ogni passaggio e verifica che gli output abbiano le forme attese.\n",
    "\n",
    "Ho mantenuto le forme coerenti con i tuoi parametri:\n",
    "\n",
    "\n",
    "Batch size: 8\n",
    "Numero di canali EEG: 3\n",
    "Numero di frequenze: 38\n",
    "Numero di timepoints (campioni temporali): 100\n",
    "Numero di classi: 2\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Parametri\n",
    "batch_size = 44\n",
    "input_channels = 61  # Canali EEG\n",
    "num_freqs = 45       # Numero di frequenze\n",
    "num_classes = 2       # Numero di classi\n",
    "\n",
    "# Creazione di dati fittizi per il test\n",
    "x = torch.randn(batch_size, num_freqs, input_channels)  # (batch, frequenze, channels)\n",
    "print(f\"Input iniziale: {x.shape}\\n\")\n",
    "\n",
    "# ---- CNN2D ----\n",
    "cnn_model = CNN2D(input_channels=input_channels, num_classes=num_classes)\n",
    "cnn_output = cnn_model(x)\n",
    "print(f\"Output CNN2D: {cnn_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n",
    "'''\n",
    "# ---- ReadMEndYou (LSTM) ----\n",
    "hidden_sizes = [24, 48, 62]\n",
    "lstm_model = ReadMEndYou(input_size=input_channels * num_freqs, hidden_sizes=hidden_sizes, output_size=num_classes)\n",
    "lstm_output = lstm_model(x)\n",
    "print(f\"Output ReadMEndYou (LSTM): {lstm_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "\n",
    "# ---- ReadMYMind (Transformer) ----\n",
    "d_model = 64   # Dimensione embedding\n",
    "num_heads = 8   # Numero di teste di attenzione\n",
    "num_layers = 3  # Numero di strati Transformer\n",
    "\n",
    "transformer_model = ReadMYMind(d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes)\n",
    "transformer_output = transformer_model(x)\n",
    "print(f\"Output ReadMYMind (Transformer): {transformer_output.shape}\\n\")  # Atteso: (batch_size, num_classes)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a441955-4785-419c-b889-5b9c9bd6ef74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Neural Network Models - EEG Spectrograms - Electrodes x Frequencies - CNN2D (DA USARE!)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e5a9354-2774-400e-bcb5-3d944b40bcae",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI - PRE LUGLIO 2025\n",
    "'''\n",
    "\n",
    "\n",
    "'''CNN2D\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, frequenze, canali). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (45)\n",
    "e la \"larghezza\" come i canali (61)\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, frequenze, canali)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "       \n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec56fcec-bf60-428e-8981-ffdb9374be34",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI - PRE LUGLIO 2025\n",
    "'''\n",
    "\n",
    "\n",
    "'''CNN2D\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, frequenze, canali). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (45)\n",
    "e la \"larghezza\" come i canali (61)\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, frequenze, canali)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "       \n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a281f-a638-4ce4-a7be-27060869fe7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI 2D FREQUENCY-CHANNELS (LUGLIO 2025!)\n",
    "\n",
    "\n",
    "\n",
    "Ora però, ragionandoci, potrei inserire dei valori da cui pescare, \n",
    "\n",
    "durante l'ottimizzazione degli iper-parametri della mia rete, che si riferiscono \n",
    "\n",
    "1) a valori di alcuni parametri generale dell'apprendimento delle reti\n",
    "2) a valori dei parametri architetturali di ciascuna delle mie singole reti neurali testate\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN1D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***OLD CNN2D***\n",
    "\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, frequenze, canali). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (45)\n",
    "e la \"larghezza\" come i canali (61)\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, frequenze, canali)\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "       \n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''CNN2D CON LE FUNZIONI DI ATTIVAZIONI DINAMICHE (LAYER CONVOLUTIVI + FC1) E DINAMISMO DEI KERNEL SIZE DI CONV e POOL LAYER e STRIDE\n",
    "\n",
    "\n",
    "\n",
    "Scusami non ho capito la differenza tra la mia e la tua, che la mia tratta le convoluzione per ogni canale separatamente\n",
    "e quindi vede le features per ogni canale mentre la tua vede le relazioni in frequenza tra i canali?\n",
    "\n",
    "mi fai capire meglio?\n",
    "\n",
    "\n",
    "ChatGPT ha detto:\n",
    "\n",
    "Ottima domanda, e sì: sei sulla strada giusta! Vediamo con precisione la differenza concettuale tra le due versioni, perché hanno implicazioni molto diverse sul tipo di informazione che la rete convoluzionale impara.\n",
    "\n",
    "🧠 Contesto: input (batch, freq, channels) → (B, 45, 61)\n",
    "✅ Versione A: .unsqueeze(1) → input = (B, 1, 45, 61)\n",
    "\n",
    "x = x.unsqueeze(1)\n",
    "# Output: (batch_size, 1, 45, 61)\n",
    "\n",
    "✅ Come la interpreta la CNN:\n",
    "Tratta tutta la matrice frequenze × canali come un'immagine 2D.\n",
    "\n",
    "Applica filtri convoluzionali 2D che coprono sia le frequenze che i canali contemporaneamente.\n",
    "\n",
    "La convoluzione può imparare pattern locali e globali che coinvolgono più canali EEG e bande di frequenza assieme.\n",
    "\n",
    "✅ Esempio di pattern che può catturare:\n",
    "\"La banda theta (4–8 Hz) è più attiva nei canali posteriori rispetto a quelli frontali\"\n",
    "\n",
    "\"Un pattern a L tra canali temporali e frequenze gamma\"\n",
    "\n",
    "➕ Vantaggi:\n",
    "Cattura interazioni tra frequenze e canali.\n",
    "\n",
    "Molto potente per pattern spatial-temporal-frequenziali complessi.\n",
    "\n",
    "➖ Svantaggi:\n",
    "Più complesso da interpretare.\n",
    "\n",
    "Può \"mescolare troppo\" se le relazioni tra canali non sono forti.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ Versione B: .permute(0, 2, 1).unsqueeze(3) → input = (B, 61, 45, 1)\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1).unsqueeze(3)\n",
    "# Output: (batch_size, 61, 45, 1)\n",
    "\n",
    "✅ Come la interpreta la CNN:\n",
    "Ogni canale EEG (dei 61) è trattato come un’immagine verticale di 45 pixel (cioè le frequenze).\n",
    "\n",
    "**Ogni filtro convoluzionale lavora indipendentemente su ciascun canale EEG.\n",
    "\n",
    "È come applicare 61 CNN monodimensionali parallele sulle frequenze di ogni canale.\n",
    "\n",
    "✅ Esempio di pattern che può catturare:\n",
    "“Nel canale Fz, la banda alpha ha un picco”\n",
    "\n",
    "“Nel canale Pz, la potenza decresce linearmente con la frequenza”\n",
    "\n",
    "➕ Vantaggi:\n",
    "Cattura pattern locali per canale (molto utile se ogni canale è considerato indipendente).\n",
    "\n",
    "Meno soggetto a overfitting se i dati sono limitati.\n",
    "\n",
    "➖ Svantaggi:\n",
    "Non cattura le relazioni tra canali.\n",
    "\n",
    "Meno potente se la distribuzione spaziale tra canali è importante (es. P300, N170, ecc.).\n",
    "\n",
    "🔍 Riassunto Visuale\n",
    "\n",
    "Variante\t        Input Shape\t      Tratta i canali come\t       Cattura relazioni tra canali?\t    Cattura pattern cross-frequenza?\n",
    ".unsqueeze(1)\t    (B, 1, 45, 61)\t    Griglia 2D\t                               ✅ Sì\t                             ✅ Sì\n",
    ".permute...\t        (B, 61, 45, 1)\t   Canali separati\t                           ❌ No\t               ✅ Solo all'interno del canale\n",
    "\n",
    "🧠 Quale scegliere?\n",
    "Usa .unsqueeze(1) (griglia 2D) se:\n",
    "Vuoi sfruttare relazioni tra canali EEG (spazialità).\n",
    "\n",
    "Stai analizzando pattern che coinvolgono distribuzioni topografiche (Pz vs Cz, emisferi, ecc.).\n",
    "\n",
    "Usa .permute(...).unsqueeze(3) se:\n",
    "Vuoi analizzare ogni canale indipendentemente.\n",
    "\n",
    "Hai pochi dati e vuoi una CNN più semplice.\n",
    "\n",
    "Tratti i canali EEG come “sorgenti indipendenti”.\n",
    "\n",
    "✅ Conclusione\n",
    "Hai centrato la differenza:\n",
    "\n",
    "La tua versione **focalizza sui pattern di frequenza all'interno di ciascun canale.\n",
    "\n",
    "L’altra versione esplora pattern condivisi tra canali e frequenze, come se fosse un'immagine EEG frequenze × elettrodi.\n",
    "\n",
    "Se vuoi, posso anche mostrarti un esempio visivo (matplotlib) per rendere ancora più chiara la differenza.\n",
    "\n",
    "\n",
    "\n",
    "Quindi, nella mia versione originale è come se dessi 61 immagini singolo canale 'verticali', \n",
    "nel senso che per analogia, sarebbe come dare la rappresentazione in formato vettore verticale di ogni canale rispetto ai pixel in frequenza \n",
    "(ossia i bins di frequenza)..\n",
    "\n",
    "Nella tua, invece, forniamo un input solo, ossia una immagine sola,\n",
    "con la rappresentazione dei bin di frequenza di ogni canale e quindi ci consente di vedere le relazioni tra i canali stessi?\n",
    "\n",
    "\n",
    "****************** ****************** ****************** ****************** ****************** ****************** ******************\n",
    "\n",
    "✅ Esattamente! Hai centrato il punto con una metafora visiva perfetta. Ti spiego meglio facendo proprio leva su questa analogia dell’immagine.\n",
    "\n",
    "🎯 Tua versione originale: (B, 61, 45, 1) → 61 “immagini verticali”\n",
    "✅ Metafora:\n",
    "È come dare 61 immagini strette e alte (una per ogni canale EEG), in cui:\n",
    "\n",
    "Altezza = 45: le frequenze (tipo pixel verticali)\n",
    "\n",
    "Larghezza = 1: inesistente (una sola colonna)\n",
    "\n",
    "Ogni immagine è indipendente\n",
    "\n",
    "👉 In pratica:\n",
    "La rete analizza separatamente ogni canale EEG.\n",
    "\n",
    "Impara “per ogni canale, com’è distribuita la potenza nelle varie frequenze?”\n",
    "\n",
    "Non sa nulla di cosa succede in altri canali.\n",
    "\n",
    "🎯 La mia versione (classica CNN2D): (B, 1, 45, 61) → una sola immagine F×C\n",
    "✅ Metafora:\n",
    "È come dare una singola immagine 2D:\n",
    "\n",
    "Asse verticale = 45 bin di frequenza\n",
    "\n",
    "Asse orizzontale = 61 canali EEG\n",
    "\n",
    "👉 In pratica:\n",
    "La rete guarda l’intera griglia frequenza × canale come un’immagine.\n",
    "\n",
    "Può imparare pattern tipo:\n",
    "\n",
    "“La potenza in alpha aumenta da O1 verso Cz”\n",
    "\n",
    "“Un cluster di attivazione nei canali frontali nella gamma”\n",
    "\n",
    "Le convoluzioni possono spostarsi sia tra frequenze che tra canali, quindi può apprendere interazioni spaziali.\n",
    "\n",
    "👁️‍🗨️ Visualizzazione (semplificata)\n",
    "Tua:\n",
    "scss\n",
    "Copia\n",
    "Modifica\n",
    "Canale Fz →  [ delta ]  \n",
    "              [ theta ]  \n",
    "              [ alpha ]  \n",
    "              [ beta  ]  \n",
    "              [ gamma ]\n",
    "\n",
    "Canale Cz →  [ delta ]  \n",
    "              [ theta ]  \n",
    "              ...\n",
    "(61 ripetizioni)\n",
    "Mia:\n",
    "matlab\n",
    "Copia\n",
    "Modifica\n",
    "        C1   C2   C3   ...  C61\n",
    "delta   ■    □    ■    ...  □\n",
    "theta   □    ■    □    ...  ■\n",
    "alpha   ■    ■    ■    ...  □\n",
    "beta    □    □    ■    ...  ■\n",
    "gamma   ■    □    □    ...  □\n",
    "Una griglia piena, dove ogni pixel rappresenta la potenza in una frequenza per un canale.\n",
    "\n",
    "✅ Conclusione\n",
    "Hai descritto benissimo:\n",
    "\n",
    "Versione\tInterpretazione\tVantaggio principale\n",
    "Tua (permute + unsqueeze)\t61 immagini verticali (1 per canale)\tFocus dettagliato su pattern per canale\n",
    "Classica (unsqueeze(1))\tUna sola immagine 2D F×C\tCattura pattern e relazioni tra canali\n",
    "\n",
    "Se vuoi fare un passo in più, puoi anche combinare le due visioni con multi-branch CNN (una per canale, una per immagine globale). \n",
    "Fammi sapere se vuoi un esempio anche per questo!\n",
    "\n",
    "****************** ****************** ****************** ****************** ****************** ****************** ******************\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        #🔁 Prima:\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        #x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        #x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "        \n",
    "        #✅ Ora:\n",
    "        #Siccome i dati arrivano come (B, 45, 61) — cioè frequenze × canali, non serve permutare. Ti basta:\n",
    "        \n",
    "        # Aggiungiamo una dimensione per il canale \"immagine\"\n",
    "        x = x.unsqueeze(1)  # → (B, 1, 45, 61)\n",
    "            \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = self.act_fns[0](x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = self.act_fns[1](x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = self.act_fns[2](x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "       \n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd746c6-84e1-45dc-aa0c-3dfa8c5640b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ecco un codice che fornisce dati di input fittizi a ciascuna rete neurale, \n",
    "stampa le dimensioni a ogni passaggio e verifica che gli output abbiano le forme attese.\n",
    "'''\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Testing shapes\n",
    "batch, frequency, channels, num_classes = 44, 45, 61, 2\n",
    "\n",
    "x = torch.randn(batch, channels, frequency)\n",
    "\n",
    "print(\"Input:\", x.shape)\n",
    "\n",
    "cnn = CNN2D(input_channels = 1, num_classes = num_classes,\n",
    "            conv_out_channels=16,\n",
    "            conv_k1_h=3,conv_k1_w=5,\n",
    "            conv_k2_h=3,conv_k2_w=5,\n",
    "            conv_k3_h=3,conv_k3_w=5,\n",
    "            conv_s1_h=1,conv_s1_w=2,\n",
    "            conv_s2_h=1,conv_s2_w=2,\n",
    "            conv_s3_h=1,conv_s3_w=2,\n",
    "            pool_p1_h=1,pool_p1_w=2,\n",
    "            pool_p2_h=1,pool_p2_w=2,\n",
    "            pool_p3_h=1,pool_p3_w=1,\n",
    "            pool_type='max',fc1_units=10,dropout=0.1,\n",
    "            cnn_act1='relu',cnn_act2='relu',cnn_act3='elu')\n",
    "\n",
    "out_cnn = cnn(x)\n",
    "print(\"CNN2D output:\", out_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944f40-e81f-4ed4-90f8-3264581858fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Neural Network Models - EEG Spectrograms - Electrodes x Frequencies - CNN3D e CNN2D_Sep (RIADATTATA PER SHAPE batch, frequenze, canali)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6ebe7-86e6-4de5-9c88-1fec4883ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ATTENZIONE, BISOGNA\n",
    "\n",
    "1) RICREARE UNA GRIGLIA EXCEL, PER I DATI EEG DI INTERROGAIT\n",
    "\n",
    "\n",
    "a) Carica questo file e ricrea i dati \n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "with open(f\"{path}EEG_channels_names.pkl\", \"rb\") as f:\n",
    "    EEG_channels_names = pickle.load(f)\n",
    "    \n",
    "\n",
    "b) Tieni presente che tu hai sia\n",
    "\n",
    "\n",
    "Familiar_Spectrograms_channels_frequencies\n",
    "Unfamiliar_Spectrograms_channels_frequencies \n",
    "\n",
    "e per ciascuna hai coppie di condizioni sperimentali....non so se valga la pena investirci...pensaci bene\n",
    "\n",
    "\n",
    "Forse conviene ri-adattare le reti neurali per fare convoluzioni separabili,\n",
    "ma sull'input frequenza x canali base ottenuto da Interrogait...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2) VERIFICARE IL NUMERO DI ELETTRODI CORRISPONDA A QUELLO USATO PER TASK MOTORIO!\n",
    "3) CREARE LA GRIGLIA CON LA DISPOSIZIONE ELETTRODICA DEI DATI DI INTERROGAIT\n",
    "4) USARE LE STESSE ARCHITETTURE DI CONVOLUZIONI SEPARABILI PER IL TASK DI INTERROGAIT!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01aaa2-47f8-42d4-b132-9d9b5455aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Perfetto — ti ho preparato l’excel con le coordinate x,y per solo i canali che hai elencato (ordine identico alla tua lista).\n",
    "Ho usato il tuo file “EEG_grid (1).xlsx” come formato di riferimento e ho estratto/ricostruito le posizioni in modo coerente \n",
    "con la topologia 10–20 che avevi già mappato sulla griglia. \n",
    "\n",
    "Gli unici due canali che mancavano nella griglia erano AF3 e AF4: li ho inseriti come punti a metà strada fra (Fp1, F3) e (Fp2, F4)\n",
    "rispettivamente, cioè sul “ring” AF tra FP e F (scelta standard nella 10–10).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bda02-7fea-4180-9ee6-55d7663228c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "VERSIONE CONVOLUZIONE 3D PURA e CONVOLUZIONI SEPARABILI 19 LUGLIO 2025\n",
    "\n",
    "\n",
    "Due versioni dell’architettura:\n",
    "\n",
    "CNN3D_LSTM_FC: usa nn.Conv3d per eseguire una vera convoluzione 3D sui cinque depth (bande di frequenza), \n",
    "mantenendo il resto del flusso identico.\n",
    "\n",
    "SeparableCNN2D_LSTM_FC: applica in sequenza una convoluzione depthwise (gruppi = canali) e una pointwise (1×1) \n",
    "per fondere i cinque canali in modo efficiente.\n",
    "\n",
    "Entrambe le classi si integrano con il tuo blocco LSTM e il classificatore come nella versione originale.\n",
    "\n",
    "\n",
    "\n",
    "******** PROBLEMA SUL GRADCAM NELLE ANALISI PRE 19 LUGLIO ********\n",
    "\n",
    "\n",
    "Per ottenere un Grad‑CAM “3D” su ciascuna delle 5 bande (cioè un volume 9×9×5) \n",
    "invece di schiacciare tutto in una mappa 9×9, bisogna:\n",
    "\n",
    "Non appiattire la dimensione di profondità (“depth” = bande) con cam.mean(dim=1).\n",
    "\n",
    "Calcolare i pesi medi dei gradienti solo su altezza e larghezza, non su depth, in modo da preservare D=5.\n",
    "\n",
    "Upsample (solo) le due dimensioni spaziali H×W, lasciando inalterata la profondità D.\n",
    "\n",
    "(Opzionale) \n",
    "\n",
    "Se il tuo primo Conv3d usa un kernel di profondità pari all’intera profondità d’ingresso, \n",
    "quella informazione viene compressa in D=1!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "******** PROBLEMA SUL CONV3D NELLE ANALISI PRE 19 LUGLIO ********\n",
    "\n",
    "\n",
    "Se il tuo primo Conv3d usa un kernel di profondità pari all’intera profondità d’ingresso, \n",
    "quella informazione viene compressa in D=1!\n",
    "\n",
    "\n",
    "Se vuoi davvero avere D=5 in uscita, devi cambiare conv1 in:\n",
    "\n",
    "\n",
    "# ❌ kernel_size=(5,3,3), padding=(0,1,1) → D_out = 1\n",
    "\n",
    "self.conv1 = nn.Conv3d(1, 32, kernel_size=(3,3,3), padding=(1,1,1))\n",
    "così la profondità si conserva da 5→5.\n",
    "\n",
    "\n",
    "\n",
    "1) Perché in conv1 useremo padding=(1,1,1) e negli altri layer padding=(0,1,1)\n",
    "Obiettivo: mantenere la profondità (numero di bande, D = 5) costante lungo tutta la rete.\n",
    "\n",
    "In conv1, abbiamo scelto kernel_size=(3,3,3) perché vogliamo che il filtro “scorra” su tutti e tre gli assi (D,H,W).\n",
    "\n",
    "Con kernel_depth=3, per avere\n",
    "\n",
    "𝐷out = (𝐷in + 2⋅𝑃 depth − 𝐾 depth)/ 𝑆 + 1 = 5\n",
    "\n",
    "Da qui (1,1,1) per (depth, height, width).\n",
    "\n",
    "Negli altri layer 3D (conv2a, conv2b, conv3) il kernel depth = 1 (kernel_size=(1,3,3)), \n",
    "quindi la profondità non cambia se mettiamo padding_depth=0 con padding (0,1,1) nel layer conv2 e conv3\n",
    "\n",
    "In altre parole, su quell’asse non serve alcun padding:\n",
    "\n",
    "se P dept = 0 allora diventa infatti\n",
    "\n",
    "𝐷out = (𝐷in + 2⋅0 − 𝐾 depth)/ 𝑆 + 1 = 5\n",
    "\n",
    "2⋅0\n",
    "\n",
    "\n",
    "Non è che la tua rete “CNN3D_LSTM_FC” sia sbagliata in senso assoluto, \n",
    "ma — proprio a causa di quel primo Conv3d con kernel_size=(5,3,3) e padding=(0,1,1) — \n",
    "\n",
    "stai automaticamente comprimendo tutte e 5 le bande nella singola fetta di profondità:\n",
    "\n",
    "\n",
    "self.conv1 = nn.Conv3d(\n",
    "    in_channels=1, out_channels=32,\n",
    "    kernel_size=(5, 3, 3),  # → D_out = (5 − 5 + 2·0)/1 + 1 = 1\n",
    "    padding=(0, 1, 1)\n",
    ")\n",
    "Quindi il tuo tensore (B, 1, 5, 9, 9) diventa (B, 32, 1, 9, 9): la dimensione depth (5) si riduce a 1 subito.\n",
    "\n",
    "Se invece vuoi davvero preservare le 5 “fette” come vera terza dimensione spaziale, hai due possibili correzioni:\n",
    "\n",
    "Usare un kernel 3×3×3 (o 1×3×3) in conv1, in modo da non “abbracciare” tutta la profondità d’ingresso:\n",
    "\n",
    "\n",
    "- self.conv1 = nn.Conv3d(1, 32, kernel_size=(5, 3, 3), padding=(0, 1, 1))\n",
    "+ # preserva D: depth out = depth in = 5\n",
    "+ self.conv1 = nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "\n",
    "Oppure, se vuoi mantenere le bande completamente indipendenti in questo layer,\n",
    "\n",
    "\n",
    "self.conv1 = nn.Conv3d(1, 32, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "che trasforma (B,1,5,9,9) → (B,32,5,9,9).\n",
    "\n",
    "Lasciare com’è, sapendo però che la rete “fonderà” le 5 bande in un’unica mappa di profondità: \n",
    "non è un bug, è una scelta architetturale.\n",
    "\n",
    "\n",
    "\n",
    "La SeparableCNN2D_LSTM_FC\n",
    "Quella architettura non comprime mai le bande all’interno di un’unica fetta, perché:\n",
    "\n",
    "Le bande diventano canali di un Conv2d depth‑wise:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # (B,5,9,9)\n",
    "self.dw_conv1 = nn.Conv2d(5, 5, kernel_size=3, padding=1, groups=5)\n",
    "Ogni “fetta” (banda) resta separata fino al pointwise e agli strati successivi.\n",
    "\n",
    "Quindi se il tuo obiettivo è avere un’uscita per banda (e poi poter plottare un Grad‑CAM 2D per ciascuna),\n",
    "la SeparableCNN2D è già configurata correttamente.\n",
    "\n",
    "Se invece vuoi un Grad‑CAM “volumetrico” 3D (5×9×9) direttamente dal modello 3D puro,\n",
    "la vera modifica necessaria è solo sul primo Conv3d, come mostrato sopra.\n",
    "\n",
    "Fammi sapere quale dei due setup stai usando e ti aiuto a integrare il Grad‑CAM 3D di conseguenza!\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Ecco l’implementazione completa di CNN3D_LSTM_FC (“approccio sequenza di profondità”) in cui:\n",
    "\n",
    "mantieni il tuo primo blocco 3D con kernel (3,3,3) e padding (1,1,1), quindi D rimane 5 fino alla fine;\n",
    "\n",
    "riduci le spatial singleton dims (H=1,W=1) e trasformi la depth D=5 in una sequenza di lunghezza 5;\n",
    "\n",
    "imposti l’input_size=128 nell’LSTM (feature per time‑step = 128);\n",
    "\n",
    "\n",
    "Con questa versione:\n",
    "\n",
    "la sequenza per l’LSTM ha lunghezza D=5;\n",
    "\n",
    "ogni passo ha 128 feature, esattamente input_size=128;\n",
    "\n",
    "non servono trucchi di reshape su scala globale.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN3D_LSTM_FC(nn.Module):\n",
    "    \"\"\"\n",
    "    Version with pure 3D convolutions treating the 5 frequency bands\n",
    "    as a sequence (depth) for the LSTM.\n",
    "    Input: Tensor of shape (B, 9, 9, 5) --> reshaped to (B, 1, 5, 9, 9)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, dropout=0.5, hidden_size=64, use_lstm=True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        # --- Block 1 (3D) ---\n",
    "        self.conv1   = nn.Conv3d(1,  32, kernel_size=(3,3,3), padding=(1,1,1))\n",
    "        self.bn1     = nn.BatchNorm3d(32)\n",
    "        self.pool3d  = nn.MaxPool3d((1,2,2))  # non tocca D\n",
    "\n",
    "        # --- Block 2 (3D Residual) ---\n",
    "        self.res_conv3d = nn.Conv3d(32, 64, kernel_size=1, bias=False)\n",
    "        self.res_bn3d   = nn.BatchNorm3d(64)\n",
    "        self.conv2a     = nn.Conv3d(32, 64, kernel_size=(1,3,3), padding=(0,1,1))\n",
    "        self.bn2a       = nn.BatchNorm3d(64)\n",
    "        self.conv2b     = nn.Conv3d(64, 64, kernel_size=(1,3,3), padding=(0,1,1))\n",
    "        self.bn2b       = nn.BatchNorm3d(64)\n",
    "\n",
    "        # --- Block 3 (3D) ---\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(1,3,3), padding=(0,1,1))\n",
    "        self.bn3   = nn.BatchNorm3d(128)\n",
    "\n",
    "        # LSTM o FC finale\n",
    "        if self.use_lstm:\n",
    "            # input_size = feature_dim per time‑step = 128\n",
    "            self.lstm       = nn.LSTM(input_size=128,\n",
    "                                      hidden_size=self.hidden_size,\n",
    "                                      num_layers=1,\n",
    "                                      batch_first=True)\n",
    "            self.classifier = nn.LazyLinear(num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 9, 9, 5)\n",
    "        if x.ndim == 4:\n",
    "            # -> (B,1,D=5,H=9,W=9)\n",
    "            x = x.permute(0, 3, 1, 2).unsqueeze(1)\n",
    "\n",
    "        # --- Block 1 ---\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B,32,5,9,9)\n",
    "        x = self.pool3d(x)                   # (B,32,5,4,4)\n",
    "\n",
    "        # --- Block 2 (Residual) ---\n",
    "        res = self.res_bn3d(self.res_conv3d(x))  # (B,64,5,4,4)\n",
    "        x   = F.relu(self.conv2a(x))             # (B,64,5,4,4)\n",
    "        x   = self.bn2b(self.conv2b(x))          # (B,64,5,4,4)\n",
    "        x   = F.relu(x + res)                    # (B,64,5,4,4)\n",
    "        x   = self.pool3d(x)                     # (B,64,5,2,2)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        x = F.relu(self.bn3(self.conv3(x)))      # (B,128,5,2,2)\n",
    "        x = self.pool3d(x)                       # (B,128,5,1,1)\n",
    "\n",
    "        # Stampa delle dimensioni prima di passare al classifier\n",
    "        #print(f\"Dimensioni prima del classifier: {x.shape}\")\n",
    "\n",
    "        if self.use_lstm:\n",
    "            # x: (B,128,5,1,1)\n",
    "            # -> squeeze spatial dims → (B,128,5)\n",
    "            x = x.squeeze(-1).squeeze(-1)\n",
    "            # -> permute per batch_first → (B, seq_len=5, feat=128)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.dropout(x)\n",
    "            out, _ = self.lstm(x)               # out: (B,5,hidden_size)\n",
    "            last    = out[:, -1, :]             # prendo l’ultimo time-step\n",
    "            logits  = self.classifier(last)     # (B, num_classes)\n",
    "        else:\n",
    "            # x: (B,128,5,1,1) → flatten → (B,128)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            logits = self.classifier(self.dropout(x))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    \n",
    "\n",
    "class SeparableCNN2D_LSTM_FC(nn.Module):\n",
    "    \"\"\"\n",
    "    Version with depthwise + pointwise separable convolutions\n",
    "    across the 5 channels.\n",
    "    Input: Tensor of shape (B, 9, 9, 5) -> (B,5,9,9)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, dropout=0.5, hidden_size=64, use_lstm=True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        # --- Block 1 separabile ---\n",
    "        self.dw_conv1 = nn.Conv2d(5, 5, kernel_size=3, padding=1, groups=5)\n",
    "        self.pw_conv1 = nn.Conv2d(5, 32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # --- Block 2 (residuo) ---\n",
    "        self.res_conv = nn.Conv2d(32, 64, kernel_size=1, bias=False)\n",
    "        self.res_bn = nn.BatchNorm2d(64)\n",
    "        self.bn2a = nn.BatchNorm2d(32)\n",
    "        self.conv2a = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=128 * 5, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=128 * 1,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (B,5,9,9)\n",
    "\n",
    "        x = F.relu(self.dw_conv1(x))\n",
    "        x = F.relu(self.bn1(self.pw_conv1(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        res = self.res_bn(self.res_conv(x))\n",
    "        x = F.relu(self.conv2a(self.bn2a(x)))\n",
    "        x = self.bn2b(self.conv2b(x))\n",
    "        x = F.relu(x + res)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)  # (B,128,1,1)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            x = x.permute(0, 2, 1, 3).reshape(x.size(0), 1, -1)  # (B,1,128)\n",
    "            out, _ = self.lstm(self.dropout(x))\n",
    "            last = out[:, -1, :]\n",
    "            logits = self.classifier(last)\n",
    "        else:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            logits = self.classifier(self.dropout(x))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdcb94-e15c-4e4a-ad4b-46aef54dbe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "if __name__ == \"__main__\":\n",
    "    cnn = CNN3D_LSTM_FC()\n",
    "    sep_conv = SeparableCNN2D_LSTM_FC()\n",
    "    \n",
    "    # Test both input orders\n",
    "    x2 = torch.randn(8, 9, 9, 5)\n",
    "    print(cnn(x2).shape)   # -> (8,2)\n",
    "    print(sep_conv(x2).shape)  # -> (8,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9991ce88-ca0f-4ad1-a879-424f40d18c74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Early Stopping - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140e872-9b7e-4e62-9019-3402651f93b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE EARLY STOPPING\n",
    "'''\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbeb6ef-f6f1-4d77-97bd-ea42de36d65f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login & REMOTE CHECKS - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c2fa9d-d87b-46c1-92fe-f293e272af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "WEIGHT AND BIASES LOGIN\n",
    "\n",
    "Il messaggio che stai ricevendo indica \n",
    "che sei già connesso al tuo account Weights & Biases (wandb).\n",
    "\n",
    "Se vuoi forzare il login, puoi usare il comando suggerito:\n",
    "\n",
    "wandb login --relogin\n",
    "\n",
    "Questo comando ti permetterà di reinserire le credenziali e riconnetterti al tuo account.\n",
    "Se non hai bisogno di disconnetterti o di cambiare l'account,\n",
    "puoi semplicemente continuare a usare wandb senza ulteriori passaggi. \n",
    "Hai bisogno di ulteriore assistenza con wandb o con il tuo progetto?\n",
    "'''\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"✅ Weights & Biases login effettuato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d4ee8-8f7f-4b49-9e21-8a5a49cd517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d81e36-6667-4e31-995b-7a866210c204",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Weight & Biases Login ERRORE CANCELLAZIONE SWEEPS - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c78b34c-3d83-4340-b03a-11f387992e98",
   "metadata": {
    "tags": []
   },
   "source": [
    "import wandb\n",
    "\n",
    "# Sostituisci con il nome del tuo progetto e dell'utente\n",
    "project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs'\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'  # Può essere vuoto se è il tuo account\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "# Stampa tutte le run e il loro numero\n",
    "for run in runs:\n",
    "    print(run.id, run.name)\n",
    "\n",
    "print(f\"Totale runs: {len(list(runs))}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "930d0e8a-2091-4a5b-9642-aee231f1d43a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Login a W&B\n",
    "wandb.login()\n",
    "\n",
    "# Crea un'istanza dell'API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Scorri tutti i progetti e stampa i nomi\n",
    "for project in api.projects():\n",
    "    print(f\"Project Name: {project.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357cfd9-6e6b-4c7d-afcc-44f8bad3044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "COME CANCELLARE GLI SWEEP DA REMOTO!\n",
    "\n",
    "                                                                                IO\n",
    "Quindi un modo potrebbe essere :\n",
    "\n",
    "1) tracciarmi il codice univoco delle sweep che avevo associate ad un progetto e salvare in una lista tipo sfruttando la base di questo codice\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Sostituisci con il nome del tuo progetto e dell'utente\n",
    "project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs'\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'  # Può essere vuoto se è il tuo account\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "# Stampa tutte le run e il loro numero\n",
    "for run in runs:\n",
    "    print(run.id, run.name)\n",
    "\n",
    "print(f\"Totale runs: {len(list(runs))}\")\n",
    "\n",
    "\n",
    "\n",
    "2) Poi creare un loop iterando sugli sweep di quel progetto e poi fare\n",
    "\n",
    "wandb delete <sweep_id>\n",
    "\n",
    "3) In questo modo sarei sicuro da remoto che gli sweep (e quindi anche le runs associate) \n",
    "verrano eliminate e di conseguenza potrei esser sicuro che, se ri-avvio il kernel, avrò sweep id nuovi, \n",
    "ma solo quelli (e non anche quelli vecchi)\n",
    "\n",
    "\n",
    "\n",
    "                                                                                CHATGPT\n",
    "\n",
    "Passaggi:\n",
    "Salvare gli sweep ID associati al progetto:\n",
    "Utilizzando il codice che hai condiviso, puoi raccogliere gli ID delle run e sweep \n",
    "(anche se, in W&B, gli ID delle run sono associati agli sweep, quindi si tratta della stessa cosa). \n",
    "\n",
    "Li puoi salvare in una lista, così da avere il riferimento completo per il progetto specifico.\n",
    "\n",
    "Eliminare gli sweep:\n",
    "Dopo aver raccolto gli ID, puoi iterare su di essi ed eliminarli usando il comando wandb delete <sweep_id>.\n",
    "Questo rimuoverà tutti gli sweep associati al progetto.\n",
    "\n",
    "Assicurarti che al riavvio del kernel vengano creati solo nuovi sweep:\n",
    "Una volta eliminati tutti gli sweep, quando ri-eseguirai il codice per la creazione di nuovi sweep, verranno generati con nuovi ID,\n",
    "senza sovrapporsi a quelli precedenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6590a08-1a9a-4d3e-a353-76c3e99ce272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "#Sweeps dei Progetti da ELIMINARE\n",
    "#1)th_resp_vs_pt_resp_spectrograms_channels_freqs\n",
    "#2 th_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "#3) pt_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "\n",
    "\n",
    "# Nome del Progetto \n",
    "project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs' \n",
    "\n",
    "# Nome Utente\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "# Connessione all'API di W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Recupera tutte le run del progetto\n",
    "runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "# Salva gli sweep_id delle run in una lista (per tracciare gli sweep ID)\n",
    "to_delete_sweep_ids = []\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Sweep ID: {run.sweep.id} - Run ID: {run.id} - Run Name: {run.name}\")\n",
    "    to_delete_sweep_ids.append(run.sweep.id)  # Aggiungi l'ID dello sweep alla lista\n",
    "\n",
    "# Numero totale di sweep trovati\n",
    "print(f\"Totale sweep: {len(to_delete_sweep_ids)}\")\n",
    "\n",
    "\n",
    "# Elimina ogni sweep trovato\n",
    "for sweep_id in to_delete_sweep_ids:\n",
    "    print(f\"Eliminando sweep ID: {sweep_id}\")\n",
    "    wandb.delete(sweep_id)  # Elimina lo sweep\n",
    "\n",
    "print(\"Tutti gli sweep sono stati eliminati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1b85b-1b55-49a5-89d1-195b83a89f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "----\n",
    "\n",
    "                                                                                IO\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "Sweeps dei Progetti da ELIMINARE\n",
    "#1)th_resp_vs_pt_resp_spectrograms_channels_freqs\n",
    "#2 th_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "#3) pt_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "\n",
    "\n",
    "# Nome del Progetto \n",
    "project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs' \n",
    "\n",
    "# Nome Utente\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "# Connessione all'API di W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Recupera tutte le run del progetto\n",
    "runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "# Salva gli sweep_id delle run in una lista (per tracciare gli sweep ID)\n",
    "to_delete_sweep_ids = []\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Sweep ID: {run.sweep.id} - Run ID: {run.id} - Run Name: {run.name}\")\n",
    "    to_delete_sweep_ids.append(run.sweep.id)  # Aggiungi l'ID dello sweep alla lista\n",
    "\n",
    "# Numero totale di sweep trovati\n",
    "print(f\"Totale sweep: {len(to_delete_sweep_ids)}\")\n",
    "\n",
    "\n",
    "# Elimina ogni sweep trovato\n",
    "for sweep_id in to_delete_sweep_ids:\n",
    "    print(f\"Eliminando sweep ID: {sweep_id}\")\n",
    "    wandb.delete(sweep_id)  # Elimina lo sweep\n",
    "\n",
    "print(\"Tutti gli sweep sono stati eliminati.\")\n",
    "\n",
    "                                                                                IO\n",
    "\n",
    "\n",
    "mi dice \n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "Cell In[8], line 4\n",
    "      2 for sweep_id in to_delete_sweep_ids:\n",
    "      3     print(f\"Eliminando sweep ID: {sweep_id}\")\n",
    "----> 4     wandb.delete(sweep_id)  # Elimina lo sweep\n",
    "      6 print(\"Tutti gli sweep sono stati eliminati.\")\n",
    "\n",
    "AttributeError: module 'wandb' has no attribute 'delete'\n",
    "ChatGPT ha detto:\n",
    "\n",
    "\n",
    "                                                                                CHATGPT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Il messaggio di errore che hai ricevuto è dovuto al fatto che la libreria WandB non ha un metodo chiamato delete() per eliminare direttamente gli sweep. \n",
    "In realtà, non esiste un metodo diretto per eliminare un sweep tramite l'API Python di WandB. L'API di WandB supporta l'eliminazione delle run, \n",
    "ma non degli sweep direttamente.\n",
    "\n",
    "Quindi, per risolvere il problema, puoi eliminare le run individuali (che sono associate agli sweep), e questo rimuoverà indirettamente anche gli sweep,\n",
    "dato che ogni run è parte di uno sweep. Ecco come puoi procedere:\n",
    "\n",
    "Codice aggiornato per eliminare le run:\n",
    "\n",
    "        import wandb\n",
    "\n",
    "        # Nome del progetto e utente\n",
    "        project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs'\n",
    "        user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "        # Connessione all'API di W&B\n",
    "        api = wandb.Api()\n",
    "\n",
    "        # Recupera tutte le run del progetto\n",
    "        runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "        # Salva gli ID delle run da eliminare\n",
    "        run_ids = []\n",
    "\n",
    "        for run in runs:\n",
    "            print(f\"Eliminando run ID: {run.id} - Run Name: {run.name}\")\n",
    "            run_ids.append(run.id)  # Aggiungi l'ID della run alla lista\n",
    "\n",
    "        # Elimina le run\n",
    "        for run_id in run_ids:\n",
    "            run = api.run(f\"{user_name}/{project_name}/{run_id}\")\n",
    "            run.delete()  # Elimina la run\n",
    "\n",
    "        print(\"Tutte le run sono state eliminate.\")\n",
    "        \n",
    "        \n",
    "Dettagli del codice:\n",
    "run.delete(): Questo comando elimina la run specificata. Poiché una run è associata a uno sweep, eliminando la run si rimuove anche lo sweep.\n",
    "\n",
    "run.id: L'ID della run viene raccolto per poter essere eliminato successivamente.\n",
    "\n",
    "api.run(f\"{user_name}/{project_name}/{run_id}\"): Carica la run specificata dall'ID e la elimina.\n",
    "\n",
    "Come funziona:\n",
    "Eliminazione delle run: Quando esegui run.delete(), WandB elimina la run e rimuove tutte le informazioni associate, inclusi gli sweep. \n",
    "Questo è il comportamento indiretto che ti consente di eliminare gli sweep, dato che le run sono legate a questi.\n",
    "\n",
    "Considerazioni:\n",
    "Verifica le eliminazioni: Dopo aver eseguito lo script, puoi fare un altro controllo per verificare che tutte le run siano state effettivamente rimosse.\n",
    "\n",
    "Nuove run/sweep: Dopo aver eliminato le run esistenti, quando ri-eseguirai il codice, verranno creati nuovi sweep con nuovi ID, senza conflitti con quelli vecchi.\n",
    "\n",
    "Questo dovrebbe risolvere il problema e consentire l'eliminazione delle run/sweep in modo corretto!\n",
    "\n",
    "\n",
    "\n",
    "                                                                                IO\n",
    "\n",
    "quindi questo mi assicura che ANCHE GLI SWEEPS (ossia i codici univoci ad essi collegati durante  la loro creazione) verranno eliminati? \n",
    "\n",
    "\n",
    "                                                                                CHATGPT\n",
    "Sì, eliminando le run tramite il metodo run.delete(), gli sweep associati verranno eliminati indirettamente. \n",
    "\n",
    "Questo succede perché:\n",
    "\n",
    "Ogni sweep è composto da una serie di run (dove ogni run rappresenta un'esecuzione del modello in un determinato set di condizioni).\n",
    "Quando elimini una run, questa non è più associata a uno sweep specifico, e se non ci sono altre run associate a quello stesso sweep,\n",
    "lo sweep verrà rimosso automaticamente.\n",
    "\n",
    "Perché succede:\n",
    "Quando crei uno sweep, viene generato un ID unico per quello sweep. Ogni volta che una run viene eseguita come parte di uno sweep, \n",
    "viene collegata a quel sweep tramite l'ID di quest'ultimo. \n",
    "Se elimini tutte le run di uno sweep, quell'ID di sweep non avrà più run associate e verrà rimosso dal sistema di WandB.\n",
    "\n",
    "Cosa succede quindi:\n",
    "Eliminando le run, gli sweep ad esse collegati saranno eliminati, dato che uno sweep non ha senso senza run associate.\n",
    "\n",
    "Se esegui di nuovo il codice per creare nuovi sweep e run, verranno creati nuovi ID di sweep senza alcun conflitto con i vecchi.\n",
    "\n",
    "In breve:\n",
    "Eliminando le run come nel codice che ti ho fornito, eliminerai anche gli sweep associati, e i nuovi sweep che creerai avranno nuovi ID univoci, \n",
    "garantendo che non ci siano sovrapposizioni con quelli eliminati.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1a171-ff0d-4c37-800f-f890dc55cf85",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Weight & Biases Login PRECEDURA CORRETTA ✅ (CANCELLAZIONE RUNS e NON SWEEPS!) - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce68fac-e95f-4ba2-8ef6-fda7d141b046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **BOZZE**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d6cd2e0-c922-4662-84a6-ec836f8ed4bb",
   "metadata": {},
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "#RUNS degli SWEEPS dei Progetti da ELIMINARE\n",
    "#1)th_resp_vs_pt_resp_spectrograms_channels_freqs\n",
    "#2 th_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "#3) pt_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "\n",
    "\n",
    "'''SELEZIONANE UNO DI QUESTE 3'''\n",
    "# Nome del progetto e utente\n",
    "\n",
    "project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs'\n",
    "project_name = 'th_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "project_name = 'pt_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "# Connessione all'API di W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Recupera tutte le run del progetto\n",
    "runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "# Salva gli ID delle run da eliminare\n",
    "run_ids_to_delete = []\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Estrazione run ID: {run.id} - Run Name: {run.name}\")\n",
    "    run_ids_to_delete.append(run.id)  # Aggiungi l'ID della run alla lista\n",
    "    \n",
    "print(f\"\\nTotale runs: \\033[1m{len(list(run_ids_to_delete))}\\033[0m\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c97235d3-a060-45f5-a74f-a4ad1bc1fc26",
   "metadata": {},
   "source": [
    "#RUNS degli SWEEPS dei Progetti da ELIMINARE\n",
    "#1)th_resp_vs_pt_resp_spectrograms_channels_freqs\n",
    "#2 th_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "#3) pt_resp_vs_shared_resp_spectrograms_channels_freqs\n",
    "\n",
    "\n",
    "# Nome del progetto e utente\n",
    "\n",
    "#project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs'\n",
    "#project_name = 'th_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "project_name = 'pt_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "# Elimina le run\n",
    "for run_id in run_ids_to_delete:\n",
    "    run = api.run(f\"{user_name}/{project_name}/{run_id}\")\n",
    "    run.delete()  # Elimina la run\n",
    "\n",
    "print(\"Tutte le run sono state eliminate.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6840617c-f85c-46e6-b918-265d1409f8df",
   "metadata": {},
   "source": [
    "# Login a W&B\n",
    "wandb.login()\n",
    "\n",
    "# Crea un'istanza dell'API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Scorri tutti i progetti e stampa i nomi\n",
    "for project in api.projects():\n",
    "    print(f\"Project Name: {project.name}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfff6e61-aff2-4f24-bab9-3ae5867ca2ac",
   "metadata": {},
   "source": [
    "# Nome del progetto e utente\n",
    "\n",
    "#project_name = 'th_resp_vs_pt_resp_spectrograms_channels_freqs'\n",
    "#project_name = 'th_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "#project_name = 'pt_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "# Connessione all'API di W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Recupera tutte le run del progetto\n",
    "runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "\n",
    "# Salva gli ID delle run da eliminare\n",
    "run_ids_to_delete = []\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"Estrazione run ID: {run.id} - Run Name: {run.name}\")\n",
    "    run_ids_to_delete.append(run.id)  # Aggiungi l'ID della run alla lista\n",
    "    \n",
    "print(f\"\\nTotale runs: \\033[1m{len(list(run_ids_to_delete))}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739f0a0-9a3c-4094-b11f-d905557dc1d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **IMPLEMENTAZIONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73e59b-74b3-40b1-88b6-ff7085f47c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TUTTE ASSIEME\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Nomi dei progetti\n",
    "project_names = [\n",
    "    'th_resp_vs_pt_resp_spectrograms_channels_freqs',\n",
    "    'th_resp_vs_shared_resp_spectrograms_channels_freqs',\n",
    "    'pt_resp_vs_shared_resp_spectrograms_channels_freqs'\n",
    "]\n",
    "\n",
    "user_name = 'stefano-bargione-universit-di-roma-tor-vergata'\n",
    "\n",
    "# Connessione all'API di W&B\n",
    "api = wandb.Api()\n",
    "\n",
    "# Itera su tutti i progetti\n",
    "for project_name in project_names:\n",
    "    print(f\"\\nSelezione delle run per il progetto: \\033[1m{project_name}\\033[0m\")\n",
    "    \n",
    "    # Recupera tutte le run del progetto\n",
    "    runs = api.runs(f\"{user_name}/{project_name}\")\n",
    "    \n",
    "    # Salva gli ID delle run da eliminare\n",
    "    run_ids_to_delete = []\n",
    "    \n",
    "    for run in runs:\n",
    "        #print(f\"Estrazione run ID: {run.id} - Run Name: {run.name}\")\n",
    "        run_ids_to_delete.append(run.id)  # Aggiungi l'ID della run alla lista\n",
    "    \n",
    "    print(f\"\\nTotale runs da eliminare: \\033[1m{len(run_ids_to_delete)}\\033[0m\")\n",
    "    \n",
    "    # Elimina le run\n",
    "    for run_id in run_ids_to_delete:\n",
    "        run = api.run(f\"{user_name}/{project_name}/{run_id}\")\n",
    "        run.delete()  # Elimina la run\n",
    "        #print(f\"Eliminata la run con ID: {run_id}\")\n",
    "    print(f\"Eliminazione runs completata\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da3f3c-feb8-416d-8079-5b940080b356",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login PRECEDURA CORRETTA ✅ (CANCELLAZIONE RUNS e NON SWEEPS!) - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000f3a3-07d6-4de1-9b91-124f4e1622ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Sì, è perfettamente normale: in W&B un progetto è semplicemente un contenitore di run e di sweep, \n",
    "e rimane visibile (anche se vuoto) fintanto che non lo archivi o lo cancelli esplicitamente. \n",
    "\n",
    "Cancellare tutte le run e tutti i sweep in un progetto non elimina il progetto stesso — lascia solo un progetto “vuoto” con zero run/sweep.\n",
    "\n",
    "\n",
    "Come rimuovere (o archiviare) anche i progetti\n",
    "\n",
    "\n",
    "1) Dal web UI\n",
    "\n",
    "Vai nella pagina del progetto che vuoi rimuovere.\n",
    "\n",
    "Clicca sui tre puntini (⋯) accanto al nome del progetto.\n",
    "Scegli “Archive project” (per nasconderlo) o, se disponibile, “Delete project” per eliminarlo definitivamente.\n",
    "\n",
    "\n",
    "2) Da CLI\n",
    "Se hai una versione recente del wandb CLI, puoi usare (da terminale):\n",
    "\n",
    "\n",
    "wandb project delete <ENTITY>/<PROJECT_NAME>\n",
    "\n",
    "Esempio:\n",
    "\n",
    "wandb project delete stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_1_20_familiar_th\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Programmaticamente\n",
    "Attualmente W&B non espone un’API Python ufficiale per cancellare progetti; l’opzione più rapida resta il CLI o la UI.\n",
    "\n",
    "In sintesi\n",
    "Cancellare run/sweep → lascia il progetto “vuoto” (0 run, 0 sweep).\n",
    "\n",
    "Progetto visibile → finché non lo archivi o elimini esplicitamente, lo vedrai nella lista.\n",
    "\n",
    "Per toglierlo di mezzo → archivia o cancella il progetto tramite i tre puntini in UI, oppure con wandb project delete da CLI.\n",
    "\n",
    "\n",
    "\n",
    "IO:\n",
    "ok ma quindi se ricreo il progetto con lo stesso nome semplicemente ripopola quel progetto ma con sweeps e runs nuovi giusto?\n",
    "\n",
    "\n",
    "ChatGPT:\n",
    "\n",
    "Esatto. In W&B il progetto è identificato dal solo nome (più l’entity) e non ha uno “stato” permanente di esistenza: se tu:\n",
    "\n",
    "Elimini o archivi totalmente il progetto (tramite UI o wandb project delete),\n",
    "\n",
    "E poi lanci di nuovo uno wandb.init(..., project=\"quel_nome\"),\n",
    "\n",
    "W&B creerà un nuovo progetto con lo stesso nome, vuoto all’inizio, in cui andranno a confluire i tuoi nuovi sweep e run.\n",
    "\n",
    "Quindi:\n",
    "\n",
    "Se archivi il vecchio progetto, è solo nascosto ma rimane là;\n",
    "\n",
    "Se lo cancelli definitivamente, sparisce e puoi ricrearlo identico per nome, e in quel momento sarà “pulito” (0 sweep, 0 run)\n",
    "pronto a riempirsi con i tuoi esperimenti successivi.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from itertools import product\n",
    "import wandb\n",
    "\n",
    "# 1) Parti delle stringhe da combinare\n",
    "prefixes = [\n",
    "    \"th_resp_vs_pt_resp\",\n",
    "    \"th_resp_vs_shared_resp\",\n",
    "    \"pt_resp_vs_shared_resp\",\n",
    "]\n",
    "middles = [\n",
    "    \"spectrograms_channels_freqs\",\n",
    "]\n",
    "\n",
    "suffixes = [\n",
    "    \"familiar_th\",\n",
    "    \"familiar_pt\",\n",
    "    \"unfamiliar_th\",\n",
    "    \"unfamiliar_pt\",\n",
    "]\n",
    "\n",
    "# 2) Genera tutti i nomi di progetto\n",
    "projects = [\n",
    "    f\"{p}_{m}_{s}\"\n",
    "    for p, m, s in product(prefixes, middles, suffixes)\n",
    "]\n",
    "\n",
    "# 3) Configura l’API e l’entity\n",
    "entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "api = wandb.Api()\n",
    "\n",
    "# 4) Itera su ogni progetto: svuota le run e poi cancella gli sweep\n",
    "for proj in projects:\n",
    "    project_path = f\"{entity}/{proj}\"\n",
    "    print(f\"\\n→ Progetto: {project_path}\")\n",
    "\n",
    "    # 4.1 Cancella tutte le run via Python API\n",
    "    try:\n",
    "        runs = api.runs(project_path)\n",
    "        if runs:\n",
    "            print(f\"   • Eliminando {len(runs)} run…\")\n",
    "            for run in runs:\n",
    "                try:\n",
    "                    run.delete()\n",
    "                except Exception as e:\n",
    "                    print(f\"     – Errore cancellando run {run.id}: {e}\")\n",
    "                else:\n",
    "                    print(f\"     – run {run.id} eliminata\")\n",
    "        else:\n",
    "            print(\"   (nessuna run trovata)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Impossibile caricare le run: {e}\")\n",
    "\n",
    "    # 4.2 Cancella tutti gli sweep via CLI Python module\n",
    "    #    Evitiamo di chiamare un eseguibile esterno, usiamo `python -m wandb`\n",
    "    #    Lo stesso interprete che esegue questo script è in sys.executable\n",
    "    cmd_list = [\n",
    "        sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "        \"--project\", project_path, \"--list\"\n",
    "    ]\n",
    "    res = subprocess.run(cmd_list, capture_output=True, text=True)\n",
    "\n",
    "    if res.returncode != 0 or not res.stdout.strip():\n",
    "        print(\"   • Nessuno sweep trovato o progetto inesistente\")\n",
    "        continue\n",
    "\n",
    "    # Ogni riga di res.stdout ha uno sweep_id come primo token\n",
    "    for line in res.stdout.splitlines():\n",
    "        sweep_id = line.split()[0]\n",
    "        print(f\"   • Cancello sweep {sweep_id}\")\n",
    "        cmd_delete = [\n",
    "            sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "            \"--delete\", f\"{project_path}/{sweep_id}\"\n",
    "        ]\n",
    "        subprocess.run(cmd_delete, check=False)\n",
    "\n",
    "    print(f\"  ✅ Run e sweep eliminati per {project_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7df42-b11b-40fe-9597-ec7f06c88af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from itertools import product\n",
    "import wandb\n",
    "\n",
    "entity   = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "api      = wandb.Api()\n",
    "\n",
    "# il solo progetto di cui voglio ripulire sweep+run\n",
    "target = \"th_resp_vs_pt_resp_spectrograms_channel_freqs_familiar_th\"\n",
    "project_path = f\"{entity}/{target}\"\n",
    "\n",
    "print(f\"\\n→ Progetto: {project_path}\")\n",
    "\n",
    "# 4.1 Cancella tutte le run via Python API\n",
    "try:\n",
    "    runs = api.runs(project_path)\n",
    "    if runs:\n",
    "        print(f\"   • Eliminando {len(runs)} run…\")\n",
    "        for run in runs:\n",
    "            try:\n",
    "                run.delete()\n",
    "            except Exception as e:\n",
    "                print(f\"     – Errore cancellando run {run.id}: {e}\")\n",
    "            else:\n",
    "                print(f\"     – run {run.id} eliminata\")\n",
    "    else:\n",
    "        print(\"   (nessuna run trovata)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Impossibile caricare le run: {e}\")\n",
    "\n",
    "# 4.2 Cancella tutti gli sweep via CLI (usando python -m wandb)\n",
    "cmd_list = [\n",
    "    sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "    \"--project\", project_path, \"--list\"\n",
    "]\n",
    "res = subprocess.run(cmd_list, capture_output=True, text=True)\n",
    "\n",
    "if res.returncode != 0 or not res.stdout.strip():\n",
    "    print(\"   • Nessuno sweep trovato o progetto inesistente\")\n",
    "else:\n",
    "    for line in res.stdout.splitlines():\n",
    "        sweep_id = line.split()[0]\n",
    "        print(f\"   • Cancello sweep {sweep_id}\")\n",
    "        cmd_delete = [\n",
    "            sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "            \"--delete\", f\"{project_path}/{sweep_id}\"\n",
    "        ]\n",
    "        subprocess.run(cmd_delete, check=False)\n",
    "\n",
    "print(f\"  ✅ Run e sweep eliminati per {project_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4ae0e-ce9a-4a9f-b3b9-940c00ecf88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea367b20-6a75-47d2-90ed-faba78ed997e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login PRECEDURA CORRETTA ✅ (CANCELLAZIONE RUNS e ANCHE SWEEPS!) - EEG Spectrograms - Electrodes x Frequencies V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cefb4-582c-421f-9198-b999a06f92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6a166-40ab-4a4b-aff0-2c2b4d5c2cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perfetto, hai due script:\n",
    "\n",
    "uno funzionale e robusto che elimina run e sweep, ma usa pattern statici,\n",
    "\n",
    "uno che usa prefisso, medio e suffisso per generare i nomi dei progetti, ma è meno dettagliato.\n",
    "\n",
    "Ti creo una versione unificata che:\n",
    "\n",
    "Usa prefissi, medii e suffissi per generare i nomi dei progetti (come nel secondo script).\n",
    "\n",
    "Per ogni progetto, cancella tutte le run (come nel primo script).\n",
    "\n",
    "Cancella tutti gli sweep usando wandb sweep --delete.\n",
    "\n",
    "Verifica che gli sweep siano stati eliminati correttamente.\n",
    "\n",
    "\n",
    "✅ Script finale combinato:\n",
    "    \n",
    "✅ Cosa fa questo script:\n",
    "\n",
    "Genera i progetti combinando prefix, middle, suffix.\n",
    "\n",
    "Cancella le run usando l’API Python (run.delete()).\n",
    "\n",
    "Cancella gli sweep usando wandb sweep --delete, richiamato tramite subprocess.\n",
    "\n",
    "Verifica se gli sweep sono stati realmente eliminati, usando proj.sweeps().\n",
    "\n",
    "📝 Dipendenze e prerequisiti:\n",
    "\n",
    "wandb dev'essere installato e autenticato (wandb login)\n",
    "\n",
    "Lo script va eseguito in un ambiente con accesso alla CLI di wandb (es. terminale Python)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from itertools import product\n",
    "import wandb\n",
    "\n",
    "# --- Configurazione ---\n",
    "entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "api = wandb.Api()\n",
    "\n",
    "# --- Parti del nome progetto ---\n",
    "prefixes = [\n",
    "    \"th_resp_vs_pt_resp\",\n",
    "    \"th_resp_vs_shared_resp\",\n",
    "    \"pt_resp_vs_shared_resp\",\n",
    "]\n",
    "middles = [\n",
    "    \"spectrograms_channels_freqs\",\n",
    "]\n",
    "suffixes = [\n",
    "    \"familiar_th\",\n",
    "    \"familiar_pt\",\n",
    "    \"unfamiliar_th\",\n",
    "    \"unfamiliar_pt\",\n",
    "]\n",
    "\n",
    "# --- Genera i nomi dei progetti ---\n",
    "project_names = [\n",
    "    f\"{p}_{m}_{s}\"\n",
    "    for p, m, s in product(prefixes, middles, suffixes)\n",
    "]\n",
    "\n",
    "# --- Itera su ogni progetto ---\n",
    "for proj_name in project_names:\n",
    "    path = f\"{entity}/{proj_name}\"\n",
    "    print(f\"\\n→ Progetto: {path}\")\n",
    "\n",
    "    # --- 1. Elimina tutte le run ---\n",
    "    try:\n",
    "        runs = api.runs(path, per_page=None)\n",
    "        runs = list(runs)\n",
    "        if runs:\n",
    "            print(f\"   • Eliminando {len(runs)} run…\")\n",
    "            for run in runs:\n",
    "                try:\n",
    "                    run.delete()\n",
    "                    print(f\"     – Run {run.id} eliminata\")\n",
    "                except Exception as e:\n",
    "                    print(f\"     – Errore eliminando run {run.id}: {e}\")\n",
    "        else:\n",
    "            print(\"   (nessuna run trovata)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Errore caricando le run: {e}\")\n",
    "        continue  # salta alla prossima\n",
    "\n",
    "    # --- 2. Ottieni e cancella gli sweep tramite CLI ---\n",
    "    cmd_list = [\n",
    "        sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "        \"--project\", path, \"--list\"\n",
    "    ]\n",
    "    res = subprocess.run(cmd_list, capture_output=True, text=True)\n",
    "\n",
    "    if res.returncode != 0 or not res.stdout.strip():\n",
    "        print(\"   • Nessuno sweep trovato o progetto inesistente\")\n",
    "        continue\n",
    "\n",
    "    sweep_ids = []\n",
    "    for line in res.stdout.strip().splitlines():\n",
    "        sweep_id = line.split()[0]\n",
    "        sweep_ids.append(sweep_id)\n",
    "        print(f\"   • Cancello sweep {sweep_id}\")\n",
    "        cmd_delete = [\n",
    "            sys.executable, \"-m\", \"wandb\", \"sweep\",\n",
    "            \"--delete\", f\"{path}/{sweep_id}\"\n",
    "        ]\n",
    "        subprocess.run(cmd_delete, check=False)\n",
    "\n",
    "    # --- 3. Verifica cancellazione sweep ---\n",
    "    print(\"   • Verifica sweep attivi dopo la cancellazione...\")\n",
    "    try:\n",
    "        project_obj = next(p for p in api.projects(entity=entity) if p.name == proj_name)\n",
    "        remaining_sweeps = project_obj.sweeps()\n",
    "        if not remaining_sweeps:\n",
    "            print(\"   ✅ Nessuno sweep attivo trovato.\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Sweep ancora attivi: {remaining_sweeps}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Errore nella verifica sweep: {e}\")\n",
    "\n",
    "    print(f\"  ✅ Run e sweep eliminati per {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba967c-d466-4a08-aa28-7428707ddab5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Login - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3977cf-e2c8-4e82-91e1-d838b6194281",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "WEIGHT AND BIASES LOGIN\n",
    "\n",
    "Il messaggio che stai ricevendo indica \n",
    "che sei già connesso al tuo account Weights & Biases (wandb).\n",
    "\n",
    "Se vuoi forzare il login, puoi usare il comando suggerito:\n",
    "\n",
    "wandb login --relogin\n",
    "\n",
    "Questo comando ti permetterà di reinserire le credenziali e riconnetterti al tuo account.\n",
    "Se non hai bisogno di disconnetterti o di cambiare l'account,\n",
    "puoi semplicemente continuare a usare wandb senza ulteriori passaggi. \n",
    "Hai bisogno di ulteriore assistenza con wandb o con il tuo progetto?\n",
    "'''\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"✅ Weights & Biases login effettuato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a8e7a-2c6a-4f77-9194-92c4eb393c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Per modificare il percorso in cui W&B salva i dati localmente,\n",
    "puoi configurare la variabile di ambiente WANDB_DIR.\n",
    "\n",
    "Questo ti permette di specificare una directory personalizzata in cui W&B salva tutti i file associati al tuo run, inclusi i dati e i log.\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "# Imposta la directory per i dati W&B:\n",
    "# Questo cambierà la cartella in cui W&B salva i dati per quella sessione di esecuzione\n",
    "\n",
    "# Definisci la cartella principale\n",
    "WB_dir = \"/home/stefano/Interrogait/WB_spectrograms_analyses_channels_frequencies\"\n",
    "os.makedirs(WB_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DIR\"] = WB_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a7e61-0708-4d60-884b-92f064503707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "\n",
    "# Apri il file in modalità lettura binaria ('rb')\n",
    "\n",
    "#path = '/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms/'\n",
    "\n",
    "#path = '/home/stefano/Interrogait/all_datas/Familiar_Spectrograms_channels_frequencies/'\n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms_channels_frequencies/'\n",
    "\n",
    "with open(f\"{path}new_all_th_concat_spectrograms_coupled_exp.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca0919-6f94-445c-b963-5b1b5870fc99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Itera sulle chiavi del dizionario principale\n",
    "for condition, values in data.items():\n",
    "    if isinstance(values, dict) and \"data\" in values and \"labels\" in values:\n",
    "        X_shape = values[\"data\"].shape\n",
    "        y_length = len(values[\"labels\"])\n",
    "        print(f\"🔹 Condizione: {condition}\")\n",
    "        print(f\"   ➡ Shape dati: {X_shape}\")\n",
    "        print(f\"   ➡ Lunghezza labels: {y_length}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee3480-96b2-4385-bfac-3d77d140647c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Datasets Loading - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a9fb1-fbd2-4cee-b81e-c193ee1e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################### CODICE UFFICIALE DEL 04/03/2025 ORE 9:30 #####################\n",
    "                                 ##################### SENZA DETTAGLI SCRITTI V°3 #####################\n",
    "        \n",
    "'''ATTENZIONE: \n",
    "\n",
    "HO SOSTITUITO LE VARIABILI DI \n",
    "\n",
    "    1) DATASET_TRAIN_LOADER -->  TRAIN_LOADER\n",
    "    2) DATASET_VAL_LOADER -->  VAL_LOADER\n",
    "\n",
    "    VEDI FUNZIONE 'PREPARE_DATA_FOR_MODEL --> NOMI DELLE VARIABILI DEI TORCH TENSOR DATASET LOADER SON  'TRAIN_LOADER' E VAL_LOADER!!!  \n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import random\n",
    "import copy as cp\n",
    "\n",
    "\n",
    "# Definisci le lista delle coppie di condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Inizializza il dizionario per caricare i dati\n",
    "data_dict = {}\n",
    "\n",
    "# Definisci la cartella principale\n",
    "\n",
    "\n",
    "#base_dir = \"/home/stefano/Interrogait/WB_spectrograms_best_results\"\n",
    "\n",
    "base_dir = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies\"\n",
    "\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "'''LOOP DI CARICAMENTO DATI'''\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    # Crea la cartella per la condizione sperimentale\n",
    "    condition_dir = os.path.join(base_dir, condition)\n",
    "    os.makedirs(condition_dir, exist_ok=True)\n",
    "    \n",
    "    # Aggiungi un livello di annidamento per ogni condizione\n",
    "    data_dict[condition] = {}\n",
    "    \n",
    "    for data_type in [\"spectrograms\"]:\n",
    "        \n",
    "        # Crea la cartella per il tipo di dato\n",
    "        data_dir = os.path.join(condition_dir, data_type)\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "        for category in [\"familiar\", \"unfamiliar\"]:\n",
    "            # Crea la cartella per la categoria\n",
    "            #category_dir = os.path.join(data_dir, category)\n",
    "            #os.makedirs(category_dir, exist_ok=True)\n",
    "            \n",
    "            for subject_type in [\"th\", \"pt\"]:\n",
    "                # Caricamento e suddivisione dei dati\n",
    "                \n",
    "                #if data_type == \"spectrograms\":\n",
    "                    \n",
    "                print(f\"Caricamento dati per: {condition} - {data_type} - {category}_{subject_type}\")\n",
    "                X, y = load_data(data_type, category, subject_type, condition=condition)\n",
    "                \n",
    "                \n",
    "                # Creazione della chiave per il dizionario annidato\n",
    "                data_dict[condition][data_type] = data_dict[condition].get(data_type, {})\n",
    "                data_dict[condition][data_type][f\"{category}_{subject_type}\"] = (X, y)\n",
    "                \n",
    "                # Stampa di conferma\n",
    "                print(f\"Dataset caricato: \\033[1m{condition}\\033[0m_\\033[1m{data_type}\\033[0m_\\033[1m{category}_{subject_type}\\033[0m - Shape X: \\033[1m{X.shape}\\033[0m, Shape y: \\033[1m{len(y)}\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dbeec-2d7d-47e5-addd-e210b9787c77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Creazione Griglia 2D per Interrogait - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7b611-e372-45c4-b21d-6a302a86db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "with open(f\"{path}EEG_channels_names.pkl\", \"rb\") as f:\n",
    "    EEG_channels_names = pickle.load(f)\n",
    "    \n",
    "# Caricare file xlsx con pickle\n",
    "path_xlsx = f'{path}EEG_grid_interrogait.xlsx'\n",
    "\n",
    "# Caricamento del file in un DataFrame\n",
    "EEG_file_interrogait = pd.read_excel(path_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680f51c-6bdc-4d97-8a1e-a0c3c6147af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_file_interrogait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f6dda-0a2a-4ef6-9bd8-b54f26b25021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def _build_grid_maps(\n",
    "    eeg_grid_df: pd.DataFrame,\n",
    "    eeg_channels_names: List[str],\n",
    "    grid_shape: Tuple[int, int] = (9, 9),\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea:\n",
    "      - label_grid: matrice (9x9) di etichette (elettrodi o 'EMPTY')\n",
    "      - electrode_grid_map: dict {elettrodo -> (y, x)} (solo elettrodi reali)\n",
    "      - placement_idx: matrice (9x9) di indici canale (>=0) o -1 per EMPTY/non presenti\n",
    "    \"\"\"\n",
    "    df = eeg_grid_df.copy()\n",
    "    df[\"Electrode\"] = df[\"Electrode\"].astype(str).str.strip()\n",
    "\n",
    "    H, W = grid_shape\n",
    "    label_grid = np.full((H, W), \"\", dtype=object)\n",
    "    electrode_grid_map = {}\n",
    "\n",
    "    # Mappa canale -> indice colonna in X_data\n",
    "    ch_to_idx = {ch: i for i, ch in enumerate(eeg_channels_names)}\n",
    "\n",
    "    # Matrice con indici canale (per riempimento veloce delle griglie)\n",
    "    placement_idx = np.full((H, W), -1, dtype=int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        elec = row[\"Electrode\"]\n",
    "        x = int(round(row[\"grid_x\"] * (W - 1)))\n",
    "        y = int(round(row[\"grid_y\"] * (H - 1)))\n",
    "\n",
    "        label_grid[y, x] = \"\" if elec == \"EMPTY\" else elec\n",
    "\n",
    "        if elec != \"EMPTY\":\n",
    "            electrode_grid_map[elec] = (y, x)\n",
    "            if elec in ch_to_idx:\n",
    "                placement_idx[y, x] = ch_to_idx[elec]\n",
    "            # se l'elettrodo non è nella lista canali, placement resta -1 (verrà messo 0 in griglia)\n",
    "\n",
    "    # Controllo elettrodi presenti nell'Excel ma non nei dati\n",
    "    excel_elec = set(df[\"Electrode\"].unique()) - {\"EMPTY\"}\n",
    "    missing = sorted(elec for elec in excel_elec if elec not in ch_to_idx)\n",
    "    if missing:\n",
    "        print(\"⚠️ Elettrodi nel file Excel ma non presenti in EEG_channels_names:\", missing)\n",
    "\n",
    "    return label_grid, electrode_grid_map, placement_idx\n",
    "\n",
    "\n",
    "def convert_fft_images_to_2d_grids_all_freqs_interrogait(\n",
    "    data_dict: Dict[str, Dict[str, Dict[str, Tuple[np.ndarray, np.ndarray]]]],\n",
    "    eeg_grid_df: pd.DataFrame,\n",
    "    eeg_channels_names: List[str],\n",
    "    grid_shape: Tuple[int, int] = (9, 9),\n",
    "    fs: int = 250,\n",
    "    n_fft_points: int = 250,\n",
    "    bands: Dict[str, Tuple[float, float]] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Dict, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Converte OGNI X_data nella struttura annidata di `data_dict`:\n",
    "      (B, n_freqs, n_channels)  →  (B, 9, 9, 5)\n",
    "    sommando la potenza sulle frequenze per ciascuna banda EEG e mappando\n",
    "    i canali nelle posizioni (y,x) definite dal file Excel della griglia.\n",
    "\n",
    "    Struttura in input (immutata nelle chiavi):\n",
    "      data_dict[condition][data_type][category_subject] = (X_data, y_data)\n",
    "\n",
    "    In output mantiene la stessa struttura ma con X_data trasformato:\n",
    "      X_grid: (B, 9, 9, 5),  y invariato.\n",
    "\n",
    "    Ritorna:\n",
    "      - new_data_dict: stessa struttura annidata con X trasformati\n",
    "      - label_grid: matrice (9x9) con le etichette\n",
    "      - electrode_grid_map: dict {elettrodo -> (y, x)}\n",
    "    \"\"\"\n",
    "    if bands is None:\n",
    "        # Ordine fisso (profondità = 5)\n",
    "        bands = {\n",
    "            \"delta\": (1, 4),\n",
    "            \"theta\": (4, 8),\n",
    "            \"alpha\": (8, 13),\n",
    "            \"beta\":  (13, 30),\n",
    "            \"gamma\": (30, 45),\n",
    "        }\n",
    "    band_order = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
    "\n",
    "    # Precostruisco mappe della griglia\n",
    "    label_grid, electrode_grid_map, placement_idx = _build_grid_maps(\n",
    "        eeg_grid_df=eeg_grid_df,\n",
    "        eeg_channels_names=eeg_channels_names,\n",
    "        grid_shape=grid_shape,\n",
    "    )\n",
    "\n",
    "    H, W = grid_shape\n",
    "\n",
    "    # Trovo un esempio per determinare n_freqs effettivi (bins) e costruire le maschere\n",
    "    example_found = False\n",
    "    n_freqs_example = None\n",
    "    n_channels_example = None\n",
    "\n",
    "    for condition, data_types in data_dict.items():\n",
    "        for data_type, categories in data_types.items():\n",
    "            for category_subject, (X_data, y_data) in categories.items():\n",
    "                if X_data is not None and len(X_data) > 0:\n",
    "                    n_freqs_example = X_data.shape[1]\n",
    "                    n_channels_example = X_data.shape[2]\n",
    "                    example_found = True\n",
    "                    break\n",
    "            if example_found:\n",
    "                break\n",
    "        if example_found:\n",
    "            break\n",
    "\n",
    "    if not example_found:\n",
    "        raise ValueError(\"Impossibile determinare n_freqs/n_channels: data_dict è vuoto?\")\n",
    "\n",
    "    # Frequenze in Hz per i bins RFFT (tronco ai primi n_freqs effettivi)\n",
    "    all_freqs_full = np.fft.rfftfreq(n_fft_points, d=1.0 / fs)\n",
    "    all_freqs = all_freqs_full[:n_freqs_example]\n",
    "\n",
    "    # Maschere per ciascuna banda sull'asse delle frequenze\n",
    "    band_masks = {\n",
    "        b: (all_freqs >= fmin) & (all_freqs <= fmax) for b, (fmin, fmax) in bands.items()\n",
    "    }\n",
    "\n",
    "    # Avvisi utili\n",
    "    if verbose:\n",
    "        print(f\"fs={fs} Hz, n_fft_points={n_fft_points}\")\n",
    "        print(f\"n_freqs in X_data = {n_freqs_example} (verranno usati i primi {n_freqs_example} bins di rfftfreq)\")\n",
    "        print(\"Bande usate:\", {b: bands[b] for b in band_order})\n",
    "\n",
    "    # Trasformazione\n",
    "    new_data_dict = {}\n",
    "    for condition, data_types in data_dict.items():\n",
    "        new_data_dict.setdefault(condition, {})\n",
    "        for data_type, categories in data_types.items():\n",
    "            new_data_dict[condition].setdefault(data_type, {})\n",
    "\n",
    "            for category_subject, (X_data, y_data) in categories.items():\n",
    "                \n",
    "                # X_data: (B, n_freqs, n_channels)\n",
    "                if X_data is None or X_data.size == 0:\n",
    "                    new_data_dict[condition][data_type][category_subject] = (X_data, y_data)\n",
    "                    continue\n",
    "\n",
    "                B, n_freqs, n_channels = X_data.shape\n",
    "                if n_freqs != n_freqs_example:\n",
    "                    # Le maschere sono state costruite su n_freqs_example; se differisce, le rigenero on-the-fly\n",
    "                    all_freqs_local = np.fft.rfftfreq(n_fft_points, d=1.0 / fs)[:n_freqs]\n",
    "                    band_masks_local = {\n",
    "                        b: (all_freqs_local >= bands[b][0]) & (all_freqs_local <= bands[b][1])\n",
    "                        for b in band_order\n",
    "                    }\n",
    "                else:\n",
    "                    band_masks_local = band_masks\n",
    "\n",
    "                if n_channels != len(eeg_channels_names):\n",
    "                    print(\n",
    "                        f\"⚠️ Attenzione: n_channels={n_channels} \"\n",
    "                        f\"diverso da len(EEG_channels_names)={len(eeg_channels_names)} \"\n",
    "                        f\"per {condition} / {data_type} / {category_subject}. \"\n",
    "                        f\"Userò SOLO i canali presenti in placement_idx (gli altri verranno ignorati).\"\n",
    "                    )\n",
    "\n",
    "                # Output per questo blocco: (B, H, W, 5)\n",
    "                X_out = np.zeros((B, H, W, len(band_order)), dtype=X_data.dtype)\n",
    "\n",
    "                # Precompute posizione valide nella griglia (non EMPTY)\n",
    "                valid_pos = placement_idx >= 0\n",
    "                idx_lin = placement_idx[valid_pos]  # indici canale per le posizioni valide\n",
    "                yy, xx = np.where(valid_pos)       # coordinate y,x da riempire\n",
    "\n",
    "                for b in range(B):\n",
    "                    # Per ciascuna banda: somma lungo le frequenze → vettore (n_channels,)\n",
    "                    per_band_grids = []\n",
    "                    sample = X_data[b]  # (n_freqs, n_channels)\n",
    "\n",
    "                    for bi, band_name in enumerate(band_order):\n",
    "                        mask = band_masks_local[band_name]\n",
    "                        if not np.any(mask):\n",
    "                            # nessun bin in banda → griglia a zero\n",
    "                            continue\n",
    "\n",
    "                        # potenza totale per canale nella banda\n",
    "                        band_power_per_ch = sample[mask, :].sum(axis=0)  # (n_channels,)\n",
    "\n",
    "                        # riempi griglia rapidamente con indicizzazione\n",
    "                        grid = np.zeros((H, W), dtype=sample.dtype)\n",
    "                        # assegna solo posizioni con elettrodi mappati (placement_idx >= 0)\n",
    "                        grid[yy, xx] = band_power_per_ch[idx_lin]\n",
    "\n",
    "                        X_out[b, :, :, bi] = grid\n",
    "\n",
    "                new_data_dict[condition][data_type][category_subject] = (X_out, y_data)\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"[OK] {condition} / {data_type} / {category_subject} : \"\n",
    "                        f\"{X_data.shape}  →  {X_out.shape}\"\n",
    "                    )\n",
    "\n",
    "    return new_data_dict, label_grid, electrode_grid_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b53192-4a32-43f9-8aae-d3a44fa12cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) converti TUTTI i blocchi del tuo data_dict\n",
    "data_dict, label_grid, electrode_grid_map = convert_fft_images_to_2d_grids_all_freqs_interrogait(\n",
    "    data_dict,\n",
    "    eeg_grid_df=EEG_file_interrogait,\n",
    "    eeg_channels_names=EEG_channels_names,\n",
    "    grid_shape=(9, 9),\n",
    "    fs=250,\n",
    "    n_fft_points=250,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939f80f-da8c-4e52-bb7c-35779a173744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "# Salvare l'intero dizionario annidato con pickle\n",
    "with open(f'{path}final_EEG_electrodes_grid_interrogait.pkl', 'wb') as f:\n",
    "    pickle.dump(label_grid, f)\n",
    "    \n",
    "# Salvare l'intero dizionario annidato con pickle\n",
    "with open(f'{path}electrode_grid_map_interrogait.pkl', 'wb') as f:\n",
    "    pickle.dump(electrode_grid_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc166ec-90a2-478f-8103-2ffe16fda087",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Se vuoi costruire una matrice 2D indicizzata (es. grid[y, x]) per usarla come immagine 2D o input per modelli CNN:\n",
    "\n",
    "→ Devi usare indici interi\n",
    "\n",
    "Quindi sì, devi fare:\n",
    "\n",
    "x = int(round(row['grid_x'] * (grid_shape[1] - 1)))\n",
    "y = int(round(row['grid_y'] * (grid_shape[0] - 1)))\n",
    "\n",
    "Questo perché una matrice NumPy grid_2d[y, x] non può accettare float come indici\n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_eeg_grid_labels(label_grid: np.ndarray, title: str = \"Posizione elettrodi sulla griglia EEG\"):\n",
    "    \"\"\"\n",
    "    Visualizza una griglia 2D con le etichette degli elettrodi (senza potenza).\n",
    "    \n",
    "    Args:\n",
    "        label_grid (np.ndarray): Griglia 2D (grid_y x grid_x) con stringhe degli elettrodi.\n",
    "        title (str): Titolo del grafico.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Disegna la griglia e annota le etichette\n",
    "    for y in range(label_grid.shape[0]):\n",
    "        for x in range(label_grid.shape[1]):\n",
    "            label = label_grid[y, x]\n",
    "            rect = plt.Rectangle((x, y), 1, 1, fill=False, edgecolor='gray', lw=1)\n",
    "            ax.add_patch(rect)\n",
    "            if label != '':\n",
    "                ax.text(x + 0.5, y + 0.5, label, ha='center', va='center', fontsize=8)\n",
    "\n",
    "    ax.set_xlim(0, label_grid.shape[1])\n",
    "    ax.set_ylim(label_grid.shape[0], 0)  # Inverti y per avere origine in alto a sinistra\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00f8db-ab98-421b-a62e-b77c50a157ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eeg_grid_labels(label_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21e98c-b552-4c73-a94c-8b2af37f2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#🔄 Se vuoi aggiungere le etichette degli elettrodi sulla griglia:\n",
    "\n",
    "def plot_eeg_grid_any(trial_img, label_grid, band=None, mode=\"single\", title=None):\n",
    "    \"\"\"\n",
    "    trial_img: (9,9) oppure (9,9,5)\n",
    "    band: 'delta'|'theta'|'alpha'|'beta'|'gamma' (usata se mode='single')\n",
    "    mode: 'single' (una banda), 'sum', 'mean', 'all' (griglia 2x3 con tutte le bande)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    band_map = {\"delta\":0, \"theta\":1, \"alpha\":2, \"beta\":3, \"gamma\":4}\n",
    "    if trial_img.ndim == 2:\n",
    "        img2d = trial_img\n",
    "        plt.figure(figsize=(6,6))\n",
    "        im = plt.imshow(img2d, cmap='viridis', origin='upper')\n",
    "        for y in range(label_grid.shape[0]):\n",
    "            for x in range(label_grid.shape[1]):\n",
    "                lab = label_grid[y, x]\n",
    "                if lab != \"\":\n",
    "                    plt.text(x, y, lab, ha='center', va='center', color='white', fontsize=8)\n",
    "        plt.colorbar(im, label=\"Potenza Totale\")\n",
    "        plt.title(title or \"EEG grid\")\n",
    "        plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "        return\n",
    "\n",
    "    # trial_img è (9,9,5)\n",
    "    if mode in (\"sum\", \"mean\"):\n",
    "        img2d = trial_img.sum(axis=-1) if mode==\"sum\" else trial_img.mean(axis=-1)\n",
    "        return plot_eeg_grid_any(img2d, label_grid, title=title or f\"Aggregato ({mode})\")\n",
    "\n",
    "    if mode == \"all\":\n",
    "        import matplotlib.pyplot as plt\n",
    "        bands = [\"delta\",\"theta\",\"alpha\",\"beta\",\"gamma\"]\n",
    "        fig = plt.figure(figsize=(10,7))\n",
    "        for i,b in enumerate(bands):\n",
    "            ax = fig.add_subplot(2,3,i+1)\n",
    "            im = ax.imshow(trial_img[:,:,band_map[b]], cmap='viridis', origin='upper')\n",
    "            for y in range(label_grid.shape[0]):\n",
    "                for x in range(label_grid.shape[1]):\n",
    "                    lab = label_grid[y, x]\n",
    "                    if lab != \"\":\n",
    "                        ax.text(x, y, lab, ha='center', va='center', color='white', fontsize=7)\n",
    "            ax.set_title(b); ax.axis('off')\n",
    "        fig.suptitle(title or \"Tutte le bande\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "        return\n",
    "\n",
    "    # mode == 'single'\n",
    "    assert band in band_map, f\"band deve essere in {list(band_map.keys())}\"\n",
    "    img2d = trial_img[:, :, band_map[band]]\n",
    "    return plot_eeg_grid_any(img2d, label_grid, title=title or f\"Banda: {band}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ff788-08c4-40f7-bcb7-1cba655dc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#th_resp_vs_pt_resp / spectrograms / familiar_th : (1586, 45, 61)  →  (1586, 9, 9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d504be-6c62-47e8-8c37-4b560d96a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_eeg_grid_any(data_dict[\"th_resp_vs_pt_resp\"][\"spectrograms\"][\"familiar_th\"][0], label_grid, \"delta\")\n",
    "\n",
    "X_grid, y = data_dict[\"th_resp_vs_shared_resp\"][\"spectrograms\"][\"familiar_th\"]\n",
    "trial_3d = X_grid[5]  # (9,9,5)\n",
    "\n",
    "\n",
    "plot_eeg_grid_any(trial_3d, label_grid, band=\"alpha\", mode=\"single\",\n",
    "                  title=\"th_resp_vs_shared_resp - Trial 0 - alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3de8f-def8-474d-92bb-d11cc6b65128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9dca59-c1f0-48ad-8124-308b740558e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "N.B. \n",
    "\n",
    "PER SAPERE A QUALE COMBINAZIONE DI FATTORI CORRISPONDONO I DATI (i.e, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "MI CREO UN DIZIONARIO ULTERIORE, 'DATA_DICT_PREPROCESSED' CHE CONTIENE PER OGNI COMBINAZIONE DI FATTORI I DATI SPLITTATI\n",
    "\n",
    "IN QUESTO MODO, QUANDO FORNISCO ALLA FUNZIONE 'TRAINING_SWEEP' LA TUPLA CON I VARI DATI ((TRAIN, VAL E TEST))\n",
    "IO POSSO CAPIRE A QUALE COMBINAZIONI DI FATTORI CORRISPONDE QUELLA TUPLA DI DATI (TRAIN, VAL E TEST)\n",
    "\n",
    "\n",
    "INOLTRE,\n",
    "MI CREO ANCHE UNA LISTA DI TUPLE DI STRINGHE, DOVE OGNI TUPLA CONTIENE LE STRINGHE DELLE CHIAVI USATE \n",
    "PER LA GENERAZIONE DI DATA_DICT_PREPROCESSED.\n",
    "\n",
    "IN QUESTO MODO, MI ASSICURO CHE SIA UNA COERENZA TRA LA CREAZIONE DEI 'NAME' E 'TAG' DELLA RUN\n",
    "E\n",
    "LA CORRETTA ESTRAZIONE DEI DATI (OSSIA I DATI DI QUALE CONDIZIONE SPERIMENTALE, QUALI EEG INPUT, E DA CHI PROVENGONO!)  \n",
    "\n",
    "\n",
    "Questo approccio permette di garantire la corrispondenza tra \n",
    "\n",
    "1) le chiavi dei dati pre‐processati e \n",
    "2) la configurazione delle runs su W&B\n",
    "\n",
    "andando a creare due strutture in parallelo:\n",
    "\n",
    "- data_dict_preprocessed – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "                            la tupla dei dati già suddivisi (X_train, X_val, X_test, y_train, y_val, y_test);\n",
    "                            \n",
    "- sweeps_id – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "              sia la stringa univoca dello sweep ID, che l'insieme delle stringhe che formano la combinazione (condition, data_type, category_subject)\n",
    "\n",
    "\n",
    "\n",
    "LOOP DI PREPARAZIONE DATI (FINO A DATASET SPLITTING)\n",
    "'''\n",
    "\n",
    "#A QUESTO PUNTO PER OGNI DATASET, FACCIO STEP PRIMA DELLO SWEEP\n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Seleziona il dispositivo (GPU o CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dizionario per salvare gli sweep ID associati a ogni condizione sperimentale\n",
    "\n",
    "'''sweep_ids_for_models contiene la struttura che mi serve da copiare per best_models''' \n",
    "sweep_ids_for_models = {}\n",
    "\n",
    "'''sweep_ids contiene la struttura che mi serve da copiare per iterare sui singoli swweps di ogni combinazione di fattori'''\n",
    "sweep_ids = {}  \n",
    "\n",
    "'''DIZIONARIO CHE VIENE FORNITO IN INGRESSO A TRAINING_SWEEP'''\n",
    "# Dizionario per salvare la tupla di dati già preprocessati\n",
    "data_dict_preprocessed = {}\n",
    "\n",
    "\n",
    "# Loop di addestramento e test per ogni condizione sperimentale\n",
    "for condition, data_types in data_dict.items():  # Itera sulle condizioni sperimentali\n",
    "    \n",
    "    data_dict_preprocessed[condition] = {}\n",
    "    \n",
    "    # Aggiungi al dizionario sweep_ids\n",
    "    if condition not in sweep_ids:\n",
    "        sweep_ids[condition] = {}\n",
    "        \n",
    "        '''sweep_ids_for_models'''\n",
    "        sweep_ids_for_models[condition] = {}\n",
    "        \n",
    "    for data_type, categories in data_types.items():  # Itera sui tipi di dati (1_20, 1_45, wavelet)\n",
    "        \n",
    "        data_dict_preprocessed[condition][data_type] = {}\n",
    "        \n",
    "        if data_type not in sweep_ids[condition]:\n",
    "            sweep_ids[condition][data_type] = {}\n",
    "            \n",
    "            '''sweep_ids_for_models'''\n",
    "            sweep_ids_for_models[condition][data_type] = {}\n",
    "            \n",
    "        for category_subject, (X_data, y_data) in categories.items():  # Itera sulle coppie category_subject\n",
    "            \n",
    "            if category_subject not in sweep_ids[condition][data_type]:\n",
    "                sweep_ids[condition][data_type][category_subject] = {}\n",
    "                \n",
    "                '''sweep_ids_for_models'''\n",
    "                sweep_ids_for_models[condition][data_type][category_subject] = {}\n",
    "                \n",
    "            print(f\"\\n\\n\\033[1mEstrazione Dati\\033[0m della Chiave \\033[1m{condition}_{data_type}_{category_subject}\\033[0m\")\n",
    "            \n",
    "            # Controlla se il dataset è già stato elaborato (se la chiave è già nel set)\n",
    "            if (condition, data_type, category_subject) in processed_datasets:\n",
    "                print(f\"⚠️ ATTENZIONE: Il dataset {condition} - {data_type} - {category_subject} è già stato elaborato! Salto iterazione...\")\n",
    "                continue  # Salta se il dataset è già stato processato\n",
    "\n",
    "            # Aggiungi il dataset al set\n",
    "            processed_datasets.add((condition, data_type, category_subject))\n",
    "\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "            \n",
    "            data_dict_preprocessed[condition][data_type][category_subject] = (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "            \n",
    "            # Puoi anche aggiungere altri print per verificare la dimensione dei set\n",
    "            print(f\"\\033[1mDataset Splitting\\033[0m: Train Set Shape: {X_train.shape}, Validation Set Shape: {X_val.shape}, Test set Shape: {X_test.shape}\")\n",
    "\n",
    "            \n",
    "print(f\"\\nCreato \\033[1mdata_dict_preprocessed\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e009d221-57e1-496d-8270-8aac9881558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094686fc-3a19-4f14-ac7f-016d331c153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_pt_resp'].keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms'].keys())\n",
    "print(type(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms'].keys()))\n",
    "\n",
    "#All'interno, c'è una tupla, di 6 elementi!\n",
    "print(type(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms']['familiar_th']))\n",
    "\n",
    "#I 6 elementi della tupla sono X_train, X_val, X_test, y_train, y_val, y_test !\n",
    "print(len(data_dict_preprocessed['th_resp_vs_pt_resp']['spectrograms']['familiar_th']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece1e31-bb89-4fa1-aa3f-ce8a27bc6350",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Sweep Configuration - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS (PER CNN2D)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a0ca115-2bce-44f1-9209-7f5834dc23e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                             PRE-LUGLIO 2025\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "#\"model_name\":{\"values\": ['CNN2D', 'BiLSTM', 'Transformer']},\n",
    "\n",
    "''' OLD VERSION!\n",
    "#Creo la configurazione dello sweep e la eseguo\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "        \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 10},\n",
    "        \"model_name\":{\"values\": ['CNN2D']},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 72, 84, 96]}, #batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\":{\"values\": [True, False]},\n",
    "        \n",
    "'''\n",
    "\n",
    "'''\n",
    "VERSIONE SOLO IPER-PARAMETRI\n",
    "\n",
    "'''\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        # Hyperaparameters di training\n",
    "        \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "        \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 10},\n",
    "        \"model_name\": {\"values\": ['CNN2D']},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 72, 84, 96]},\n",
    "        \"standardization\": {\"values\": [True, False]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4af295ed-40d3-4a28-83b9-a5191be25537",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                POST 12 LUGLIO 2025\n",
    "                                                                \n",
    "                                                                \n",
    "Adesso, nello sweep config ho esteso per estendere il valore degli iper-parametri\n",
    "\n",
    "1) “classici” (lr, weight_decay, beta1, beta2, eps, ecc.) \n",
    "2) \"architetturali\", ossia specifici per ogni specifico modello scelto\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ad esempio, per la mia rete CNN1D, potrei impostare un set di valori per:\n",
    "\n",
    "\n",
    "^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^\n",
    "\n",
    "Ad esempio, rispetto alla mia rete CNN1D, potrei far variare\n",
    "\n",
    "\n",
    "1) All'interno di ogni layer convolutivo \n",
    "\n",
    "(https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels \n",
    "\n",
    "(ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente,se capisco bene) \n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 12 con step di 2)\n",
    "c) la grandezza del passo, ossia dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    " \n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo \n",
    "\n",
    "(https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "\n",
    "^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^\n",
    "Invece, per la mia BiLSTM vorrei far variare\n",
    "\n",
    "\n",
    "1) il valore di hidden_sizes ossia dello spazio di embedding multidimensionale dei miei punti temporali \n",
    "del dato EEG (tutti i valori tra 16 e 32 con step di 2, ossia 16, 18, 20.. e così via)\n",
    "\n",
    "2) il valore di dropout (tra 0.0 e 0.5)\n",
    "\n",
    "3) la scelta sulla bidirezionalità o meno (True o False)\n",
    "\n",
    "^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^ ^^^^^^^^^^^\n",
    "\n",
    "\n",
    "\n",
    "Invece, per il mio Transformer vorrei far variare\n",
    "\n",
    "\n",
    "1) il valore dell'embedding, ossia \"d_model\" (con valori tra 8 e 64 con step di 8)\n",
    "2) il valore di head attenzionali, ossia \"num_heads\" (con valori tra 2 e 12 con step di 2) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "N.B. \n",
    "\n",
    "In questo modo WandB proverà tutte le 27 triplette possibili di funzioni di attivazione sui tre blocchi della CNN 1D\n",
    "\n",
    "\n",
    "Sostanzialmente vorrei replicare gli stessi valori presenti nell'approccio CNN1D ma nel CNN2D ossia\n",
    "\n",
    "\n",
    "1) per kernel sizes dovrebbe esser gli stessi di \n",
    "kernel_combinations = list(product([2, 4, 6, 8], repeat=3))\n",
    "\n",
    "nel CNN1D, ogni valore veniva associato ad ogni kernel convolutivo di ogni layer (perché si creava una tripletta sostanzialmente...\n",
    "Ora invece, per ogni layer convolutivo, avro una tripletta di tuple, ciascuna da associare ad ogni layer convolutivo,\n",
    "e dovrebbe estrarre se capisco bene da questi set di coppie di valori, \n",
    "dove ciascuna coppia rappresenta la grandezza del kernel size per ogni layer convolutivo nella CNN2D..\n",
    "\n",
    "E che dovrebbero essere (2,2), (4,4), (6,6), (8,8), per cui si potrebbero creare triplette di o stesse coppie di valori,\n",
    "o di triplette di coppie di valori diversi tra i layer convolutivi... \n",
    "\n",
    "\n",
    "2)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#Genera tutte le 27 combinazioni di 3 funzioni di attivazione\n",
    "activation_combinations = list(product(['relu','selu','elu'], repeat=3))\n",
    "\n",
    "\n",
    "# I valori base “CNN1D” che prima erano [2,4,6,8], ora diventano coppie (h,w)\n",
    "\n",
    "kernel2d_sizes = [(2,2), (4,4), (6,6), (8,8)]\n",
    "\n",
    "# tutte le possibili triplette di coppie, una per ciascun conv‐layer\n",
    "conv2d_kernel_combinations = list(product(kernel2d_sizes, repeat=3))\n",
    "\n",
    "# per lo stride, usi (1,1) e (2,2) invece di 1 e 2\n",
    "stride2d_sizes = [(1,1), (2,2)]\n",
    "\n",
    "conv2d_stride_combinations = list(product(stride2d_sizes, repeat=3))\n",
    "\n",
    "# per il pooling, coppie (1,1) e (2,2)\n",
    "pool2d_sizes = [(1,1), (2,2)]\n",
    "\n",
    "pool2d_kernel_combinations = list(product(pool2d_sizes, repeat=3))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Ecco un esempio di sweep_config pensato solo per le tre architetture che lavorano su input tempo × frequenza \n",
    "(CNN2D, BiLSTM, Transformer). Ho rimosso CNN1D e ho parametrizzato:\n",
    "\n",
    "CNN2D\n",
    "\n",
    "    conv_out_channels\n",
    "    conv2d_kernel_size (tuple h×w)\n",
    "    conv2d_stride (tuple h×w)\n",
    "    pool_type\n",
    "    pool2d_kernel_size (tuple h×w)\n",
    "    fc1_units\n",
    "    activations (tuple di 3 funzioni)\n",
    "    dropout\n",
    "\n",
    "BiLSTM\n",
    "\n",
    "    hidden_size\n",
    "    bidirectional\n",
    "    dropout\n",
    "\n",
    "Transformer\n",
    "    d_model\n",
    "    num_heads\n",
    "\n",
    "Con le dipendenze condizionali via conditional.\n",
    "\n",
    "In questo modo, quando scegli CNN2D, i parametri architetturali 2D (kernel, stride, pool, ecc.) verranno pescati dalla griglia; \n",
    "Quando scegli BiLSTM o Transformer, verranno usati solo i loro iper-parametri specifici.\n",
    "\n",
    "'''\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        # Ottimizzatore\n",
    "        \"lr\":            {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":         {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "        \"beta2\":         {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "        \"eps\":           {\"values\": [1e-8, 1e-7, 1e-6, 1e-5]},\n",
    "\n",
    "        # Training\n",
    "        \"n_epochs\":      {\"value\": 100},\n",
    "        \"patience\":      {\"value\": 12},\n",
    "\n",
    "        # Scelta del modello\n",
    "        \"model_name\":    {\"values\": [\"CNN2D\", \"BiLSTM\", \"Transformer\"]},\n",
    "\n",
    "        # Dati e regolarizzazione generale\n",
    "        \"batch_size\":    {\"values\": [16, 24, 32, 48, 52, 64, 72, 84, 96]},\n",
    "        \"standardization\":{\"values\":[True, False]},\n",
    "        \n",
    "        # ================================================\n",
    "        # SOLO per CNN2D\n",
    "        # ================================================\n",
    "        \"conv_out_channels\": {\n",
    "            \"values\": list(range(16,33,4)),  # 16,20,24,28,32\n",
    "            \"conditional\": {\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"conv_kernel_size_2d\": {\n",
    "            \"values\": conv2d_kernel_combinations,\n",
    "            \"conditional\": {\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"conv_stride_2d\": {\n",
    "            \"values\": conv2d_stride_combinations,\n",
    "            \"conditional\": {\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"pool_type\": {\n",
    "            \"values\":[\"max\",\"avg\"],\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"pool_kernel_size_2d\": {\n",
    "            \"values\": pool2d_kernel_combinations,\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"fc1_units\": {\n",
    "            \"values\": list(range(8,18,2)),  # 8,10,...,16\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \"activations\": {\n",
    "            \"values\": activation_combinations,\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "        \n",
    "        # Il dropout in CNN2D e BiLSTM\n",
    "        \"dropout\": {\n",
    "            \"values\":[0.0,0.1,0.2,0.3,0.4,0.5],\n",
    "            \"conditional\":{\"model_name\":\"CNN2D\"}\n",
    "        },\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eeaafd26-c14b-4812-be4f-e5e082acce90",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''OGNI IPER-PARAMETRO DI OGNI RETE\n",
    "\n",
    "\n",
    "ALLO STESSO LIVELLO DI PARAMETERS!\n",
    "\n",
    "\n",
    "                                                                POST 14 LUGLIO 2025\n",
    "                                                                \n",
    "                                                                \n",
    "                                                                \n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN1D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        # Ottimizzatore\n",
    "        \"lr\":            {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":         {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "        \"beta2\":         {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "        \"eps\":           {\"values\": [1e-8, 1e-7, 1e-6, 1e-5]},\n",
    "\n",
    "        # Training\n",
    "        \"n_epochs\":      {\"value\": 100},\n",
    "        \"patience\":      {\"value\": 12},\n",
    "\n",
    "        # Scelta del modello\n",
    "        \"model_name\":    {\"values\": [\"CNN2D\"]},\n",
    "\n",
    "        # Dati e regolarizzazione generale\n",
    "        \"batch_size\":    {\"values\": [16, 24, 32, 48, 52, 64, 72, 84, 96]},\n",
    "        \"standardization\":{\"values\":[True, False]},\n",
    "        \n",
    "        # --- CNN1D solo quando model_name==\"CNN2D\" ---\n",
    "        \"conv_out_channels\":{\"values\":[16,20,24,28,32]},\n",
    "\n",
    "        \"conv_k1_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k1_w\":{\"values\":[3,5,7,9]},\n",
    "        \n",
    "        \"conv_k2_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k2_w\":{\"values\":[3,5,7,9]},\n",
    "        \n",
    "        \"conv_k3_h\":{\"values\":[3,5,7,9]},\n",
    "        \"conv_k3_w\":{\"values\":[3,5,7,9]},\n",
    "\n",
    "        \"conv_s1_h\":{\"values\":[1,2]},\n",
    "        \"conv_s1_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"conv_s2_h\":{\"values\":[1,2]},\n",
    "        \"conv_s2_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"conv_s3_h\":{\"values\":[1,2]},\n",
    "        \"conv_s3_w\": {\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p1_h\":{\"values\":[1,2]},\n",
    "        \"pool_p1_w\":{\"values\":[1,2]},\n",
    "        \n",
    "        \"pool_p2_h\":{\"values\":[1,2]},\n",
    "        \"pool_p2_w\":{\"values\":[1,2]},\n",
    "    \n",
    "        \n",
    "        \"pool_p3_h\":{\"values\":[1]},\n",
    "        \"pool_p3_w\":{\"values\":[1]},\n",
    "\n",
    "        \"pool_type\":{\"values\":[\"max\",\"avg\"]},\n",
    "        \"fc1_units\":{\"values\":[8,10,12,14,16]},\n",
    "\n",
    "        \"cnn_act1\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \"cnn_act2\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \"cnn_act3\":{\"values\":[\"relu\",\"selu\",\"elu\"]},\n",
    "        \n",
    "        \n",
    "        # --- BiLSTM solo quando model_name==\"BiLSTM\" ---\n",
    "        #\"hidden_size\":{\"values\":list(range(16,34,2))},\n",
    "        #\"bidirectional\":{\"values\":[0,1]},\n",
    "\n",
    "        # --- Transformer solo quando model_name==\"Transformer\" ---\n",
    "        #\"d_model\":{\"values\":list(range(8,65,8))},\n",
    "    \n",
    "        #\"num_heads\":{\"values\":[2,4,8]}, # solo divisori di tutti i d_model\n",
    "        \n",
    "        #\"num_layers\":{\"values\":[1,2,3]},\n",
    "        #\"ff_mult\":{\"values\":[2,4]},\n",
    "        #\"transformer_activations\":{\"values\":[\"relu\",\"gelu\"]},\n",
    "\n",
    "        # comune\n",
    "        \"dropout\":{\"values\":[0.0,0.1,0.2,0.3,0.4,0.5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33735f25-b266-40bb-8882-79550e3b2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''OGNI IPER-PARAMETRO DI OGNI RETE\n",
    "\n",
    "\n",
    "ALLO STESSO LIVELLO DI PARAMETERS!\n",
    "\n",
    "\n",
    "                                                                POST 22 SETTEMBRE 2025\n",
    "                                                                \n",
    "                                                                \n",
    "                                                                \n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN1D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ok, adesso invece dovrei cercare di modificare lo \"sweep config\" associato al modello CNN2D che processerà invece i dati EEG, nella sua rappresentazione frequenza x canali che ho creato. \n",
    "In questo formulazione di dato di input, i segnali EEG hanno shape (batch, frequenze, canali). \n",
    "\n",
    "\n",
    "Lo sweep config dovrebbe avere questi valori nella sua configurazione \"generica\"/\"generale\"\n",
    "\n",
    "\n",
    "sweep_config_cnn2d = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN2D_LSTM_TF\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        #ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "    \n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "a cui, però,  vorrei che venissero aggiunti anche nello sweep config stesso i parametri architetturali specifici della rete CNN2D che ho creato quando ho generato il costruttore della rete stessa,z che sarebbe questa qui\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        #🔁 Prima:\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        #x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        #x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "        \n",
    "        #✅ Ora:\n",
    "        #Siccome i dati arrivano come (B, 45, 61) — cioè frequenze × canali, non serve permutare. Ti basta:\n",
    "        \n",
    "        # Aggiungiamo una dimensione per il canale \"immagine\"\n",
    "        x = x.unsqueeze(1)  # → (B, 1, 45, 61)\n",
    "            \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = self.act_fns[0](x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = self.act_fns[1](x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = self.act_fns[2](x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "       \n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "Vorrei quindi che nello sweep config io dia un SOLO valore ad ogni parametro architetturale (quindi CNN2D specifico) della rete stessa, in modo che in realtà nello sweep config sia 'fisso',  in modo che quando lo richiamo dovrebbe avere questi valori qui \n",
    "\n",
    "cnn = CNN2D(input_channels = 1, num_classes = num_classes,\n",
    "            conv_out_channels=16,\n",
    "            conv_k1_h=3,conv_k1_w=5,\n",
    "            conv_k2_h=3,conv_k2_w=5,\n",
    "            conv_k3_h=3,conv_k3_w=5,\n",
    "            conv_s1_h=1,conv_s1_w=2,\n",
    "            conv_s2_h=1,conv_s2_w=2,\n",
    "            conv_s3_h=1,conv_s3_w=2,\n",
    "            pool_p1_h=1,pool_p1_w=2,\n",
    "            pool_p2_h=1,pool_p2_w=2,\n",
    "            pool_p3_h=1,pool_p3_w=1,\n",
    "            pool_type='max',fc1_units=10,dropout=0.5,\n",
    "            cnn_act1='relu',cnn_act2='relu',cnn_act3='relu')\n",
    "\n",
    " quindi, se ragiono correttamente....\n",
    "\n",
    "\n",
    "il mio sweep config dovrebbe diventare così\n",
    "\n",
    "\n",
    "sweep_config_cnn2d = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN2D_LSTM_TF\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \n",
    "        \"standardization\": {\"values\": [True]}, #        ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE\n",
    "        \n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "\n",
    "a cui si dovrebbero aggiungere però appunto i valori dei parametri architetturali della CNN2D, e quindi dovrebbero essere\n",
    "\n",
    "\n",
    "        # --- CNN1D solo quando model_name==\"CNN2D\" ---\n",
    "        \"conv_out_channels\":{\"values\":[16]},\n",
    "\n",
    "        \"conv_k1_h\":{\"values\":[3]},\n",
    "        \"conv_k1_w\":{\"values\":[5]},\n",
    "        \n",
    "        \"conv_k2_h\":{\"values\":[3]},\n",
    "        \"conv_k2_w\":{\"values\":[5]},\n",
    "        \n",
    "        \"conv_k3_h\":{\"values\":[3]},\n",
    "        \"conv_k3_w\":{\"values\":[5]},\n",
    "\n",
    "        \"conv_s1_h\":{\"values\":[1]},\n",
    "        \"conv_s1_w\": {\"values\":[2]},\n",
    "        \n",
    "        \"conv_s2_h\":{\"values\":[1]},\n",
    "        \"conv_s2_w\": {\"values\":[2]},\n",
    "        \n",
    "        \"conv_s3_h\":{\"values\":[1]},\n",
    "        \"conv_s3_w\": {\"values\":[2]},\n",
    "        \n",
    "        \"pool_p1_h\":{\"values\":[1]},\n",
    "        \"pool_p1_w\":{\"values\":[2]},\n",
    "        \n",
    "        \"pool_p2_h\":{\"values\":[1]},\n",
    "        \"pool_p2_w\":{\"values\":[2]},\n",
    "    \n",
    "        \n",
    "        \"pool_p3_h\":{\"values\":[1]},\n",
    "        \"pool_p3_w\":{\"values\":[1]},\n",
    "\n",
    "        \"pool_type\":{\"values\":[\"max\",\"avg\"]},\n",
    "        \"fc1_units\":{\"values\":[10]},\n",
    "\n",
    "        \"cnn_act1\":{\"values\":[\"relu\"]},\n",
    "        \"cnn_act2\":{\"values\":[\"relu\"]},\n",
    "        \"cnn_act3\":{\"values\":[\"relu\"]},\n",
    "        \n",
    "     \n",
    "        # comune\n",
    "        \"dropout\":{\"values\":[0.5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "giusto?\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        # --- setup generale ---\n",
    "        \"model_name\":   {\"values\": [\"CNN2D\"]},\n",
    "        \"n_epochs\":     {\"value\": 100},\n",
    "        \"patience\":     {\"value\": 12},\n",
    "        \"batch_size\":   {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"value\": True},   # fisso a True\n",
    "\n",
    "        # --- ottimizzatore ---\n",
    "        \"lr\":           {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":        {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\":        {\"values\": [0.99, 0.995]},\n",
    "        \"eps\":          {\"values\": [1e-8, 1e-7]},\n",
    "\n",
    "        # --- iperparametri architettura CNN2D (fissi) ---\n",
    "        \"conv_out_channels\": {\"value\": 16},\n",
    "\n",
    "        \"conv_k1_h\": {\"value\": 3}, \"conv_k1_w\": {\"value\": 5},\n",
    "        \"conv_k2_h\": {\"value\": 3}, \"conv_k2_w\": {\"value\": 5},\n",
    "        \"conv_k3_h\": {\"value\": 3}, \"conv_k3_w\": {\"value\": 5},\n",
    "\n",
    "        \"conv_s1_h\": {\"value\": 1}, \"conv_s1_w\": {\"value\": 2},\n",
    "        \"conv_s2_h\": {\"value\": 1}, \"conv_s2_w\": {\"value\": 2},\n",
    "        \"conv_s3_h\": {\"value\": 1}, \"conv_s3_w\": {\"value\": 2},\n",
    "\n",
    "        \"pool_p1_h\": {\"value\": 1}, \"pool_p1_w\": {\"value\": 2},\n",
    "        \"pool_p2_h\": {\"value\": 1}, \"pool_p2_w\": {\"value\": 2},\n",
    "        \"pool_p3_h\": {\"value\": 1}, \"pool_p3_w\": {\"value\": 1},\n",
    "\n",
    "        \"pool_type\":  {\"values\": [\"max\", \"avg\"]},     # se vuoi fissarlo; se vuoi provarlo, usa {\"values\":[\"max\",\"avg\"]}\n",
    "        \"fc1_units\":  {\"value\": 12},\n",
    "        \"cnn_act1\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act2\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act3\":   {\"value\": \"relu\"},\n",
    "        \"dropout\":    {\"value\": 0.5}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8a3ab-e031-4562-b955-c99e62bd9219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4928ac9-2de5-4754-a013-4e550b0c6efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(best_models)\n",
    "#print(sweep_ids_for_models)\n",
    "#print(sweep_ids)\n",
    "#print(data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba962ef-7845-427d-b251-46d95b7f8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebe06b-f9f9-47b1-8258-bfa3e04d62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce515965-d757-407b-a7de-86b1064de804",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pprint\n",
    "pprint.pprint(sweep_config)**NOTA BENE**\n",
    "\n",
    "Come output, io otterrò **quando crei gli sweeps** una cosa come questa, ad esempio:\n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw\n",
    "        Create sweep with ID: 3b6o28jt\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/3b6o28jt\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - BiLSTM: n° sweep 3b6o28jt\n",
    "        Create sweep with ID: q6yp4fas\n",
    "\n",
    "        .....\n",
    "\n",
    "Vedendole bene, per **ogni condizione sperimentale (3)**, **per ogni dato EEG (3)** e **per ogni provenienza del dato EEG (4)**, \n",
    "Io **DOVREI OTTENERE** in totale = **3x3x4 = 36 sweeps** per **OGNI CONDIZIONE SPERIMENTALE**\n",
    "\n",
    "\n",
    "Per **ognuna di queste sweeps**, io se ho capito bene creerò **15 esperimenti** (le mie runs), che corrispondo alle **diverse configurazioni di iper-parametri testati per lo stesso specifico sweep**!\n",
    "\n",
    "(ad esempio, solo questo \n",
    "\n",
    "<br> \n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw)\n",
    "\n",
    "Dove, le diverse configurazioni, son determinate randomicamente a partire dai valori dentro la variabile \"**sweep_config**\"  che è questa \n",
    "\n",
    "\n",
    "    #Creo la configurazione dello sweep e la eseguo\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "            \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "            \"n_epochs\": {\"value\": 100},\n",
    "            \"patience\": {\"value\": 10},\n",
    "            \"model_name\":{\"values\": ['CNN1D', 'BiLSTM', 'Transformer']},\n",
    "            \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "            \"standardization\":{\"values\": [True, False]},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d83e547-fb9a-4202-b949-4f9172d0bc82",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Popolamento di sweep_ids e lancio degli agenti:\n",
    "\n",
    "Obiettivo: \n",
    "\n",
    "Per ogni combinazione (condition, data_type, category_subject, model_name), \n",
    "Se la lista è vuota, crei uno sweep usando wandb.sweep(sweep_config, project=condition) e lo inserisci nella lista. \n",
    "In seguito, iteri su quella lista (che ora contiene IL TUO SPECIFICO sweep_id) e lanci wandb.agent() per eseguire il training.\n",
    "\n",
    "\n",
    "\n",
    "Nota importante:\n",
    "L'ID restituito da wandb.sweep() è una STRINGA UNIVOCA generata automaticamente da WandB.\n",
    "Non puoi assegnargli direttamente una stringa personalizzata, ma puoi comunque usarlo per mappare nel tuo dizionario la combinazione di fattori! \n",
    "\n",
    "In questo ciclo, il fatto che la lista parta vuota è normale: il codice la popola se necessario e poi lancia l'agente per ogni sweep_id presente.\n",
    "\n",
    "\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "INOLTRE, BISOGNA CONTROLLARE CHE SI STIA ITERANDO CORRETTAMENTE SOLO SULLA COMBINAZIONE CORRENTE DI \n",
    "\n",
    "                CONDITION, DATA_TYPE, CATEGORY_SUBJECT E MODEL_NAME\n",
    "                \n",
    "QUESTO PERCHÉ SE UN CICLO SI RIPETE PER UNA CONDIZIONE IN PIÙ UNA COMBINAZIONE, POTREBBE GENERARE PIÙ  SWEEP IDS DI QUELLI CHE TI ASPETTI!\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "SOLUZIONE:\n",
    "\n",
    "Un buon approccio per evitare la creazione ripetuta di Sweep ID \n",
    "per la stessa combinazione di condition, data_type, category_subject e model_name \n",
    "è quello di utilizzare un SET per tenere traccia delle combinazioni già processate.\n",
    "Se una combinazione è già presente nel set, non dovresti creare un nuovo Sweep ID, ma semplicemente saltare quella parte del codice\n",
    "\n",
    "\n",
    "Inoltre, ho avuto una idea ad un certo punto! \n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "\n",
    "Quando creo ogni sweep singolarmente, si genera una stringa univoca di quello sweep, che si riferisce ad un dataset che è il prodotto di diversi fattori:\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Di conseguenza, iterando su ogni sweep_ids (che ho fatto in modo avesse la STESSA struttura dei miei dati già splittati i.e, data_dict_preprocessed\n",
    "io posso, \n",
    "\n",
    "1) da un lato eseguire la creazione della stringa univoca associata a quello sweep,\n",
    "2) crearmi una 'combination_key', che sarebbe l'insieme delle stringhe che descrivono quel dataset specifico di data_dict_preprocessed\n",
    "\n",
    "che sarà costituito da\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Poiché quindi so già la corrispondenza tra ogni Sweep ID e la sua combinazione di fattori (condition, data_type, category_subject), \n",
    "posso creare un MAPPING, che associ, ad certo Sweep ID e la stringa che descrive i suoi fattori associati!\n",
    "\n",
    "\n",
    "In questo modo, forse, si riesce a risolvere il PROBLEMA 2 NELLA CELLA DI CREAZIONE DELLA FUNZIONE DI TRAINING (VEDI SOTTO!)\n",
    "\n",
    "'''\n",
    "\n",
    "#Inizializza un set per tenere traccia delle combinazioni già elaborate\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms_channels_freqs\")\n",
    "\n",
    "                    '''QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                     CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA '''\n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf914a1-5837-4e20-ab5e-63592a72c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ATTENZIONE: A DIFFERENZA DI PRIMA, DOVE GLI SWEEPS ERANO CREATI SOLO PER OGNI CONDIZIONE SPERIMENTALE,\n",
    "ADESSO INVECE VENGONO CREATI PER OGNI COMBINAZIONI DI FATTORI, CHE INCLUDONO:\n",
    "\n",
    "1) DATI DI COPPIE DI CONDIZIONI SPERIMENTALI\n",
    "2) PROVEVIENZA DEI DATI (IN QUESTO SPETTOGRAMMI TIME-FREQUENCY\n",
    "3) PROVENIENZA DEI DATI STESSI (FAMILIAR VS UNFAMILIAR; THERAPIST VS PATIENT)\n",
    "\n",
    "'''\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    \n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_{data_type}_channels_freqs_{category_subject}\")\n",
    "\n",
    "                    '''QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                     CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA '''\n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dedd5c-2e37-488c-9d1d-508630f3af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola e stampa il numero totale di combinazioni uniche (e quindi di sweep creati)\n",
    "\n",
    "total_sweeps = len(created_combinations)\n",
    "total_runs = total_sweeps * 200\n",
    "\n",
    "print(f\"Numero totale di sweep creati: {total_sweeps}\")\n",
    "print(f\"Numero totale di runs da eseguire: {total_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302f178-23d2-4cb6-8939-d109256ab86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''ESEGUI QUI QUESTA CELLA PER VEDERE COME SI STRUTTURA SWEEP_IDS'''\n",
    "\n",
    "#sweep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564c459-b776-4f51-b374-b233be704a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_ids.keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8522f5-2684-46d1-a95b-2834bd31b010",
   "metadata": {},
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "\n",
    "I **numeri degli sweeps** tornano e son corretti! \n",
    "Tuttavia, avendo solo preparato l'inizializzazione degli sweeps dentro 'sweep_ids', \n",
    "Sul sito di weight and biases, io vedo le tre condizioni sperimentali, create ciascuna come un progetto separato, che è corretto, ma ancora le runs di ciascuna le vedo a 0\n",
    "\n",
    "Deduco che questo comportamento, dovrebbe esser normale, dato che ancora non ho avviato l'agente appunto wandb.agent(), con cui gli fornisco lo sweep_id generato adesso in questo loop precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765dc61-9630-427e-9f8d-2c0c89972ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(sweep_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e5387-8c6d-4d12-b98c-8f066b6ae4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf418940-756a-4e20-8d24-2a70e9a17f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b35e47-6438-44af-9d2f-b2046a5a58c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235a1d7-5ad9-494a-b3bf-c79264385dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VERSIONE DEL 6 MARZO (RISOLUZIONE DEFINITIVA) OLD VERSION**\n",
    "\n",
    "##### **Training Function Edits - EEG Spectrograms - Electrodes x Frequencies BOTH HYPER-PARAMS & MODEL PARAMAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0990b08-ae99-42a4-a435-f828c14567cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Allora adesso, credo ci sia una delle parti più complesse, ossia: \n",
    "\n",
    "aggiungere una stringa formata da 'v_' e 'un valore numerico progressivo' al nome del file, quindi a 'best_model_name' quindi in riferimento a questa parte del codice qui...\n",
    "\n",
    "\n",
    "    # Salva un dizionario contenente sia i pesi che le configurazioni\n",
    "        torch.save({\n",
    "            \"state_dict\": best_model.state_dict(),\n",
    "            \"config\": training_config,\n",
    "            \"model_config\": model_config\n",
    "        }, model_file)\n",
    "\n",
    "in base alla configurazione dei parametri della architettura.. l'idea è questa:\n",
    "\n",
    "è possibile che venga trovato lo stesso modello (ossia gli stessi valori di model_config ossia dei PARAMETRI del modello), per la stessa combinazioni di fattori che costituiscono in dat (LEGGI SOTTO BENE PER CAPIRE QUESTO PASSAGGIO) MA con diverse configurazioni di IPER-PARAMETRI.... giusto?\n",
    "\n",
    "quindi, potrei salvarmi i modelli, in base alla configurazione dei loro PARAMETRI... e tenere traccia della loro combinazione di valori dentro ad un set, che contiene la combinazione dei valori associati ai parametri di UN CERTO MODELLO...\n",
    "\n",
    "dovendo provare a scegliere quale tra I DUE (immaginiamo che lo sweep crea lo STESSO MODELLO più volte) lui dovrà SALVARE (o sovrascrivere, quindi, secondo la logica che ho scritto) quello che, tra i due, abbia ad esempio ottenuto una migliore VALIDATION ACCURACY...\n",
    "\n",
    "ora abbiamo detto che  \n",
    "\n",
    "\n",
    "1) best_model_name dovrà avere alla fine un 'v_' e 'un valore numerico progressivo' al nome del file che indentifica quale tipologia di modello CNN2D è stato configurato...\n",
    "\n",
    "quindi sarà tipo alla fine \"v_1\"\n",
    "\n",
    "a questo punto si dovrebbe, credo, creare una ulteriore funzione a parte che \n",
    "\n",
    "1) accetta come argomento \n",
    "\n",
    "A) il 'model_config' corrente (quindi quello che è associato alla creazione di 'best_model_name' con l'aggiunta del \"v_1\" FINALE, giusto?) e \n",
    "B) la stringa appunto del nome del modello ( che sarà diventata appunto \"best_model_name\" con il suffisso \"v_1\")\n",
    "\n",
    "A questo punto, questa funzione potrebbe creare un set, che tiene traccia appunto di questa configurazione creata, appena creata per il modello CNN2D, ma che tiene conto e che si riferisce allo specifico modello di una certa combinazione di fattori che costituiscono i DATI! \n",
    "\n",
    "perché RICORDATI CHE \n",
    "\n",
    "best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "!\n",
    "\n",
    "quindi, l'idea è che\n",
    " \n",
    "2) se non l'aveva mai trovata quella combinazioni di PARAMETRI per il modello CNN2D, per quella combinazioni di dati, si salva la stringa del nome del modello dentro al set (con anche il suffisso) e la configurazione di parametri!\n",
    "\n",
    "3) SUCCESSIVAMENTE, mettiamo siamo nel caso in cui siamo nell'ipotesi di PRIMA, ossia che \n",
    "\n",
    "è possibile che venga trovato lo stesso modello (ossia gli stessi valori di model_config ossia dei PARAMETRI del modello), di una certa combinazione di fattori che costituiscono i DATI, MA con diverse configurazioni di IPER-PARAMETRI.... giusto?\n",
    " \n",
    "ALLORA, se capita questa cosa, lui dovrebbe fare il confronto tra \n",
    "\n",
    "1) la prima instanza del modello CNN2D (dentro al set) con la stessa configurazione e QUELLA DEL MODELLO CORRENTE, che ha la stessa configurazione dei PARAMETRI DEL MODELLO.... MA magari ha una diversa configurazione di IPER-PARAMETRI\n",
    "\n",
    "2) si dovrebbe confrontare appunto QUALE validation accuracy sia la migliore tra i DUE MODELLI e SE LA VALIDATION ACCURACY DEL MODELLO CORRENTE, che ha la stessa configurazione dei PARAMETRI DEL MODELLO.... MA magari ha una diversa configurazione di IPER-PARAMETRI è MIGLIORE di quella DEL MODELLO confrontato dentro al SET.. ALLORA SI ANDRA' A SOVRASCRIVERE IL FILE CON \n",
    "\n",
    "LO STESSO MODELLO (STESSA CONFIGURAZIONE DI PARAMETRI) MA ORA AVRA' UNA NUOVA (MIGLIORE) CONFIGURAZIONE DI IPER-PARAMETRI....\n",
    "\n",
    "l'idea è replicare la stessa logica che si usava prima, ma in questo caso è più complesso perché la verifica non è più solo SUGLI IPER-PARAMETRI ma anche SUI PARAMETRI STESSI DEL MODELLO, che sì possono cambiare, MA POSSONO ANCHE RIPETERSI MAGARI NEI VARI SWEEPS ...\n",
    "\n",
    "SPERO DI ESSER STATO CHIARO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Aspetta ti do il codice che ho ad ora:\n",
    "\n",
    "    \"import re\n",
    "\n",
    "    def parse_combination_key(combination_key):\n",
    "        \"\"\"\n",
    "        Estrae condition_experiment e subject_key da combination_key\n",
    "        dove il data_type è fisso a \"spectrograms\".\n",
    "\n",
    "        Esempio di chiave: \n",
    "        \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "        \"\"\"\n",
    "        match = re.match(\n",
    "            r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "            combination_key\n",
    "        )\n",
    "        if match:\n",
    "            condition_experiment = match.group(1)\n",
    "            subject_key = match.group(2)\n",
    "            return condition_experiment, subject_key\n",
    "        else:\n",
    "            raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "\n",
    "    def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "        sweep_id, combination_key = sweep_tuple\n",
    "\n",
    "        exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "        data_type = \"spectrograms\"\n",
    "\n",
    "        if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "            raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "        run_name = f\"{exp_cond}_{data_type}_{category_subject}_channels_freqs\"\n",
    "        tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "        wandb.init(project=f\"{exp_cond}_spectrograms_channels_freqs\", name=run_name, tags=tags)\n",
    "\n",
    "        print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "        print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "        print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        try:\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "            print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "            print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "            print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "            print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "        if config.standardization:\n",
    "            # Standardizzazione\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "        else:\n",
    "            print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "        # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "        )\n",
    "\n",
    "        '''ULTIMA VERSIONE: QUI CARICO LA FUNZIONE PER CREARE\n",
    "         LA CONFIGURAZIONE DI PARAMETRI (RANDOMICA) DEL MODELLO\n",
    "         A PARTIRE DA SWEEP CONFIG\n",
    "        '''\n",
    "\n",
    "        #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "\n",
    "        if config.model_name == \"CNN2D\":\n",
    "            model = build_cnn2d(config)\n",
    "            print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "            print(f\"\\Configurazione Modello CNN2D: \\n{dict(config)}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # Parametri di training\n",
    "        n_epochs = config.n_epochs\n",
    "        patience = config.patience\n",
    "        early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "\n",
    "        best_model = None\n",
    "        max_val_acc = 0\n",
    "        best_epoch = 0\n",
    "\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "\n",
    "        for epoch in pbar:\n",
    "            train_loss_tmp = []\n",
    "            correct_train = 0\n",
    "            y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss_tmp.append(loss.item())\n",
    "                _, predicted_train = torch.max(y_pred, 1)\n",
    "                correct_train += (predicted_train == y).sum().item()\n",
    "                y_true_train_list.extend(y.cpu().numpy())\n",
    "                y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "            accuracy_train = correct_train / len(train_loader.dataset)\n",
    "            loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "            precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "            recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "            f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "            auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "            loss_val_tmp = []\n",
    "            correct_val = 0\n",
    "            y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, y in val_loader:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    y_pred = model(x)\n",
    "\n",
    "                    loss = criterion(y_pred, y.view(-1))\n",
    "                    loss_val_tmp.append(loss.item())\n",
    "                    _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                    correct_val += (predicted_val == y).sum().item()\n",
    "                    y_true_val_list.extend(y.cpu().numpy())\n",
    "                    y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "            accuracy_val = correct_val / len(val_loader.dataset)\n",
    "            loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": loss_train,\n",
    "                \"train_accuracy\": accuracy_train,\n",
    "                \"train_precision\": precision_train,\n",
    "                \"train_recall\": recall_train,\n",
    "                \"train_f1\": f1_train,\n",
    "                \"train_auc\": auc_train,\n",
    "                \"val_loss\": loss_val,\n",
    "                \"val_accuracy\": accuracy_val\n",
    "            })\n",
    "\n",
    "            if accuracy_val > max_val_acc:\n",
    "                max_val_acc = accuracy_val\n",
    "                best_epoch = epoch\n",
    "                best_model = cp.deepcopy(model)\n",
    "\n",
    "            early_stopping(accuracy_val)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"🛑 Early stopping attivato!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            # Crea un dizionario separato per prelevarsi i correnti valori della configurazione interna della rete\n",
    "            model_config = {\n",
    "                \"conv_channels\": config.conv_channels,\n",
    "                \"kernel_sizes\": config.kernel_sizes,\n",
    "                \"strides\": config.strides,\n",
    "                \"paddings\": config.paddings,\n",
    "                \"pooling_type\": config.pooling_type\n",
    "                \"dropout_rate\": config.dropout_rate,\n",
    "                \"activations\": config.activations,\n",
    "\n",
    "            }\n",
    "\n",
    "\n",
    "            training_config = {key: config[key] for key in config if key not in model_config}\n",
    "\n",
    "            if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "                # Salvo il primo best_model per quella combinazione\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": cp.deepcopy(model),\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": epoch,\n",
    "                    \"config\": training_config,        # Iperparametri di training\n",
    "                    \"model_config\": model_config      # Parametri del modello\n",
    "\n",
    "                }\n",
    "\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                torch.save({\n",
    "                    \"state_dict\": best_model.state_dict(),\n",
    "                    \"config\": training_config,\n",
    "                    \"model_config\": model_config\n",
    "                }, model_file)\n",
    "\n",
    "                print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "\n",
    "            elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                    best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                        \"model\": best_model,\n",
    "                        \"max_val_acc\": accuracy_val,\n",
    "                        \"best_epoch\": best_epoch,\n",
    "                        \"config\": training_config,        # Iperparametri di training\n",
    "                        \"model_config\": model_config      # Parametri del modello\n",
    "                    }\n",
    "\n",
    "                    best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                    model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                    print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                    print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                    model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                    if os.path.exists(model_file):\n",
    "\n",
    "                        print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                        # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                        torch.save({\n",
    "                            \"state_dict\": best_model.state_dict(),\n",
    "                            #\"config\": dict(config)\n",
    "                            \"config\": training_config,\n",
    "                            \"model_config\": model_config\n",
    "                        }, model_file)\n",
    "\n",
    "                        print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            else:\n",
    "                ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "                print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "        return best_models\n",
    "\n",
    "    \" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ORA, voglio capire come queste funzioni che mi hai proposto, si vadano ad integrare nel codice che già ho\n",
    "\n",
    "\n",
    "    \"import json\n",
    "\n",
    "    # Funzione per ottenere una stringa chiave univoca dalla configurazione del modello.\n",
    "    def get_model_config_key(model_config):\n",
    "        # Convertiamo il dizionario in una stringa in modo canonico\n",
    "        return json.dumps(model_config, sort_keys=True)\n",
    "\n",
    "    # Funzione per aggiornare (o assegnare) la versione del modello\n",
    "    def update_model_version(model_config, base_model_name, current_val_acc, data_key, model_versions):\n",
    "        \"\"\"\n",
    "        - model_config: dizionario con i parametri interni del modello\n",
    "        - base_model_name: stringa base (es. \"CNN2D_pt_resp_vs_shared_resp_spectrograms_familiar_th\")\n",
    "        - current_val_acc: validation accuracy della run corrente\n",
    "        - data_key: chiave identificativa dei dati, ad esempio una stringa composta da exp_cond, data_type, category_subject\n",
    "        - model_versions: dizionario che tiene traccia delle versioni per ciascuna configurazione\n",
    "        \"\"\"\n",
    "        # Creiamo una chiave unica per il model_config\n",
    "        model_config_key = get_model_config_key(model_config)\n",
    "        # La chiave completa include anche la combinazione di fattori dati e il nome base del modello.\n",
    "        full_key = (data_key, model_config_key)\n",
    "\n",
    "        if full_key not in model_versions:\n",
    "            # Nuova configurazione per questi dati: assegnamo la versione 1\n",
    "            version = 1\n",
    "            new_model_name = f\"{base_model_name}_v_{version}\"\n",
    "            model_versions[full_key] = {\n",
    "                \"version\": version,\n",
    "                \"best_val_acc\": current_val_acc,\n",
    "                \"model_name\": new_model_name\n",
    "            }\n",
    "            return new_model_name\n",
    "        else:\n",
    "            # Configurazione già esistente: confrontiamo la validation accuracy\n",
    "            record = model_versions[full_key]\n",
    "            if current_val_acc > record[\"best_val_acc\"]:\n",
    "                # Aggiorniamo la best accuracy, manteniamo la stessa versione (lo stesso suffisso)\n",
    "                record[\"best_val_acc\"] = current_val_acc\n",
    "                return record[\"model_name\"]\n",
    "            else:\n",
    "                # Il modello corrente è peggiore, ritorna il nome già salvato\n",
    "                return record[\"model_name\"]\n",
    "\n",
    "    # Esempio di uso nel training loop:\n",
    "    # Supponiamo di avere:\n",
    "    # - config: il dizionario di configurazione completo (contenente sia iperparametri che parametri del modello)\n",
    "    # - exp_cond, data_type, category_subject: identificatori dei dati\n",
    "    # - best_models: il dizionario in cui salvi i modelli migliori\n",
    "    # - model_versions: un dizionario globale (o locale) che tiene traccia delle versioni per ciascuna configurazione\n",
    "    # - accuracy_val: validation accuracy della run corrente\n",
    "    # - model: il modello corrente, best_model: il modello best salvato\n",
    "\n",
    "    # Preleviamo la configurazione del modello\n",
    "    model_config = {\n",
    "        \"conv_channels\": config.conv_channels,\n",
    "        \"kernel_sizes\": config.kernel_sizes,\n",
    "        \"strides\": config.strides,\n",
    "        \"paddings\": config.paddings,\n",
    "        \"dropout_rate\": config.dropout_rate,\n",
    "        \"activations\": config.activations,\n",
    "        \"pooling_type\": config.pooling_type  # se lo hai aggiunto\n",
    "    }\n",
    "\n",
    "    # Filtriamo gli iperparametri di training\n",
    "    training_config = {key: config[key] for key in config if key not in model_config}\n",
    "\n",
    "    # Costruiamo la chiave dei dati, per esempio:\n",
    "    data_key = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "    # Ora usiamo la funzione di versioning per ottenere il nome del modello (con suffisso v_X)\n",
    "    # model_versions deve essere un dizionario definito prima, per esempio all'esterno del loop.\n",
    "    # Esempio: model_versions = {} (all'inizio dell'esperimento)\n",
    "    base_model_name = data_key\n",
    "    new_model_name = update_model_version(model_config, base_model_name, accuracy_val, data_key, model_versions)\n",
    "\n",
    "    # Quindi, il nome finale del file sarà new_model_name:\n",
    "    model_file = os.path.join(base_dir, exp_cond, data_type, category_subject, f\"{new_model_name}.pkl\")\n",
    "\n",
    "    # Ora, applica la logica per il salvataggio, ad esempio:\n",
    "    if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": epoch,\n",
    "            \"config\": training_config,        # Iperparametri di training\n",
    "            \"model_config\": model_config      # Parametri del modello\n",
    "        }\n",
    "\n",
    "        os.makedirs(os.path.join(base_dir, exp_cond, data_type, category_subject), exist_ok=True)\n",
    "        torch.save({\n",
    "            \"state_dict\": best_model.state_dict(),\n",
    "            \"config\": training_config,\n",
    "            \"model_config\": model_config\n",
    "        }, model_file)\n",
    "        print(f\"Il modello \\033[1m{new_model_name}\\033[0m verrà salvato in \\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": best_model,\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": training_config,\n",
    "            \"model_config\": model_config\n",
    "        }\n",
    "\n",
    "        os.makedirs(os.path.join(base_dir, exp_cond, data_type, category_subject), exist_ok=True)\n",
    "        print(f\"Il modello in {os.path.join(base_dir, exp_cond, data_type, category_subject)} ha un MIGLIORAMENTO!\")\n",
    "\n",
    "        if os.path.exists(model_file):\n",
    "            print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{new_model_name}\\033[0m verrà AGGIORNATO in \\033[1m{model_file}\\033[0m\")\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": training_config,\n",
    "                \"model_config\": model_config\n",
    "            }, model_file)\n",
    "            print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{new_model_name}\\033[0m\")\n",
    "        else:\n",
    "            continue\n",
    "    \"\n",
    "\n",
    "Nello specifico:\n",
    "\n",
    "1) get_model_config_key cosa fa esattamente e perché c'è 'sort_keys=True'?\n",
    "\n",
    "2) se capisco bene 'update_model_version' si crea la chiave unica full_key basandosi su \n",
    "\n",
    "a) model_config_key che però non capisco che cosa sarebbe alla fine di base...?\n",
    "b) la combinazione di stringhe che si riferiscono ai fattori da cui provengono i dati ...? \n",
    "\n",
    "cioè, se capisco, tu semplifichi la creazione delle \"versioni\" date dalla combinazione di parametri del modello e set di iper-parametri, perché sai che qui tanto il modello sarà sempre e solo uno, ossia CNN2D?\n",
    "\n",
    "dopodiché, vedo che metti quella versione del modello dentro 'model_versions' se non è mai esistita.. e questo è ok, ma \n",
    "\n",
    "1)  il valore di 'version' non deve essere sempre necessariamente 1, ma sarebbe un numero progressivo.. nel senso che, la versione del modello è dipesa principalmente, nella mia idea originale, dal set di valori che compongo i parametri del modello che viene creato...\n",
    "\n",
    "(NOTA BENE: \n",
    "\n",
    "il suffisso \"v_\" e \"digit\" progressivo è solo per tenermi diciamo traccia di quante combinazioni di modelli (ognuna con la sua specific combinazioni di parametri) viene 'ri-creata' magari più volte all'interno degli sweep...\n",
    "\n",
    "è un modo diciamo per sapere alla fine di quanti modelli CNN2D siano stati effettivamente creati.. )\n",
    "\n",
    "Quindi, mettiamo caso che la versione 1 è stata creata ed ok.. ora nello sweep magari, ne viene creata un'altra nuova... questa versione sarà quella con valore \"2\", (i.e., v_2),  in modo da dare un identificativo univoco a questo NUOVO modello, che avrà una sua NUOVA specifica combinazione di parametri (rispetto a quella 'v_1')....\n",
    "\n",
    "Ora, immagina che, magari, negli sweep lo stesso modello (supponiamo che sia la versione 2 ossia \"v_2\" ) viene re-instanziata, più volte.... mettiamo il caso che capiti che venga re-instanziata una seconda volta..\n",
    "\n",
    "\n",
    "ma, in questa seconda volta, magari questa versione \"v_2\" avrà una NUOVA combinazione di iper-parametri rispetto alla prima volta in cui era stata istanziata (sempre la \"v_2\" ma il cui file già esiste, perché è stata già incontrata almeno una volta...) \n",
    "\n",
    "ora, mettiamo il caso che, questa \"v_2\", con questa NUOVA combinazione di iper-parametri \n",
    "\n",
    "ha una accuratezza MIGLIORE \n",
    "\n",
    "di quella stessa versione, ma rispetto alla prima volta in cui era stata istanziata ... e che, come ti dicevo è già stata salvata dentro 'model_versions' (che sarebbe in questo esempio sempre la versione 2) , \n",
    "\n",
    "allora.. vorrei che, quello che il codice facesse è che, per questa versione del modello (v2) verrà salvata \n",
    "\n",
    "a) la NUOVA configurazione di iper-parametri\n",
    "b) i pesi e bias che hanno contribuito al MIGLIORAMENTO della validation accuracy... \n",
    "c) vedi se necessita di essere ANCHE ri-salvata nuovamente anche la stessa configurazioni di parametri del modello (che sarebbe in questo esempio sempre la versione 2, e per cui è già in memoria nel file che era già stato salvato precedentemente..) \n",
    "\n",
    "l'importante è che, \n",
    "\n",
    "b) il salvataggio dei pesi e bias e\n",
    "a) della NUOVA configurazione di iper-parametri (che ha portato al miglioramenteo della validation accuracy per lo stesso modello \"v_2\")  avvenga rispetto a quello stesso associato alla modello della stessa versione \n",
    "\n",
    "ora, ti chiedo appunto.. come potrebbe essere integrata questa cosa, nel codice che già ho ?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**CHATGPT**\n",
    "\n",
    "\n",
    "Ti spiego nel dettaglio cosa fanno le due funzioni e come integrarle nel tuo codice.\n",
    "\n",
    "1. Funzione get_model_config_key\n",
    "\n",
    "        def get_model_config_key(model_config):\n",
    "            # Convertiamo il dizionario in una stringa in modo canonico,\n",
    "            # usando sort_keys=True per garantire che l’ordinamento delle chiavi\n",
    "            # sia sempre lo stesso indipendentemente dall’ordine in cui sono state definite.\n",
    "            return json.dumps(model_config, sort_keys=True)\n",
    "        \n",
    "Cosa fa e perché sort_keys=True\n",
    "Obiettivo: Convertire il dizionario model_config in una stringa che rappresenti in modo univoco la configurazione interna del modello.\n",
    "sort_keys=True:\n",
    "Questo parametro assicura che le chiavi siano ordinate alfabeticamente. In questo modo, due dizionari che contengono gli stessi elementi (anche se inseriti in ordine diverso) daranno sempre la stessa stringa. Questo è fondamentale per creare una chiave univoca e consistente.\n",
    "\n",
    "2. Funzione update_model_version\n",
    "\n",
    "        def update_model_version(model_config, base_model_name, current_val_acc, data_key, model_versions):\n",
    "            \"\"\"\n",
    "            - model_config: dizionario con i parametri interni del modello.\n",
    "            - base_model_name: stringa base, ad esempio \"CNN2D_pt_resp_vs_shared_resp_spectrograms_familiar_th\".\n",
    "            - current_val_acc: validation accuracy della run corrente.\n",
    "            - data_key: stringa identificativa dei dati (ad es. combinazione di exp_cond, data_type, category_subject e model_name).\n",
    "            - model_versions: dizionario che tiene traccia delle versioni per ciascuna configurazione.\n",
    "\n",
    "            Ritorna il nome del modello con suffisso versionale (es. \"_v_1\", \"_v_2\", ecc.).\n",
    "            \"\"\"\n",
    "            # Creiamo una chiave univoca per la configurazione del modello\n",
    "            model_config_key = get_model_config_key(model_config)\n",
    "            # La chiave completa include anche la combinazione dei fattori dati (data_key)\n",
    "            full_key = (data_key, model_config_key)\n",
    "\n",
    "            if full_key not in model_versions:\n",
    "                # Se non abbiamo mai visto questa combinazione di dati e configurazione del modello,\n",
    "                # la versione inizia a 1.\n",
    "                version = 1\n",
    "                new_model_name = f\"{base_model_name}_v_{version}\"\n",
    "                model_versions[full_key] = {\n",
    "                    \"version\": version,\n",
    "                    \"best_val_acc\": current_val_acc,\n",
    "                    \"model_name\": new_model_name\n",
    "                }\n",
    "                return new_model_name\n",
    "            else:\n",
    "                # Se esiste già questa configurazione\n",
    "                record = model_versions[full_key]\n",
    "                if current_val_acc > record[\"best_val_acc\"]:\n",
    "                    # Se la nuova validation accuracy è migliore, aggiorniamo il record.\n",
    "                    record[\"best_val_acc\"] = current_val_acc\n",
    "                    # Manteniamo lo stesso suffisso (la stessa versione) e ritorniamo il nome.\n",
    "                    return record[\"model_name\"]\n",
    "                else:\n",
    "                    # Altrimenti, non aggiorniamo e ritorniamo il nome già salvato.\n",
    "                    return record[\"model_name\"]\n",
    "                    \n",
    "Cosa fa update_model_version\n",
    "model_config_key: È la stringa canonica (ottenuta con get_model_config_key) che rappresenta i parametri interni del modello.\n",
    "full_key: È una tupla composta da data_key (che identifica la combinazione dei dati, ad esempio \"CNN2D_pt_resp_vs_shared_resp_spectrograms_familiar_th\") e dalla stringa della configurazione del modello.\n",
    "\n",
    "In questo modo, la chiave è univoca per:\n",
    "La combinazione dei dati (ad esempio: exp_cond, data_type, category_subject, model_name)\n",
    "La configurazione interna del modello (i parametri architetturali)\n",
    "\n",
    "Versioning:\n",
    "Se questa chiave non è presente in model_versions, viene assegnata la versione 1 (ossia suffisso \"_v_1\"). Se già esiste, si confronta la validation accuracy:\n",
    "Se la nuova accuracy è migliore, si aggiorna il record (ma si mantiene lo stesso suffisso).\n",
    "Altrimenti, si ritorna il nome già salvato.\n",
    "\n",
    "\n",
    "3. Integrazione nel Training Loop\n",
    "All'interno del tuo loop di training, subito prima del salvataggio, puoi fare quanto segue:\n",
    "\n",
    "Creare model_config (che contiene i parametri del modello)\n",
    "Filtrare training_config (gli iperparametri di training)\n",
    "Costruire la chiave data_key (ad esempio usando:\n",
    "\n",
    "    data_key = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "\n",
    "Utilizzare update_model_version per ottenere il nome del modello con suffisso versionale.\n",
    "Ecco come integrare queste parti nel tuo codice:\n",
    "\n",
    "\n",
    "        import json  # Assicurati di importare json all'inizio del file\n",
    "\n",
    "        # Supponiamo che model_versions sia definito all'esterno del loop (o all'inizio dell'esperimento)\n",
    "        # Ad esempio, all'inizio del file:\n",
    "        model_versions = {}\n",
    "\n",
    "        # ... all'interno del loop (dopo early stopping, ad esempio) ...\n",
    "\n",
    "        # Crea il dizionario dei parametri del modello:\n",
    "        model_config = {\n",
    "            \"conv_channels\": config.conv_channels,\n",
    "            \"kernel_sizes\": config.kernel_sizes,\n",
    "            \"strides\": config.strides,\n",
    "            \"paddings\": config.paddings,\n",
    "            \"pooling_type\": config.pooling_type,\n",
    "            \"dropout_rate\": config.dropout_rate,\n",
    "            \"activations\": config.activations\n",
    "        }\n",
    "\n",
    "        # Filtra gli iperparametri di training:\n",
    "        training_config = {key: config[key] for key in config if key not in model_config}\n",
    "\n",
    "        # Costruisci la chiave dei dati:\n",
    "        data_key = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "        base_model_name = data_key\n",
    "\n",
    "        # Ottieni il nome del modello con suffisso versione (es. v_1, v_2, etc.)\n",
    "        new_model_name = update_model_version(model_config, base_model_name, accuracy_val, data_key, model_versions)\n",
    "\n",
    "        # Costruisci il percorso del file:\n",
    "        model_file = os.path.join(base_dir, exp_cond, data_type, category_subject, f\"{new_model_name}.pkl\")\n",
    "\n",
    "        # Ora, integra la logica di salvataggio:\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \"config\": training_config,        # Iperparametri di training\n",
    "                \"model_config\": model_config      # Parametri del modello\n",
    "            }\n",
    "\n",
    "            os.makedirs(os.path.join(base_dir, exp_cond, data_type, category_subject), exist_ok=True)\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": training_config,\n",
    "                \"model_config\": model_config\n",
    "            }, model_file)\n",
    "            print(f\"Il modello \\033[1m{new_model_name}\\033[0m verrà salvato in \\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": best_model,\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"config\": training_config,\n",
    "                \"model_config\": model_config\n",
    "            }\n",
    "\n",
    "            os.makedirs(os.path.join(base_dir, exp_cond, data_type, category_subject), exist_ok=True)\n",
    "            print(f\"Il modello in {os.path.join(base_dir, exp_cond, data_type, category_subject)} ha un MIGLIORAMENTO!\")\n",
    "\n",
    "            if os.path.exists(model_file):\n",
    "                print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{new_model_name}\\033[0m verrà AGGIORNATO in \\033[1m{model_file}\\033[0m\")\n",
    "                torch.save({\n",
    "                    \"state_dict\": best_model.state_dict(),\n",
    "                    \"config\": training_config,\n",
    "                    \"model_config\": model_config\n",
    "                }, model_file)\n",
    "                print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{new_model_name}\\033[0m\")\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            # Se non c'è miglioramento, stampi un messaggio.\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\033[1m{model_path}\\033[0m, ossia \\033[1m{model_file}\\033[0m\")\n",
    "            \n",
    "            \n",
    "Riepilogo delle risposte alle tue domande specifiche:\n",
    "get_model_config_key\n",
    "\n",
    "Converte il dizionario model_config in una stringa con le chiavi ordinate (grazie a sort_keys=True).\n",
    "Questo assicura che la stessa configurazione (anche se definita in ordine diverso) dia sempre la stessa stringa, usata come chiave univoca.\n",
    "update_model_version\n",
    "\n",
    "Calcola model_config_key usando get_model_config_key.\n",
    "Costruisce una chiave completa (full_key) composta da data_key (identificativo dei dati) e dalla stringa della configurazione del modello.\n",
    "Se full_key non esiste in model_versions, assegna la versione 1 (quindi il nome sarà \"base_model_name_v_1\").\n",
    "Se esiste già, confronta la validation accuracy: se la nuova è migliore, aggiorna best_val_acc (senza cambiare il suffisso) e ritorna il nome già salvato; altrimenti, ritorna il nome già esistente.\n",
    "In questo modo tieni traccia di quante volte (e quale versione) una specifica configurazione del modello è stata ottenuta e aggiornata se migliora.\n",
    "Integrazione nel codice\n",
    "\n",
    "Dopo ogni epoch (o al termine dell’addestramento), costruisci i dizionari model_config e training_config.\n",
    "Costruisci una chiave identificativa data_key basata su exp_cond, data_type, category_subject e model_name.\n",
    "Usa update_model_version per ottenere il nome del file (con suffisso versione, es. \"_v_1\", \"_v_2\", ecc.).\n",
    "Salva il modello usando questo nome e aggiorna il dizionario best_models.\n",
    "Con questo schema, se lo sweep istanzia lo stesso modello (stessa configurazione architetturale) più volte con iperparametri differenti, verrà usato lo stesso suffisso (versione) e, se la nuova run ha una migliore validation accuracy, verrà aggiornata la configurazione di iperparametri e i pesi del modello associato a quella versione.\n",
    "\n",
    "Spero che ora il meccanismo sia chiaro! Se hai ulteriori dubbi o desideri altre modifiche, fammi sapere!\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**IO**\n",
    "Quindi, se capisco bene \n",
    "\n",
    "1) \"get_model_config_key\" crea sostanzialmente in forma stringa json, tutto il set di parametro del modello CNN2D corrente, giusto?\n",
    "\n",
    "e questo serve per avere un formato UNIVOCO e CANONICO del di parametri interni della rete\n",
    "e si impone con 'sort_keys=True:' per dare una STESSA SEQUENZA all'ORDINAMENTO delle STRINGHE associati ai PARAMETRI INTERNI DELLA RETE... giusto?\n",
    "\n",
    "2) \"update_model_version\" richiama \"get_model_config_key\" (che sarebbe la stringa json, che sarebbe il set di STRINGHE (UNIVOCO e CANONICO ed ORDINATO con la STESSA SEQUENZA) dei parametri interni della rete.\n",
    "\n",
    "Costruisce poi, con \"full_key\" una tupla che contiene tutta la STRINGA che si riferisce a\n",
    "\n",
    "1) nome del modello e combinazione dei fattori dei dati  + la rappresentazione canonica della configurazione del modello (in formato stringa)... quindi immagino sarà una stringa bella lunga...?\n",
    "\n",
    "se questa non si trova dentro 'model_versions' allora la salva dentro a 'model_versions'..\n",
    "\n",
    "però forse non son stato chiaro o forse non ti ho posto la domanda... \n",
    "\n",
    "quando io creo la 'version' IO intendevo che la rappresentazione canonica della configurazione del modello è costituita sì dalla stessa combinazioni di parametri, e quello va bene...\n",
    "\n",
    "ma ci dovrebbe essere anche il check di rispetto al fatto che,  due modelli per essere della stessa VERSION, devono per forza (ma quella è comune a tutti)\n",
    "\n",
    "1) la rappresentazione canonica della configurazione del modello, ma oltretutto quella rappresentazione canonica della configurazione del modello deve avere, per ogni parametro che la costituisce, LO STESSO SET DI VALORI.. perché è quello che rende \n",
    "\n",
    "della STESSA VERSIONE due modelli.. cioè la categorizzazione di due modelli come della stessa VERSION deriva non tanto dalla rappresentazione canonica della configurazione del modello (che è COMUNE per tutti diciamo), ma dal fatto che quei due modelli abbiano \n",
    "\n",
    "GLI STESSI VALORI nella rappresentazione canonica della configurazione DEI PARAMETRI del modello....\n",
    "\n",
    "forse mancava specificare questo passaggio....\n",
    "\n",
    "per questo dicevo che, il suffisso può esser diverso... perché io posso avere, per uno stesso set di dati (es. combinazione di exp_cond, data_type, category_subject e il nome base del modello) diverse VERSIONI di instanziazione del modello, ma con DIVERSI VALORI (appunto) all'interno della rappresentazione canonica della configurazione DEI PARAMETRI del modello stesso...\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**CHATGPT**\n",
    "\n",
    "\n",
    "Esattamente, hai colto il punto. Riepilogo e chiarisco:\n",
    "\n",
    "get_model_config_key:\n",
    "\n",
    "Questa funzione converte il dizionario dei parametri interni del modello (model_config) in una stringa JSON.\n",
    "L'uso di sort_keys=True garantisce che le chiavi vengano ordinate sempre nello stesso ordine, così che due dizionari contenenti gli stessi valori (anche se definiti in ordine diverso) producano esattamente la stessa stringa.\n",
    "In questo modo, ottieni una rappresentazione univoca e canonica della configurazione del modello.\n",
    "update_model_version:\n",
    "\n",
    "Richiama get_model_config_key per ottenere quella rappresentazione canonica (model_config_key).\n",
    "Quindi costruisce una full_key che è una tupla formata da:\n",
    "a) un identificatore dei dati (ad esempio una stringa costruita con il nome base del modello e i fattori dati, come \"CNN2D_pt_resp_vs_shared_resp_spectrograms_familiar_th\")\n",
    "b) la stringa canonica dei parametri del modello ottenuta (che conterrà tutti i valori dei parametri interni, nell'ordine standardizzato).\n",
    "Questa full_key identifica in modo univoco una configurazione del modello per quella specifica combinazione di dati.\n",
    "Se full_key non esiste in model_versions, viene assegnata la versione 1 (il suffisso \"_v_1\") e si memorizza quella combinazione.\n",
    "Se la full_key è già presente, si confronta la validation accuracy:\n",
    "Se la nuova run migliora la best accuracy per quella configurazione, si aggiorna il record (ma si mantiene lo stesso suffisso, perché la configurazione interna è identica).\n",
    "Se invece la run corrente non migliora, viene semplicemente restituito il nome già salvato.\n",
    "Quindi, per due modelli affinché siano considerati della stessa versione (ad es. \"v_1\"), devono avere esattamente lo stesso set di valori nella rappresentazione canonica dei parametri interni. Se invece la rappresentazione cambia (cioè, i valori effettivi differiscono), la full_key sarà diversa e verrà assegnata una nuova versione (ad esempio \"v_1\" per quella nuova chiave, indipendentemente dal fatto che un'altra configurazione con la stessa chiave sia già presente per altri dati).\n",
    "\n",
    "In sintesi:\n",
    "\n",
    "La versione (suffisso \"v_X\") è determinata dalla combinazione di:\n",
    "\n",
    "I fattori dei dati (exp_cond, data_type, category_subject, model_name)\n",
    "La rappresentazione canonica dei parametri del modello (ottenuta con get_model_config_key)\n",
    "Due modelli avranno lo stesso suffisso (stessa versione) se e solo se la loro rappresentazione canonica dei parametri interni è identica.\n",
    "Se i valori interni sono diversi, anche se il modello è dello stesso tipo (CNN2D), avranno full_key differenti e quindi verrà considerata una nuova versione.\n",
    "\n",
    "Questo meccanismo ti permette di tenere traccia in modo univoco delle diverse istanze (o \"versioni\") di modelli creati in base alla configurazione dei parametri della rete, e di aggiornare il salvataggio (con la migliore validation accuracy) per ogni versione specifica.\n",
    "\n",
    "Spero che questo chiarisca come funzionano e come si integrano le due funzioni nel tuo codice. Se hai altri dubbi o necessiti ulteriori dettagli, chiedi pure!\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**IO**\n",
    "\n",
    "ok ma allora domanda, tu hai detto\n",
    "\n",
    "\"\n",
    "La versione (suffisso \"v_X\") è determinata dalla combinazione di:\n",
    "\n",
    "I fattori dei dati (exp_cond, data_type, category_subject, model_name)\n",
    "La rappresentazione canonica dei parametri del modello (ottenuta con get_model_config_key)\n",
    "\n",
    "Due modelli avranno lo stesso suffisso (stessa versione) SE E SOLO SE la loro rappresentazione canonica dei parametri interni è identica.\n",
    "Se i valori interni sono diversi, anche se il modello è dello stesso tipo (CNN2D), avranno full_key differenti e quindi verrà considerata una nuova versione.\n",
    "\"\n",
    "\n",
    "ma allora, se i il modello è lo stesso ed i fattori dei dati (exp_cond, data_type, category_subject, model_name) sono gli stessi MA SOLO i valori interni dei parametri sono diversi...\n",
    "\n",
    "DATO CHE ho già una versione \"v_1\".. per non confondere.. non conviene aumentare il valore progressivo del numero di di \"v_*\" dove * sarebbe il digit progressivo?\n",
    "\n",
    "te lo dico perché il salvataggio è stato fatto in modo che ogni modello, relativo ad una certa combinazioni di fattori dei dati, venga salvata in una directory specifica...\n",
    "\n",
    "automatizzando questa \"aumento progressivo\" del valore del digit nella striga di salvataggio del modello aiuta così a capire quante diverse versioni di modelli son state provate per ogni configurazione di fattori dei dati.. giusto?\n",
    "\n",
    "se è corretto il mio ragionamento.. allora, si può fare questa modifica?\n",
    "\n",
    "in questo modo, la mia idea è che, alla fine della procedura su Weight & Biases, io abbia \n",
    "\n",
    "1) per ogni combinazione di fattori dei dati, \n",
    "2) provenienti da un certo soggetto, si avranno \n",
    "\n",
    "3) specifiche versioni dei modelli CNN2D (e non solo una diciamo) e si salverà, così, \n",
    "\n",
    "per ognuna delle versioni, non solo \n",
    "\n",
    "i migliori modelli con la loro relativa configurazione di parametri (e dei suoi valori), ma anche degli iper-parametri associati... giusto? \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82ab2a-8efe-48b3-8db5-ea32baea86fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Training Function IMPLEMENTATION - EEG Spectrograms - Electrodes x Frequencies PESI E HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f560723-5cff-4b0a-bb73-a93bc212e53c",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                    **** SALVATAGGIO SOLO PESI E BIAS DI UN CERTO MODELLO ****      \n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "\n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "\n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "    #    model = CNN1D(input_channels=3, num_classes=2)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1m{config.model_name}\\033[0m\")\n",
    "    #elif config.model_name == \"BiLSTM\":\n",
    "    #    model = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1m{config.model_name}\\033[0m\")\n",
    "    #elif config.model_name == \"Transformer\":\n",
    "    #    model = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1m{config.model_name}\\033[0m\")\n",
    "        \n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    if config.model_name == \"CNN2D\":\n",
    "        model = CNN2D(input_channels = 61, num_classes = 2)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "        \n",
    "    #elif config.model_name == \"BiLSTM\":\n",
    "    #    # Qui, input_size = canali * frequenze = 3 * 38 = 114\n",
    "    #    model = ReadMEndYou(input_size= 3 * 38, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1mBiLSTM\\033[0m\")\n",
    "        \n",
    "    #elif config.model_name == \"Transformer\":\n",
    "    #    # Per il Transformer, passiamo anche i parametri channels e freqs per adattare l'embedding\n",
    "    #    model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=38)\n",
    "    #    print(f\"\\nInizializzazione Modello \\033[1mTransformer\\033[0m\")\n",
    "\n",
    "        \n",
    "    #ORIGINAL VERSION OF TIME SERIES EEG DATA REPRESENTATION  \n",
    "    #def initialize_models():\n",
    "        #model_CNN = CNN1D(input_channels=3, num_classes=2)\n",
    "        #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        \n",
    "        #return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "\n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val\n",
    "        })\n",
    "\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "\n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        '''\n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "            torch.save(best_model.state_dict(), model_file)\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    torch.save(best_model.state_dict(), model_file)\n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "        \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7c26c1d-c896-46e4-884e-c0e37c06413c",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                                    **** SALVATAGGIO DI **** \n",
    "                                                        \n",
    "                                                        1) PESI E BIAS DI UN CERTO MODELLO \n",
    "                                                        2) CONFIGURAZIONE IPER-PARAMETRI DI UN CERTO MODELLO\n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "\n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "\n",
    "        \n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    if config.model_name == \"CNN2D\":\n",
    "        model = CNN2D(input_channels = 61, num_classes=2)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "        \n",
    "    elif config.model_name == \"BiLSTM\":\n",
    "        # Qui, input_size = canali * frequenze = 3 * 26 = 78\n",
    "        model = ReadMEndYou(input_size= 3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mBiLSTM\\033[0m\")\n",
    "        \n",
    "    elif config.model_name == \"Transformer\":\n",
    "        # Per il Transformer, passiamo anche i parametri channels e freqs per adattare l'embedding\n",
    "        model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mTransformer\\033[0m\")\n",
    "\n",
    "        \n",
    "    #ORIGINAL VERSION OF TIME SERIES EEG DATA REPRESENTATION  \n",
    "    #def initialize_models():\n",
    "        #model_CNN = CNN1D(input_channels=3, num_classes=2)\n",
    "        #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        \n",
    "        #return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "\n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val\n",
    "        })\n",
    "\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "\n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        \n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        IMPORTANTISSIMO: COME SALVARSI LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI UN CERTO MODELLO, DI UN DATO DI UNA CERTA COMBINAZIONE DI FATTORI\n",
    "        (CONDIZIONE SPERIMENTALE, TIPO DI DATO, PROVENIENZA DEL DATO!)\n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        \n",
    "        CHATGPT:\n",
    "        \n",
    "        Nei run eseguiti con W&B ogni esecuzione registra automaticamente la configurazione degli iper-parametri (tramite wandb.config) \n",
    "        insieme alle metriche e ai log. \n",
    "        Quindi, a meno che tu non abbia modificato il comportamento predefinito, \n",
    "        ogni run con il tuo sweep ha già la configurazione associata registrata nei run logs di W&B.\n",
    "\n",
    "        Tuttavia, per associare in modo “automatico” e diretto la migliore configurazione agli specifici modelli salvati in .pth, \n",
    "        potresti considerare di fare uno o più di questi aggiustamenti:\n",
    "\n",
    "        Salvare la configurazione nel dizionario dei best_models:\n",
    "        Quando aggiorni il dizionario best_models (cioè quando salvi il miglior modello per una determinata combinazione), \n",
    "        puoi salvare anche una copia della configurazione corrente. \n",
    "        \n",
    "        Ad esempio, potresti modificare il blocco in cui aggiorni best_models in questo modo:\n",
    "        \n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": dict(config)  # Salva la configurazione degli iper-parametri\n",
    "        }\n",
    "        \n",
    "        In questo modo, ogni volta che un modello viene considerato il migliore per quella combinazione,\n",
    "        la sua configurazione sarà salvata insieme ai pesi.\n",
    "        Questo ti permetterà, in seguito, di sapere esattamente quali iper-parametri sono stati usati per ottenere quel modello.\n",
    "        \n",
    "        \n",
    "        In sintesi, se hai già usato wandb.config e hai loggato le configurazioni durante le run,\n",
    "        W&B le ha automaticamente salvate nei run logs. \n",
    "        \n",
    "        Se vuoi rendere più esplicita l'associazione tra il modello salvato (.pth) e la sua configurazione, \n",
    "        è utile modificare il tuo codice di TRAINING per salvare ANCHE \n",
    "        \n",
    "        1) il dizionario di configurazione insieme a \n",
    "        2) i pesi nel dizionario best_models oppure nei metadati del file salvato.\n",
    "        \n",
    "        Questo piccolo accorgimento ti consentirà di recuperare facilmente la configurazione ottimale per ogni modello salvato.\n",
    "        \n",
    "        OSSIA\n",
    "        Aggiungendo la chiave \"config\": dict(config) nel dizionario che memorizza il best model,\n",
    "        salvi anche la configurazione degli iper-parametri utilizzata in quella run.\n",
    "        \n",
    "        In questo modo, per ogni modello salvato (.pth) potrai recuperare facilmente sia i pesi che la configurazione ottimale che li ha generati.\n",
    "        \n",
    "        Questo approccio garantisce che ogni modello sia associato in modo esplicito al set di iper-parametri che ha prodotto le migliori performance, \n",
    "        rendendo più semplice il successivo confronto o la replica degli esperimenti.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # ***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "        #1)Al posto di salvarmi solo i migliori pesi (i.e.,  model_file = f\"{model_path}/{best_model_name}.pth\")\n",
    "        #  ora mi salvo anche la MIGLIORE configurazione di iper-parametri trovata rispetto alle 15 RUNS di un certo SWEEP\n",
    "        #  di un certo MODELLO, applicato su un DATASET con una SPECIFICA COMBINAZIONE DI FATTORI\n",
    "        #  condizione sperimentale, tipo di dato e provenienza del dato!\n",
    "        \n",
    "    \n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \n",
    "                #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "                #***** AGGIUNTA DELLA CHIAVE CONFIG CHE PRELEVA AUTOMATICAMENTE LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DENTRO 'BEST_MODELS'\n",
    "                \n",
    "                # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                # una certa combinazione di fattori: \n",
    "                # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                \"config\": dict(config)  \n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "            #***** SALVATAGGIO DI UN FILE .PKL, CHE CONTIENE \n",
    "            \n",
    "            # I PESI E BIAS DEL MODELLO DERIVATO DALLA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI OTTENUTA DALLA MIGLIORE RUN DI UN CERTO SWEEP\n",
    "            # IN RELAZIONE AD UN CERTO DATASET COSTITUITO DA UNA CERTA COMBINAZIONE DI FATTORI\n",
    "            \n",
    "            '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "            #model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "            \n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            \n",
    "            '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "            #torch.save(best_model.state_dict(), model_file)\n",
    "            \n",
    "            # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": dict(config)\n",
    "            }, model_file)\n",
    "\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                    \n",
    "                    # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                    # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                    # una certa combinazione di fattori: \n",
    "                    # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                    \"config\": dict(config)  \n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "                #model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    \n",
    "                    '''OLD VERSION (SOLO SALVATAGGIO PESI E BIAS DEL MODELLO!'''\n",
    "                    #torch.save(best_model.state_dict(), model_file)\n",
    "\n",
    "                    # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                    torch.save({\n",
    "                        \"state_dict\": best_model.state_dict(),\n",
    "                        \"config\": dict(config)\n",
    "                    }, model_file)\n",
    "                    \n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pth\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "        \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b74738-01ac-438a-a062-b8624d72b2ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **PARTI DA QUI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa6124-60e4-4c5f-93c1-cabc55571110",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "#Test\n",
    "combination_key = \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "condition_experiment, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "print(\"Condizione:\", condition_experiment)\n",
    "print(\"Soggetto:\", subject_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58d746-cd47-4790-9e48-b843d39b7c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                                    **** SALVATAGGIO DI **** \n",
    "                                                        \n",
    "                                                        1) PESI E BIAS DI UN CERTO MODELLO \n",
    "                                                        2) CONFIGURAZIONE IPER-PARAMETRI DI UN CERTO MODELLO\n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''NEW VERSION\n",
    "    \n",
    "    Questo assicura la coerenza tra la creazione degli sweep e le run che li eseguono,\n",
    "    e permette di tracciare meglio ogni combinazione anche su W&B.\n",
    "    '''\n",
    "    wandb.init(project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", name = run_name, tags = tags)\n",
    "    \n",
    "    \n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "        #model = CNN2D(input_channels = 61, num_classes = 2)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "\n",
    "    #class CNN2D(nn.Module):\n",
    "        #def __init__(\n",
    "            #self,\n",
    "            #input_channels: int,              # numero di canali (es. 61)\n",
    "            #num_classes: int,                 # numero di classi di output\n",
    "            #conv_out_channels: int,           # parametro dallo sweep\n",
    "            #conv2d_kernel_size: tuple,        # es. (h, w) dallo sweep\n",
    "            #conv2d_stride: tuple,             # es. (h, w) dallo sweep\n",
    "            #pool_type: str,                   # \"max\" o \"avg\" dallo sweep\n",
    "            #pool2d_kernel_size: tuple,        # es. (h, w) dallo sweep\n",
    "            #fc1_units: int,                   # unità del primo fully connected\n",
    "            #dropout: float,                   # dropout dallo sweep\n",
    "            #activations: tuple                # tupla di 3 stringhe, es. ('relu','selu','elu')\n",
    "        #):\n",
    "    \n",
    "    '''PRENDO LA SHAPE DEI DATI PER FORNIRE VALORI GIUSTI PER OGNI INPUt DI CIASCUNA RETE'''\n",
    "    \n",
    "    # Appena caricato X_train, X_val, X_test, etc.\n",
    "    # X_train.shape == (N, freq_bins, channels)\n",
    "    \n",
    "    _, freq_bins, channels = X_train.shape\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    if config.model_name == \"CNN2D\":\n",
    "        \n",
    "        #model = CNN2D(\n",
    "            #input_channels   = 1,\n",
    "            #num_classes      = 2,\n",
    "            #conv_out_channels= config.conv_out_channels,\n",
    "            #conv2d_kernel_size = tuple(config.conv2d_kernel_size),\n",
    "            #conv2d_stride      = tuple(config.conv2d_stride),\n",
    "            #pool_type        = config.pool_type,\n",
    "            #pool2d_kernel_size = tuple(config.pool2d_kernel_size),\n",
    "            #fc1_units        = config.fc1_units,\n",
    "            #dropout          = config.dropout,\n",
    "            #activations      = tuple(config.activations)\n",
    "        #)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "        \n",
    "    \n",
    "        model = CNN2D(\n",
    "                input_channels   = 1,\n",
    "                num_classes      = num_classes,\n",
    "                conv_out_channels= config.conv_out_channels,\n",
    "\n",
    "                conv_k1_h = config.conv_k1_h, \n",
    "                conv_k1_w = config.conv_k1_w,\n",
    "\n",
    "                conv_k2_h = config.conv_k2_h, \n",
    "                conv_k2_w = config.conv_k2_w,\n",
    "\n",
    "                conv_k3_h = config.conv_k3_h,\n",
    "                conv_k3_w = config.conv_k3_w,\n",
    "\n",
    "                conv_s1_h = config.conv_s1_h, \n",
    "                conv_s1_w = config.conv_s1_w,\n",
    "\n",
    "                conv_s2_h = config.conv_s2_h,\n",
    "                conv_s2_w = config.conv_s2_w,\n",
    "\n",
    "                conv_s3_h = config.conv_s3_h,\n",
    "                conv_s3_w = config.conv_s3_w,\n",
    "\n",
    "                pool_p1_h = config.pool_p1_h,\n",
    "                pool_p1_w = config.pool_p1_w,\n",
    "\n",
    "                pool_p2_h = config.pool_p2_h,\n",
    "                pool_p2_w = config.pool_p2_w,\n",
    "\n",
    "                pool_p3_h = config.pool_p3_h,\n",
    "                pool_p3_w = config.pool_p3_w,\n",
    "\n",
    "                pool_type = config.pool_type,\n",
    "\n",
    "                fc1_units = config.fc1_units,\n",
    "                dropout   = config.dropout,\n",
    "\n",
    "                cnn_act1  = config.cnn_act1,\n",
    "                cnn_act2  = config.cnn_act2,\n",
    "                cnn_act3  = config.cnn_act3,\n",
    "            )\n",
    "    \n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    # 1) Optimizer con betas, eps, weight_decay\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        eps=config.eps,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode ='min',      # monitoriamo val_loss\n",
    "        factor = 0.1,      # dimezza lr\n",
    "        patience = 8,      # 4 epoche di plateau\n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val\n",
    "        })\n",
    "        \n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "            \n",
    "        '''OLD VERSION'''\n",
    "        #early_stopping(accuracy_val)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"🛑 Early stopping attivato!\")\n",
    "            #break\n",
    "            \n",
    "        '''NEW VERSION'''\n",
    "        scheduler.step(loss_val)\n",
    "        early_stopping(loss_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        \n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        IMPORTANTISSIMO: COME SALVARSI LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI UN CERTO MODELLO, DI UN DATO DI UNA CERTA COMBINAZIONE DI FATTORI\n",
    "        (CONDIZIONE SPERIMENTALE, TIPO DI DATO, PROVENIENZA DEL DATO!)\n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        \n",
    "        CHATGPT:\n",
    "        \n",
    "        Nei run eseguiti con W&B ogni esecuzione registra automaticamente la configurazione degli iper-parametri (tramite wandb.config) \n",
    "        insieme alle metriche e ai log. \n",
    "        Quindi, a meno che tu non abbia modificato il comportamento predefinito, \n",
    "        ogni run con il tuo sweep ha già la configurazione associata registrata nei run logs di W&B.\n",
    "\n",
    "        Tuttavia, per associare in modo “automatico” e diretto la migliore configurazione agli specifici modelli salvati in .pth, \n",
    "        potresti considerare di fare uno o più di questi aggiustamenti:\n",
    "\n",
    "        Salvare la configurazione nel dizionario dei best_models:\n",
    "        Quando aggiorni il dizionario best_models (cioè quando salvi il miglior modello per una determinata combinazione), \n",
    "        puoi salvare anche una copia della configurazione corrente. \n",
    "        \n",
    "        Ad esempio, potresti modificare il blocco in cui aggiorni best_models in questo modo:\n",
    "        \n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": dict(config)  # Salva la configurazione degli iper-parametri\n",
    "        }\n",
    "        \n",
    "        In questo modo, ogni volta che un modello viene considerato il migliore per quella combinazione,\n",
    "        la sua configurazione sarà salvata insieme ai pesi.\n",
    "        Questo ti permetterà, in seguito, di sapere esattamente quali iper-parametri sono stati usati per ottenere quel modello.\n",
    "        \n",
    "        \n",
    "        In sintesi, se hai già usato wandb.config e hai loggato le configurazioni durante le run,\n",
    "        W&B le ha automaticamente salvate nei run logs. \n",
    "        \n",
    "        Se vuoi rendere più esplicita l'associazione tra il modello salvato (.pth) e la sua configurazione, \n",
    "        è utile modificare il tuo codice di TRAINING per salvare ANCHE \n",
    "        \n",
    "        1) il dizionario di configurazione insieme a \n",
    "        2) i pesi nel dizionario best_models oppure nei metadati del file salvato.\n",
    "        \n",
    "        Questo piccolo accorgimento ti consentirà di recuperare facilmente la configurazione ottimale per ogni modello salvato.\n",
    "        \n",
    "        OSSIA\n",
    "        Aggiungendo la chiave \"config\": dict(config) nel dizionario che memorizza il best model,\n",
    "        salvi anche la configurazione degli iper-parametri utilizzata in quella run.\n",
    "        \n",
    "        In questo modo, per ogni modello salvato (.pth) potrai recuperare facilmente sia i pesi che la configurazione ottimale che li ha generati.\n",
    "        \n",
    "        Questo approccio garantisce che ogni modello sia associato in modo esplicito al set di iper-parametri che ha prodotto le migliori performance, \n",
    "        rendendo più semplice il successivo confronto o la replica degli esperimenti.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # ***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "        #1)Al posto di salvarmi solo i migliori pesi (i.e.,  model_file = f\"{model_path}/{best_model_name}.pth\")\n",
    "        #  ora mi salvo anche la MIGLIORE configurazione di iper-parametri trovata rispetto alle 15 RUNS di un certo SWEEP\n",
    "        #  di un certo MODELLO, applicato su un DATASET con una SPECIFICA COMBINAZIONE DI FATTORI\n",
    "        #  condizione sperimentale, tipo di dato e provenienza del dato!\n",
    "        \n",
    "    \n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \n",
    "                #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "                #***** AGGIUNTA DELLA CHIAVE CONFIG CHE PRELEVA AUTOMATICAMENTE LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DENTRO 'BEST_MODELS'\n",
    "                \n",
    "                # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                # una certa combinazione di fattori: \n",
    "                # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                \"config\": dict(config)  \n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "            #***** SALVATAGGIO DI UN FILE .PKL, CHE CONTIENE \n",
    "            \n",
    "            # I PESI E BIAS DEL MODELLO DERIVATO DALLA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI OTTENUTA DALLA MIGLIORE RUN DI UN CERTO SWEEP\n",
    "            # IN RELAZIONE AD UN CERTO DATASET COSTITUITO DA UNA CERTA COMBINAZIONE DI FATTORI\n",
    "            \n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            \n",
    "            # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": dict(config)\n",
    "            }, model_file)\n",
    "\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                    \n",
    "                    # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                    # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                    # una certa combinazione di fattori: \n",
    "                    # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                    \"config\": dict(config)  \n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                    torch.save({\n",
    "                        \"state_dict\": best_model.state_dict(),\n",
    "                        \"config\": dict(config)\n",
    "                    }, model_file)\n",
    "                    \n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ff786-95a0-4ef3-88f6-108013e0938d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure Edits - EEG Spectrograms - Electrodes x Frequencies BOTH HYPER-PARAMS & MODEL PARAMAS (NON GUARDARE!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d6916-e023-42e0-a7a0-43847dadbbfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **PRE AGGIORNAMENTO: Weight & Biases Procedure Edits - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS (VEDI SOTTO!!!)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3394d3a1-ab2f-4cec-a2c1-735e833b7e69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "''' \n",
    "\n",
    "                                    QUI IL LOOP LO ESEGUO SU UN SINGOLO SWEEP DI UNA SPECIFICA COMBINAZIONE DI FATTORI\n",
    "\n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "'''\n",
    "\n",
    "#Allora la mia idea è far così\n",
    "#1) questo loop rimane così, con la differenza che, oltre al singolo sweep ID, mi carico in input alla funzione 'training_sweep' anche sweep_tuple\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    \n",
    "    #eseguo l'unpacking della tupla per prendermi solo il primo elmento della tupla (sweep_id, ...) dove ... sarebbe il combination_key\n",
    "    sweep_id, _ =  sweep_tuple\n",
    "    '''\n",
    "    Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "    per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "    In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "    '''\n",
    "    \n",
    "    # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "    \n",
    "    def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "        def train_wrapper():\n",
    "            \n",
    "            training_sweep(\n",
    "                data_dict_preprocessed, \n",
    "                sweep_config,\n",
    "                sweep_ids,\n",
    "                sweep_id,\n",
    "                sweep_tuple,\n",
    "                best_models # Viene aggiornato dentro la funzione training_sweep\n",
    "            )\n",
    "        return train_wrapper\n",
    "\n",
    "    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "    \n",
    "    # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "    wandb.agent(sweep_id, function=agent_function, count=15)\n",
    "\n",
    "    print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione: \\033[1m{condition} - {data_type} - {category_subject}\\033[0m\\n\")\n",
    "\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb4d0e57-596f-448c-8b8f-86f71641d00e",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE A \n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "        \n",
    "'''\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "# Itera attraverso il primo livello (tipo di combinazione)\n",
    "for condition, sub_dict in sweep_ids.items():\n",
    "    #print(f\"Processing sweep_id: {condition}\")\n",
    "    \n",
    "    # Itera attraverso il secondo livello (e.g., '1_20', '1_45', 'wavelet')\n",
    "    for data_type, condition_data in sub_dict.items():\n",
    "        #print(f\"  Processing condition: {data_type}\")\n",
    "        \n",
    "        # Itera attraverso il terzo livello (e.g., 'familiar_th', 'unfamiliar_pt', ...)\n",
    "        for category_subject, data_tuples in condition_data.items():\n",
    "            #print(f\"    Processing data type: {category_subject}\")\n",
    "            \n",
    "            # Itera sulle tuple (sweep_id, sweep_name)\n",
    "            for sweep_tuple in data_tuples:\n",
    "                #print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, _ = sweep_tuple\n",
    "                \n",
    "                #Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                #per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                #In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    def train_wrapper():\n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        print(f\"Eseguendo il training per {condition} - {data_type} - {category_subject} con sweep_id {sweep_id}\")\n",
    "                        training_sweep(\n",
    "                            data_dict_preprocessed, \n",
    "                            sweep_config,\n",
    "                            sweep_ids,\n",
    "                            sweep_id,\n",
    "                            sweep_tuple,\n",
    "                            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        )\n",
    "                    return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_id}\\033[0m\")\n",
    "                wandb.agent(sweep_id, function=agent_function, count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione: \\033[1m{condition} - {data_type} - {category_subject}\\033[0m\\n\")\n",
    "                \n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba3a3b6a-f89b-4669-9812-b1be6deacabb",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE B\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE B (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                PRIMA DELL'AGGIORNAMENTO \n",
    "\n",
    "'''\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, combination_key = sweep_tuple\n",
    "                \n",
    "                # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    def train_wrapper():\n",
    "                        \n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "                        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "                        training_sweep(\n",
    "                            data_dict_preprocessed, \n",
    "                            sweep_config,\n",
    "                            sweep_ids,\n",
    "                            sweep_id,\n",
    "                            sweep_tuple,\n",
    "                            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        )\n",
    "                    return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                   ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "                \n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs\", count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e80f5-c9cd-4c2f-b85f-977245e963fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE C \n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                W&B SWEEPS AND TRAING LAUNCH WITH MULTIPLE GPUs MANAGEMENT\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE C (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "SPIEGAZIONE\n",
    "\n",
    "GPU counter: Ho aggiunto un contatore (gpu_counter) che cicla tra le GPU disponibili. \n",
    "\n",
    "In questo modo, il primo sweep sarà eseguito sulla GPU 0, il secondo sulla GPU 1, e così via. \n",
    "Quando il contatore raggiunge il numero di GPU disponibili, torna a 0 per riusare la prima GPU.\n",
    "\n",
    "Rotazione delle GPU: All'interno del loop, per ogni sweep, viene assegnata una GPU diversa. \n",
    "Se ci sono più di 1 GPU, il contatore incrementa, e la variabile CUDA_VISIBLE_DEVICES cambia automaticamente per assegnare la GPU corretta.\n",
    "\n",
    "Esecuzione parallela: Ogni sweep viene eseguito su una GPU separata. Se ci sono 2 GPU, il primo sweep va su GPU 0, il secondo su GPU 1, il terzo su GPU 0, e così via.\n",
    "\n",
    "Risposta alla tua domanda:\n",
    "In questo modo, ogni sweep_id viene eseguito una sola volta, ma su GPU diverse (se disponibili). Non ci sono duplicati dello stesso sweep su entrambe le GPU.\n",
    "\n",
    "\n",
    "DOMANDE SUL NUOVO CODICE\n",
    "\n",
    "1) Gli sweep sono eseguiti già in parallelo giusto?\n",
    "No, in questo caso gli sweep non sono eseguiti in parallelo in modo esplicito tramite il codice che hai scritto.\n",
    "\n",
    "Anche se hai assegnato ciascun sweep a una GPU diversa, il codice esegue sequenzialmente ogni sweep, solo che li distribuisce su GPU differenti in modo rotazionale.\n",
    "Ogni volta che il ciclo passa ad un nuovo sweep, assegna un ID GPU e poi esegue l'agent su quella GPU. Non vengono eseguiti in parallelo a livello di codice.\n",
    "\n",
    "2) O semplicemente in questo modo faccio in modo di distribuire ogni sweep sull'altra GPU rispetto a quella usata dallo sweep precedente\n",
    "per \"ottimizzare\" il carico computazionale di ogni GPU?\n",
    "\n",
    "Esatto! Quello che stai facendo è distribuire i vari sweep su GPU diverse, assicurandoti che ogni sweep venga eseguito su una GPU separata (se ne hai di disponibili).\n",
    "Questo permette di ottimizzare l'uso delle risorse, evitando che una GPU venga sovraccaricata da più sweep. Se il numero di GPU disponibili è maggiore di 1, \n",
    "allora i sweep sono distribuiti sulle diverse GPU, ma ogni sweep sarà ancora eseguito singolarmente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sì, con il codice che hai fornito, stai distribuendo gli sweep tra le diverse GPU, in modo da ottimizzare il carico computazionale e non sovraccaricare una sola GPU.\n",
    "\n",
    "Dettaglio del funzionamento:\n",
    "Distribuzione delle GPU (rotazionale):\n",
    "\n",
    "Quando ci sono più di una GPU, il codice assegna a ciascun sweep una GPU diversa in modo rotazionale.\n",
    "\n",
    "Per ogni ciclo del loop, la variabile gpu_counter determina a quale GPU assegnare il prossimo sweep.\n",
    "\n",
    "Se ci sono 2 GPU, il primo sweep viene eseguito sulla GPU 0, il secondo sulla GPU 1, il terzo di nuovo sulla GPU 0, e così via.\n",
    "\n",
    "Gestione della GPU:\n",
    "\n",
    "Se hai più di una GPU, os.environ[\"CUDA_VISIBLE_DEVICES\"] imposta il dispositivo GPU corrente su cui il codice deve girare (GPU 0 o GPU 1). Questo permette di gestire quale GPU eseguirà l'addestramento per ciascun sweep.\n",
    "\n",
    "Quando num_gpus > 1, il codice alterna l'assegnazione della GPU per ogni sweep, evitando di sovraccaricare una singola GPU con troppe operazioni contemporaneamente.\n",
    "\n",
    "Ottimizzazione del carico computazionale:\n",
    "\n",
    "L'alternanza tra GPU successive per ciascun sweep aiuta a distribuire il carico in modo equilibrato, specialmente se il numero di sweep è alto.\n",
    "\n",
    "In pratica, se hai 2 GPU e 10 sweep da eseguire, ogni GPU eseguirà 5 sweep, evitando di saturare una singola GPU.\n",
    "\n",
    "Parallelizzazione effettiva:\n",
    "Nel tuo codice non c'è parallelismo vero e proprio tra le GPU (come quello che otterresti con DataParallel o DistributedDataParallel), ma piuttosto distribuisci gli sweep su GPU diverse. Ogni sweep è eseguito sequenzialmente su una GPU diversa, ma non stai parallelizzando il training dello stesso sweep su più GPU.\n",
    "\n",
    "Se volessi fare in modo che un singolo sweep fosse parallelizzato su più GPU (in modo che il lavoro venga diviso tra le GPU per uno stesso sweep), dovresti usare DataParallel o DistributedDataParallel, come descritto in precedenza. Ma in questo caso, ogni sweep è isolato e viene eseguito su una sola GPU, quindi non c'è un vero parallelismo dentro ogni singolo sweep.\n",
    "\n",
    "Domande che potrebbero sorgere:\n",
    "Gli sweep vengono eseguiti in parallelo?\n",
    "\n",
    "No, i sweep sono eseguiti uno alla volta, ma su GPU diverse. Quindi, mentre il primo sweep usa la GPU 0, il secondo usa la GPU 1 e così via. Ogni sweep viene gestito separatamente, ma sfrutti più GPU per parallelizzare l'esecuzione di più sweep contemporaneamente.\n",
    "\n",
    "La distribuzione delle GPU tra gli sweep è ottimizzata?\n",
    "\n",
    "Sì, stai bilanciando il carico computazionale tra le GPU, assegnando a ogni GPU uno sweep alternato. Se hai molte GPU, puoi ottimizzare ulteriormente distribuendo i sweep su più dispositivi.\n",
    "\n",
    "Se volessi parallelizzare più agenti W&B su diverse GPU, il codice che stai utilizzando sarebbe corretto, ma per ottimizzare ulteriormente i tempi di esecuzione, potresti prendere in considerazione anche l'utilizzo di tecniche come DataParallel o DistributedDataParallel per far sì che più GPU lavorino contemporaneamente sullo stesso sweep. Ma la logica che hai già implementato va bene per distribuire più sweep tra le GPU.\n",
    "\n",
    "Se hai bisogno di ulteriori dettagli su come implementare il parallelismo vero e proprio (inclusi DataParallel o DistributedDataParallel), fammi sapere!\n",
    "\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "\n",
    "# Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "    def train_wrapper():\n",
    "\n",
    "        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "        training_sweep(\n",
    "            data_dict_preprocessed, \n",
    "            sweep_config,\n",
    "            sweep_ids,\n",
    "            sweep_id,\n",
    "            sweep_tuple,\n",
    "            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "        )\n",
    "    return train_wrapper\n",
    "                        \n",
    "                \n",
    "# Verifica quante GPU sono disponibili\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "# Crea un contatore per assegnare un GPU diversa a ciascun sweep\n",
    "gpu_counter = 0\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, combination_key = sweep_tuple\n",
    "                \n",
    "                # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                \n",
    "                # Se ci sono più di 1 GPU, assegna a ciascuna GPU uno sweep diverso\n",
    "                if num_gpus > 1:\n",
    "                    \n",
    "                    # Assegna la GPU in modo rotazionale\n",
    "                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_counter)\n",
    "                    \n",
    "                    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                    #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project=f\"{condition}_spectrograms_channels_freqs\", count=100)\n",
    "                    \n",
    "                    wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", count=200)\n",
    "                    \n",
    "                    # Passa alla prossima GPU per il prossimo sweep\n",
    "                    gpu_counter = (gpu_counter + 1) % num_gpus\n",
    "\n",
    "                else:\n",
    "                    # Se c'è una sola GPU, esegui il sweep sulla GPU 0\n",
    "                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "                    \n",
    "                    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                    \n",
    "                    wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", count=200)\n",
    "\n",
    "                    \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                #def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    #def train_wrapper():\n",
    "                        \n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "                        #print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "                        #training_sweep(\n",
    "                            #data_dict_preprocessed, \n",
    "                            #sweep_config,\n",
    "                            #sweep_ids,\n",
    "                            #sweep_id,\n",
    "                            #sweep_tuple,\n",
    "                            #best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        #)\n",
    "                    #return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                '''COMMENTATO'''\n",
    "                #agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                   ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "                \n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                \n",
    "                '''COMMENTATO'''\n",
    "                #wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs_new_2d_grid_multiband_topomap\", count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2e526-ad6f-4bc4-92d6-7452693e42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finito\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a98b8-0009-4b09-ad13-492be77848b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VERSIONE DEL 6 MARZO (RISOLUZIONE DEFINITIVA) NEW VERSION (PER CNN2D)**\n",
    "\n",
    "##### **Training Function Edits - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c98425-36ba-4728-9a6b-6bb75be0cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                                    **** SALVATAGGIO DI **** \n",
    "                                                        \n",
    "                                                        1) PESI E BIAS DI UN CERTO MODELLO \n",
    "                                                        2) CONFIGURAZIONE IPER-PARAMETRI DI UN CERTO MODELLO\n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''NEW VERSION\n",
    "    \n",
    "    Questo assicura la coerenza tra la creazione degli sweep e le run che li eseguono,\n",
    "    e permette di tracciare meglio ogni combinazione anche su W&B.\n",
    "    '''\n",
    "    wandb.init(project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", name = run_name, tags = tags)\n",
    "    \n",
    "    \n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "        #model = CNN2D(input_channels = 61, num_classes = 2)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "\n",
    "    #class CNN2D(nn.Module):\n",
    "        #def __init__(\n",
    "            #self,\n",
    "            #input_channels: int,              # numero di canali (es. 61)\n",
    "            #num_classes: int,                 # numero di classi di output\n",
    "            #conv_out_channels: int,           # parametro dallo sweep\n",
    "            #conv2d_kernel_size: tuple,        # es. (h, w) dallo sweep\n",
    "            #conv2d_stride: tuple,             # es. (h, w) dallo sweep\n",
    "            #pool_type: str,                   # \"max\" o \"avg\" dallo sweep\n",
    "            #pool2d_kernel_size: tuple,        # es. (h, w) dallo sweep\n",
    "            #fc1_units: int,                   # unità del primo fully connected\n",
    "            #dropout: float,                   # dropout dallo sweep\n",
    "            #activations: tuple                # tupla di 3 stringhe, es. ('relu','selu','elu')\n",
    "        #):\n",
    "    \n",
    "    '''PRENDO LA SHAPE DEI DATI PER FORNIRE VALORI GIUSTI PER OGNI INPUt DI CIASCUNA RETE'''\n",
    "    \n",
    "    # Appena caricato X_train, X_val, X_test, etc.\n",
    "    # X_train.shape == (N, freq_bins, channels)\n",
    "    \n",
    "    _, freq_bins, channels = X_train.shape\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    if config.model_name == \"CNN2D\":\n",
    "        \n",
    "        #model = CNN2D(\n",
    "            #input_channels   = 1,\n",
    "            #num_classes      = 2,\n",
    "            #conv_out_channels= config.conv_out_channels,\n",
    "            #conv2d_kernel_size = tuple(config.conv2d_kernel_size),\n",
    "            #conv2d_stride      = tuple(config.conv2d_stride),\n",
    "            #pool_type        = config.pool_type,\n",
    "            #pool2d_kernel_size = tuple(config.pool2d_kernel_size),\n",
    "            #fc1_units        = config.fc1_units,\n",
    "            #dropout          = config.dropout,\n",
    "            #activations      = tuple(config.activations)\n",
    "        #)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "        \n",
    "    \n",
    "        model = CNN2D(\n",
    "                input_channels   = 1,\n",
    "                num_classes      = num_classes,\n",
    "                conv_out_channels= config.conv_out_channels,\n",
    "\n",
    "                conv_k1_h = config.conv_k1_h, \n",
    "                conv_k1_w = config.conv_k1_w,\n",
    "\n",
    "                conv_k2_h = config.conv_k2_h, \n",
    "                conv_k2_w = config.conv_k2_w,\n",
    "\n",
    "                conv_k3_h = config.conv_k3_h,\n",
    "                conv_k3_w = config.conv_k3_w,\n",
    "\n",
    "                conv_s1_h = config.conv_s1_h, \n",
    "                conv_s1_w = config.conv_s1_w,\n",
    "\n",
    "                conv_s2_h = config.conv_s2_h,\n",
    "                conv_s2_w = config.conv_s2_w,\n",
    "\n",
    "                conv_s3_h = config.conv_s3_h,\n",
    "                conv_s3_w = config.conv_s3_w,\n",
    "\n",
    "                pool_p1_h = config.pool_p1_h,\n",
    "                pool_p1_w = config.pool_p1_w,\n",
    "\n",
    "                pool_p2_h = config.pool_p2_h,\n",
    "                pool_p2_w = config.pool_p2_w,\n",
    "\n",
    "                pool_p3_h = config.pool_p3_h,\n",
    "                pool_p3_w = config.pool_p3_w,\n",
    "\n",
    "                pool_type = config.pool_type,\n",
    "\n",
    "                fc1_units = config.fc1_units,\n",
    "                dropout   = config.dropout,\n",
    "\n",
    "                cnn_act1  = config.cnn_act1,\n",
    "                cnn_act2  = config.cnn_act2,\n",
    "                cnn_act3  = config.cnn_act3,\n",
    "            )\n",
    "    \n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    # 1) Optimizer con betas, eps, weight_decay\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        eps=config.eps,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode ='min',      # monitoriamo val_loss\n",
    "        factor = 0.1,      # dimezza lr\n",
    "        patience = 8,      # 4 epoche di plateau\n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "\n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val\n",
    "        })\n",
    "        \n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "            \n",
    "        '''OLD VERSION'''\n",
    "        #early_stopping(accuracy_val)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"🛑 Early stopping attivato!\")\n",
    "            #break\n",
    "            \n",
    "        '''NEW VERSION'''\n",
    "        scheduler.step(loss_val)\n",
    "        early_stopping(loss_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        \n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        IMPORTANTISSIMO: COME SALVARSI LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI UN CERTO MODELLO, DI UN DATO DI UNA CERTA COMBINAZIONE DI FATTORI\n",
    "        (CONDIZIONE SPERIMENTALE, TIPO DI DATO, PROVENIENZA DEL DATO!)\n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        \n",
    "        CHATGPT:\n",
    "        \n",
    "        Nei run eseguiti con W&B ogni esecuzione registra automaticamente la configurazione degli iper-parametri (tramite wandb.config) \n",
    "        insieme alle metriche e ai log. \n",
    "        Quindi, a meno che tu non abbia modificato il comportamento predefinito, \n",
    "        ogni run con il tuo sweep ha già la configurazione associata registrata nei run logs di W&B.\n",
    "\n",
    "        Tuttavia, per associare in modo “automatico” e diretto la migliore configurazione agli specifici modelli salvati in .pth, \n",
    "        potresti considerare di fare uno o più di questi aggiustamenti:\n",
    "\n",
    "        Salvare la configurazione nel dizionario dei best_models:\n",
    "        Quando aggiorni il dizionario best_models (cioè quando salvi il miglior modello per una determinata combinazione), \n",
    "        puoi salvare anche una copia della configurazione corrente. \n",
    "        \n",
    "        Ad esempio, potresti modificare il blocco in cui aggiorni best_models in questo modo:\n",
    "        \n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": dict(config)  # Salva la configurazione degli iper-parametri\n",
    "        }\n",
    "        \n",
    "        In questo modo, ogni volta che un modello viene considerato il migliore per quella combinazione,\n",
    "        la sua configurazione sarà salvata insieme ai pesi.\n",
    "        Questo ti permetterà, in seguito, di sapere esattamente quali iper-parametri sono stati usati per ottenere quel modello.\n",
    "        \n",
    "        \n",
    "        In sintesi, se hai già usato wandb.config e hai loggato le configurazioni durante le run,\n",
    "        W&B le ha automaticamente salvate nei run logs. \n",
    "        \n",
    "        Se vuoi rendere più esplicita l'associazione tra il modello salvato (.pth) e la sua configurazione, \n",
    "        è utile modificare il tuo codice di TRAINING per salvare ANCHE \n",
    "        \n",
    "        1) il dizionario di configurazione insieme a \n",
    "        2) i pesi nel dizionario best_models oppure nei metadati del file salvato.\n",
    "        \n",
    "        Questo piccolo accorgimento ti consentirà di recuperare facilmente la configurazione ottimale per ogni modello salvato.\n",
    "        \n",
    "        OSSIA\n",
    "        Aggiungendo la chiave \"config\": dict(config) nel dizionario che memorizza il best model,\n",
    "        salvi anche la configurazione degli iper-parametri utilizzata in quella run.\n",
    "        \n",
    "        In questo modo, per ogni modello salvato (.pth) potrai recuperare facilmente sia i pesi che la configurazione ottimale che li ha generati.\n",
    "        \n",
    "        Questo approccio garantisce che ogni modello sia associato in modo esplicito al set di iper-parametri che ha prodotto le migliori performance, \n",
    "        rendendo più semplice il successivo confronto o la replica degli esperimenti.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # ***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "        #1)Al posto di salvarmi solo i migliori pesi (i.e.,  model_file = f\"{model_path}/{best_model_name}.pth\")\n",
    "        #  ora mi salvo anche la MIGLIORE configurazione di iper-parametri trovata rispetto alle 15 RUNS di un certo SWEEP\n",
    "        #  di un certo MODELLO, applicato su un DATASET con una SPECIFICA COMBINAZIONE DI FATTORI\n",
    "        #  condizione sperimentale, tipo di dato e provenienza del dato!\n",
    "        \n",
    "    \n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \n",
    "                #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "                #***** AGGIUNTA DELLA CHIAVE CONFIG CHE PRELEVA AUTOMATICAMENTE LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DENTRO 'BEST_MODELS'\n",
    "                \n",
    "                # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                # una certa combinazione di fattori: \n",
    "                # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                \"config\": dict(config)  \n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "            #***** SALVATAGGIO DI UN FILE .PKL, CHE CONTIENE \n",
    "            \n",
    "            # I PESI E BIAS DEL MODELLO DERIVATO DALLA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI OTTENUTA DALLA MIGLIORE RUN DI UN CERTO SWEEP\n",
    "            # IN RELAZIONE AD UN CERTO DATASET COSTITUITO DA UNA CERTA COMBINAZIONE DI FATTORI\n",
    "            \n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            \n",
    "            # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": dict(config)\n",
    "            }, model_file)\n",
    "\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                    \n",
    "                    # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                    # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                    # una certa combinazione di fattori: \n",
    "                    # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                    \"config\": dict(config)  \n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                    torch.save({\n",
    "                        \"state_dict\": best_model.state_dict(),\n",
    "                        \"config\": dict(config)\n",
    "                    }, model_file)\n",
    "                    \n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8598d-f7c6-409d-a501-5b51560a9d5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure Edits - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS - NEW VERSION (PER CNN2D)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c41f24-551d-4037-ae55-12ec1de5f2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE C \n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                W&B SWEEPS AND TRAING LAUNCH WITH MULTIPLE GPUs MANAGEMENT\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE C (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "SPIEGAZIONE\n",
    "\n",
    "GPU counter: Ho aggiunto un contatore (gpu_counter) che cicla tra le GPU disponibili. \n",
    "\n",
    "In questo modo, il primo sweep sarà eseguito sulla GPU 0, il secondo sulla GPU 1, e così via. \n",
    "Quando il contatore raggiunge il numero di GPU disponibili, torna a 0 per riusare la prima GPU.\n",
    "\n",
    "Rotazione delle GPU: All'interno del loop, per ogni sweep, viene assegnata una GPU diversa. \n",
    "Se ci sono più di 1 GPU, il contatore incrementa, e la variabile CUDA_VISIBLE_DEVICES cambia automaticamente per assegnare la GPU corretta.\n",
    "\n",
    "Esecuzione parallela: Ogni sweep viene eseguito su una GPU separata. Se ci sono 2 GPU, il primo sweep va su GPU 0, il secondo su GPU 1, il terzo su GPU 0, e così via.\n",
    "\n",
    "Risposta alla tua domanda:\n",
    "In questo modo, ogni sweep_id viene eseguito una sola volta, ma su GPU diverse (se disponibili). Non ci sono duplicati dello stesso sweep su entrambe le GPU.\n",
    "\n",
    "\n",
    "DOMANDE SUL NUOVO CODICE\n",
    "\n",
    "1) Gli sweep sono eseguiti già in parallelo giusto?\n",
    "No, in questo caso gli sweep non sono eseguiti in parallelo in modo esplicito tramite il codice che hai scritto.\n",
    "\n",
    "Anche se hai assegnato ciascun sweep a una GPU diversa, il codice esegue sequenzialmente ogni sweep, solo che li distribuisce su GPU differenti in modo rotazionale.\n",
    "Ogni volta che il ciclo passa ad un nuovo sweep, assegna un ID GPU e poi esegue l'agent su quella GPU. Non vengono eseguiti in parallelo a livello di codice.\n",
    "\n",
    "2) O semplicemente in questo modo faccio in modo di distribuire ogni sweep sull'altra GPU rispetto a quella usata dallo sweep precedente\n",
    "per \"ottimizzare\" il carico computazionale di ogni GPU?\n",
    "\n",
    "Esatto! Quello che stai facendo è distribuire i vari sweep su GPU diverse, assicurandoti che ogni sweep venga eseguito su una GPU separata (se ne hai di disponibili).\n",
    "Questo permette di ottimizzare l'uso delle risorse, evitando che una GPU venga sovraccaricata da più sweep. Se il numero di GPU disponibili è maggiore di 1, \n",
    "allora i sweep sono distribuiti sulle diverse GPU, ma ogni sweep sarà ancora eseguito singolarmente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sì, con il codice che hai fornito, stai distribuendo gli sweep tra le diverse GPU, in modo da ottimizzare il carico computazionale e non sovraccaricare una sola GPU.\n",
    "\n",
    "Dettaglio del funzionamento:\n",
    "Distribuzione delle GPU (rotazionale):\n",
    "\n",
    "Quando ci sono più di una GPU, il codice assegna a ciascun sweep una GPU diversa in modo rotazionale.\n",
    "\n",
    "Per ogni ciclo del loop, la variabile gpu_counter determina a quale GPU assegnare il prossimo sweep.\n",
    "\n",
    "Se ci sono 2 GPU, il primo sweep viene eseguito sulla GPU 0, il secondo sulla GPU 1, il terzo di nuovo sulla GPU 0, e così via.\n",
    "\n",
    "Gestione della GPU:\n",
    "\n",
    "Se hai più di una GPU, os.environ[\"CUDA_VISIBLE_DEVICES\"] imposta il dispositivo GPU corrente su cui il codice deve girare (GPU 0 o GPU 1). Questo permette di gestire quale GPU eseguirà l'addestramento per ciascun sweep.\n",
    "\n",
    "Quando num_gpus > 1, il codice alterna l'assegnazione della GPU per ogni sweep, evitando di sovraccaricare una singola GPU con troppe operazioni contemporaneamente.\n",
    "\n",
    "Ottimizzazione del carico computazionale:\n",
    "\n",
    "L'alternanza tra GPU successive per ciascun sweep aiuta a distribuire il carico in modo equilibrato, specialmente se il numero di sweep è alto.\n",
    "\n",
    "In pratica, se hai 2 GPU e 10 sweep da eseguire, ogni GPU eseguirà 5 sweep, evitando di saturare una singola GPU.\n",
    "\n",
    "Parallelizzazione effettiva:\n",
    "Nel tuo codice non c'è parallelismo vero e proprio tra le GPU (come quello che otterresti con DataParallel o DistributedDataParallel), ma piuttosto distribuisci gli sweep su GPU diverse. Ogni sweep è eseguito sequenzialmente su una GPU diversa, ma non stai parallelizzando il training dello stesso sweep su più GPU.\n",
    "\n",
    "Se volessi fare in modo che un singolo sweep fosse parallelizzato su più GPU (in modo che il lavoro venga diviso tra le GPU per uno stesso sweep), dovresti usare DataParallel o DistributedDataParallel, come descritto in precedenza. Ma in questo caso, ogni sweep è isolato e viene eseguito su una sola GPU, quindi non c'è un vero parallelismo dentro ogni singolo sweep.\n",
    "\n",
    "Domande che potrebbero sorgere:\n",
    "Gli sweep vengono eseguiti in parallelo?\n",
    "\n",
    "No, i sweep sono eseguiti uno alla volta, ma su GPU diverse. Quindi, mentre il primo sweep usa la GPU 0, il secondo usa la GPU 1 e così via. Ogni sweep viene gestito separatamente, ma sfrutti più GPU per parallelizzare l'esecuzione di più sweep contemporaneamente.\n",
    "\n",
    "La distribuzione delle GPU tra gli sweep è ottimizzata?\n",
    "\n",
    "Sì, stai bilanciando il carico computazionale tra le GPU, assegnando a ogni GPU uno sweep alternato. Se hai molte GPU, puoi ottimizzare ulteriormente distribuendo i sweep su più dispositivi.\n",
    "\n",
    "Se volessi parallelizzare più agenti W&B su diverse GPU, il codice che stai utilizzando sarebbe corretto, ma per ottimizzare ulteriormente i tempi di esecuzione, potresti prendere in considerazione anche l'utilizzo di tecniche come DataParallel o DistributedDataParallel per far sì che più GPU lavorino contemporaneamente sullo stesso sweep. Ma la logica che hai già implementato va bene per distribuire più sweep tra le GPU.\n",
    "\n",
    "Se hai bisogno di ulteriori dettagli su come implementare il parallelismo vero e proprio (inclusi DataParallel o DistributedDataParallel), fammi sapere!\n",
    "\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "\n",
    "# Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "    def train_wrapper():\n",
    "\n",
    "        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "        training_sweep(\n",
    "            data_dict_preprocessed, \n",
    "            sweep_config,\n",
    "            sweep_ids,\n",
    "            sweep_id,\n",
    "            sweep_tuple,\n",
    "            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "        )\n",
    "    return train_wrapper\n",
    "                        \n",
    "                \n",
    "# Verifica quante GPU sono disponibili\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "# Crea un contatore per assegnare un GPU diversa a ciascun sweep\n",
    "gpu_counter = 0\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, combination_key = sweep_tuple\n",
    "                \n",
    "                # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                \n",
    "                # Se ci sono più di 1 GPU, assegna a ciascuna GPU uno sweep diverso\n",
    "                if num_gpus > 1:\n",
    "                    \n",
    "                    # Assegna la GPU in modo rotazionale\n",
    "                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_counter)\n",
    "                    \n",
    "                    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                    #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project=f\"{condition}_spectrograms_channels_freqs\", count=100)\n",
    "                    \n",
    "                    wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", count=200)\n",
    "                    \n",
    "                    # Passa alla prossima GPU per il prossimo sweep\n",
    "                    gpu_counter = (gpu_counter + 1) % num_gpus\n",
    "\n",
    "                else:\n",
    "                    # Se c'è una sola GPU, esegui il sweep sulla GPU 0\n",
    "                    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "                    \n",
    "                    agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                    \n",
    "                    wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject), project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", count=200)\n",
    "\n",
    "                    \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                #def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject):\n",
    "                    #def train_wrapper():\n",
    "                        \n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "                        #print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "                        #training_sweep(\n",
    "                            #data_dict_preprocessed, \n",
    "                            #sweep_config,\n",
    "                            #sweep_ids,\n",
    "                            #sweep_id,\n",
    "                            #sweep_tuple,\n",
    "                            #best_models  # Best models viene aggiornato all'interno della funzione\n",
    "                        #)\n",
    "                    #return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                '''COMMENTATO'''\n",
    "                #agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "                \n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                   ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "                \n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                \n",
    "                '''COMMENTATO'''\n",
    "                #wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs_new_2d_grid_multiband_topomap\", count=15)\n",
    "                \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f068127-23fa-4504-b769-012be30297ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Sweep Configuration - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS**\n",
    "\n",
    "\n",
    "#### **Sweep separati per ciascuno dei modelli CNN3D e CNN Sep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f24b2-b780-42a6-8fad-7824124e9f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "N.B. \n",
    "\n",
    "PER SAPERE A QUALE COMBINAZIONE DI FATTORI CORRISPONDONO I DATI (i.e, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "MI CREO UN DIZIONARIO ULTERIORE, 'DATA_DICT_PREPROCESSED' CHE CONTIENE PER OGNI COMBINAZIONE DI FATTORI I DATI SPLITTATI\n",
    "\n",
    "IN QUESTO MODO, QUANDO FORNISCO ALLA FUNZIONE 'TRAINING_SWEEP' LA TUPLA CON I VARI DATI ((TRAIN, VAL E TEST))\n",
    "IO POSSO CAPIRE A QUALE COMBINAZIONI DI FATTORI CORRISPONDE QUELLA TUPLA DI DATI (TRAIN, VAL E TEST)\n",
    "\n",
    "\n",
    "INOLTRE,\n",
    "MI CREO ANCHE UNA LISTA DI TUPLE DI STRINGHE, DOVE OGNI TUPLA CONTIENE LE STRINGHE DELLE CHIAVI USATE \n",
    "PER LA GENERAZIONE DI DATA_DICT_PREPROCESSED.\n",
    "\n",
    "IN QUESTO MODO, MI ASSICURO CHE SIA UNA COERENZA TRA LA CREAZIONE DEI 'NAME' E 'TAG' DELLA RUN\n",
    "E\n",
    "LA CORRETTA ESTRAZIONE DEI DATI (OSSIA I DATI DI QUALE CONDIZIONE SPERIMENTALE, QUALI EEG INPUT, E DA CHI PROVENGONO!)  \n",
    "\n",
    "\n",
    "Questo approccio permette di garantire la corrispondenza tra \n",
    "\n",
    "1) le chiavi dei dati pre‐processati e \n",
    "2) la configurazione delle runs su W&B\n",
    "\n",
    "andando a creare due strutture in parallelo:\n",
    "\n",
    "- data_dict_preprocessed – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "                            la tupla dei dati già suddivisi (X_train, X_val, X_test, y_train, y_val, y_test);\n",
    "                            \n",
    "- sweeps_id – che contiene, per ogni combinazione (condition, data_type, category_subject), \n",
    "              sia la stringa univoca dello sweep ID, che l'insieme delle stringhe che formano la combinazione (condition, data_type, category_subject)\n",
    "\n",
    "\n",
    "\n",
    "LOOP DI PREPARAZIONE DATI (FINO A DATASET SPLITTING)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#A QUESTO PUNTO PER OGNI DATASET, FACCIO STEP PRIMA DELLO SWEEP\n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Seleziona il dispositivo (GPU o CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Modelli che useremo nei sweep\n",
    "MODEL_LIST = [\"CNN3D_LSTM_FC\", \"SeparableCNN2D_LSTM_FC\"]\n",
    "\n",
    "\n",
    "# Dizionario per salvare gli sweep ID associati a ogni condizione sperimentale\n",
    "\n",
    "'''sweep_ids_for_models contiene la struttura che mi serve da copiare per best_models''' \n",
    "sweep_ids_for_models = {}\n",
    "\n",
    "'''sweep_ids contiene la struttura che mi serve da copiare per iterare sui singoli swweps di ogni combinazione di fattori'''\n",
    "sweep_ids = {}  \n",
    "\n",
    "'''DIZIONARIO CHE VIENE FORNITO IN INGRESSO A TRAINING_SWEEP'''\n",
    "# Dizionario per salvare la tupla di dati già preprocessati\n",
    "data_dict_preprocessed = {}\n",
    "\n",
    "\n",
    "# Loop di addestramento e test per ogni condizione sperimentale\n",
    "for condition, data_types in data_dict.items():  # Itera sulle condizioni sperimentali\n",
    "    \n",
    "    data_dict_preprocessed[condition] = {}\n",
    "    \n",
    "    # Aggiungi al dizionario sweep_ids\n",
    "    if condition not in sweep_ids:\n",
    "        sweep_ids[condition] = {}\n",
    "        \n",
    "        '''sweep_ids_for_models'''\n",
    "        sweep_ids_for_models[condition] = {}\n",
    "        \n",
    "    for data_type, categories in data_types.items():  # Itera sui tipi di dati (1_20, 1_45, wavelet)\n",
    "        \n",
    "        data_dict_preprocessed[condition][data_type] = {}\n",
    "        \n",
    "        if data_type not in sweep_ids[condition]:\n",
    "            sweep_ids[condition][data_type] = {}\n",
    "            \n",
    "            '''sweep_ids_for_models'''\n",
    "            sweep_ids_for_models[condition][data_type] = {}\n",
    "            \n",
    "        for category_subject, (X_data, y_data) in categories.items():  # Itera sulle coppie category_subject\n",
    "            \n",
    "            # 1. Prepara spazio nei dizionari: sotto category_subject, un dict per ogni modello\n",
    "            \n",
    "            data_dict_preprocessed[condition][data_type][category_subject] = None\n",
    "            \n",
    "            if category_subject not in sweep_ids[condition][data_type]:\n",
    "                \n",
    "                sweep_ids[condition][data_type][category_subject] = {}\n",
    "                \n",
    "                '''NUOVA MODIFICA'''\n",
    "                sweep_ids[condition][data_type][category_subject] = {\n",
    "                model: [] for model in MODEL_LIST\n",
    "                }\n",
    "\n",
    "                '''sweep_ids_for_models'''\n",
    "                sweep_ids_for_models[condition][data_type][category_subject] = {}\n",
    "                \n",
    "                '''NUOVA MODIFICA'''\n",
    "                sweep_ids_for_models[condition][data_type][category_subject] = {\n",
    "                model: [] for model in MODEL_LIST\n",
    "                }\n",
    "                \n",
    "            print(f\"\\n\\n\\033[1mEstrazione Dati\\033[0m della Chiave \\033[1m{condition}_{data_type}_{category_subject}\\033[0m\")\n",
    "            \n",
    "            # Controlla se il dataset è già stato elaborato (se la chiave è già nel set)\n",
    "            if (condition, data_type, category_subject) in processed_datasets:\n",
    "                print(f\"⚠️ ATTENZIONE: Il dataset {condition} - {data_type} - {category_subject} è già stato elaborato! Salto iterazione...\")\n",
    "                continue  # Salta se il dataset è già stato processato\n",
    "\n",
    "            # Aggiungi il dataset al set\n",
    "            processed_datasets.add((condition, data_type, category_subject))\n",
    "\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "            \n",
    "            data_dict_preprocessed[condition][data_type][category_subject] = (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "            \n",
    "            # Puoi anche aggiungere altri print per verificare la dimensione dei set\n",
    "            print(f\"\\033[1mDataset Splitting\\033[0m: Train Set Shape: {X_train.shape}, Validation Set Shape: {X_val.shape}, Test Set Shape: {X_test.shape}\")\n",
    "\n",
    "            \n",
    "print(f\"\\nCreato \\033[1mdata_dict_preprocessed\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92d055-a13c-4d8c-a665-47ede4f9b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['th_resp_vs_shared_resp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385873e-d6f0-4b24-8f9a-63f1bc8d1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_shared_resp'].keys())\n",
    "print(data_dict_preprocessed['th_resp_vs_shared_resp']['spectrograms'].keys())\n",
    "print(type(data_dict_preprocessed['th_resp_vs_shared_resp']['spectrograms'].keys()))\n",
    "\n",
    "#All'interno, c'è una tupla, di 6 elementi!\n",
    "print(type(data_dict_preprocessed['th_resp_vs_shared_resp']['spectrograms']['familiar_th']))\n",
    "\n",
    "#I 6 elementi della tupla sono X_train, X_val, X_test, y_train, y_val, y_test !\n",
    "print(len(data_dict_preprocessed['th_resp_vs_shared_resp']['spectrograms']['familiar_th']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebd62f-b3ef-44e5-8ec5-3f6df4fc50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8907f7-af5c-4de3-b0b6-aaa67aba031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f00125-2c4b-4a31-904a-4bada2486db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids_for_models)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8737fdc-262c-4a85-99cf-f2d96b1c48bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        #\"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "        \n",
    "        \"lr\": {\"values\": [1e-3]}, # fissato al valore di default del paper\n",
    "        \n",
    "        #\"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "        \n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \n",
    "        #\"model_name\":{\"values\": ['CNN2D', 'BiLSTM', 'Transformer']},\n",
    "        \n",
    "        #\"model_name\":{\"values\": ['CNN2D_LSTM_FC', 'BiLSTM', 'Transformer']},\n",
    "        \"model_name\":{\"values\": ['CNN2D_LSTM_FC']},\n",
    "        \n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 72, 84, 96]},\n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 72, 84]},\n",
    "        \n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 72, 84, 96]},\n",
    "        #\"batch_size\": {\"values\": [16, 24, 32, 48, 64]},\n",
    "        \"standardization\":{\"values\": [True, False]},\n",
    "        \n",
    "        \"beta1\": {\"value\": 0.9},\n",
    "        \"beta2\": {\"value\": 0.999},\n",
    "        \"eps\": {\"value\": 1e-8},\n",
    "        \n",
    "    }\n",
    "}\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "for condition in sweep_ids_for_models:\n",
    "    for data_type in sweep_ids_for_models[condition]:\n",
    "        for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4aebc7-74f3-4e9a-b0fa-2153fd031e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "                                                                    AGGIORNATA AL 19 LUGLIO\n",
    "                                                                    \n",
    "                                                                    \n",
    "#\"learning rate : {\"value\"[1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\"}\n",
    "#\"n_epochs\": {\"value\": 100},\n",
    "# \"patience\": {\"value\": 12},\n",
    "#\"batch_size\": {\"values\": [16, 24, 32, 48, 64, 72, 84, 96]}\n",
    "#\"standardization\": {\"values\": [True, False]}, \n",
    "# \"beta1\": {\"values\": [0.8, 0.85, 0.9, 0.95]},\n",
    "#  \"beta2\": {\"values\": [0.98, 0.99, 0.995, 0.999]},\n",
    "#  \"eps\": {\"value\": [1e-8, 1e-7, 1e-6, 1e-5]}                                                                                                                            \n",
    "\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]}, # fissato al valore di default del paper\n",
    "\n",
    "        \"weight_decay\":  {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \n",
    "        \n",
    "        \"model_name\":{\"values\": ['CNN3D_LSTM_FC']},\n",
    "\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "\n",
    "        \"standardization\":{\"values\": [True, False]},\n",
    "\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        #In questo modo:\n",
    "        \n",
    "        \"use_lstm\":      {\"values\":[True, False]},\n",
    "        \"lstm_hidden\":   {\"values\":[32]},\n",
    "        \"dropout\":       {\"values\":[0.5]},\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#Tutti gli sweep saranno organizzati sotto lo stesso progetto,\n",
    "#che corrisponde alla coppia di condizioni sperimentali corrente (i.e., exp_cond).\n",
    "\n",
    "#Questo significa che tutte le runs che verranno lanciate con quello sweep, \n",
    "#saranno associate a quella specifica coppia di condizioni sperimentali corrente.\n",
    "\n",
    "#Dato che sto iterando su ogni coppia di condizioni sperimentali, \n",
    "#ogni sweep verrà automaticamente salvato all'interno del progetto corrispondente \n",
    "#della specifica condizione sperimentale (exp_cond).\n",
    "\n",
    "#In pratica, se hai più condizioni sperimentali \n",
    "#(ad esempio, \"Condizione_A\", \"Condizione_B\", ecc.),\n",
    "#WandB creerà automaticamente sweep separati all'interno dei rispettivi progetti\n",
    "\n",
    "\n",
    "#Creo la configurazione dello sweep e la eseguo:\n",
    "\n",
    "#uno per il modello CNN3D_LSTM_FC, uno oer \n",
    "\n",
    "\n",
    "# 2.1 – Sweep config per ciascun modello\n",
    "sweep_config_cnn3d = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN3D_LSTM_FC\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"use_lstm\": {\"values\": [True, False]},\n",
    "        \"lstm_hidden\": {\"values\": [32]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config_cnn_sep = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"SeparableCNN2D_LSTM_FC\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"values\": [True]}, #        '''ATTENZIONE QUI IMPOSTIAMO SEMPRE A TRUE'''\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"use_lstm\": {\"values\": [True, False]},\n",
    "        \"lstm_hidden\": {\"values\": [32]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "'''SWEEP_IDS_FOR_MODELS\n",
    "\n",
    "# 2) Popolo sweep_ids_for_models in base a MODEL_LIST (già inizializzato nella prima cella)\n",
    "'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids_for_models (lo aggiorno inserendo il livello delle chiavi dei modelli, per copiare poi la struttura per creare best_models)\n",
    "\n",
    "#for condition in sweep_ids_for_models:\n",
    "    #for data_type in sweep_ids_for_models[condition]:\n",
    "        #for category_subject in sweep_ids_for_models[condition][data_type]:\n",
    "            #for model_name in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "                \n",
    "                # Aggiungi il modello al dizionario, se non esiste già\n",
    "                #if model_name not in sweep_ids_for_models[condition][data_type][category_subject]:\n",
    "                    #sweep_ids_for_models[condition][data_type][category_subject][model_name] = []\n",
    "\n",
    "                    \n",
    "print(f\"\\nAggiornato \\033[1msweep_ids_for_models\\033[0m\")\n",
    "\n",
    "\n",
    "'''BEST_MODELS\n",
    "\n",
    "# 3) Creo best_models da sweep_ids_for_models\n",
    "'''\n",
    "\n",
    "#Preparazione del dizionario best_models (facendo una copia della struttura di 'sweep_ids_for_models')\n",
    "\n",
    "#In questo modo potrò, per ogni condizione sperimentale, tipo di dato EEG e combinazione di ruolo/gruppo,\n",
    "#accedere facilmente al miglior modello (cioè ai suoi pesi e bias) e gestirlo in maniera separata!\n",
    "\n",
    "import copy\n",
    "best_models = copy.deepcopy(sweep_ids_for_models)\n",
    "\n",
    "# Inizializzo il dizionario che contiene il migliori modello tra quelli degli sweep testati, \n",
    "# relativi ad una certa combinazione di fattori,\n",
    "#per ogni condizione sperimentale\n",
    "#tipo di dato EEG \n",
    "#combinazione di ruolo/gruppo\n",
    "\n",
    "for condition in best_models:\n",
    "    for data_type in best_models[condition]:\n",
    "        for category_subject in best_models[condition][data_type]:\n",
    "            for model_name in best_models[condition][data_type][category_subject]:\n",
    "                best_models[condition][data_type][category_subject][model_name] = {\n",
    "                    \"model\": None,\n",
    "                    \"max_val_acc\": -float('inf'),\n",
    "                    \"best_epoch\": None,\n",
    "                    \n",
    "                    #ATTENZIONE! CREATA ALTRA CHIAVE PER SALVARE \n",
    "                    #LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI OGNI MODELLO!\n",
    "                    \"config\": None}\n",
    "                \n",
    "print(f\"\\nCreato \\033[1mbest_models\\033[0m\")\n",
    "\n",
    "\n",
    "#'''SWEEP_IDS'''\n",
    "\n",
    "#Preparazione del dizionario sweep_ids (lo aggiorno inserendo solo una lista all'ultimo livello)\n",
    "\n",
    "# Itera su sweep_ids e crea le chiavi per category_subject con liste vuote\n",
    "#for condition in sweep_ids:\n",
    "    #for data_type in sweep_ids[condition]:\n",
    "        #for category_subject in sweep_ids[condition][data_type]:\n",
    "            # Inizializza una lista vuota se non esiste già\n",
    "            #if not isinstance(sweep_ids[condition][data_type][category_subject], list):\n",
    "                #sweep_ids[condition][data_type][category_subject] = []\n",
    "                    \n",
    "#print(f\"\\nAggiornato \\033[1msweep_ids\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0dfd9-1485-48d6-9004-a69c90af44fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(best_models)\n",
    "#print(sweep_ids_for_models)\n",
    "#print(sweep_ids)\n",
    "#print(data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf5aca-cd6c-4819-875f-f21fd0bb55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bf85b-b4e7-4e91-bc16-6faebcf99c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b5609-8cd8-4afc-9e10-42ea9696b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cb15a-13ab-4219-86a4-ca67eaaf4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict_preprocessed['th_resp_vs_pt_resp']['1_20']['familiar_th'][5].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fdd54-58cc-4a2b-8295-6cb2cbfb7472",
   "metadata": {
    "tags": []
   },
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "Come output, io otterrò **quando crei gli sweeps** una cosa come questa, ad esempio:\n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw\n",
    "        Create sweep with ID: 3b6o28jt\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/3b6o28jt\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - BiLSTM: n° sweep 3b6o28jt\n",
    "        Create sweep with ID: q6yp4fas\n",
    "\n",
    "        .....\n",
    "\n",
    "Vedendole bene, per **ogni condizione sperimentale (3)**, **per ogni dato EEG (3)** e **per ogni provenienza del dato EEG (4)**, \n",
    "Io **DOVREI OTTENERE** in totale = **3x3x4 = 36 sweeps** per **OGNI CONDIZIONE SPERIMENTALE**\n",
    "\n",
    "\n",
    "Per **ognuna di queste sweeps**, io se ho capito bene creerò **15 esperimenti** (le mie runs), che corrispondo alle **diverse configurazioni di iper-parametri testati per lo stesso specifico sweep**!\n",
    "\n",
    "(ad esempio, solo questo \n",
    "\n",
    "<br> \n",
    "\n",
    "        Create sweep with ID: y73iajvw\n",
    "        Sweep URL: https://wandb.ai/stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp/sweeps/y73iajvw\n",
    "        Sweep ID creato per th_resp_vs_pt_resp - 1_20 - familiar_th - CNN1D: n° sweep y73iajvw)\n",
    "\n",
    "Dove, le diverse configurazioni, son determinate randomicamente a partire dai valori dentro la variabile \"**sweep_config**\"  che è questa \n",
    "\n",
    "\n",
    "    #Creo la configurazione dello sweep e la eseguo\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"lr\": {\"values\": [0.01, 0.001, 0.0005, 0.0001]},\n",
    "            \"weight_decay\": {\"values\": [0, 0.01, 0.001, 0.0001]},\n",
    "            \"n_epochs\": {\"value\": 100},\n",
    "            \"patience\": {\"value\": 10},\n",
    "            \"model_name\":{\"values\": ['CNN1D', 'BiLSTM', 'Transformer']},\n",
    "            \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "            \"standardization\":{\"values\": [True, False]},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746dd38-4253-4417-ad10-40259fc5c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ATTENZIONE CHE A QUESTO PUNTO\n",
    "\n",
    "\n",
    "1) sweep_ids[cond][dtype][cat][model_name] contiene le tuple (sweep_id, combo_key) per ciascun modello, che ancora non esistono perché devo esser create durante la creazione degli sweeps, ma ho solo una lista\n",
    "\n",
    "{'rest_vs_left_fist': {'spectrograms': {'familiar_th': []}}, \n",
    "'rest_vs_right_fist': {'spectrograms': {'familiar_th': []}}, \n",
    "'left_fist_vs_right_fist': {'spectrograms': {'familiar_th': []}}}\n",
    "\n",
    "\n",
    "2) sweep_ids_for_models e best_models sono paralleli a sweep_ids con lo stesso livello model_name\n",
    "\n",
    "ossia \n",
    "\n",
    "sweep_ids_for_models come\n",
    "\n",
    "{'rest_vs_left_fist': {'spectrograms': {'familiar_th': {'CNN3D_LSTM_FC': [], 'SeparableCNN2D_LSTM_FC': []}}},\n",
    "'rest_vs_right_fist': {'spectrograms': {'familiar_th': {'CNN3D_LSTM_FC': [], 'SeparableCNN2D_LSTM_FC': []}}},\n",
    "'left_fist_vs_right_fist': {'spectrograms': {'familiar_th': {'CNN3D_LSTM_FC': [], 'SeparableCNN2D_LSTM_FC': []}}}}\n",
    "\n",
    "best_models come\n",
    "\n",
    "{'rest_vs_left_fist': {'spectrograms': {'familiar_th': \n",
    "{'CNN3D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}, \n",
    "'SeparableCNN2D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}}}},\n",
    "\n",
    "'rest_vs_right_fist': {'spectrograms': {'familiar_th': \n",
    "{'CNN3D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}, \n",
    "'SeparableCNN2D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}}}}, \n",
    "\n",
    "'left_fist_vs_right_fist': {'spectrograms': {'familiar_th': \n",
    "{'CNN3D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None},\n",
    "'SeparableCNN2D_LSTM_FC': {'model': None, 'max_val_acc': -inf, 'best_epoch': None, 'config': None}}}}}\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a75f5-d555-4beb-b9e0-840e15854a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Popolamento di sweep_ids e lancio degli agenti:\n",
    "\n",
    "Obiettivo: \n",
    "\n",
    "Per ogni combinazione (condition, data_type, category_subject, model_name), \n",
    "Se la lista è vuota, crei uno sweep usando wandb.sweep(sweep_config, project=condition) e lo inserisci nella lista. \n",
    "In seguito, iteri su quella lista (che ora contiene IL TUO SPECIFICO sweep_id) e lanci wandb.agent() per eseguire il training.\n",
    "\n",
    "\n",
    "\n",
    "Nota importante:\n",
    "L'ID restituito da wandb.sweep() è una STRINGA UNIVOCA generata automaticamente da WandB.\n",
    "Non puoi assegnargli direttamente una stringa personalizzata, ma puoi comunque usarlo per mappare nel tuo dizionario la combinazione di fattori! \n",
    "\n",
    "In questo ciclo, il fatto che la lista parta vuota è normale: il codice la popola se necessario e poi lancia l'agente per ogni sweep_id presente.\n",
    "\n",
    "\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "INOLTRE, BISOGNA CONTROLLARE CHE SI STIA ITERANDO CORRETTAMENTE SOLO SULLA COMBINAZIONE CORRENTE DI \n",
    "\n",
    "                CONDITION, DATA_TYPE, CATEGORY_SUBJECT E MODEL_NAME\n",
    "                \n",
    "QUESTO PERCHÉ SE UN CICLO SI RIPETE PER UNA CONDIZIONE IN PIÙ UNA COMBINAZIONE, POTREBBE GENERARE PIÙ  SWEEP IDS DI QUELLI CHE TI ASPETTI!\n",
    "****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******  ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "SOLUZIONE:\n",
    "\n",
    "Un buon approccio per evitare la creazione ripetuta di Sweep ID \n",
    "per la stessa combinazione di condition, data_type, category_subject e model_name \n",
    "è quello di utilizzare un SET per tenere traccia delle combinazioni già processate.\n",
    "Se una combinazione è già presente nel set, non dovresti creare un nuovo Sweep ID, ma semplicemente saltare quella parte del codice\n",
    "\n",
    "\n",
    "Inoltre, ho avuto una idea ad un certo punto! \n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "\n",
    "Quando creo ogni sweep singolarmente, si genera una stringa univoca di quello sweep, che si riferisce ad un dataset che è il prodotto di diversi fattori:\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Di conseguenza, iterando su ogni sweep_ids (che ho fatto in modo avesse la STESSA struttura dei miei dati già splittati i.e, data_dict_preprocessed\n",
    "io posso, \n",
    "\n",
    "1) da un lato eseguire la creazione della stringa univoca associata a quello sweep,\n",
    "2) crearmi una 'combination_key', che sarebbe l'insieme delle stringhe che descrivono quel dataset specifico di data_dict_preprocessed\n",
    "\n",
    "che sarà costituito da\n",
    "\n",
    "- una certa condizione sperimentale,  \n",
    "- una certo preprocessing sui dati EEG (1_20, 1_45, wavelet)\n",
    "- una certa provenienza del dato proprio (in termini di ruolo e gruppo --> th o pt, familiar o unfamiliar)\n",
    "\n",
    "\n",
    "Poiché quindi so già la corrispondenza tra ogni Sweep ID e la sua combinazione di fattori (condition, data_type, category_subject), \n",
    "posso creare un MAPPING, che associ, ad certo Sweep ID e la stringa che descrive i suoi fattori associati!\n",
    "\n",
    "\n",
    "In questo modo, forse, si riesce a risolvere il PROBLEMA 2 NELLA CELLA DI CREAZIONE DELLA FUNZIONE DI TRAINING (VEDI SOTTO!)\n",
    "\n",
    "\n",
    "\n",
    "                                                        ******IMPORTANTE MODIFICA*****\n",
    "                                                        \n",
    "Ora lo sweep_ids non si deve sdoppiare ora, perché sostanzialmente, \n",
    "per ogni modello si creano gli sweeps ids corrispondenti e salvati come valore\n",
    "dentro la chiave del modello corrispondente, sotto forma di tupla...\n",
    "\n",
    "cioè non più così\n",
    "\n",
    "\"sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\"\n",
    "\n",
    "ma una cosa del genere\n",
    "\n",
    "\"sweep_ids[condition][data_type][category_subject][model_name].append((new_sweep_id, combination_key))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COME ERA PRIMA\n",
    "\n",
    "#Inizializza un set per tenere traccia delle combinazioni già elaborate\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\")\n",
    "\n",
    "                    #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                    #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA \n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n",
    "                \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "ADESSO\n",
    "\n",
    "\n",
    "Cosa fa questo snippet\n",
    "\n",
    "Cicla su ogni (condition, data_type, category_subject) una volta sola grazie a created_combinations.\n",
    "\n",
    "All’interno, fa un sottoloop su MODEL_LIST (i tuoi due modelli).\n",
    "\n",
    "In base a model_name, sceglie sweep_config_cnn3d o sweep_config_cnn_sep.\n",
    "\n",
    "Chiama wandb.sweep(...) con il config giusto e salva il risultato in\n",
    "\n",
    "\n",
    "sweep_ids[condition][data_type][category_subject][model_name]\n",
    "anziché nella lista “piatta” che avevi prima.\n",
    "\n",
    "\n",
    "In questo modo:\n",
    "\n",
    "sweep_ids[cond][dtype][cat] resta un dict con due chiavi (\"CNN3D_LSTM_FC\" e \"SeparableCNN2D_LSTM_FC\")\n",
    "\n",
    "Ognuna di quelle chiavi punta a una propria lista di tuple (sweep_id, combo_key)\n",
    "\n",
    "Non serve sdoppiare l’intero sweep_ids, perché tiene già separati gli sweep di ciascun modello\n",
    "\n",
    "Più tardi, quando lancerai gli agent, ti basterà:\n",
    "\n",
    "\n",
    "for model_name, sweeps in sweep_ids[cond][dtype][cat].items():\n",
    "    for sweep_id, combo_key in sweeps:\n",
    "        # qui scegli il train_fn in base a model_name\n",
    "        wandb.agent(sweep_id, function=train_fn_map[model_name], count=200)\n",
    "e ogni modello girerà solo i suoi sweep.\n",
    "\n",
    "\n",
    "\n",
    "Alla fine, sweep_ids avrà la forma:\n",
    "\n",
    "{\n",
    "  'rest_vs_left_fist': {\n",
    "    'spectrograms': {\n",
    "      'familiar_th': {\n",
    "         'CNN3D_LSTM_FC':       [(sweep_id_1, 'rest_vs_left_fist_spectrograms_familiar_th')],\n",
    "         'SeparableCNN2D_LSTM_FC': [(sweep_id_2, 'rest_vs_left_fist_spectrograms_familiar_th')]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  …\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "#Ecco come puoi riscrivere solo la TERZA CELLA (quella in cui crei effettivamente gli sweep) \n",
    "#mantenendo la tua struttura “a celle” e usando per ognuno il sweep_config giusto in base al model_name.\n",
    "\n",
    "#Creazione degli sweep (Terza cella)\n",
    "#Ecco il solo snippet che devi usare per creare gli sweep ripartiti per modello, usando i due sweep_config_*:\n",
    "\n",
    "\n",
    "'''\n",
    "Per mantenere la stessa logica di prima ma tenendo conto che ora stai lavorando con modelli separati, \n",
    "dovresti modificare il controllo in modo che verifichi se una combinazione di condition, data_type, category_subject\n",
    "è già stata processata per ciascun modello.\n",
    "\n",
    "Quindi, il controllo dovrebbe essere fatto separatamente per ogni modello dentro il loop che itera sui modelli (MODEL_LIST).\n",
    "Di seguito ti mostro la versione modificata che tiene conto di questo:\n",
    "\n",
    "\n",
    "\n",
    "#Inizializza un set per tenere traccia delle combinazioni già elaborate\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms_channels_freqs_new_3d_grid_multiband\")\n",
    "\n",
    "                    #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                    #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA \n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n",
    "                \n",
    "                \n",
    "                \n",
    "'''\n",
    "\n",
    "                    \n",
    "'''\n",
    "\n",
    "Cosa è stato cambiato rispetto alla versione precedente?\n",
    "Controllo della combinazione di modello:\n",
    "La logica del controllo della combinazione (combination_key, model_name) nel set created_combinations è corretta, \n",
    "perché vogliamo evitare di creare più volte lo stesso sweep per una combinazione di condition, data_type, category_subject, e model_name.\n",
    "\n",
    "Controllo e creazione dello sweep:\n",
    "Il codice controlla prima se la combinazione con il modello non è stata già processata \n",
    "con il controllo if (combination_key, model_name) not in created_combinations. \n",
    "\n",
    "Se non è stata processata, procede a creare lo sweep corrispondente. \n",
    "Se la combinazione esiste già, salta la creazione dello sweep per quel modello.\n",
    "\n",
    "Aggiunta del nuovo sweep ID:\n",
    "Una volta creato il nuovo sweep per il modello, viene aggiunto correttamente \n",
    "alla lista del modello specifico sotto sweep_ids[condition][data_type][category_subject][model_name].\n",
    "\n",
    "Aggiunta al set delle combinazioni:\n",
    "Dopo aver creato lo sweep, aggiungiamo (combination_key, model_name) al set created_combinations\n",
    "per tenere traccia delle combinazioni già elaborate.\n",
    "\n",
    "Verifica della logica:\n",
    "La combinazione (combination_key, model_name) deve essere unica per ciascun modello, \n",
    "e quindi il controllo che evita duplicazioni nel set è corretto.\n",
    "\n",
    "La creazione dello sweep per ciascun modello separato è mantenuta, \n",
    "e viene applicata solo quando la combinazione specifica non è già stata elaborata per quel modello.\n",
    "\n",
    "In questo modo, la logica funziona come nel codice precedente, ma ora si tiene conto anche dei modelli separati, \n",
    "creando un sweep per ciascuno di essi e mantenendo la traccia delle combinazioni in modo appropriato.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "COME ERA PER CNN2D\n",
    "\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "\n",
    "            # Controlla se la combinazione è già stata elaborata\n",
    "            if combination_key not in created_combinations:\n",
    "\n",
    "                if not sweep_ids[condition][data_type][category_subject]:\n",
    "                    #new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_spectrograms\")\n",
    "                    \n",
    "                    new_sweep_id = wandb.sweep(sweep_config, project=f\"{condition}_{data_type}_channels_freqs_{category_subject}\")\n",
    "\n",
    "                    #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                     #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA\n",
    "                \n",
    "                    sweep_ids[condition][data_type][category_subject].append((new_sweep_id, combination_key))\n",
    "                    \n",
    "                    print(f\"Sweep ID creato per \\033[1m{combination_key}\\033[0m: n° sweep \\033[1m{new_sweep_id}\\033[0m\")\n",
    "\n",
    "                # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                created_combinations.add(combination_key)\n",
    "            else:\n",
    "                # Se la combinazione è già stata creata, salta\n",
    "                print(f\"Sweep ID per {combination_key} già esistente.\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "created_combinations = set()\n",
    "\n",
    "# Per semplicità, tieni MODEL_LIST a portata di mano\n",
    "MODEL_LIST = [\"CNN3D_LSTM_FC\", \"SeparableCNN2D_LSTM_FC\"]\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "            \n",
    "            # per ciascun modello, creo uno sweep separato\n",
    "            for model_name in MODEL_LIST:\n",
    "\n",
    "                # Controlla se la combinazione di condition, data_type, category_subject + modello è già stata elaborata\n",
    "                if (combination_key, model_name) not in created_combinations:\n",
    "\n",
    "                    # Scegli il config in base al model_name\n",
    "                    if model_name == \"CNN3D_LSTM_FC\":\n",
    "                        sweep_conf = sweep_config_cnn3d\n",
    "                        \n",
    "                    else:  # SeparableCNN2D_LSTM_FC\n",
    "                        sweep_conf = sweep_config_cnn_sep\n",
    "                    \n",
    "                    # Controllo se la lista per il modello specifico è vuota\n",
    "                    if not sweep_ids[condition][data_type][category_subject][model_name]:\n",
    "\n",
    "                        # Crea lo sweep e lo appendo nella lista dedicata a quel modello\n",
    "                        #new_sweep_id = wandb.sweep(sweep_conf, project=f\"{condition}_spectrograms_channels_freqs_new_imagery_3d_grid_multiband\")\n",
    "                        \n",
    "                        new_sweep_id = wandb.sweep(sweep_conf, project=f\"{condition}_{data_type}_channels_freqs_{category_subject}\")\n",
    "                        \n",
    "                        #QUI, viene creata la mappatura tra Sweep ID e la descrizione della combinazione (in formato di stringhe)\n",
    "                        #CON LA CREAZIONE DI UNA TUPLA, DENTRO LA LISTA \n",
    "                        \n",
    "                        sweep_ids[condition][data_type][category_subject][model_name].append((new_sweep_id, combination_key))\n",
    "\n",
    "                    print(f\"▶ Sweep \\033[1m{new_sweep_id}\\033[0m creato per \\033[1m{combination_key}\\033[0m, modello \\033[1m{model_name}\\033[0m\")\n",
    "                    \n",
    "                    # Aggiungi la combinazione al set per evitare duplicazioni\n",
    "                    created_combinations.add((combination_key, model_name))  # Aggiungi la combinazione con il modello\n",
    "                else:\n",
    "                    # Se la combinazione è già stata creata, salta\n",
    "                    print(f\"⚠️ {combination_key} già processato per il modello {model_name}, skip.\")\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba077514-e9b5-41f7-9fef-96351a466526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola e stampa il numero totale di combinazioni uniche (e quindi di sweep creati)\n",
    "\n",
    "total_sweeps = len(created_combinations)\n",
    "total_runs = total_sweeps * 200\n",
    "\n",
    "print(f\"Numero totale di sweep creati: {total_sweeps}\")\n",
    "print(f\"Numero totale di runs da eseguire: {total_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83e983-f03f-4549-8b6d-bd3a635fddb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''ESEGUI QUI QUESTA CELLA PER VEDERE COME SI STRUTTURA SWEEP_IDS'''\n",
    "\n",
    "#sweep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9485f-2954-4d1f-b697-f556750f8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_ids.keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()\n",
    "#sweep_ids['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bdab41-3947-4d36-9719-9856c2cd51fc",
   "metadata": {},
   "source": [
    "**NOTA BENE**\n",
    "\n",
    "\n",
    "I **numeri degli sweeps** tornano e son corretti! \n",
    "Tuttavia, avendo solo preparato l'inizializzazione degli sweeps dentro 'sweep_ids', \n",
    "Sul sito di weight and biases, io vedo le tre condizioni sperimentali, create ciascuna come un progetto separato, che è corretto, ma ancora le runs di ciascuna le vedo a 0\n",
    "\n",
    "Deduco che questo comportamento, dovrebbe esser normale, dato che ancora non ho avviato l'agente appunto wandb.agent(), con cui gli fornisco lo sweep_id generato adesso in questo loop precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9916c-2e32-4217-898d-268efd6f5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict_preprocessed.keys())\n",
    "print(sweep_ids.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f108c-099e-4124-902d-1eac1158575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024bdca-a5fe-4568-98bd-a722d71e6620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dict_preprocessed['th_resp_vs_pt_resp'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9ba1c-9234-461d-be41-43eb83e1bbf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VERSIONE DEL 6 MARZO (RISOLUZIONE DEFINITIVA) NEW VERSION (PER CNN3D e CNN2D Sep)**\n",
    "\n",
    "##### **Training Function Edits - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d98abe-7d3d-42a3-8344-f57d3ddac916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "                                                                ***** FUNZIONE DI TRAINING *****\n",
    "                                                                ***** VERSIONE DEL 5 MARZO *****\n",
    "                                                                \n",
    "                                                                    **** SALVATAGGIO DI **** \n",
    "                                                        \n",
    "                                                        1) PESI E BIAS DI UN CERTO MODELLO \n",
    "                                                        2) CONFIGURAZIONE IPER-PARAMETRI DI UN CERTO MODELLO\n",
    "                                                                \n",
    "Il punto critico è garantire che ogni configurazione di iperparametri estratta randomicamente da W&B per OGNI SWEEP sia coerente con:\n",
    "\n",
    "Il dataset giusto (ossia la coppia di condizioni sperimentali corrispondente).\n",
    "Il tipo di dato EEG usato (1_20, 1_45, wavelet ecc.).\n",
    "L'origine dei dati tra le quattro tipologie di soggetti.\n",
    "\n",
    "\n",
    "che io andrei a prelevare ogni volta da 'data_dict_preprocessed'!\n",
    "\n",
    "Quindi, ad ogni iterazione del loop sui dati (i.e., data_dict_preprocessed?)\n",
    "il codice dovrebbe assicurarsi/verificare che, \n",
    "\n",
    "\n",
    "1) la configurazione selezionata da W&B presa da uno SPECIFICO SWEEP,  \n",
    "sia quella che effettivamente corrisponde ad un certo dataset in termini di combinazione di fattori \n",
    "\n",
    "- una specifica condizione sperimentale\n",
    "- una specifico tipo di dato EEG \n",
    "- una specifica combinazione di ruolo/gruppo\n",
    "\n",
    "\n",
    "2) che le run di quella sweep siano inserita nel progetto del dataset di quella specifica condizione sperimentale,\n",
    "\n",
    "\n",
    "(3 PLUS OPZIONALE\n",
    "\n",
    "e che il \"name\" e i \"tag\" (eventualmente, delle runs associate a quello sweep)\n",
    "siano costruiti in maniera coerente con la combinazione di fattori associata allo sweep (e quindi alla condizione sperimentale corrente)\n",
    "\n",
    "\n",
    "\n",
    "****************************** ******************************\n",
    "CONCLUSIONE A CUI SON ARRIVATO LA MATTINA DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "Dato che ogni sweep si applica per verificare, tra le 15 diversi set di iper-parametri diversi, \n",
    "quale sia la configurazione migliore, per uno specifico set di dati in termini di combinazione di fattori, che sono\n",
    "\n",
    "- relativi ad una certa condizione sperimentale,  \n",
    "- con un certo preprocessing\n",
    "- con un certa provenienza del dato\n",
    "\n",
    "\n",
    "Son arrivato ad un punto in cui credo che sia davvero molto complesso controllare la corrispondenza esatta tra \n",
    "\n",
    "1) di chi esegue lo sweep\n",
    "2) la definizione del nome della sue 15 runs (cioè di quale dato si riferisca etc. in termini di combinazione di fattori) ...\n",
    "\n",
    "Quindi l'unica cosa che ha senso è forse solo creare le runs in modo da inserirle tutte assieme in base al solo nome del progetto,\n",
    "che però è prelevabile dalla prima chiave di 'data_dict_preprocessed'.. \n",
    "\n",
    "in questo modo, pur non avendo il controllo sul nome della run e del suo tag,\n",
    "almeno dovrei esser sicuro che comunque le runs associate all'uso dei dati di ALMENO \n",
    "una certa condizione sperimentale vengano inserite nel relativo progetto su weight and biases...\n",
    "\n",
    "\n",
    "\n",
    "TUTTAVIA, \n",
    "\n",
    "****************************** ******************************\n",
    "ILLUMINAZIONE DEL POMERIGGIO DEL 04/03/2025: \n",
    "****************************** ******************************\n",
    "\n",
    "MI HA PORTATO A PENSARE A PROVARE A CAPIRE ANCORA SE RIESCO A RISOLVERE IL PROBLEMA ...\n",
    "'''\n",
    "\n",
    "\n",
    "#VERSIONE NUOVA!\n",
    "\n",
    "#Fase 2: Creazione della funzione di 'training_sweep' \n",
    "    \n",
    "'''Questa funzione parse_combination_key serve per estrarre \n",
    "le varie stringhe che compongono la combinazioni di fattori (condizione sperimentale, tipo di dato EEG e provenienza del dato EEG) \n",
    "che si riferiscono allo sweep ID corrente.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "Lo tupla sweep (sweep ID, combinazioni di fattori in stringa) è la seguente:\n",
    "\n",
    "Inizio l'agent per sweep_id: ('4u94ovth', 'pt_resp_vs_shared_resp_wavelet_unfamiliar_pt') dove\n",
    "- sweep ID: 4u94ovth\n",
    "- combinazioni di fattori in stringa: pt_resp_vs_shared_resp_wavelet_unfamiliar_pt\n",
    "\n",
    "Di conseguenza, quando avvio l'agent per quella condizione sperimentale nel loop, \n",
    "dentro la funzione di 'training_sweep' io prenderò in input la tupla\n",
    "\n",
    "\n",
    "\"\"\" Esegue il training per uno specifico sweep \"\"\"\n",
    "\n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "\n",
    "sweep_id, combination_key = sweep_tuple\n",
    "exp_cond, data_type, category_subject = parse_combination_key(combination_key)\n",
    "\n",
    "\n",
    "E lui estrarrà la combinazione di fattori che la compongono, in questo caso è \n",
    "\n",
    "1) Condizione Sperimentale = pt_resp_vs_shared_resp\n",
    "2) Tipo di Dato EEG = wavelet\n",
    "3) Provenienza del Tipo di Dato EEG unfamiliar_pt\n",
    "\n",
    "Successivamente, confronta se questa combinazione di stringhe si trova dentro la mia struttura dati e, se la trova\n",
    "\n",
    "1) creerà il progetto con il nome della condizione sperimentale combaciante tra \n",
    " \n",
    " - la combination_key associata allo Sweep ID corrente e\n",
    " - il sottodizionario di data_dict_preprocessed \n",
    " \n",
    "2) le relative run di quello specifico Sweep, verranno nominate con la combinazioni di fattori combaciante su W&B\n",
    "\n",
    "3) Esegue e gestisce il salvataggio della migliore configurazione di iper-parametri del relativo modello preso in esame (CNN1D, BiLSTM e Transformer)\n",
    "   tra le 15 runs di OGNI SWEEP\n",
    "   \n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae condition_experiment e subject_key da combination_key\n",
    "    dove il data_type è fisso a \"spectrograms\".\n",
    "    \n",
    "    Esempio di chiave: \n",
    "    \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(pt_resp_vs_shared_resp|th_resp_vs_pt_resp|th_resp_vs_shared_resp)_spectrograms_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        condition_experiment = match.group(1)\n",
    "        subject_key = match.group(2)\n",
    "        return condition_experiment, subject_key\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "        \n",
    "def training_sweep(data_dict_preprocessed, sweep_config, sweep_ids, sweep_id, sweep_tuple, best_models): \n",
    "    \n",
    "    # Per ogni sweep, che viene iterato nel loop, io prendo \n",
    "    #1) la stringa univoca dello Sweep ID\n",
    "    #2) la sua combinazione di fattori stringa (che mi serviranno per prelevare il dato corrispondente da 'data_dict_preprocessed'\n",
    "    \n",
    "    sweep_id, combination_key = sweep_tuple\n",
    "    \n",
    "    # Ora la funzione restituisce solo (exp_condition, subject_key)\n",
    "    exp_cond, category_subject = parse_combination_key(combination_key)\n",
    "    \n",
    "    # Poiché ora i dati sono solo di tipo \"spectrograms\", li impostiamo in modo fisso:\n",
    "    data_type = \"spectrograms\"\n",
    "\n",
    "    if not (exp_cond in data_dict_preprocessed and category_subject in data_dict_preprocessed[exp_cond][data_type]):\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    run_name = f\"{exp_cond}_{data_type}_{category_subject}\"\n",
    "    tags = [exp_cond, data_type, category_subject]\n",
    "\n",
    "    #Inizializza la run dello specifico Sweep dentro Weights & Biases (W&B) con\n",
    "\n",
    "    #1) un nome del progetto pari alla condizione sperimentale corrente\n",
    "    #2) il nome e tag della run in base alla combinazione di fattori corrispondente\n",
    "    #3) la congiurazione di iper-parametri è pari a quella passata in input a 'training_sweep'\n",
    "\n",
    "    #Vedi questo link su wandb.init() per vedere i suoi parametri --> #https://docs.wandb.ai/ref/python/init/\n",
    "    \n",
    "    # Inizializza la run in W&B nel progetto che termina con \"_spectrograms\"\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #wandb.init(project=f\"{exp_cond}_spectrograms\", name=run_name, tags=tags)\n",
    "    \n",
    "    '''NEW VERSION\n",
    "    \n",
    "    Questo assicura la coerenza tra la creazione degli sweep e le run che li eseguono,\n",
    "    e permette di tracciare meglio ogni combinazione anche su W&B.\n",
    "    '''\n",
    "    wandb.init(project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", name = run_name, tags = tags)\n",
    "    \n",
    "    \n",
    "    print(f\"\\nCreo wandb project per: \\033[1m{exp_cond}_spectrograms\\033[0m\")\n",
    "    print(f\"Lo sweep corrente è \\033[1m{sweep_tuple}\\033[0m\")\n",
    "    print(f\"\\nInizio addestramento sul dataset \\033[1m{exp_cond}\\033[0m con dati EEG \\033[1m{data_type}\\033[0m di \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "    # Parametri dell'esperimento presi da wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Recupera i dati pre-processati per la combinazione corrente una volta verificata l'esatta corrispondenza tra:\n",
    "    #1)il combination_key dello sweep\n",
    "    #2)l'esistenza di specifico dataset con le stesse 'combination_key' dentro data_dict_preprocessed\n",
    "\n",
    "    try:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data_dict_preprocessed[exp_cond][data_type][category_subject]\n",
    "        print(f\"\\nCarico i dati di \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\\n\")\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"❌ ERRORE - Combinazione \\033[1mNON TROVATA\\033[0m in data_dict_preprocessed: \\033[1m{exp_cond}\\033[0m, \\033[1m{data_type}\\033[0m, \\033[1m{category_subject}\\033[0m\")\n",
    "\n",
    "\n",
    "    if config.standardization:\n",
    "        # Standardizzazione\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "        print(f\"\\nUso DATI \\033[1mSTANDARDIZZATI\\033[0m!\")\n",
    "    else:\n",
    "        print(f\"\\nUso DATI \\033[1mNON STANDARDIZZATI\\033[0m!\")\n",
    "\n",
    "    # Preparazione dei dataloaders (N.B. prendo uno dei modelli considerati dentro config.model_name)\n",
    "    train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, model_type=config.model_name, batch_size = config.batch_size\n",
    "    )\n",
    "\n",
    "    #Qui estraggo il relativo modello su cui sto iterando al momento corrente e lo inizializzo\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    # Inizializza il modello in base al valore scelto in config.model_name\n",
    "    #if config.model_name == \"CNN2D\":\n",
    "        #model = CNN2D(input_channels = 61, num_classes = 2)\n",
    "        #print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "\n",
    "    #class CNN2D(nn.Module):\n",
    "        #def __init__(\n",
    "            #self,\n",
    "            #input_channels: int,              # numero di canali (es. 61)\n",
    "            #num_classes: int,                 # numero di classi di output\n",
    "            #conv_out_channels: int,           # parametro dallo sweep\n",
    "            #conv2d_kernel_size: tuple,        # es. (h, w) dallo sweep\n",
    "            #conv2d_stride: tuple,             # es. (h, w) dallo sweep\n",
    "            #pool_type: str,                   # \"max\" o \"avg\" dallo sweep\n",
    "            #pool2d_kernel_size: tuple,        # es. (h, w) dallo sweep\n",
    "            #fc1_units: int,                   # unità del primo fully connected\n",
    "            #dropout: float,                   # dropout dallo sweep\n",
    "            #activations: tuple                # tupla di 3 stringhe, es. ('relu','selu','elu')\n",
    "        #):\n",
    "    \n",
    "    if config.model_name == \"CNN3D_LSTM_FC\":\n",
    "        #model = CNN2D_LSTM_FC(n_freq =45, input_channels=64, num_classes=2, dropout=0.2)\n",
    "        \n",
    "        '''OCCHIO QUI CAMBIATO PER GRIGLIA 3D'''\n",
    "    \n",
    "        model = CNN3D_LSTM_FC(\n",
    "            num_classes=2,\n",
    "            dropout=config.dropout,\n",
    "            hidden_size=config.lstm_hidden,\n",
    "            use_lstm=config.use_lstm\n",
    "        )\n",
    "\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mCNN3D_LSTM_FC\\033[0m\")\n",
    "    \n",
    "    \n",
    "    elif config.model_name == \"SeparableCNN2D_LSTM_FC\":\n",
    "        model = SeparableCNN2D_LSTM_FC(\n",
    "            num_classes=2,\n",
    "            dropout=config.dropout,\n",
    "            hidden_size=config.lstm_hidden,\n",
    "            use_lstm=config.use_lstm\n",
    "        )\n",
    "        print(f\"\\nInizializzazione Modello \\033[1mSeparableCNN2D_LSTM_FC\\033[0m\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Modello sconosciuto: {config.model_name}\")\n",
    "    \n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    # 1) Optimizer con betas, eps, weight_decay\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        eps=config.eps,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode ='min',      # monitoriamo val_loss\n",
    "        factor = 0.1,      # dimezza lr\n",
    "        patience = 8,      # 4 epoche di plateau\n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Parametri di training\n",
    "    n_epochs = config.n_epochs\n",
    "    patience = config.patience\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    '''NEW VERSION'''\n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    best_model = None\n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    #'''AGGIORNAMENTI FINALI'''\n",
    "    #from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        # ---------------------- TRAIN ----------------------\n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #model.train()  \n",
    "        \n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list, y_pred_train_list = [], []\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_tmp.append(loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "            \n",
    "            #'''AGGIORNAMENTI FINALI'''\n",
    "            \n",
    "            # 👇 NOVITÀ: SCORE CONTINUO PER AUC TRAIN (usa la Softmax):\n",
    "            # OPZIONE A: puoi usare la Softmax per avere le probabilità,\n",
    "            # OPZIONE B: oppure direttamente CrossEntropy y_pred[:,1] (logit della classe 1).\n",
    "            \n",
    "            # Opzione A: usare le probabilità (softmax) \n",
    "            \n",
    "            #DECOMMENTA QUESTE 2 RIGHE PER USARE SOFTMAX\n",
    "            \n",
    "            #probs_train = torch.softmax(y_pred, dim=1)\n",
    "            #y_score_train_list.extend(probs_train[:, 1].detach().cpu().numpy())\n",
    "            \n",
    "            # Opzione B: usare direttamente i logits della classe 1 (consigliata, compatibile con CrossEntropy)\n",
    "            \n",
    "            #DECOMMENTA QUESTA RIGA PER USARE CROSSENTROPY\n",
    "            \n",
    "            # y_score_train_list.extend(y_pred[:, 1].detach().cpu().numpy())\n",
    "\n",
    "        accuracy_train = correct_train / len(train_loader.dataset)\n",
    "        loss_train = np.mean(train_loss_tmp)\n",
    "\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        '''come dovrebbe essere calcolato se non si dovesse passare al load_best_run_results'''\n",
    "        #auc_train = roc_auc_score(y_true_train_list, y_pred_train_list)\n",
    "        \n",
    "        '''come è stato calcolato se si dovesse passare al load_best_run_results'''\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #try:\n",
    "            #auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        #except ValueError:\n",
    "            #print(\"⚠️ AUC non calcolabile: nel train set c'è una sola classe.\")\n",
    "            #auc_val = np.nan\n",
    "        \n",
    "        # ---------------------- VALIDATION ----------------------\n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #model.eval()\n",
    "        \n",
    "        loss_val_tmp = []\n",
    "        correct_val = 0\n",
    "        y_true_val_list, y_pred_val_list = [], []\n",
    "        \n",
    "                \n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #y_score_val_list = []  # per AUC valida\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y.view(-1))\n",
    "                loss_val_tmp.append(loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_val_list.extend(y.cpu().numpy())\n",
    "                y_pred_val_list.extend(predicted_val.cpu().numpy())\n",
    "                \n",
    "                #'''AGGIORNAMENTI FINALI'''\n",
    "                \n",
    "                # 👇 NOVITÀ: SCORE CONTINUO PER AUC TRAIN (usa la Softmax):\n",
    "                \n",
    "                # OPZIONE A: puoi usare la Softmax per avere le probabilità,\n",
    "                # OPZIONE B: oppure direttamente CrossEntropy y_pred[:,1] (logit della classe 1).\n",
    "                \n",
    "                # Opzione A: usare le probabilità (softmax) \n",
    "                \n",
    "                #DECOMMENTA QUESTE 2 RIGHE PER USARE SOFTMAX\n",
    "                \n",
    "                #probs_val = torch.softmax(y_pred, dim=1)\n",
    "                #y_score_val_list.extend(probs_val[:, 1].detach().cpu().numpy())\n",
    "                \n",
    "                # Opzione B: usare direttamente i logits della classe 1 (consigliata, compatibile con CrossEntropy)\n",
    "                \n",
    "                #DECOMMENTA QUESTA RIGA PER USARE CROSSENTROPY\n",
    "                # y_score_val_list.extend(y_pred[:, 1].detach().cpu().numpy())\n",
    "                \n",
    "\n",
    "        accuracy_val = correct_val / len(val_loader.dataset)\n",
    "        loss_val = np.mean(loss_val_tmp)\n",
    "        \n",
    "        #'''AGGIORNAMENTI FINALI'''\n",
    "        #precision_val = precision_score(y_true_val_list, y_pred_val_list, average='weighted')\n",
    "        #recall_val    = recall_score(y_true_val_list, y_pred_val_list, average='weighted')\n",
    "        #f1_val        = f1_score(y_true_val_list, y_pred_val_list, average='weighted')\n",
    "        \n",
    "        #try:\n",
    "            # ATTENZIONE: qui usiamo gli score continui, NON le etichette\n",
    "            #auc_val = roc_auc_score(y_true_val_list, y_score_val_list, average='weighted')\n",
    "        #except ValueError:\n",
    "            #print(\"⚠️ AUC non calcolabile: nel validation set c'è una sola classe.\")\n",
    "            #auc_val = np.nan\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \n",
    "            # TRAIN\n",
    "            \"train_loss\": loss_train,\n",
    "            \"train_accuracy\": accuracy_train,\n",
    "            \"train_precision\": precision_train,\n",
    "            \"train_recall\": recall_train,\n",
    "            \"train_f1\": f1_train,\n",
    "            \"train_auc\": auc_train,\n",
    "            \n",
    "            # VALIDATION\n",
    "            \n",
    "            \"val_loss\": loss_val,\n",
    "            \"val_accuracy\": accuracy_val,\n",
    "            \n",
    "            # se vuoi loggare anche queste (consigliato):\n",
    "            \n",
    "            #\"val_precision\": precision_val,\n",
    "            #\"val_recall\": recall_val,\n",
    "            #\"val_f1\": f1_val,\n",
    "            #\"val_auc\": auc_val,\n",
    "        })\n",
    "        \n",
    "        #Nota: questa patch qua sopra (correzione su train e validation) rende corretto anche train_auc per le run future, \n",
    "        #quindi non avrai più bisogno della “correzione a posteriori” in load_best_run_results \n",
    "        #per i nuovi esperimenti (ma la puoi lasciare per compatibilità coi vecchi run).\n",
    "        \n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "            \n",
    "        '''OLD VERSION'''\n",
    "        #early_stopping(accuracy_val)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"🛑 Early stopping attivato!\")\n",
    "            #break\n",
    "            \n",
    "        '''NEW VERSION'''\n",
    "        scheduler.step(loss_val)\n",
    "        early_stopping(loss_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "        '''\n",
    "        Qui, si usa config.model_name tra le chiavi di best_models, \n",
    "        così che gestisca automaticamente il salvataggio del best model estratto dalla configurazione randomica di iper-parametri\n",
    "        della specifica run di un determinato sweep, che è relativa allo specifico modello correntemente estratto randomicamente dalla sweep_config!\n",
    "        \n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        IMPORTANTISSIMO: COME SALVARSI LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DI UN CERTO MODELLO, DI UN DATO DI UNA CERTA COMBINAZIONE DI FATTORI\n",
    "        (CONDIZIONE SPERIMENTALE, TIPO DI DATO, PROVENIENZA DEL DATO!)\n",
    "        ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** *****\n",
    "        \n",
    "        CHATGPT:\n",
    "        \n",
    "        Nei run eseguiti con W&B ogni esecuzione registra automaticamente la configurazione degli iper-parametri (tramite wandb.config) \n",
    "        insieme alle metriche e ai log. \n",
    "        Quindi, a meno che tu non abbia modificato il comportamento predefinito, \n",
    "        ogni run con il tuo sweep ha già la configurazione associata registrata nei run logs di W&B.\n",
    "\n",
    "        Tuttavia, per associare in modo “automatico” e diretto la migliore configurazione agli specifici modelli salvati in .pth, \n",
    "        potresti considerare di fare uno o più di questi aggiustamenti:\n",
    "\n",
    "        Salvare la configurazione nel dizionario dei best_models:\n",
    "        Quando aggiorni il dizionario best_models (cioè quando salvi il miglior modello per una determinata combinazione), \n",
    "        puoi salvare anche una copia della configurazione corrente. \n",
    "        \n",
    "        Ad esempio, potresti modificare il blocco in cui aggiorni best_models in questo modo:\n",
    "        \n",
    "        best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "            \"model\": cp.deepcopy(model),\n",
    "            \"max_val_acc\": accuracy_val,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"config\": dict(config)  # Salva la configurazione degli iper-parametri\n",
    "        }\n",
    "        \n",
    "        In questo modo, ogni volta che un modello viene considerato il migliore per quella combinazione,\n",
    "        la sua configurazione sarà salvata insieme ai pesi.\n",
    "        Questo ti permetterà, in seguito, di sapere esattamente quali iper-parametri sono stati usati per ottenere quel modello.\n",
    "        \n",
    "        \n",
    "        In sintesi, se hai già usato wandb.config e hai loggato le configurazioni durante le run,\n",
    "        W&B le ha automaticamente salvate nei run logs. \n",
    "        \n",
    "        Se vuoi rendere più esplicita l'associazione tra il modello salvato (.pth) e la sua configurazione, \n",
    "        è utile modificare il tuo codice di TRAINING per salvare ANCHE \n",
    "        \n",
    "        1) il dizionario di configurazione insieme a \n",
    "        2) i pesi nel dizionario best_models oppure nei metadati del file salvato.\n",
    "        \n",
    "        Questo piccolo accorgimento ti consentirà di recuperare facilmente la configurazione ottimale per ogni modello salvato.\n",
    "        \n",
    "        OSSIA\n",
    "        Aggiungendo la chiave \"config\": dict(config) nel dizionario che memorizza il best model,\n",
    "        salvi anche la configurazione degli iper-parametri utilizzata in quella run.\n",
    "        \n",
    "        In questo modo, per ogni modello salvato (.pth) potrai recuperare facilmente sia i pesi che la configurazione ottimale che li ha generati.\n",
    "        \n",
    "        Questo approccio garantisce che ogni modello sia associato in modo esplicito al set di iper-parametri che ha prodotto le migliori performance, \n",
    "        rendendo più semplice il successivo confronto o la replica degli esperimenti.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # ***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "        #1)Al posto di salvarmi solo i migliori pesi (i.e.,  model_file = f\"{model_path}/{best_model_name}.pth\")\n",
    "        #  ora mi salvo anche la MIGLIORE configurazione di iper-parametri trovata rispetto alle 15 RUNS di un certo SWEEP\n",
    "        #  di un certo MODELLO, applicato su un DATASET con una SPECIFICA COMBINAZIONE DI FATTORI\n",
    "        #  condizione sperimentale, tipo di dato e provenienza del dato!\n",
    "        \n",
    "    \n",
    "\n",
    "        if best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"] == -float('inf'):\n",
    "\n",
    "            # Salvo il primo best_model per quella combinazione\n",
    "            best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                \"model\": cp.deepcopy(model),\n",
    "                \"max_val_acc\": accuracy_val,\n",
    "                \"best_epoch\": epoch,\n",
    "                \n",
    "                #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "                #***** AGGIUNTA DELLA CHIAVE CONFIG CHE PRELEVA AUTOMATICAMENTE LA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI DENTRO 'BEST_MODELS'\n",
    "                \n",
    "                # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                # una certa combinazione di fattori: \n",
    "                # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                \"config\": dict(config)  \n",
    "            }\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            #***** ATTENZIONE: CAMBIAMENTI ESEGUITI RISPETTO A PRIMA *****\n",
    "            #***** SALVATAGGIO DI UN FILE .PKL, CHE CONTIENE \n",
    "            \n",
    "            # I PESI E BIAS DEL MODELLO DERIVATO DALLA MIGLIORE CONFIGURAZIONE DI IPER-PARAMETRI OTTENUTA DALLA MIGLIORE RUN DI UN CERTO SWEEP\n",
    "            # IN RELAZIONE AD UN CERTO DATASET COSTITUITO DA UNA CERTA COMBINAZIONE DI FATTORI\n",
    "            \n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            \n",
    "            # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "            torch.save({\n",
    "                \"state_dict\": best_model.state_dict(),\n",
    "                \"config\": dict(config)\n",
    "            }, model_file)\n",
    "\n",
    "            print(f\"Il modello \\n\\033[1m{best_model_name}\\033[0m verrà salvato in questa folder directory: \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "            #Condizione di aggiornamento:\n",
    "            #Se l'accuracy corrente (accuracy_val) di quel modello di quello sweep supera il valore già salvato in best_models[...], \n",
    "            #allora aggiorniamo il dizionario e sovrascriviamo il file del best model, di quel modello, di quella combinazione di fattori.\n",
    "\n",
    "\n",
    "            # Puoi confrontare e salvare il modello solo se il nuovo è migliore\n",
    "\n",
    "\n",
    "            #Questo assicura che il salvataggio del modello avvenga solo se\n",
    "            #il nuovo modello ha un'accuratezza di validazione (max_val_acc) migliore \n",
    "            #rispetto a quella già memorizzata per la condizione specifica (exp_cond).\n",
    "\n",
    "            #In questo modo, si evita di sovrascrivere il modello salvato con uno peggiore\n",
    "\n",
    "\n",
    "            # Nuovo modello migliore per questa combinazione: aggiorna e sovrascrivi il file\n",
    "\n",
    "\n",
    "        elif accuracy_val > best_models[exp_cond][data_type][category_subject][config.model_name][\"max_val_acc\"]:\n",
    "                best_models[exp_cond][data_type][category_subject][config.model_name] = {\n",
    "                    \"model\": best_model,\n",
    "                    \"max_val_acc\": accuracy_val,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                    \n",
    "                    # Salva la configurazione degli iper-parametri della migliore run di uno sweep \n",
    "                    # in relazione ad un certo modello applicato su un dataset costituito da \n",
    "                    # una certa combinazione di fattori: \n",
    "                    # condizione sperimentale, tipo di dato EEG usato, provenienza del dato usato\n",
    "                    \"config\": dict(config)  \n",
    "                }\n",
    "                best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "                model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                print(f\"Il modello di questa folder directory:\\n\\033[1m{model_path}\\033[0m\")\n",
    "                print(f\"\\nHa un MIGLIORAMENTO!\")\n",
    "\n",
    "                model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "\n",
    "                if os.path.exists(model_file):\n",
    "\n",
    "                    # Se il file esiste, stampiamo un messaggio di aggiornamento\n",
    "                    print(f\"\\n⚠️ ATTENZIONE: \\nIl modello \\033[1m{best_model_name}\\033[0m verrà AGGIORNATO in \\n\\033[1m{model_path}\\033[0m\")\n",
    "\n",
    "                    # Salva il miglior modello solo se è stato aggiornato\n",
    "                    # Salva un dizionario contenente sia i pesi che la configurazione\n",
    "                    torch.save({\n",
    "                        \"state_dict\": best_model.state_dict(),\n",
    "                        \"config\": dict(config)\n",
    "                    }, model_file)\n",
    "                    \n",
    "                    print(f\"\\nIl nome del modello AGGIORNATO è:\\n\\033[1m{best_model_name}\\033[0m\")\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                #Condizione \"nessun miglioramento\":\n",
    "                #Se il modello corrente non migliora il best già salvato, viene semplicemente stampato un messaggio.\n",
    "\n",
    "                #Questa logica garantisce che per ogni combinazione il file .pth contenga \n",
    "                #sempre i pesi del miglior modello (secondo la validation accuracy) fino a quel momento.\n",
    "                #Adatta eventualmente i nomi delle variabili (es. accuracy_val vs max_val_acc) per essere coerente con il resto del tuo codice.\n",
    "        else:\n",
    "            ''''QUI VA RIDEFINITO LA MODEL_PATH (e anche se vuoi MODE_FILE) ALTRIMENTI IN QUESTO ELSE NON ESISTONO!'''\n",
    "\n",
    "            best_model_name = f\"{config.model_name}_{exp_cond}_{data_type}_{category_subject}\"\n",
    "            model_path = os.path.join(base_dir, exp_cond, data_type, category_subject)\n",
    "            model_file = f\"{model_path}/{best_model_name}.pkl\"\n",
    "            print(f\"Nessun miglioramento per il modello \\033[1m{config.model_name}\\033[0m in \\n\\033[1m{model_path}\\033[0m, ossia \\n\\033[1m{model_file}\\033[0m\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21ef57-0c55-455e-9a80-655badd11738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Weight & Biases Procedure Final Edits - EEG Spectrograms - Electrodes x Frequencies ONLY HYPER-PARAMS**\n",
    "#### **Sweep separati per ciascuno dei modelli CNN3D e CNN Sep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f598da7-c90a-490f-a66f-5e7be18ffba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE C \n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                W&B SWEEPS AND TRAING LAUNCH WITH MULTIPLE GPUs MANAGEMENT\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE C (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "SPIEGAZIONE\n",
    "\n",
    "GPU counter: Ho aggiunto un contatore (gpu_counter) che cicla tra le GPU disponibili. \n",
    "\n",
    "In questo modo, il primo sweep sarà eseguito sulla GPU 0, il secondo sulla GPU 1, e così via. \n",
    "Quando il contatore raggiunge il numero di GPU disponibili, torna a 0 per riusare la prima GPU.\n",
    "\n",
    "Rotazione delle GPU: All'interno del loop, per ogni sweep, viene assegnata una GPU diversa. \n",
    "Se ci sono più di 1 GPU, il contatore incrementa, e la variabile CUDA_VISIBLE_DEVICES cambia automaticamente per assegnare la GPU corretta.\n",
    "\n",
    "Esecuzione parallela: Ogni sweep viene eseguito su una GPU separata. Se ci sono 2 GPU, il primo sweep va su GPU 0, il secondo su GPU 1, il terzo su GPU 0, e così via.\n",
    "\n",
    "Risposta alla tua domanda:\n",
    "In questo modo, ogni sweep_id viene eseguito una sola volta, ma su GPU diverse (se disponibili). Non ci sono duplicati dello stesso sweep su entrambe le GPU.\n",
    "\n",
    "\n",
    "DOMANDE SUL NUOVO CODICE\n",
    "\n",
    "1) Gli sweep sono eseguiti già in parallelo giusto?\n",
    "No, in questo caso gli sweep non sono eseguiti in parallelo in modo esplicito tramite il codice che hai scritto.\n",
    "\n",
    "Anche se hai assegnato ciascun sweep a una GPU diversa, il codice esegue sequenzialmente ogni sweep, solo che li distribuisce su GPU differenti in modo rotazionale.\n",
    "Ogni volta che il ciclo passa ad un nuovo sweep, assegna un ID GPU e poi esegue l'agent su quella GPU. Non vengono eseguiti in parallelo a livello di codice.\n",
    "\n",
    "2) O semplicemente in questo modo faccio in modo di distribuire ogni sweep sull'altra GPU rispetto a quella usata dallo sweep precedente\n",
    "per \"ottimizzare\" il carico computazionale di ogni GPU?\n",
    "\n",
    "Esatto! Quello che stai facendo è distribuire i vari sweep su GPU diverse, assicurandoti che ogni sweep venga eseguito su una GPU separata (se ne hai di disponibili).\n",
    "Questo permette di ottimizzare l'uso delle risorse, evitando che una GPU venga sovraccaricata da più sweep. Se il numero di GPU disponibili è maggiore di 1, \n",
    "allora i sweep sono distribuiti sulle diverse GPU, ma ogni sweep sarà ancora eseguito singolarmente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sì, con il codice che hai fornito, stai distribuendo gli sweep tra le diverse GPU, in modo da ottimizzare il carico computazionale e non sovraccaricare una sola GPU.\n",
    "\n",
    "Dettaglio del funzionamento:\n",
    "Distribuzione delle GPU (rotazionale):\n",
    "\n",
    "Quando ci sono più di una GPU, il codice assegna a ciascun sweep una GPU diversa in modo rotazionale.\n",
    "\n",
    "Per ogni ciclo del loop, la variabile gpu_counter determina a quale GPU assegnare il prossimo sweep.\n",
    "\n",
    "Se ci sono 2 GPU, il primo sweep viene eseguito sulla GPU 0, il secondo sulla GPU 1, il terzo di nuovo sulla GPU 0, e così via.\n",
    "\n",
    "Gestione della GPU:\n",
    "\n",
    "Se hai più di una GPU, os.environ[\"CUDA_VISIBLE_DEVICES\"] imposta il dispositivo GPU corrente su cui il codice deve girare (GPU 0 o GPU 1). Questo permette di gestire quale GPU eseguirà l'addestramento per ciascun sweep.\n",
    "\n",
    "Quando num_gpus > 1, il codice alterna l'assegnazione della GPU per ogni sweep, evitando di sovraccaricare una singola GPU con troppe operazioni contemporaneamente.\n",
    "\n",
    "Ottimizzazione del carico computazionale:\n",
    "\n",
    "L'alternanza tra GPU successive per ciascun sweep aiuta a distribuire il carico in modo equilibrato, specialmente se il numero di sweep è alto.\n",
    "\n",
    "In pratica, se hai 2 GPU e 10 sweep da eseguire, ogni GPU eseguirà 5 sweep, evitando di saturare una singola GPU.\n",
    "\n",
    "Parallelizzazione effettiva:\n",
    "Nel tuo codice non c'è parallelismo vero e proprio tra le GPU (come quello che otterresti con DataParallel o DistributedDataParallel), ma piuttosto distribuisci gli sweep su GPU diverse. Ogni sweep è eseguito sequenzialmente su una GPU diversa, ma non stai parallelizzando il training dello stesso sweep su più GPU.\n",
    "\n",
    "Se volessi fare in modo che un singolo sweep fosse parallelizzato su più GPU (in modo che il lavoro venga diviso tra le GPU per uno stesso sweep), dovresti usare DataParallel o DistributedDataParallel, come descritto in precedenza. Ma in questo caso, ogni sweep è isolato e viene eseguito su una sola GPU, quindi non c'è un vero parallelismo dentro ogni singolo sweep.\n",
    "\n",
    "Domande che potrebbero sorgere:\n",
    "Gli sweep vengono eseguiti in parallelo?\n",
    "\n",
    "No, i sweep sono eseguiti uno alla volta, ma su GPU diverse. Quindi, mentre il primo sweep usa la GPU 0, il secondo usa la GPU 1 e così via. Ogni sweep viene gestito separatamente, ma sfrutti più GPU per parallelizzare l'esecuzione di più sweep contemporaneamente.\n",
    "\n",
    "La distribuzione delle GPU tra gli sweep è ottimizzata?\n",
    "\n",
    "Sì, stai bilanciando il carico computazionale tra le GPU, assegnando a ogni GPU uno sweep alternato. Se hai molte GPU, puoi ottimizzare ulteriormente distribuendo i sweep su più dispositivi.\n",
    "\n",
    "Se volessi parallelizzare più agenti W&B su diverse GPU, il codice che stai utilizzando sarebbe corretto, ma per ottimizzare ulteriormente i tempi di esecuzione, potresti prendere in considerazione anche l'utilizzo di tecniche come DataParallel o DistributedDataParallel per far sì che più GPU lavorino contemporaneamente sullo stesso sweep. Ma la logica che hai già implementato va bene per distribuire più sweep tra le GPU.\n",
    "\n",
    "Se hai bisogno di ulteriori dettagli su come implementare il parallelismo vero e proprio (inclusi DataParallel o DistributedDataParallel), fammi sapere!\n",
    "\n",
    "\n",
    "\n",
    "****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Per modificare il loop in modo che accetti i sweeps per ogni modello e gestisca correttamente\n",
    "l'esecuzione del training per ciascun modello con il relativo sweep, dobbiamo fare alcune modifiche.\n",
    "\n",
    "\n",
    "Modifiche principali:\n",
    "\n",
    "1) Funzione make_train_wrapper:\n",
    "La funzione dovrà essere adattata per passare correttamente la configurazione di sweep per ogni modello, \n",
    "invece di passare un'unica configurazione generica (sweep_config).\n",
    "\n",
    "2) Identificazione corretta del modello: \n",
    "Nel loop, per ogni combinazione (condition, data_type, category_subject)\n",
    "e per ogni modello (ad esempio, CNN3D_LSTM_FC e SeparableCNN2D_LSTM_FC), \n",
    "\n",
    "dobbiamo passare al wandb.agent il relativo sweep ID per il modello e la sua configurazione.\n",
    "\n",
    "3) Modifica della funzione make_train_wrapper per gestire ogni modello separatamente: \n",
    "Ogni modello avrà il proprio sweep e la propria configurazione.\n",
    "\n",
    "\n",
    "Spiegazione delle modifiche:\n",
    "\n",
    "1) Funzione make_train_wrapper:\n",
    "\n",
    "Adesso prende anche model_name per passare il relativo sweep_config dal dizionario sweep_config_dict.\n",
    "Passa il sweep_config corretto per ogni modello, a seconda del model_name passato nel ciclo.\n",
    "\n",
    "2) Dizionario sweep_config_dict:\n",
    "\n",
    "Ho creato un dizionario sweep_config_dict che associa ciascun modello (\"CNN3D_LSTM_FC\" e \"SeparableCNN2D_LSTM_FC\")\n",
    "alla sua configurazione di sweep (sweep_config_cnn3d e sweep_config_cnn_sep).\n",
    "Questo permette di usare la corretta configurazione per ogni modello.\n",
    "\n",
    "3) Modifica nel ciclo:\n",
    "\n",
    "Il ciclo ora scorre su model_name (i.e., i modelli CNN3D_LSTM_FC e SeparableCNN2D_LSTM_FC) \n",
    "per ogni combinazione di condition, data_type, category_subject.\n",
    "\n",
    "Per ogni modello, il relativo sweep viene creato ed eseguito.\n",
    "\n",
    "\n",
    "Risultato:\n",
    "Ora, per ogni combinazione di condition, data_type, e category_subject, \n",
    "il codice creerà e gestirà separatamente gli sweeps per ciascun modello,\n",
    "e li eseguirà utilizzando la funzione training_sweep con la relativa configurazione specifica per ogni modello.\n",
    "\n",
    "Questa modifica ti consente di avere il corretto flusso di lavoro per eseguire\n",
    "il training separato per ogni modello con la sua configurazione.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "\n",
    "# Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "\n",
    "'''ATTENZIONE AGGIUNTO model_name tra i parametri di --> make_train_wrapper'''\n",
    "\n",
    "def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name):\n",
    "    def train_wrapper():\n",
    "\n",
    "        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "        #print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "        \n",
    "        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m, modello \\033[1m{model_name}\\033[0m\")\n",
    "        training_sweep(\n",
    "            data_dict_preprocessed, \n",
    "            sweep_config_dict[model_name], # Prendi la configurazione per il modello specifico\n",
    "            sweep_ids,\n",
    "            sweep_id,\n",
    "            sweep_tuple,\n",
    "            best_models  # Best models viene aggiornato all'interno della funzione\n",
    "        )\n",
    "    return train_wrapper\n",
    "                        \n",
    "\n",
    "# Dizionari di configurazione per ogni modello\n",
    "sweep_config_dict = {\n",
    "    \"CNN3D_LSTM_FC\": sweep_config_cnn3d,\n",
    "    \"SeparableCNN2D_LSTM_FC\": sweep_config_cnn_sep\n",
    "}\n",
    "\n",
    "# Verifica quante GPU sono disponibili\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "\n",
    "# Crea un contatore per assegnare un GPU diversa a ciascun sweep\n",
    "gpu_counter = 0\n",
    "\n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for model_name in sweep_ids[condition][data_type][category_subject]:  # Aggiunto loop per il modello\n",
    "                \n",
    "                #for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                 for sweep_tuple in sweep_ids[condition][data_type][category_subject][model_name]:  # Itera sugli sweep per ciascun modello\n",
    "\n",
    "                    # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                    sweep_id, combination_key = sweep_tuple\n",
    "                    \n",
    "                    \n",
    "                    combination_key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "                    \n",
    "                    # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                    # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                    # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "\n",
    "\n",
    "                    # Se ci sono più di 1 GPU, assegna a ciascuna GPU uno sweep diverso\n",
    "                    if num_gpus > 1:\n",
    "\n",
    "                        '''ATTENZIONE AGGIUNTO model_name tra i parametri di --> make_train_wrapper''' \n",
    "                        \n",
    "                        # Assegna la GPU in modo rotazionale\n",
    "                        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_counter)\n",
    "                        \n",
    "                        agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name)\n",
    "                    \n",
    "                        #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_channels_freqs_new_imagery_3d_grid_multiband\", count=200)\n",
    "                        wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", count=202)\n",
    "                        \n",
    "                        \n",
    "                        # Passa alla prossima GPU per il prossimo sweep\n",
    "                        gpu_counter = (gpu_counter + 1) % num_gpus\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        # Se c'è una sola GPU, esegui il sweep sulla GPU 0\n",
    "                        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "                        \n",
    "                        agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        #wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project=f\"{condition}_spectrograms_channels_freqs_new_imagery_3d_grid_multiband\", count=200)\n",
    "                        wandb.agent(sweep_id, function=make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_name), project = f\"{condition}_{data_type}_channels_freqs_{category_subject}\", count=202)\n",
    "\n",
    "\n",
    "                    # Crea la funzione wrapper per l'agent\n",
    "                    '''COMMENTATO'''\n",
    "                    #agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject)\n",
    "\n",
    "\n",
    "                    # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                    '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                       ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "\n",
    "                    print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "\n",
    "                    '''COMMENTATO'''\n",
    "                    #wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs_new_2d_grid_multiband_topomap\", count=15)\n",
    "\n",
    "                    print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139170b-5849-4a44-acae-8828260266c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Impostazione **Recupero DL Optimized Models** - EEG Spectrograms - **Electrodes x Frequencies (2D)**\n",
    "\n",
    "### IMPLEMENTAZIONE DEI BEST MODELS DOPO W&B - EEG SPECTROGRAMS **+ GRADCAM FREQUENCY x CHANNELS (ALL SUBJECTS)**! "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6085871f-ee67-4a64-8699-99af2f914abf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import mne\n",
    "print(mne.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2de8e85-be5b-418b-979b-37d5950fa38c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import mne\n",
    "\n",
    "# Percorso del file .vhdr\n",
    "file_path = '/home/stefano/Interrogait/raw_EEG_task/HS39_HS40_EEG_base_Hyper.vhdr'\n",
    "\n",
    "# Carica il file .vhdr con mne\n",
    "raw_data = mne.io.read_raw_brainvision(file_path, preload=True)\n",
    "\n",
    "# Visualizza le informazioni sui dati\n",
    "print(raw_data.info)\n",
    "\n",
    "# Estrai i dati EEG\n",
    "eeg_data = raw_data.get_data()\n",
    "\n",
    "# Estrai il nome dei canali\n",
    "channel_names = raw_data.info['ch_names']\n",
    "\n",
    "print(\"Canali:\", channel_names)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4c6ae6b-fca6-4d23-9042-fc3cc08269ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Filtra i canali che terminano con '_1' e rimuovi il suffisso\n",
    "filtered_channel_names = [ch[:-2] for ch in channel_names if ch.endswith('_1')]\n",
    "\n",
    "print(\"Canali filtrati:\", filtered_channel_names)\n",
    "print()\n",
    "print(len(filtered_channel_names))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbe7de3d-7c33-4d4c-a240-c716236fc67a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "import pickle \n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "with open(f\"{path}EEG_channels_names.pkl\", \"wb\") as f:\n",
    "    pickle.dump(filtered_channel_names, f)\n",
    "\n",
    "#print(f\"Lista salvata in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8825653-db7c-414a-9b49-7864b9de5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Importing \n",
    "    \n",
    "import os\n",
    "import math\n",
    "import copy as cp \n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random \n",
    "\n",
    "#import mne \n",
    "import scipy\n",
    "\n",
    "import numpy as np  # NumPy per operazioni numeriche\n",
    "import matplotlib.pyplot as plt  # Matplotlib per la visualizzazione dei dati\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f9f9f5-e9f6-491c-85ea-e8be82ebfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "with open(f\"{path}EEG_channels_names.pkl\", \"rb\") as f:\n",
    "    EEG_channels_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97688943-4b9f-42df-b37e-88651b0b6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 352 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_size = os.stat(f\"{path}EEG_channels_names.pkl\").st_size\n",
    "print(f\"File size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fe9c2a-b494-424f-b0bc-7cbf5cf96f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EEG_channels_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2dbe6b-ad87-473a-bad9-72f03656d56d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **NUOVE MODIFICHE SPECIFICHE PER I DATI NON HYPER POST W&B CON GRADCAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba56c4-3a37-47ba-8dfa-e55515f23b93",
   "metadata": {},
   "source": [
    "Allora le modifche che ho ultimato quindi sono:\n",
    "\n",
    "- **1)Creazione della classe GradCAM**\n",
    "\n",
    "\n",
    "    **GRADCAM CLASS**\n",
    "\n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        class GradCAM:\n",
    "            def __init__(self, model, target_layer):\n",
    "                self.model = model\n",
    "                self.target_layer = target_layer\n",
    "                self.activations = None\n",
    "                self.gradients = None\n",
    "                # Registra hook per catturare attivazioni e gradienti\n",
    "                self.target_layer.register_forward_hook(self.save_activation)\n",
    "                self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "            def save_activation(self, module, input, output):\n",
    "                self.activations = output.detach()\n",
    "\n",
    "            def save_gradient(self, module, grad_input, grad_output):\n",
    "                self.gradients = grad_output[0].detach()\n",
    "\n",
    "\n",
    "- **2)** Creazione della funzione per generare delle immagini associate alla GradCAM compution**\n",
    "\n",
    "    \n",
    "    **FUNCTION FOR CREATING GRAD-CAM MAPS & FIGURES ASSOCIATED TO GRADCAM COMPUTATION**\n",
    "\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import io\n",
    "\n",
    "        def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "\n",
    "            \"\"\"\n",
    "            Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "            calcola la GradCAM e costruisce una figura con:\n",
    "              - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "              - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "            I titoli della figura vengono personalizzati con exp_cond, data_type, category_subject.\n",
    "            \"\"\"\n",
    "\n",
    "            # Assumiamo che il modello sia CNN2D e che il layer target sia model.conv3\n",
    "            target_layer = model.conv3\n",
    "            gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "            # Dizionari per salvare il campione per ogni classe\n",
    "            samples = {}      # Salveremo il sample input per ogni classe\n",
    "            labels_found = {} # Per tenere traccia delle etichette già trovate\n",
    "\n",
    "            # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                for i, label in enumerate(labels):\n",
    "                    label_int = int(label.item())\n",
    "                    if label_int not in labels_found:\n",
    "                        samples[label_int] = inputs[i].unsqueeze(0)  # salva come tensore 4D\n",
    "                        labels_found[label_int] = True\n",
    "                    if 0 in labels_found and 1 in labels_found:\n",
    "                        break\n",
    "                if 0 in labels_found and 1 in labels_found:\n",
    "                    break\n",
    "\n",
    "            # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "            if 0 not in samples or 1 not in samples:\n",
    "                print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "                return None\n",
    "\n",
    "            # Per ciascun campione, calcola GradCAM\n",
    "            cams = {}\n",
    "            overlays = {}\n",
    "            for cls in [0, 1]:\n",
    "                sample_input = samples[cls]\n",
    "                sample_input.requires_grad = True  # Abilita gradiente per il campione\n",
    "                cam = gradcam.generate_cam(sample_input)\n",
    "                cams[cls] = cam\n",
    "\n",
    "                # Converti il sample in immagine numpy per la visualizzazione\n",
    "                img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "                # Normalizza l'immagine in scala 0-255\n",
    "                img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "                # Applica la heatmap\n",
    "                heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "                heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "                # Sovrapponi la heatmap all'immagine originale\n",
    "                overlay = cv2.addWeighted(img_norm, 0.6, heatmap, 0.4, 0)\n",
    "                overlays[cls] = overlay\n",
    "\n",
    "            # Crea la figura con due righe e due colonne\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "            # Titolo per la prima riga\n",
    "            title_row1 = f\"Grad-CAM mapping of experimental condition {exp_cond}, EEG {data_type}, Subject {category_subject}\"\n",
    "            # Titolo per la seconda riga\n",
    "            title_row2 = f\"Grad-CAM mapping superimposition over EEG Spectrogram of experimental condition {exp_cond}, Subject {category_subject}\"\n",
    "\n",
    "            # Prima riga: solo le heatmap\n",
    "            for j, cls in enumerate([0, 1]):\n",
    "                axs[0, j].imshow(cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB))\n",
    "                axs[0, j].set_title(f\"Class {cls} Heatmap\")\n",
    "                axs[0, j].axis('off')\n",
    "            axs[0, 0].set_ylabel(title_row1, fontsize=10)\n",
    "\n",
    "            # Seconda riga: overlay della heatmap sullo spettrogramma originale\n",
    "            for j, cls in enumerate([0, 1]):\n",
    "                axs[1, j].imshow(overlays[cls])\n",
    "                axs[1, j].set_title(f\"Class {cls} Overlay\")\n",
    "                axs[1, j].axis('off')\n",
    "            axs[1, 0].set_ylabel(title_row2, fontsize=10)\n",
    "\n",
    "            # Ottimizza la disposizione della figura\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            fig_image = buf.getvalue()\n",
    "            buf.close()\n",
    "            plt.close(fig)\n",
    "\n",
    "            return fig_image\n",
    "\n",
    "\n",
    "- **3) Modifica delle funzioni per il salvataggio delle immagini create tramite la GradCAM compution**\n",
    "\n",
    "    **FUNCTIONS FOR GRADCAM COMPUTATION & SAVING**\n",
    "    \n",
    "    Questa modifica consente di creare ed adattare le path di salvataggio ANCHE delle immagini calcolate dalla classe customizzata di GradCAM, \n",
    "    delle mappe di attivazione prodotte dalle feature maps e della sovrapposizione delle stesse aree decisionali\n",
    "    rilevanti per la migliore classificazione dei dati di esempio di una certa classe,\n",
    "    a partire da un certo dataset composto da una certa combinazione di fattori\n",
    "    (i.e., exp_cond, data_type, category_subject)\n",
    "\n",
    "\n",
    "#NEW VERSIONS FOR SPECTROGRAMS WITH GRADCAM COMPUTATION ON CNN2D!\n",
    "\n",
    "    **Funzione per determinare a quale subfolder appartiene la chiave**\n",
    "    def get_subfolder_from_key(key, model_standardization):\n",
    "\n",
    "        #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "        if '_familiar_th' in key:\n",
    "            return 'th_fam'\n",
    "        elif '_unfamiliar_th' in key:\n",
    "            return 'th_unfam'\n",
    "        elif '_familiar_pt' in key:\n",
    "            return 'pt_fam'\n",
    "        elif '_unfamiliar_pt' in key:\n",
    "            return 'pt_unfam'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    from PIL import Image\n",
    "    import io\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    **Funzione per salvare i risultati**\n",
    "    def save_performance_results(model_name, \n",
    "                                 my_train_results,\n",
    "                                 my_test_results, \n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder,\n",
    "                                 gradcam_image = None):\n",
    "        \"\"\"\n",
    "        Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "        Se gradcam_image è fornita, la salva anche in formato PNG con un nome che inizia con 'GradCAM_results'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Identificazione del subfolder in base alla chiave\n",
    "        subfolder = get_subfolder_from_key(key, model_standardization)\n",
    "\n",
    "        # Debug: controllo sulla subfolder\n",
    "        print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "\n",
    "        if subfolder is None:\n",
    "            print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "            return\n",
    "\n",
    "        # Determinazione del tipo di dato direttamente dalla chiave\n",
    "        if \"spectrograms\" in key:\n",
    "            data_type_str = \"spectrograms\"\n",
    "        else:\n",
    "            print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "            return\n",
    "\n",
    "        # Creazione del nome del file pickle con l'inclusione della combinazione key + model_name\n",
    "        if model_standardization:\n",
    "            file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}_std.pkl\"\n",
    "            folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "        else:\n",
    "            file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}.pkl\"\n",
    "            folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "\n",
    "        # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Creazione del dizionario con i risultati\n",
    "        results_dict = {\n",
    "            'my_train_results': my_train_results,\n",
    "            'my_test_results': my_test_results\n",
    "        }\n",
    "\n",
    "        # Salvataggio del dizionario con i risultati\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(results_dict, f)\n",
    "            print(f\"\\n🔬Risultati salvati con successo 👍 in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌Errore durante il salvataggio dei risultati: {e}\")\n",
    "\n",
    "        # Se è stata fornita l'immagine GradCAM, salvala come file PNG\n",
    "        if gradcam_image is not None:\n",
    "            if model_standardization:\n",
    "                gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}_std.png\"\n",
    "            else:\n",
    "                gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}.png\"\n",
    "\n",
    "            gradcam_file_path = os.path.join(folder_path, gradcam_file_name)\n",
    "\n",
    "            #try:\n",
    "            #    with open(gradcam_file_path, \"wb\") as f_img:\n",
    "            #        f_img.write(gradcam_image)\n",
    "            #    print(f\"\\n📸Immagine GradCAM salvata con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                '''\n",
    "                Se gradcam_image è un oggetto BytesIO, allora rappresenta un flusso di dati binari in memoria.\n",
    "                Quando si leggono dati da un BytesIO, il cursore interno avanza come in un file normale. \n",
    "                Se il cursore non è all'inizio, Image.open() potrebbe non leggere correttamente l'immagine.\n",
    "                👉 seek(0) riporta il cursore all'inizio del buffer prima di leggerlo con Image.open()\n",
    "\n",
    "                Per maggior info leggi cella successiva!\n",
    "                '''\n",
    "\n",
    "                # 🔄 Se gradcam_image è un buffer, convertirlo in immagine PIL\n",
    "                if isinstance(gradcam_image, io.BytesIO):\n",
    "                    gradcam_image.seek(0)  # 🔄 Reset puntatore del buffer\n",
    "                    gradcam_image = Image.open(gradcam_image)\n",
    "\n",
    "                print(f\"\\n📸Immagine GradCAM salvata con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "                # 🔄 Salvare l'immagine nel percorso specificato\n",
    "                gradcam_image.save(gradcam_file_path, format = \"PNG\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌Errore durante il salvataggio dell'immagine GradCAM: {e}\")\n",
    "\n",
    "\n",
    "- **4) Integrazione nel loop di training e test dei punti 1), 2) e 3)**\n",
    "\n",
    "    **INTEGRATION OF GRADCAM COMPUTATION IN THE TRAINING E FOR LOOP**\n",
    "\n",
    "    Nel loop che esegue il training ed il testing, integrazione della parte di inizializzazione della classe custom di GradCAM, con cui si esegue \n",
    "\n",
    "    il calcolo delle mappe di attivazione e della sovrapposizione delle mappe di attivazione stesse sullo spettogramma originale, \n",
    "    riportate poi in due immagini distinte create nella stessa figura che vengono salvate correttamente nella stessa directory path. \n",
    "\n",
    "    Le due immagini dovrebbero rappresentare l'heatmap activation e la sovrapposizione della mappa di attivazione sullo spettogramma originale,\n",
    "    relativo ad un esempio rappresentativo per ciascuna delle due classi possibili presenti nello stesso dataset correntemente iterato.\n",
    "\n",
    "    Il loro contributo è di descrivere se la CNN2D abbia identificato delle (possibili) differenti aree decisionali delle feature maps \n",
    "    (e dunque dello spettrogramma) maggiormente utili ai fini della discriminazione delle due condizioni sperimentali inserite all'interno del dataset correntemente iterato.\n",
    "\n",
    "\n",
    "        ** Dizionario per tracciare la standardizzazione usata per ogni combinazione di dati**\n",
    "        ** Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)**\n",
    "\n",
    "        models_info = {}\n",
    "\n",
    "        ** Set per tenere traccia dei dataset già elaborati**\n",
    "        processed_datasets = set()\n",
    "\n",
    "        ** Set per tenere traccia delle combinazioni già elaborate**\n",
    "        processed_models = set()\n",
    "\n",
    "        ** Path delle performance dei modelli ottimizzati con weight and biases**\n",
    "        ** Path per trovare le best performances di ogni modello per ogni combinazione dei dati**\n",
    "        base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results\"\n",
    "\n",
    "        ** Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder**\n",
    "        save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_post_WB\"\n",
    "\n",
    "\n",
    "        ** --- LOOP PRINCIPALE (con minime modifiche) ---**\n",
    "        for key, (X_data, y_data) in data_dict.items():\n",
    "\n",
    "            print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "\n",
    "            if key in processed_datasets:\n",
    "                print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "                continue\n",
    "\n",
    "            processed_datasets.add(key)\n",
    "\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "            print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "\n",
    "            for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "\n",
    "                model_key = f\"{model_name}_{key}\"\n",
    "                if model_key in processed_models:\n",
    "                    print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "                    continue\n",
    "                processed_models.add(model_key)\n",
    "\n",
    "                print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "\n",
    "                # Prova a caricare la configurazione e i pesi ottimali dal file .pkl\n",
    "\n",
    "                '''\n",
    "                load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "                parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "\n",
    "                exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "                pesi del modello e i suoi iper-parametri\n",
    "\n",
    "                Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "                '''\n",
    "\n",
    "                config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "\n",
    "                if config is None:\n",
    "                    raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "\n",
    "                '''\n",
    "                Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "                MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "                '''\n",
    "\n",
    "                # Parsifica la chiave una volta sola e memorizza i valori\n",
    "                exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "\n",
    "                '''\n",
    "                Dpodiché, \n",
    "\n",
    "                1) si carica i vari valori degli iper-parametri,\n",
    "                2) si esegue la standardizzazione se servisse,\n",
    "                3) prepara il modello per la divisione in train_loader etc.,\n",
    "                4) si carica la configurazione dei pesi del modello, \n",
    "                5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "\n",
    "                6) esegue il training e il test e poi\n",
    "\n",
    "                7) si salva il tutto nella path corrispondente...\n",
    "\n",
    "                '''\n",
    "\n",
    "                '''\n",
    "                PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "                DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "\n",
    "                model_batch_size = config[\"batch_size\"]\n",
    "                model_n_epochs = config[\"n_epochs\"]\n",
    "                model_patience = config[\"patience\"]\n",
    "                model_lr = config[\"lr\"]\n",
    "                model_weight_decay = config[\"weight_decay\"]\n",
    "                model_standardization = config[\"standardization\"]\n",
    "\n",
    "                print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, weight_decay= \\033[1m{model_weight_decay}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "\n",
    "                # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "                models_info[model_key] = {\"standardization\": model_standardization}\n",
    "\n",
    "\n",
    "                '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "                IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "                '''\n",
    "\n",
    "                if model_standardization:\n",
    "                    X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "                    print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "                else:\n",
    "                    print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "\n",
    "                # Sposta il modello sulla GPU (se disponibile)\n",
    "                device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "                # Preparazione dei dataloaders\n",
    "                train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "                    X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "\n",
    "                # Inizializzazione del modello\n",
    "                if model_name == \"CNN2D\":\n",
    "                    model = CNN2D(input_channels=3, num_classes=2)\n",
    "                elif model_name == \"BiLSTM\":\n",
    "                    model = ReadMEndYou(input_size= 3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "                elif model_name == \"Transformer\":\n",
    "                    model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "                else:\n",
    "                    raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "\n",
    "                # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "                if best_weights is not None:\n",
    "                    try:\n",
    "                        model.load_state_dict(best_weights)\n",
    "                        print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "                        continue\n",
    "\n",
    "\n",
    "                # Definizione del criterio di perdita\n",
    "                criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "\n",
    "                # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "\n",
    "                print(f\"🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "                my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "\n",
    "                print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "                my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "\n",
    "                '''\n",
    "                GRADCAM COMPUTATION PER IL MODELLO CNN2D\n",
    "\n",
    "                La funzione compute_gradcam_figure estrae due campioni (uno per ogni classe) e crea una figura con le due righe richieste.\n",
    "\n",
    "                Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "                'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "\n",
    "                La funzione 'save_performance_results' è stata modificata \n",
    "                per gestire ANCHE questo nuovo input dell'immagine \n",
    "\n",
    "                (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "                seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "\n",
    "                - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "                - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "                - provenienza del dato stesso (i.e., familiar_th)\n",
    "                )\n",
    "\n",
    "                '''\n",
    "\n",
    "                # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "                gradcam_image = None\n",
    "\n",
    "                if model_name == \"CNN2D\":\n",
    "                    gradcam_image = compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device)\n",
    "                    if gradcam_image is not None:\n",
    "                        print(f\"GradCAM image computed successfully for {model_name}.\")\n",
    "\n",
    "                print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "                save_performance_results(model_name,\n",
    "                                         my_train_results,\n",
    "                                         my_test_results,\n",
    "                                         key,\n",
    "                                         exp_cond,\n",
    "                                         model_standardization,\n",
    "                                         base_folder = save_path_folder,\n",
    "                                         gradcam_image = gradcam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8301f-2499-4020-8f76-7981c2147848",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **UTILS DATI NON HYPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addc35a5-dcea-4cb4-8a7c-5da900c0d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(data_type, category, subject_type, condition = \"th_resp_vs_pt_resp\"):\n",
    "    \"\"\"\n",
    "    Carica i dati EEG dalla directory appropriata, già salvati con la finestra temporale (50°-300° punto)\n",
    "\n",
    "    Parameters:\n",
    "    - data_type: str, \"spectrograms\",\n",
    "    - category: str, \"familiar\" o \"unfamiliar\"\n",
    "    - subject_type: str, \"th\" (terapisti) o \"pt\" (pazienti)\n",
    "    - condition: str, condizione sperimentale da selezionare\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - X: Dati EEG sotto-selezionati (50°-300° punto e canali selezionati se applicabile)\n",
    "    - y: Etichette corrispondenti\n",
    "    \"\"\"\n",
    "\n",
    "    # Definizione dei percorsi base\n",
    "    base_paths = {\n",
    "        \"spectrograms\": {\n",
    "            \"familiar\": \"/home/stefano/Interrogait/all_datas/Familiar_Spectrograms_channels_frequencies/\",\n",
    "            \"unfamiliar\": \"/home/stefano/Interrogait/all_datas/Unfamiliar_Spectrograms_channels_frequencies/\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Seleziona il path corretto\n",
    "    base_path = base_paths[data_type][category]\n",
    "\n",
    "    # Determina il nome del file corretto\n",
    "    if data_type in [\"spectrograms\"]:\n",
    "        filename = f\"new_all_{subject_type}_concat_spectrograms_coupled_exp.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"data_type non valido!\")\n",
    "        \n",
    "    # Caricamento del file\n",
    "    filepath = base_path + filename\n",
    "    \n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    '''\n",
    "    Per i dati spectrogram, la funzione seleziona la condizione desiderata (i.e., condition = \"th_resp_vs_pt_resp\") \n",
    "    e preleva i dati e le etichette associati a quella condizione.\n",
    "    '''\n",
    "    \n",
    "    # Selezione della finestra temporale e delle etichette\n",
    "    X = data[condition][\"data\"]\n",
    "    y = data[condition][\"labels\"]\n",
    "\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def select_channels(data, channels=[12, 30, 48]):\n",
    "    \"\"\"\n",
    "    Seleziona i canali EEG specificati SOLO per i dati 1-20 e 1-45.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array NumPy, dati EEG con shape (n_trials, n_channels, n_timepoints)\n",
    "    - channels: list, indici dei canali da selezionare\n",
    "\n",
    "    Returns:\n",
    "    - data filtrato sui canali specificati\n",
    "    \"\"\"\n",
    "    return data[:, channels, :]\n",
    "\n",
    "\n",
    "# Funzione per train-test split\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "'''ATTENZIONE MODIFICATA FUNZIONE DI STANDARDIZZAZIONE'''\n",
    "# Funzione per standardizzare i dati\n",
    "# Con questa modifica eviti che std==0 produca NaN e i tuoi loss torneranno numeri sensati.\n",
    "def standardize_data(X_train, X_val, X_test, eps = 1e-8):\n",
    "    \n",
    "    mean = X_train.mean(axis=0, keepdims=True)\n",
    "    std = X_train.std(axis=0, keepdims=True)\n",
    "    \n",
    "    #aggiungo eps per evitare divisione per zero\n",
    "    X_train = (X_train - mean) / (std + eps)\n",
    "    X_val = (X_val - mean) / (std + eps)\n",
    "    X_test = (X_test - mean) / (std + eps)\n",
    "    \n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "# Import modelli (definisci le classi CNN1D, ReadMEndYou, ReadMYMind)\n",
    "#from models import CNN1D, ReadMEndYou, ReadMYMind  # Assicurati di avere i modelli definiti in 'models.py'\n",
    "\n",
    "# Funzione per inizializzare i modelli\n",
    "def initialize_models():\n",
    "    #model = CNN1D(input_channels=3, num_classes=2)\n",
    "    model_CNN = CNN2D(input_channels=61, num_classes=2)\n",
    "    #model_LSTM = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    model_LSTM = ReadMEndYou(input_size=3 * 26, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "    #model_Transformer = ReadMYMind(num_channels=3, seq_length=250, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "    model_Transformer = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=3, freqs=26)\n",
    "    \n",
    "    return model_CNN, model_LSTM, model_Transformer\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "'''\n",
    "Questa funzione prende in input i dati di training, validation e test, \n",
    "il tipo di modello scelto e la dimensione del batch. Si occupa di:\n",
    "\n",
    "Calcolare i pesi delle classi.\n",
    "Convertire i dati in tensori PyTorch, con le opportune trasformazioni per CNN, LSTM o Transformer.\n",
    "Creare i dataset e i dataloader per il training.\n",
    "'''\n",
    "\n",
    "\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48):\n",
    "    \n",
    "    # Calcolo dei pesi delle classi\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                         classes=np.unique(y_train), \n",
    "                                         y=y_train)\n",
    "    \n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    class_weights_tensor = class_weights_tensor.to(dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch con permutazione se necessario\n",
    "    \n",
    "    '''ATTENZIONE CAMBIATO QUI!'''\n",
    "    #if model_type == \"CNN2D\":\n",
    "    \n",
    "    if model_type == \"CNN2D_LSTM_FC\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    elif model_type == \"TopomapNet\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    elif model_type == \"CNN3D_LSTM_FC\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    elif model_type == \"SeparableCNN2D_LSTM_FC\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    #BiLSTM (ReadMEndYou):\n",
    "    #Ora il modello si aspetta l’input con shape (batch, canali, frequenze, tempo) \n",
    "    #e, al suo interno, \n",
    "    #esegue la permutazione per avere il tempo come dimensione sequenziale. \n",
    "    #Non serve quindi applicare una permutazione anche qui.\n",
    "    \n",
    "    elif model_type == \"BiLSTM\":\n",
    "            \n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    #Transformer (ReadMYMind):\n",
    "    #Analogamente, il modello gestisce internamente la riorganizzazione dell’input, quindi lasciamo i dati nella loro forma originale.\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Modello non riconosciuto. Scegli tra 'CNN', 'LSTM' o 'Transformer'.\")\n",
    "    \n",
    "    # Conversione delle etichette in tensori\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Creazione dei dataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Creazione dei dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "OLD VERSIONS BEFORE GRADCAM COMPUTATION ON CNN2D\n",
    "\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, model_standardization):\n",
    "    \n",
    "    #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "    if '_familiar_th' in key:\n",
    "        return 'th_fam'\n",
    "    elif '_unfamiliar_th' in key:\n",
    "        return 'th_unfam'\n",
    "    elif '_familiar_pt' in key:\n",
    "        return 'pt_fam'\n",
    "    elif '_unfamiliar_pt' in key:\n",
    "        return 'pt_unfam'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "     \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, my_train_results, my_test_results, key, exp_cond, model_standardization, base_folder):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, model_standardization)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"spectrograms\" in key:\n",
    "        data_type_str = \"spectrograms\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file con l'inclusione della combinazione key + model_name\n",
    "    if model_standardization:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}_std.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "        \n",
    "    else:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        \n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\nRisultati salvati con successo 👍 in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌Errore durante il salvataggio dei risultati: {e}\")\n",
    "'''\n",
    "\n",
    "\n",
    "#NEW VERSIONS FOR SPECTROGRAMS WITH GRADCAM COMPUTATION ON CNN2D!\n",
    "\n",
    "# Funzione per determinare a quale subfolder appartiene la chiave\n",
    "def get_subfolder_from_key(key, model_standardization):\n",
    "    \n",
    "    #DEFINIZIONE DELLA PATH DOVE VIENE SALVATO IL FILE\n",
    "    if '_familiar_th' in key:\n",
    "        return 'th_fam'\n",
    "    elif '_unfamiliar_th' in key:\n",
    "        return 'th_unfam'\n",
    "    elif '_familiar_pt' in key:\n",
    "        return 'pt_fam'\n",
    "    elif '_unfamiliar_pt' in key:\n",
    "        return 'pt_unfam'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "     \n",
    "# Funzione per salvare i risultati\n",
    "def save_performance_results(model_name, \n",
    "                             my_train_results,\n",
    "                             my_test_results, \n",
    "                             key,\n",
    "                             exp_cond,\n",
    "                             model_standardization,\n",
    "                             base_folder,\n",
    "                             gradcam_image = None):\n",
    "    \"\"\"\n",
    "    Funzione che salva i risultati del modello in base alla combinazione di 'key' e 'model_name'.\n",
    "    Se gradcam_image è fornita, la salva anche in formato PNG con un nome che inizia con 'GradCAM_results'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificazione del subfolder in base alla chiave\n",
    "    subfolder = get_subfolder_from_key(key, model_standardization)\n",
    "    \n",
    "    # Debug: controllo sulla subfolder\n",
    "    print(f\"\\nDEBUG - Chiave: \\033[1m{key}\\033[0m, Subfolder ottenuto: \\033[1m{subfolder}\\033[0m\")\n",
    "    \n",
    "    if subfolder is None:\n",
    "        print(f\"Errore: La chiave \\033[1m{key}\\033[0m non corrisponde a nessun subfolder valido.\\n\")\n",
    "        return\n",
    "    \n",
    "    # Determinazione del tipo di dato direttamente dalla chiave\n",
    "    if \"spectrograms\" in key:\n",
    "        data_type_str = \"spectrograms\"\n",
    "    else:\n",
    "        print(f\"Errore: Tipo di dato non riconosciuto nella chiave '{key}'.\")\n",
    "        return\n",
    "\n",
    "    # Creazione del nome del file pickle con l'inclusione della combinazione key + model_name\n",
    "    if model_standardization:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}_std.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "    else:\n",
    "        file_name = f\"{model_name}_performances_{exp_cond}_{data_type_str}_{subfolder}.pkl\"\n",
    "        folder_path = os.path.join(base_folder, exp_cond, data_type_str, subfolder)\n",
    "    \n",
    "    # Verifica se la cartella di destinazione esiste, altrimenti creala\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Creazione del dizionario con i risultati\n",
    "    results_dict = {\n",
    "        'my_train_results': my_train_results,\n",
    "        'my_test_results': my_test_results\n",
    "    }\n",
    "\n",
    "    # Salvataggio del dizionario con i risultati\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(results_dict, f)\n",
    "        print(f\"\\n🔬Risultati salvati con successo 👍 in: \\n\\033[1m{file_path}\\033[0m\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌Errore durante il salvataggio dei risultati: {e}\")\n",
    "    \n",
    "    # Se è stata fornita l'immagine GradCAM, salvala come file PNG\n",
    "    if gradcam_image is not None:\n",
    "        if model_standardization:\n",
    "            gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}_std.png\"\n",
    "        else:\n",
    "            gradcam_file_name = f\"GradCAM_results_{model_name}_{exp_cond}_{data_type_str}_{subfolder}.png\"\n",
    "        \n",
    "        gradcam_file_path = os.path.join(folder_path, gradcam_file_name)\n",
    "        \n",
    "        #try:\n",
    "        #    with open(gradcam_file_path, \"wb\") as f_img:\n",
    "        #        f_img.write(gradcam_image)\n",
    "        #    print(f\"\\n📸Immagine GradCAM salvata con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            '''\n",
    "            Se gradcam_image è un oggetto BytesIO, allora rappresenta un flusso di dati binari in memoria.\n",
    "            Quando si leggono dati da un BytesIO, il cursore interno avanza come in un file normale. \n",
    "            Se il cursore non è all'inizio, Image.open() potrebbe non leggere correttamente l'immagine.\n",
    "            👉 seek(0) riporta il cursore all'inizio del buffer prima di leggerlo con Image.open()\n",
    "            \n",
    "            Per maggior info leggi cella successiva!\n",
    "            '''\n",
    "            \n",
    "            # 🔄 Se gradcam_image è un buffer, convertirlo in immagine PIL\n",
    "            if isinstance(gradcam_image, io.BytesIO):\n",
    "                gradcam_image.seek(0)  # 🔄 Reset puntatore del buffer\n",
    "                gradcam_image = Image.open(gradcam_image)\n",
    "            \n",
    "            '''\n",
    "            Il messaggio di errore indica che il tuo oggetto gradcam_image è di tipo bytes e non ha il metodo save(), \n",
    "            che è tipico di un oggetto PIL. \n",
    "            \n",
    "            Per risolvere questo, devi convertire i byte in un'immagine PIL. \n",
    "            Per farlo, controlla se gradcam_image sia un oggetto di tipo bytes e,\n",
    "            in tal caso, usa io.BytesIO per creare un buffer da passare a Image.open(). \n",
    "            \n",
    "            Inserisci questa conversione all'interno del blocco che salva l'immagine, così da assicurarti che,\n",
    "            indipendentemente dal tipo, gradcam_image diventi un oggetto PIL e possa chiamare il metodo save().\n",
    "            '''\n",
    "            \n",
    "            if isinstance(gradcam_image, bytes):\n",
    "                gradcam_image = io.BytesIO(gradcam_image)\n",
    "                gradcam_image.seek(0)\n",
    "                gradcam_image = Image.open(gradcam_image)\n",
    "            \n",
    "            \n",
    "            print(f\"\\n📸Immagine \\033[1mGradCAM salvata\\033[0m con successo 👍 in: \\n\\033[1m{gradcam_file_path}\\033[0m\\n\")\n",
    "            # 🔄 Salvare l'immagine nel percorso specificato\n",
    "            gradcam_image.save(gradcam_file_path, format = \"PNG\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌Errore durante il salvataggio dell'immagine GradCAM: {e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e4f612b-21c0-409e-87ee-1793488e5934",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Quando si parla di buffer o file, immagina che ci sia un piccolo cursore invisibile che tiene traccia di dove ci troviamo nella lettura/scrittura.\n",
    "\n",
    "📌 Cos'è il cursore di un file o buffer?\n",
    "Il cursore interno è un puntatore che indica la posizione attuale nel file (o buffer).\n",
    "\n",
    "Quando scrivi dati, il cursore avanza alla fine di ciò che hai scritto.\n",
    "Quando leggi, il cursore avanza man mano che scorri i dati.\n",
    "Se tenti di leggere senza riportare il cursore all'inizio, potresti ottenere dati incompleti o un errore.\n",
    "📌 Esempio con un file\n",
    "Immagina un file di testo chiamato esempio.txt con questo contenuto:\n",
    "\n",
    "Copia\n",
    "Modifica\n",
    "Ciao, come stai?\n",
    "Ora vediamo cosa succede quando lo leggiamo:\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "# Apriamo il file in modalità lettura\n",
    "with open(\"esempio.txt\", \"r\") as f:\n",
    "    print(f.read())  # ✅ Leggiamo tutto -> \"Ciao, come stai?\"\n",
    "    \n",
    "    print(f.read())  # ❌ Ora il cursore è alla fine -> \"\" (stringa vuota!)\n",
    "Il secondo read() non restituisce nulla perché il cursore è già alla fine del file.\n",
    "Per rileggere il file dobbiamo spostare il cursore all'inizio con seek(0):\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "with open(\"esempio.txt\", \"r\") as f:\n",
    "    print(f.read())  # ✅ Legge tutto -> \"Ciao, come stai?\"\n",
    "    \n",
    "    f.seek(0)  # 🔄 Riporta il cursore all'inizio\n",
    "    \n",
    "    print(f.read())  # ✅ Ora rilegge tutto -> \"Ciao, come stai?\"\n",
    "📌 Esempio con BytesIO (buffer in memoria)\n",
    "Un BytesIO funziona come un file, ma è in RAM. Vediamo cosa succede senza seek(0):\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "import io\n",
    "\n",
    "# Creiamo un buffer\n",
    "buffer = io.BytesIO()\n",
    "buffer.write(b\"Ciao, come stai?\")  # ✍️ Scriviamo qualcosa\n",
    "\n",
    "print(buffer.read())  # ❌ \"\" perché il cursore è alla fine!\n",
    "\n",
    "buffer.seek(0)  # 🔄 Riportiamo il cursore all'inizio\n",
    "print(buffer.read())  # ✅ \"Ciao, come stai?\"\n",
    "📌 Applicazione al tuo codice GradCAM\n",
    "Nel tuo caso, la sequenza è questa:\n",
    "\n",
    "1️⃣ Crei un BytesIO()\n",
    "2️⃣ Salvi l'immagine nel buffer → il cursore ora è alla fine\n",
    "3️⃣ Per poterla leggere con Image.open(), devi riportarlo all'inizio con seek(0)\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "buf = io.BytesIO()\n",
    "plt.savefig(buf, format='png')  # ✍️ Salvataggio avanza il cursore\n",
    "buf.seek(0)  # 🔄 Riporta il cursore all'inizio per poterlo leggere\n",
    "fig_image = buf.getvalue()  # Ora possiamo leggere i byte correttamente!\n",
    "buf.close()\n",
    "E quando passi il buffer a save_performance_results, devi ripetere il seek(0) prima di aprirlo con Image.open().\n",
    "\n",
    "💡 Conclusione:\n",
    "Il cursore è come un segnalibro in un file o buffer. Se non lo riporti all'inizio, leggere i dati successivamente potrebbe fallire! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbf3b9-76c6-48c9-8138-3f387b71fba6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **NUOVE UTILS DATI NON HYPER POST W&B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be40e4-67b0-406c-ab48-c224607e44c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **SUGGERIMENTI DI MODIFICA CHATGPT DELLE UTILS DATI NON HYPER POST W&B**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab9b0d5d-f49f-4b26-93be-a175022be9a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### COME ANDREBBE CAMBIATA 'PREPARE_DATA_FOR_MODEL' IN CASO VOLESSI INTEGRARE LA STANDARDIZATION AL SUO INTERNO\n",
    "\n",
    "'''\n",
    "# Esempio di funzione di data preparation (OPZIONE 1)\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48, standardize=True):\n",
    "    # Calcolo dei pesi delle classi\n",
    "    # (Si assume che compute_class_weight sia già definita altrove)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(torch.float32)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Se la standardizzazione è richiesta, la eseguo qui (funzione ipotetica standardize_data)\n",
    "    if standardize:\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "    else:\n",
    "        print(\"Standardizzazione NON eseguita!\")\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch\n",
    "    if model_type == \"CNN1D\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        # Permutazione se necessario\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Modello {model_type} non riconosciuto nella preparazione dei dati.\")\n",
    "    \n",
    "    # Creazione dei DataLoader (si assume che DataLoader sia importato)\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, torch.tensor(y_train))\n",
    "    val_dataset   = TensorDataset(X_val_tensor, torch.tensor(y_val))\n",
    "    test_dataset  = TensorDataset(X_test_tensor, torch.tensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc22f9fb-219c-494e-9941-645c98b417c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### COME ANDREBBE CAMBIATA LA LOGICA DEL LOOP PER CHATGTP\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"1_20|1_45|wavelet\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(1_20|1_45|wavelet)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\", \n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "# Funzione per caricare il file .pkl con la configurazione e i pesi ottimali\n",
    "def load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path):\n",
    "    \"\"\"\n",
    "    Costruisce il path usando:\n",
    "        base_path / exp_cond / data_type / category_subject\n",
    "    e il nome del file:\n",
    "        {model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\n",
    "    Se il file esiste, lo carica e restituisce (config, state_dict).\n",
    "    \"\"\"\n",
    "    file_name = f\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "    file_path = os.path.join(base_path, exp_cond, data_type, category_subject, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Caricamento file pkl: {file_path}\")\n",
    "        # Si assume che il file .pkl sia salvato con torch.save() e contenga un dizionario con chiavi \"config\" e \"state_dict\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = torch.load(f)\n",
    "        return data[\"config\"], data[\"state_dict\"]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file_path} non trovato.\")\n",
    "\n",
    "# Esempio di funzione di data preparation (OPZIONE 1)\n",
    "def prepare_data_for_model(X_train, X_val, X_test, y_train, y_val, y_test, model_type, batch_size=48, standardize=True):\n",
    "    # Calcolo dei pesi delle classi\n",
    "    # (Si assume che compute_class_weight sia già definita altrove)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(torch.float32)\n",
    "    \n",
    "    # Conversione delle etichette in interi\n",
    "    y_train = y_train.astype(int)\n",
    "    y_val = y_val.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Se la standardizzazione è richiesta, la eseguo qui (funzione ipotetica standardize_data)\n",
    "    if standardize:\n",
    "        X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "    else:\n",
    "        print(\"Standardizzazione NON eseguita!\")\n",
    "    \n",
    "    # Conversione dei dati in tensori PyTorch\n",
    "    if model_type == \"CNN1D\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        # Permutazione se necessario\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32).permute(0, 2, 1)\n",
    "    elif model_type == \"Transformer\":\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_val_tensor   = torch.tensor(X_val, dtype=torch.float32)\n",
    "        X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Modello {model_type} non riconosciuto nella preparazione dei dati.\")\n",
    "    \n",
    "    # Creazione dei DataLoader (si assume che DataLoader sia importato)\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, torch.tensor(y_train))\n",
    "    val_dataset   = TensorDataset(X_val_tensor, torch.tensor(y_val))\n",
    "    test_dataset  = TensorDataset(X_test_tensor, torch.tensor(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, class_weights_tensor\n",
    "\n",
    "# Funzione di training (come da te definita)\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs=100, patience=10):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loss_train_history = []\n",
    "    loss_val_history = []\n",
    "    accuracy_train_history = []\n",
    "    accuracy_val_history = []\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_model = None\n",
    "\n",
    "    pbar = tqdm(range(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        train_loss_tmp = []\n",
    "        correct_train = 0\n",
    "        y_true_train_list = []\n",
    "        y_pred_train_list = []\n",
    "        \n",
    "        # Fase di training\n",
    "        for x, y in dataset_train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_tmp.append(train_loss.item())\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Fase di validazione\n",
    "        loss_tmp_val = []\n",
    "        correct_val = 0\n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataset_val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "                loss_tmp_val.append(val_loss.item())\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val))\n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            best_model = cp.deepcopy(model)\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "    \n",
    "    print(f\"Miglior epoca: {best_epoch} con Val Acc: {max_val_acc:.4f}\")\n",
    "    # Restituisco un dizionario con i risultati (semplificato)\n",
    "    train_results = {\n",
    "        \"best_model\": best_model,\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history\n",
    "    }\n",
    "    return train_results\n",
    "\n",
    "# --- LOOP PRINCIPALE INTEGRAZIONE CONFIG DA .pkl ---\n",
    "\n",
    "# Set per tenere traccia dei dataset e dei modelli già elaborati\n",
    "processed_datasets = set()\n",
    "processed_models = set()\n",
    "\n",
    "# Supponiamo che data_dict sia già stato creato con chiavi nel formato:\n",
    "# \"th_resp_vs_pt_resp_wavelet_familiar_th\" (che contiene anche exp_cond)\n",
    "# e con valori (X_data, y_data)\n",
    "# Ad esempio:\n",
    "# data_dict = {\"th_resp_vs_pt_resp_wavelet_familiar_th\": (X, y), ...}\n",
    "\n",
    "# Definisci il base_path in cui sono salvati i file .pkl\n",
    "base_path = \"/home/stefano/Interrogait/WB_time_domain_best_results/\"\n",
    "\n",
    "# Seleziona il dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loop sui dataset\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: {key}\")\n",
    "    \n",
    "    # Controlla se il dataset è già stato elaborato\n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    # (Qui va la suddivisione in train, validation, test)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Iterazione sui modelli\n",
    "    for model_name in [\"CNN1D\", \"BiLSTM\", \"Transformer\"]:\n",
    "        \n",
    "        model_key = f\"{key}_{model_name}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        # Imposta il seme per la riproducibilità\n",
    "        torch.manual_seed(32)\n",
    "        np.random.seed(32)\n",
    "        random.seed(32)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(32)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset {key} e il modello {model_name}...\")\n",
    "        \n",
    "        # --- Estrazione della configurazione ottimale dal file .pkl ---\n",
    "        try:\n",
    "            # La chiave key contiene già l'experimental condition; per sicurezza, la parsifichiamo:\n",
    "            exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "            config, best_weights = load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento della configurazione per {model_name} su {key}: {e}\")\n",
    "            continue  # Passa al prossimo modello se non troviamo il file\n",
    "        \n",
    "        # Estrai i parametri di interesse dal dizionario di configurazione\n",
    "        batch_size      = config.get(\"batch_size\", 48)\n",
    "        n_epochs        = config.get(\"n_epochs\", 40)\n",
    "        patience        = config.get(\"patience\", 10)\n",
    "        lr              = config.get(\"lr\", 1e-4)\n",
    "        weight_decay    = config.get(\"weight_decay\", 0)\n",
    "        standardization = config.get(\"standardization\", False)\n",
    "        \n",
    "        print(f\"Parametri estratti dal file pkl per {model_name}: batch_size={batch_size}, n_epochs={n_epochs}, patience={patience}, lr={lr}, weight_decay={weight_decay}, standardization={standardization}\")\n",
    "        \n",
    "        # --- Preparazione dei dataloaders usando i parametri estratti ---\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type=model_name, batch_size=batch_size, standardize=standardization\n",
    "        )\n",
    "        \n",
    "        # --- Inizializzazione del modello (OPZIONE 2) ---\n",
    "        if model_name == \"CNN1D\":\n",
    "            model = CNN1D(input_channels=3, num_classes=2)\n",
    "        elif model_name == \"BiLSTM\":\n",
    "            model = ReadMEndYou(input_size=3, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        elif model_name == \"Transformer\":\n",
    "            model = ReadMYMind(num_channels=3, seq_length=75, d_model=16, num_heads=4, num_layers=2, num_classes=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Carica lo stato dei pesi ottimali nel modello\n",
    "        try:\n",
    "            model.load_state_dict(best_weights)\n",
    "            print(f\"Modello {model_name} inizializzato con i pesi ottimali dal file .pkl.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Definizione del criterio di perdita con i class weights\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        \n",
    "        # Definizione dell'ottimizzatore usando lr e weight_decay dal file di config\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        print(f\"Avvio del training per il modello {model_name} sul dataset {key}...\")\n",
    "        my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs=n_epochs, patience=patience)\n",
    "        \n",
    "        print(f\"Avvio del testing per il modello {model_name} sul dataset {key}...\")\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        print(f\"Salvataggio dei risultati per il modello {model_name} sul dataset {key}...\")\n",
    "        save_performance_results(model_name, my_train_results, my_test_results, key, data_type, condition=exp_cond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50745d47-e386-4ce7-920c-892d3916fa5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### **IMPLEMENTAZIONE ADOTTATA (ONLY HYPERPARAMS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4666f702-f197-475b-afb5-3e505f0a26bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condizione: pt_resp_vs_shared_resp\n",
      "Data Type: spectrograms\n",
      "Soggetto: familiar_th\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Parsing della chiave e costruzione del path:\n",
    "Usando la funzione parse_combination_key si estraggono \n",
    "\n",
    "exp_cond, data_type e category_subject dalla chiave del dataset. \n",
    "\n",
    "Questi vengono usati per costruire il percorso in cui cercare i file .pkl.\n",
    "'''\n",
    "import re \n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"1_20|1_45|wavelet\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\", \n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "\n",
    "'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "# Test\n",
    "combination_key = \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "condition_experiment, data_type, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "print(\"Condizione:\", condition_experiment)\n",
    "print(\"Data Type:\", data_type)\n",
    "print(\"Soggetto:\", subject_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab556242-973d-4a25-aa8d-086b75f6c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Verifica del file .pkl:\n",
    "La funzione load_config_if_available cerca, per ogni modello, il file con nome del tipo\n",
    "\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "all’interno della struttura di cartelle basata su base_path. \n",
    "\n",
    "Se il file esiste, allora viene passata poi a load_model_config_and_weights, \n",
    "che carica il dizionario di partenza \n",
    "e da questo estrae i 2 sotto-dizionari 'config' e 'state_dict'.\n",
    "'''\n",
    "\n",
    "def load_config_if_available(dataset_key, model_name, base_path):\n",
    "    \"\"\"\n",
    "    Data una chiave (es. \"th_resp_vs_pt_resp_wavelet_familiar_th\") e il nome del modello,\n",
    "    cerca il file .pkl corrispondente e ritorna (config, state_dict).\n",
    "    Se non esiste, restituisce (None, None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(dataset_key)\n",
    "        config, state_dict = load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path)\n",
    "        print(f\"✅ File .pkl trovato per \\033[1m{model_name}\\033[0m su \\033[1m{dataset_key}\\033[0m\")\n",
    "        \n",
    "        return config, state_dict\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Nessun file .pkl per {model_name} su {dataset_key} - uso parametri di default. ({e})\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13aad267-4e82-486b-b2a1-42fbd0196f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Caricamento del file .pkl:\n",
    "La funzione load_model_config_and_weights cerca, per ogni modello, il file con nome del tipo\n",
    "\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "all’interno della struttura di cartelle basata su base_path. Se il file esiste, vengono restituiti config e state_dict.\n",
    "'''\n",
    "\n",
    "# Funzione per caricare il file .pkl con la configurazione e i pesi ottimali\n",
    "def load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path):\n",
    "    \"\"\"\n",
    "    Costruisce il path usando:\n",
    "        base_path / exp_cond / data_type / category_subject\n",
    "    e il nome del file:\n",
    "        {model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\n",
    "    Se il file esiste, lo carica e restituisce (config, state_dict).\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = f\"{model_name}_{exp_cond}_{data_type}_{category_subject}.pkl\"\n",
    "    file_path = os.path.join(base_path, exp_cond, data_type, category_subject, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"🕵️‍♂️🔍Caricamento file .pkl: \\033[1m{file_path}\\033[0m\")\n",
    "        \n",
    "        # Il file .pkl è stato salvato con torch.save() e contiene un dizionario con chiavi al suo interno che sono: \"config\" e \"state_dict\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data = torch.load(f)\n",
    "        return data[\"config\"], data[\"state_dict\"]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File {file_path} non trovato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7b8eb-53ce-45f1-9685-7037bd8280f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **IMPLEMENTAZIONE ADOTTATA (PARAMS ED HYPERPARAMS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665bb38-d89a-412e-b043-f97e3c24aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Prima si cerca il modello migliore con queste \"scan_folder_for_best_model\" (e \"clean_config\")\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def clean_config(config):\n",
    "    \"\"\"Rimuove la chiave '_wandb' dal dizionario di configurazione, se presente.\"\"\"\n",
    "    if \"_wandb\" in config:\n",
    "        del config[\"_wandb\"]\n",
    "    return config\n",
    "\n",
    "def scan_folder_for_best_model(folder_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scansiona la cartella folder_path per file .pkl, estrae il valore di 'max_val_acc'\n",
    "    da ciascun file e restituisce (best_file, best_val) dove best_file è il file con \n",
    "    il valore più alto di max_val_acc.\n",
    "    \"\"\"\n",
    "    best_val = -float('inf')\n",
    "    best_file = None\n",
    "    \n",
    "    # Imposta la device per la GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "            try:\n",
    "            #    data = torch.load(file_path, map_location = torch.device('cpu'))\n",
    "            \n",
    "                with torch.serialization.safe_globals({\"CNN2D\": CNN2D}):\n",
    "                    data = torch.load(file_path, map_location = device, weights_only=False)\n",
    "\n",
    "                # Pulizia opzionale della configurazione\n",
    "                data['config'] = clean_config(data['config'])\n",
    "                \n",
    "                current_val = data.get(\"max_val_acc\", -float('inf'))\n",
    "                \n",
    "                print(f\"File {file}: max_val_acc = {current_val}\")\n",
    "                \n",
    "                if current_val > best_val:\n",
    "                    best_val = current_val\n",
    "                    best_file = file_path\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "    \n",
    "    if best_file:\n",
    "        print(f\"\\nIl file con la migliore accuratezza (max_val_acc) è \\n\\033[1m{best_file}\\033[0m con un valore di \\033[1m{best_val}\\033[0m.\")\n",
    "    return best_file #questa è una stringa!\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "#folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/familiar_th\"\n",
    "#best_file, best_val = scan_folder_for_best_model(folder)\n",
    "#print(f\"\\nIl file con il miglior modello è: {best_file} con max_val_acc = {best_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f54db-658c-4a2a-b001-c9ccf3284e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Si deve passare a 'load_config_if_available' \"best_file\" trovato da 'scan_folder_for_best_model' \n",
    "#e si deve estrapolare dalla sua stringa il suffisso che è '_v_*' dove * è un stringa di un numero variabile (da 1 a 15)\n",
    "\n",
    "import re\n",
    "\n",
    "'''\n",
    "# Funzione per estrarre il suffisso\n",
    "def extract_suffix(string):\n",
    "    match = re.search(r'_v_\\d+', string)  # Cerca \"_v_\" seguito da uno o più numeri\n",
    "    if match:\n",
    "        return match.group(0)  # Restituisce il suffisso trovato\n",
    "    return None  # Se non trovato, restituisce None\n",
    "\n",
    "# Stringa di esempio\n",
    "#s1 = 'CNN2D_th_resp_vs_pt_resp_spectrograms_familiar_th_v_1'\n",
    "#s2 = 'CNN2D_th_resp_vs_pt_resp_spectrograms_familiar_th_v_10'\n",
    "\n",
    "# Test\n",
    "#print(extract_suffix(s1))  # Output: _v_1\n",
    "#print(extract_suffix(s2))  # Output: _v_10\n",
    "'''\n",
    "\n",
    "\n",
    "def extract_suffix(filename):\n",
    "    match = re.search(r'_v_\\d+', filename)\n",
    "    if match:\n",
    "        suffix = match.group(0)\n",
    "        print(f\"\\n\\033[1mSuffix estratto\\033[0m da \\n{filename}: \\033[1m{suffix}\\033[0m\\n\")\n",
    "        return suffix\n",
    "    print(f\"Nessun suffisso trovato in {filename}\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd746522-f853-4cd5-8e65-47f9bb2f64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Dentro a  'load_config_if_available\", un volta estratto anche il suffisso da \"best_file\", \n",
    "\n",
    "#4) A quel punto, 'load_config_if_available' si fa PRIMA il parsing delle key corrente \n",
    "# (richiamando 'parse_combination_key') \n",
    "\n",
    "\n",
    "'''\n",
    "Parsing della chiave e costruzione del path:\n",
    "Usando la funzione parse_combination_key si estraggono \n",
    "\n",
    "exp_cond, data_type e category_subject dalla chiave del dataset. \n",
    "\n",
    "Questi vengono usati per costruire il percorso in cui cercare i file .pkl.\n",
    "'''\n",
    "import re \n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"1_20|1_45|wavelet\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\", \n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "\n",
    "#'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "# Test\n",
    "#combination_key = \"pt_resp_vs_shared_resp_spectrograms_familiar_th\"\n",
    "#condition_experiment, data_type, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "#print(\"Condizione:\", condition_experiment)\n",
    "#print(\"Data Type:\", data_type)\n",
    "#print(\"Soggetto:\", subject_key)\n",
    "\n",
    "\n",
    "#5) A quel punto, si chiama 'load_model_config_and_weights', \n",
    "# la quale si porta appresso ANCHE il 'suffix' tra i suoi argomenti di input per accedere al file .pkl corrispondente \n",
    "\n",
    "\n",
    "#6)Vengono ritornati in output da 'load_model_config_and_weights':\n",
    "\n",
    "#a) optimized_model\n",
    "#b)state_dict\n",
    "#c) model_config\n",
    "#d) training_config\n",
    "\n",
    "#che saranno che sono le variabili, il cui valore stringa, viene usato per accedere alle chiavi del dizionario dentro il file .pkl per prelevarsi, rispettivamente\n",
    "\n",
    "#- la versione del modello migliore corrente  \n",
    "#- pesi e bias della versione migliore del modello corrente della specifica combinazione di dati (i..e, state_dict ) \n",
    "#- valori dei parametri della versione migliore del modello corrente della specifica combinazione di dati (i..e, model_config) \n",
    "#- valore degli iper-parametri della versione migliore del modello corrente della specifica combinazione di dati (i..e, training_config )\n",
    "\n",
    "\n",
    "\n",
    "def load_config_if_available(dataset_key, model_name, base_path, best_file):\n",
    "    \n",
    "    \"\"\"\n",
    "    Si estrapola il suffisso del file della versione del modello migliore per la specifica combinazione di dati\n",
    "    che è variabile come '_v*' dove * è un unità od un indice numerico (tra 1 e 15)\n",
    "    \n",
    "    \n",
    "    Data una chiave (es. \"th_resp_vs_pt_resp_wavelet_familiar_th\") e il nome del modello,\n",
    "    1) cerca il file .pkl corrispondente (con suffisso opzionale)\n",
    "    2) e ritorna (config, state_dict).\n",
    "    Se non esiste, restituisce (None, None).\n",
    "    \"\"\"\n",
    "    \n",
    "    suffix = extract_suffix(best_file)\n",
    "    \n",
    "    try:\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(dataset_key)\n",
    "        \n",
    "        # Carica state_dict, model_config e training_config usando il suffisso\n",
    "        #optimized_model, state_dict, model_config, training_config = load_model_config_and_weights(\n",
    "        #    exp_cond, data_type, category_subject, model_name, base_path, suffix = suffix\n",
    "        #)\n",
    "        \n",
    "        optimized_model, model_config, training_config = load_model_config_and_weights(\n",
    "            exp_cond, data_type, category_subject, model_name, base_path, suffix = suffix\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ File .pkl trovato per {model_name} su {dataset_key} con suffisso: {suffix}\")\n",
    "    \n",
    "        # Ritorna un dizionario con, al suo interno, i dati caricati\n",
    "        #return {\"optimized_model\": optimized_model, \"state_dict\": state_dict, \"model_config\": model_config, \"config\": training_config}\n",
    "        return {\"optimized_model\": optimized_model, \"model_config\": model_config, \"config\": training_config}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Nessun file .pkl per {model_name} su {dataset_key} con suffisso '{suffix}' - uso parametri di default. ({e})\")\n",
    "        return None, None, None    \n",
    "\n",
    "    \n",
    "def load_model_config_and_weights(exp_cond, data_type, category_subject, model_name, base_path, suffix = \"\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Costruisce il path usando:\n",
    "    base_path / exp_cond / data_type / category_subject\n",
    "    e il nome del file:\n",
    "    {model_name}_{exp_cond}_{data_type}_{category_subject}{suffix}.pkl\n",
    "    Se il file esiste, lo carica e restituisce (config, state_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = f\"{model_name}_{exp_cond}_{data_type}_{category_subject}{suffix}.pkl\"\n",
    "    file_path = os.path.join(base_path, exp_cond, data_type, category_subject, file_name)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"\\n🕵️‍♂️🔍 Caricamento file .pkl: \\033[1m{file_path}\\033[0m\")\n",
    "\n",
    "        #with open(file_path, \"rb\") as f:\n",
    "        #    data = torch.load(f)\n",
    "        \n",
    "        '''\n",
    "        Questa parte di codice serve per caricare un modello salvato precedentemente in un file,\n",
    "        utilizzando la funzionalità di serializzazione sicura di PyTorch. \n",
    "        \n",
    "        Vediamo cosa succede in dettaglio:\n",
    "\n",
    "        Contesto sicuro con torch.serialization.safe_globals:\n",
    "\n",
    "            La funzione safe_globals consente di caricare il file di modello in un contesto controllato. \n",
    "            Questo è particolarmente utile se il modello salvato fa riferimento a classi o funzioni che non sono definite nel contesto corrente, \n",
    "            evitando problemi di sicurezza o errori di caricamento.\n",
    "\n",
    "            Nel tuo caso, il contesto sicuro assicura che la classe CNN2D, che è il modello, sia disponibile durante il caricamento del file. \n",
    "            Senza questa precauzione, se la classe CNN2D non fosse definita, PyTorch non saprebbe come ricostruirla dal file salvato.\n",
    "\n",
    "        Caricamento del modello con torch.load:\n",
    "\n",
    "            torch.load(file_path, map_location=torch.device('cpu')) carica il modello salvato dal percorso specificato (file_path), \n",
    "            assicurandosi che venga mappato sulla CPU \n",
    "            (INDIPENDENTEMENTE da dove sia stato salvato originariamente, se su GPU, ad esempio,\n",
    "            il parametro map_location si assicura che il modello venga caricato sulla CPU).\n",
    "\n",
    "            Quindi, questo codice che hai fornito permette di \n",
    "            \n",
    "            1) caricare un modello SALVATO, che è la versione \"modello migliore\" ,che vuoi confrontare successivamente con \n",
    "            \n",
    "            2) il modello che sarà RICOSTRUITO dinamicamente dalla funzione load_best_cnn2d  \n",
    "            \n",
    "            (presumibilmente una funzione che carica o costruisce il modello da zero o da alcuni parametri).\n",
    "        \n",
    "        '''\n",
    "        # Usa il contesto safe_globals per permettere il caricamento della classe CNN2D\n",
    "        with torch.serialization.safe_globals({\"CNN2D\": CNN2D}):\n",
    "            data = torch.load(file_path, map_location=torch.device('cpu'), weights_only = False)    \n",
    "    \n",
    "        optimized_model = data['model']\n",
    "        #state_dict = data[\"state_dict\"]\n",
    "        model_config = data[\"model_config\"]\n",
    "        training_config = data[\"config\"]\n",
    "            \n",
    "        #return optimized_model, state_dict, model_config, training_config\n",
    "        return optimized_model, model_config, training_config\n",
    "    else:\n",
    "        #raise FileNotFoundError(f\"File {file_path} non trovato.\")\n",
    "        #raise ValueError(f\"⚠️ FileNotFoundError(f\"File {file_path} non trovato.\")\n",
    "        raise ValueError(f\"⚠️ FileNotFoundError: \\nFile {file_path} non trovato.\")\n",
    "\n",
    "#Alla fine, questi 3 valori vengono ritornati da 'load_model_config_and_weights' come nuove variabili\n",
    "#(best_state_dict, best_model_config, best_training_config) e...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4efa8-f9e1-49d2-bc69-1b29a94bc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "#NEL FRATTEMPO, si chiamerà anche il costruttore dinamico del modello CNN2D ---> VEDI CELLA SOTTO \n",
    "\n",
    "                                        #MODELLI CNN2D, BILSTM e TRANSFORMER\n",
    "\n",
    "\n",
    "######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### ######### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d57aa-60ea-4ec7-9bc4-6fd40404716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quindi a quel punto, come dicevo si forniscono in ingresso questi valori\n",
    "\n",
    "#- best_state_dict : pesi e bias della versione migliore del modello corrente della specifica combinazione di dati (i..e, state_dict ) \n",
    "#- best_model_config: valori dei parametri della versione migliore del modello corrente della specifica combinazione di dati (i..e, model_config) \n",
    "#- best_training_config; valore degli iper-parametri della versione migliore del modello corrente della specifica combinazione di dati (i..e, training_config )\n",
    "\n",
    "\n",
    "#def load_best_cnn2d(best_model, best_state_dict, best_model_config, best_training_config):\n",
    "\n",
    "def load_best_cnn2d(best_model, best_model_config, best_training_config):\n",
    "\n",
    "    \"\"\"\n",
    "    Si carica i 4 dizionari che contengono\n",
    "    \n",
    "    - la versione migliore del modello ottimizzato corrente della specifica combinazione di dati (i..e, optimized_model ) \n",
    "    - pesi e bias della versione migliore del modello corrente della specifica combinazione di dati (i..e, state_dict ) \n",
    "    - valori dei parametri della versione migliore del modello corrente della specifica combinazione di dati (i..e, model_config) \n",
    "    - valore degli iper-parametri della versione migliore del modello corrente della specifica combinazione di dati (i..e, training_config )\n",
    "    \n",
    "    Args:\n",
    "        model_path (dict): I 3 dizionari estratti dal file .pkl.\n",
    "        \n",
    "    Returns:\n",
    "        model (CNN2D): Modello con i suoi pesi caricati.\n",
    "        training_config (dict): Iperparametri usati nel training (per quella versione di modello migliore di quella combinazione di dati)\n",
    "    \"\"\"\n",
    "       \n",
    "    #Qui, si inizializza la versione del modello migliore corrente, richiamando il dizionario che contiene i valori dei parametri interni del modello stesso\n",
    "\n",
    "    #model = build_cnn2d(best_model_config)\n",
    "    \n",
    "    model = best_model\n",
    "    \n",
    "    #Qui, facciamo un ulteriore check, per verificare che effettivamente, il modello ricostruito dinamicamente corrisponda proprio allo stesso modello\n",
    "    #che era stato salvato in precedenza\n",
    "    \n",
    "    '''\n",
    "    Sì, hai ragione nel voler fare un controllo per verificare che la struttura del modello appena ricostruito sia la stessa di quella salvata nel best_model. Esistono diversi modi per fare questo controllo, ma dobbiamo tenere presente che la struttura del modello, se ricostruita correttamente tramite build_cnn2d(best_model_config), dovrebbe essere identica a quella del modello salvato, e i parametri di configurazione dovrebbero essere quelli corretti.\n",
    "\n",
    "    Per verificare che la struttura sia la stessa, puoi eseguire due controlli:\n",
    "\n",
    "    1) Controllo della struttura (architettura del modello): \n",
    "        \n",
    "        Questo si può fare confrontando l'architettura del modello appena ricostruito (model) con quella del modello salvato (best_model)\n",
    "        In pratica, possiamo confrontare la lista dei layer o gli attributi del modello.\n",
    "        \n",
    "        Poiché entrambi dovrebbero essere della stessa classe e configurazione, un confronto diretto potrebbe rivelarsi utile.\n",
    "        \n",
    "        # Confronta l'architettura del modello ricostruito con quella del modello salvato\n",
    "        if model.__class__ != best_model.__class__:\n",
    "            raise ValueError(\"Le architetture dei modelli non sono uguali!\")\n",
    "\n",
    "\n",
    "    2) Controllo dei parametri: \n",
    "        Confrontare i parametri del modello, come i pesi e le configurazioni (iper-parametri). \n",
    "        Questo controllo assicura che il modello ricostruito abbia esattamente gli stessi valori nei parametri di configurazione e nei pesi.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #Questa soluzione ti permette di fare un check completo sulla corrispondenza tra \n",
    "    #la struttura del modello,\n",
    "    #i parametri di configurazione e i pesi,\n",
    "    #assicurandoti che il modello ricostruito sia identico a quello salvato\n",
    "    \n",
    "    # Verifica che l'architettura sia la stessa, tra modello ricostruito al momento correte (model) e modellom salvato (best_model)\n",
    "    #if model.__class__ != best_model.__class__:\n",
    "    #    raise ValueError(\"Le architetture dei modelli non sono uguali!\")\n",
    "    \n",
    "    \n",
    "    # Verifica che i pesi siano gli stessi, tra modello ricostruito al momento correte (model) e modellom salvato (best_model)\n",
    "    #if not all(torch.equal(model.state_dict()[key], best_state_dict[key]) for key in model.state_dict()):\n",
    "    #    raise ValueError(\"I pesi del modello ricostruito non corrispondono a quelli salvati!\")\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Dopodiché, una volta che si è caricato i pesi e bias di quella versione migliore del modello, ‘load_best_cnn2d’ si tira fuori in output\n",
    "    \n",
    "    1) La versione MIGLIORE del modello ESTRATTO CORRENTEMENTE da 'load_best_cnn2d con già configurato con \n",
    "        a) i valori corretti dei suoi parametri interni\n",
    "        b) il caricamento dei suoi pesi e bias\n",
    "    \n",
    "    2) Gli iper-parametri associati al modello (che dovranno esser caricati successivamente nella fase di training del modello stesso)\n",
    "    '''\n",
    "    \n",
    "    #data.keys()\n",
    "    #dict_keys(['state_dict', 'config', 'model_config', 'max_val_acc', 'best_epoch'])\n",
    "    \n",
    "    #data['model_config'].keys()\n",
    "    #dict_keys(['conv_channels', 'kernel_sizes', 'strides', 'paddings', 'pooling_type', 'dropout_rate', 'activations'])\n",
    "    \n",
    "    #data['config'].keys()\n",
    "    #dict_keys(['_wandb', 'batch_size', 'lr', 'model_name', 'n_epochs', 'patience', 'standardization', 'weight_decay'])\n",
    "    \n",
    "    #if best_state_dict and best_model_config and best_training_config is not None:\n",
    "    #    try:\n",
    "    #        model.load_state_dict(best_state_dict)\n",
    "    #        print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "    \n",
    "    if best_model and best_model_config and best_training_config is not None:\n",
    "        #DESCRIZIONE DEI PARAMETRI DELLA VERSIONE DEL MIGLIORE MODELLO SELEZIONATO\n",
    "        print(f\"\\nParametri Modello \\033[1m{model_name}\\033[0m:\")\n",
    "        print(f\"conv_channels per \\033[1m{best_model_config['conv_channels']}\\033[0m\")\n",
    "        print(f\"kernel_sizes = \\033[1m{best_model_config['kernel_sizes']}\\033[0m\")\n",
    "        print(f\"strides = \\033[1m{best_model_config['strides']}\\033[0m\")\n",
    "        print(f\"paddings = \\033[1m{best_model_config['paddings']}\\033[0m\")\n",
    "        print(f\"pooling_type = \\033[1m{best_model_config['pooling_type']}\\033[0m\")\n",
    "        print(f\"dropout_rate = \\033[1m{best_model_config['dropout_rate']}\\033[0m\")\n",
    "        print(f\"activations = \\033[1m{best_model_config['activations']}\\033[0m\")\n",
    "        \n",
    "        #DESCRIZIONE DEGLI IPER-PARAMETRI DELLA VERSIONE DEL MIGLIORE MODELLO SELEZIONATO\n",
    "        print(f\"\\nIperparametri Modello \\033[1m{model_name}\\033[0m:\") \n",
    "        print(f\"batch size = \\033[1m{best_training_config['batch_size']}\\033[0m\")\n",
    "        print(f\"patience = \\033[1m{best_training_config['patience']}\\033[0m\")\n",
    "        print(f\"learning rate = \\033[1m{best_training_config['lr']}\\033[0m\")\n",
    "        print(f\"weight decay = \\033[1m{best_training_config['weight_decay']}\\033[0m\")\n",
    "        print(f\"standardization = \\033[1m{best_training_config['standardization']}\\033[0m\")\n",
    "           \n",
    "    else: \n",
    "        raise ValueError(f\"⚠️ Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a6709-5266-4a47-9ba2-546aba17d9aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **MODELLI CNN2D, BiLSTM e Transformer (PARAMS E HYPEPARAMS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd702e15-fe1e-4f6c-8326-c46118282d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "QUELLA ORIGINALE DI PARTENZA\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x: (batch, canali, frequenze, tempo)\n",
    "        \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        #x = torch.tanh(x)  # Sostituito ELU con tanh\n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- -----  ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- ----- \n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(self, input_channels = 61, num_classes=2,\n",
    "                 # Parametri fissi:\n",
    "                 conv_channels = [16, 32, 48],\n",
    "                 kernel_sizes = [(2,2), (2,2), (2,2)],\n",
    "                 strides = [[(1,1), (1,1), (1,1)], [(2,2), (2,2), (2,2)]],\n",
    "                 paddings = [[1, 1, 1], [2, 2, 2]],\n",
    "                 max_layers = 3,\n",
    "                 \n",
    "                 # Parametri dinamici (da sweep config):\n",
    "                 activations = [\"elu\", \"elu\", \"tanh\"],\n",
    "                 dropout_rate = 0.5,\n",
    "                 pooling_type = \"max\"):\n",
    "        \"\"\"\n",
    "        Costruttore della rete CNN2D con configurazione parzialmente dinamica.\n",
    "        \n",
    "        Parametri fissi (definiti internamente):\n",
    "          - conv_channels: [16, 32, 48]\n",
    "          - kernel_sizes: [(2,2), (2,2), (2,2)]\n",
    "          - max_layers: 3\n",
    "        \n",
    "        Parametri dinamici (da ottimizzare tramite sweep config):\n",
    "          - strides: [[(1,1), (1,1), (1,1)], [(2,2), (2,2), (2,2)]],\n",
    "          - paddings: [[1, 1, 1], [2, 2, 2]],\n",
    "          - activations: lista di funzioni di attivazione da usare per ogni layer\n",
    "          - dropout_rate: valore del dropout\n",
    "          - pooling_type: \"max\" oppure \"avg\"\n",
    "        \"\"\"\n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Salva i parametri fissi\n",
    "    \n",
    "        '''\n",
    "        Anche se conv_channels e kernel_sizes hanno valori fissi nello sweep config,\n",
    "        assegnarli esplicitamente all'istanza del modello permette di evitare ambiguità\n",
    "        quando la rete viene inizializzata con configurazioni diverse.\n",
    "        '''\n",
    "        \n",
    "        self.conv_channels = conv_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "\n",
    "        # Salva i parametri dinamici\n",
    "        \n",
    "        self.activations = activations\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.pooling_type = pooling_type\n",
    "        \n",
    "        self.strides = strides  # Aggiungi il salvataggio del parametro 'strides'\n",
    "        self.paddings = paddings  # Aggiungi il salvataggio del parametro 'paddings'\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = input_channels\n",
    "        \n",
    "        # Costruisci max_layers blocchi convoluzionali usando parametri fissi\n",
    "        for i in range(max_layers):\n",
    "            out_channels = conv_channels[i]\n",
    "            ks = kernel_sizes[i]\n",
    "            stride = strides[i]\n",
    "            padding = paddings[i]\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=ks, stride=stride, padding=padding)\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "            \n",
    "            '''\n",
    "            In questo modo, il kernel_size per ogni layer di pooling verrà scelto dinamicamente, \n",
    "            in base al valore presente nella lista kernel_sizes,\n",
    "            proprio come per i kernel_size delle convoluzioni.\n",
    "            \n",
    "            Perché questa modifica?\n",
    "            Flessibilità: Ora puoi configurare il kernel_size per ogni livello di pooling in modo indipendente, \n",
    "            come nel caso delle convoluzioni, permettendo un controllo maggiore sull'architettura del modello.\n",
    "            \n",
    "            Configurabilità: Puoi passare liste diverse di kernel_sizes per le convoluzioni e il pooling,\n",
    "            e ognuna sarà applicata correttamente al suo livello.\n",
    "            \n",
    "            Con questa modifica, il comportamento del pooling sarà molto più simile al comportamento delle convoluzioni, \n",
    "            ed entrambe le operazioni potranno essere configurate dinamicamente tramite i parametri di input.\n",
    "            '''\n",
    "            \n",
    "            # Selezione dinamica del pooling\n",
    "            if pooling_type.lower() == \"avg\":\n",
    "                pool = nn.AvgPool2d(kernel_size=ks)  # Usa lo stesso kernel_size per pooling\n",
    "            else:\n",
    "                pool = nn.MaxPool2d(kernel_size=ks)\n",
    "            \n",
    "            block = nn.Sequential(conv, bn, pool)\n",
    "            self.layers.append(block)\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        self.fc1 = nn.LazyLinear(8)  # LazyLinear deduce automaticamente la dimensione d'ingresso\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Adattamento dell'input: \n",
    "        \n",
    "        # Original Shape: (batch, frequenze, canali)\n",
    "        \n",
    "        # Novel Shape: (batch, canali, frequenze, 1)\n",
    "        x = x.permute(0, 2, 1)  # (batch, canali, frequenze)\n",
    "        x = x.unsqueeze(3)      # (batch, canali, frequenze, 1)\n",
    "        \n",
    "        for i, block in enumerate(self.layers):\n",
    "            # Il blocco contiene conv, bn e pooling.\n",
    "            x = block[0](x)      # Convoluzione\n",
    "            x = block[1](x)      # Batch Normalization\n",
    "            \n",
    "            # Applica la funzione di attivazione mappata dalla stringa\n",
    "            act_fn = self.get_activation(self.activations[i])\n",
    "            x = act_fn(x)\n",
    "            x = block[2](x)      # Pooling (Max o Average in base a pooling_type)\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)  # Qui potresti parametrizzare anche questa attivazione se lo desideri\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    #def get_activation(self, act_str):\n",
    "        \"\"\"\n",
    "        Mappa la stringa dell'attivazione alla funzione corrispondente.\n",
    "        Le chiavi sono in minuscolo per garantire la corrispondenza (es. \"relu\", \"elu\", \"selu\", \"tanh\").\n",
    "        \"\"\"\n",
    "   #     mapping = {\n",
    "   #         \"relu\": F.relu,\n",
    "   #         \"elu\": F.elu,\n",
    "   #         \"selu\": F.selu,\n",
    "   #         \"tanh\": torch.tanh\n",
    "   #     }\n",
    "   #     return mapping.get(act_str.lower(), F.elu)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    La nuova versione della funzione get_activation è più robusta perché gestisce in modo chiaro e diretto le stringhe \n",
    "    senza ambiguità legate alla capitalizzazione. \n",
    "    \n",
    "    Inoltre, sollevare un'eccezione (ValueError) in caso di attivazione sconosciuta è una scelta efficace\n",
    "    per intercettare errori e mantenere il codice più sicuro.\n",
    "    '''\n",
    "    \n",
    "    def get_activation(self, activation_name):\n",
    "        if activation_name == 'relu':\n",
    "            return F.relu\n",
    "        elif activation_name == 'elu':\n",
    "            return F.elu\n",
    "        elif activation_name == 'selu':\n",
    "            return F.selu\n",
    "        elif activation_name == 'tanh':\n",
    "            return torch.tanh\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function: \" + activation_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff1226-cbb4-4337-816c-42c1873daa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "È buona norma definire una funzione factory separata che, \n",
    "a partire dalla configurazione (ottenuta ad es. da wandb.config),\n",
    "istanzia la rete.\n",
    "\n",
    "Questa funzione può essere definita in un file di utilità o all’inizio del file di training e poi richiamata nel loop di training.\n",
    "\n",
    "Dove metterla?\n",
    "La funzione può essere definita in un modulo (ad es. model_factory.py) oppure all’inizio del file in cui gestisci il training. \n",
    "\n",
    "Nel tuo loop di training, dopo aver ottenuto config = wandb.config, puoi usarla così:\n",
    "\n",
    "\n",
    "if config.model_name == \"CNN2D\":\n",
    "    model = build_cnn2d(config)\n",
    "    print(f\"\\nInizializzazione Modello CNN2D con configurazione: {dict(config)}\")\n",
    "\n",
    "\n",
    "\n",
    "#ATTENZIONE CHE QUI NELLA FUNZIONE DI RICHIAMO, DEVO IMPORRE INPUT_CHANNELS A 61 E NUM_CHANNELS = 2 \n",
    "\n",
    "def build_cnn2d(config):\n",
    "    \"\"\"\n",
    "    Costruisce una CNN2D usando i parametri da config.\n",
    "    Assumiamo che config contenga:\n",
    "       - activations, dropout_rate, pooling_type (dinamici)\n",
    "    Gli altri parametri sono fissi.\n",
    "    \"\"\"\n",
    "    \n",
    "    return CNN2D(\n",
    "        input_channels = 61,\n",
    "        num_classes = 2,\n",
    "        conv_channels = [16, 32, 48],\n",
    "        kernel_sizes = [(2,2), (2,2), (2,2)],\n",
    "        strides = config.strides, \n",
    "        paddings = config.strides,\n",
    "        max_layers = 3,\n",
    "        activations=config.activations,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        pooling_type=config.pooling_type\n",
    "    )\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Sì, conviene rendere anche conv_channels e kernel_sizes dinamici in build_cnn2d se vuoi garantire che \n",
    "tutti i parametri chiave della rete siano tracciati in modo chiaro dallo sweep_config. Questo aiuta per diversi motivi:\n",
    "\n",
    "Maggiore trasparenza – Tutti i parametri vengono centralizzati in config, semplificando il debugging e il logging.\n",
    "\n",
    "Flessibilità futura – Anche se ora sono fissi, in futuro potresti voler sperimentare diverse architetture CNN senza dover modificare la funzione.\n",
    "\n",
    "Coerenza con il resto del codice – Stai già passando strides, activations, dropout_rate e pooling_type da config, \n",
    "quindi è logico che anche conv_channels e kernel_sizes seguano lo stesso principio.\n",
    "\n",
    "Quindi, la nuova versione della funzione è più modulare e allineata con la gestione dinamica dei parametri.\n",
    "\n",
    "In questo modo, tutto è centralizzato in config, rendendo il codice più pulito e scalabile! 🚀\n",
    "\n",
    "\n",
    "\n",
    "def build_cnn2d(config):\n",
    "    \"\"\"\n",
    "    Costruisce una CNN2D usando i parametri da config.\n",
    "    Assumiamo che config contenga:\n",
    "       - activations, dropout_rate, pooling_type (dinamici)\n",
    "    Gli altri parametri sono fissi.\n",
    "    \"\"\"\n",
    "    \n",
    "    return CNN2D(\n",
    "        input_channels = 61,\n",
    "        num_classes = 2,\n",
    "        conv_channels = config.conv_channels,\n",
    "        kernel_sizes = config.kernel_sizes,\n",
    "        strides = config.strides, \n",
    "        paddings = config.strides,\n",
    "        max_layers = 3,\n",
    "        activations=config.activations,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        pooling_type=config.pooling_type\n",
    "    )\n",
    "    \n",
    "'''\n",
    "\n",
    "#7) A quel punto, best_state_dict, best_model_config e best_training_config vengono forniti in ingresso a ‘load_best_cnn2d’, \n",
    "    #che al suo interno richiamerà la funzione per instanziare il modello che è ‘build_cnn2d’\n",
    "\n",
    "    \n",
    "# Funzione factory per creare il modello CNN2D con parametri dinamici migliori tra i 15 modelli della relativa path!\n",
    "\n",
    "'''\n",
    "def build_cnn2d(best_model_config):\n",
    "    \n",
    "    \"\"\"\n",
    "    Costruisce una CNN2D usando i parametri da config.\n",
    "    Assumiamo che config contenga:\n",
    "       - activations, dropout_rate, pooling_type (dinamici)\n",
    "    Gli altri parametri sono fissi.\n",
    "    \"\"\"\n",
    "    \n",
    "    return CNN2D(\n",
    "        input_channels = 61,\n",
    "        num_classes = 2,\n",
    "        conv_channels = [16, 32, 48],\n",
    "        kernel_sizes =[(2,2), (2,2), (2,2)],\n",
    "        strides = best_model_config['strides'],\n",
    "        paddings = best_model_config['paddings'],\n",
    "        max_layers = 3,\n",
    "        activations = best_model_config['activations'],\n",
    "        dropout_rate = best_model_config['dropout_rate'],\n",
    "        pooling_type = best_model_config['pooling_type']\n",
    "    )\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Passando conv_channels e kernel_sizes come parametri dinamici,\n",
    "puoi adattare meglio la funzione build_cnn2d per supportare una maggiore flessibilità con i parametri definiti nel tuo best_model_config.\n",
    "La nuova versione della funzione ti permetterà di personalizzare più facilmente il modello, \n",
    "modificando anche la configurazione delle convoluzioni (numero di canali e dimensioni dei kernel), oltre agli altri parametri che restano fissi.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def build_cnn2d(best_model_config):\n",
    "    \n",
    "    \"\"\"\n",
    "    Costruisce una CNN2D usando i parametri da config.\n",
    "    Assumiamo che config contenga:\n",
    "       - activations, dropout_rate, pooling_type (dinamici)\n",
    "       - conv_channels, kernel_sizes, strides, paddings (ora configurabili dinamicamente)\n",
    "    \"\"\"\n",
    "    \n",
    "    return CNN2D(\n",
    "        input_channels = 61,\n",
    "        num_classes = 2,\n",
    "        conv_channels = best_model_config['conv_channels'],  # Fissato dinamicamente\n",
    "        kernel_sizes = best_model_config['kernel_sizes'],  # Fissato dinamicamente\n",
    "        strides = best_model_config['strides'],\n",
    "        paddings = best_model_config['paddings'],\n",
    "        max_layers = 3,\n",
    "        activations = best_model_config['activations'],\n",
    "        dropout_rate = best_model_config['dropout_rate'],\n",
    "        pooling_type = best_model_config['pooling_type']\n",
    "    )\n",
    "\n",
    "\n",
    "#Dove ‘strides’, ‘paddings’, ‘activations’, ‘dropout_rate’, ‘pooling_type’ sono quelli estratti \n",
    "#dalla versione migliore del modello corrente trovato nella path della specifica combinazione di dati\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121ad99-d1c4-4573-a1c6-84d3c5146b11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **MODELLO CNN2D o CNN3D_FC_LST o CNN_2D Sep (ONLY HYPERPARAMS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d886fd-9cc2-471c-b29e-5f18fbc0df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFINIZIONE DEI MODELLI NEW VERSION PER SPETTROGRAMMI 2D FREQUENCY-CHANNELS (LUGLIO 2025!)\n",
    "\n",
    "\n",
    "\n",
    "Ora però, ragionandoci, potrei inserire dei valori da cui pescare, \n",
    "\n",
    "durante l'ottimizzazione degli iper-parametri della mia rete, che si riferiscono \n",
    "\n",
    "1) a valori di alcuni parametri generale dell'apprendimento delle reti\n",
    "2) a valori dei parametri architetturali di ciascuna delle mie singole reti neurali testate\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***CNN2D NEW*** \n",
    "\n",
    "1) All'interno di ogni layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "a) il numero di output channels (ossia 16 impostato di default qui sotto, ma che potrebbe variare da 16 a 32 con step di 4 \n",
    "come grandezza della feature map sostanzialmente\n",
    "\n",
    "b) la grandezza del kernel size (tra 2 e 8 con step di 2)\n",
    "c) la grandezza dello stride (metti solo valori tra 1 e 2) \n",
    "\n",
    "\n",
    "2) Per il layer di batch normalisation del relativo layer convolutivo (https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#batchnorm1d\n",
    "\n",
    "deve avere il valore del numero di features di quel layer di batch normalisation\n",
    "(che deve corrispondere come valore a quello dell'output channels del layer convolutivo che lo precede sostanzialmente) \n",
    "\n",
    "\n",
    "3) Al layer di pooling del relativo strato della della CNN1D, far variare la scelta tra\n",
    "\n",
    "a) max pooling ed average pooling \n",
    "\n",
    "b) Il valore del kernel_size del layer di max od average pooling (a seconda di quello che viene scelto tra i due), \n",
    "che può variare tra 1 e 2 \n",
    "\n",
    "4) Al solo primo layer fully connected della CNN1D, far variare la scelta del suo valore \n",
    "(che nella mia rete sarebbe \"self.fc1 = nn.LazyLinear(8)\") in questo set di valori, ossia tra i valori 8,10,12,14,16\n",
    "\n",
    "5) Il valore del dropout layer (con valori tra  0.0 e 0.5) \n",
    "\n",
    "\n",
    "6) Il valore della possibile funzione di attivazione tra 3 (relu, selu ed elu)\n",
    "\n",
    " a) per gli strati convolutivi (3) +\n",
    " b) per il primo fully connected layer (FC1) (prendendone una a caso tra quelle 3 possibili\n",
    "\n",
    "\n",
    "\n",
    "TABELLA FINALE RIASSUNTIVA - CNN1D \n",
    "\n",
    "\n",
    "| Iper-parametro                     | Descrizione                                             | Valori possibili                 |\n",
    "| ---------------------------------- | ------------------------------------------------------- | -------------------------------- |\n",
    "| `conv_out_channels`                | Numero di feature-map di base                           | `[16, 20, 24, 28, 32]`           |\n",
    "| `conv_k1`, `conv_k2`, `conv_k3`    | Kernel size rispettivamente per i 3 blocchi convolutivi | `[2, 4, 6, 8]`                   |\n",
    "| `conv_s1`, `conv_s2`, `conv_s3`    | Stride rispettivamente per i 3 blocchi convolutivi      | `[1, 2]`                         |\n",
    "| `pool_type`                        | Tipo di pooling                                         | `[\"max\",\"avg\"]`                  |\n",
    "| `pool_p1`, `pool_p2`, `pool_p3`    | Kernel size rispettivamente per i 3 blocchi di pooling  | `[1, 2]`                         |\n",
    "| `fc1_units`                        | Numero di unità nel primo fully-connected               | `[8, 10, 12, 14, 16]`            |\n",
    "| `cnn_act1`, `cnn_act2`, `cnn_act3` | Funzione di attivazione per ciascun blocco (layer1,2,3) | `[\"relu\",\"selu\",\"elu\"]`          |\n",
    "| **+ comune**                       | `dropout`                                               | `[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]` |\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                ***OLD CNN2D***\n",
    "\n",
    "\n",
    "Una CNN2D si aspetta input in forma (batch, frequenze, canali). \n",
    "Nel tuo caso, puoi interpretare l’\"altezza\" come i bin di frequenza (45)\n",
    "e la \"larghezza\" come i canali (61)\n",
    "\n",
    "Quindi, la tua CNN2D lavorerebbe direttamente con:\n",
    "Shape: (batch, frequenze, canali)\n",
    "\n",
    "\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        \n",
    "        super(CNN2D, self).__init__()\n",
    "        \n",
    "        # Ipotizziamo kernel 3x3 con padding per mantenere le dimensioni (puoi adattare a tuo piacimento)\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=(2, 2), stride=(1, 1), padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(48)\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Utilizzo LazyLinear per evitare di calcolare manualmente la dimensione piatta finale\n",
    "        self.fc1 = nn.LazyLinear(8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.LazyLinear(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = F.elu(x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "       \n",
    "\n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''CNN2D CON LE FUNZIONI DI ATTIVAZIONI DINAMICHE (LAYER CONVOLUTIVI + FC1) E DINAMISMO DEI KERNEL SIZE DI CONV e POOL LAYER e STRIDE\n",
    "\n",
    "\n",
    "\n",
    "Scusami non ho capito la differenza tra la mia e la tua, che la mia tratta le convoluzione per ogni canale separatamente\n",
    "e quindi vede le features per ogni canale mentre la tua vede le relazioni in frequenza tra i canali?\n",
    "\n",
    "mi fai capire meglio?\n",
    "\n",
    "\n",
    "ChatGPT ha detto:\n",
    "\n",
    "Ottima domanda, e sì: sei sulla strada giusta! Vediamo con precisione la differenza concettuale tra le due versioni, perché hanno implicazioni molto diverse sul tipo di informazione che la rete convoluzionale impara.\n",
    "\n",
    "🧠 Contesto: input (batch, freq, channels) → (B, 45, 61)\n",
    "✅ Versione A: .unsqueeze(1) → input = (B, 1, 45, 61)\n",
    "\n",
    "x = x.unsqueeze(1)\n",
    "# Output: (batch_size, 1, 45, 61)\n",
    "\n",
    "✅ Come la interpreta la CNN:\n",
    "Tratta tutta la matrice frequenze × canali come un'immagine 2D.\n",
    "\n",
    "Applica filtri convoluzionali 2D che coprono sia le frequenze che i canali contemporaneamente.\n",
    "\n",
    "La convoluzione può imparare pattern locali e globali che coinvolgono più canali EEG e bande di frequenza assieme.\n",
    "\n",
    "✅ Esempio di pattern che può catturare:\n",
    "\"La banda theta (4–8 Hz) è più attiva nei canali posteriori rispetto a quelli frontali\"\n",
    "\n",
    "\"Un pattern a L tra canali temporali e frequenze gamma\"\n",
    "\n",
    "➕ Vantaggi:\n",
    "Cattura interazioni tra frequenze e canali.\n",
    "\n",
    "Molto potente per pattern spatial-temporal-frequenziali complessi.\n",
    "\n",
    "➖ Svantaggi:\n",
    "Più complesso da interpretare.\n",
    "\n",
    "Può \"mescolare troppo\" se le relazioni tra canali non sono forti.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "✅ Versione B: .permute(0, 2, 1).unsqueeze(3) → input = (B, 61, 45, 1)\n",
    "\n",
    "\n",
    "x = x.permute(0, 2, 1).unsqueeze(3)\n",
    "# Output: (batch_size, 61, 45, 1)\n",
    "\n",
    "✅ Come la interpreta la CNN:\n",
    "Ogni canale EEG (dei 61) è trattato come un’immagine verticale di 45 pixel (cioè le frequenze).\n",
    "\n",
    "**Ogni filtro convoluzionale lavora indipendentemente su ciascun canale EEG.\n",
    "\n",
    "È come applicare 61 CNN monodimensionali parallele sulle frequenze di ogni canale.\n",
    "\n",
    "✅ Esempio di pattern che può catturare:\n",
    "“Nel canale Fz, la banda alpha ha un picco”\n",
    "\n",
    "“Nel canale Pz, la potenza decresce linearmente con la frequenza”\n",
    "\n",
    "➕ Vantaggi:\n",
    "Cattura pattern locali per canale (molto utile se ogni canale è considerato indipendente).\n",
    "\n",
    "Meno soggetto a overfitting se i dati sono limitati.\n",
    "\n",
    "➖ Svantaggi:\n",
    "Non cattura le relazioni tra canali.\n",
    "\n",
    "Meno potente se la distribuzione spaziale tra canali è importante (es. P300, N170, ecc.).\n",
    "\n",
    "🔍 Riassunto Visuale\n",
    "\n",
    "Variante\t        Input Shape\t      Tratta i canali come\t       Cattura relazioni tra canali?\t    Cattura pattern cross-frequenza?\n",
    ".unsqueeze(1)\t    (B, 1, 45, 61)\t    Griglia 2D\t                               ✅ Sì\t                             ✅ Sì\n",
    ".permute...\t        (B, 61, 45, 1)\t   Canali separati\t                           ❌ No\t               ✅ Solo all'interno del canale\n",
    "\n",
    "🧠 Quale scegliere?\n",
    "Usa .unsqueeze(1) (griglia 2D) se:\n",
    "Vuoi sfruttare relazioni tra canali EEG (spazialità).\n",
    "\n",
    "Stai analizzando pattern che coinvolgono distribuzioni topografiche (Pz vs Cz, emisferi, ecc.).\n",
    "\n",
    "Usa .permute(...).unsqueeze(3) se:\n",
    "Vuoi analizzare ogni canale indipendentemente.\n",
    "\n",
    "Hai pochi dati e vuoi una CNN più semplice.\n",
    "\n",
    "Tratti i canali EEG come “sorgenti indipendenti”.\n",
    "\n",
    "✅ Conclusione\n",
    "Hai centrato la differenza:\n",
    "\n",
    "La tua versione **focalizza sui pattern di frequenza all'interno di ciascun canale.\n",
    "\n",
    "L’altra versione esplora pattern condivisi tra canali e frequenze, come se fosse un'immagine EEG frequenze × elettrodi.\n",
    "\n",
    "Se vuoi, posso anche mostrarti un esempio visivo (matplotlib) per rendere ancora più chiara la differenza.\n",
    "\n",
    "\n",
    "\n",
    "Quindi, nella mia versione originale è come se dessi 61 immagini singolo canale 'verticali', \n",
    "nel senso che per analogia, sarebbe come dare la rappresentazione in formato vettore verticale di ogni canale rispetto ai pixel in frequenza \n",
    "(ossia i bins di frequenza)..\n",
    "\n",
    "Nella tua, invece, forniamo un input solo, ossia una immagine sola,\n",
    "con la rappresentazione dei bin di frequenza di ogni canale e quindi ci consente di vedere le relazioni tra i canali stessi?\n",
    "\n",
    "\n",
    "****************** ****************** ****************** ****************** ****************** ****************** ******************\n",
    "\n",
    "✅ Esattamente! Hai centrato il punto con una metafora visiva perfetta. Ti spiego meglio facendo proprio leva su questa analogia dell’immagine.\n",
    "\n",
    "🎯 Tua versione originale: (B, 61, 45, 1) → 61 “immagini verticali”\n",
    "✅ Metafora:\n",
    "È come dare 61 immagini strette e alte (una per ogni canale EEG), in cui:\n",
    "\n",
    "Altezza = 45: le frequenze (tipo pixel verticali)\n",
    "\n",
    "Larghezza = 1: inesistente (una sola colonna)\n",
    "\n",
    "Ogni immagine è indipendente\n",
    "\n",
    "👉 In pratica:\n",
    "La rete analizza separatamente ogni canale EEG.\n",
    "\n",
    "Impara “per ogni canale, com’è distribuita la potenza nelle varie frequenze?”\n",
    "\n",
    "Non sa nulla di cosa succede in altri canali.\n",
    "\n",
    "🎯 La mia versione (classica CNN2D): (B, 1, 45, 61) → una sola immagine F×C\n",
    "✅ Metafora:\n",
    "È come dare una singola immagine 2D:\n",
    "\n",
    "Asse verticale = 45 bin di frequenza\n",
    "\n",
    "Asse orizzontale = 61 canali EEG\n",
    "\n",
    "👉 In pratica:\n",
    "La rete guarda l’intera griglia frequenza × canale come un’immagine.\n",
    "\n",
    "Può imparare pattern tipo:\n",
    "\n",
    "“La potenza in alpha aumenta da O1 verso Cz”\n",
    "\n",
    "“Un cluster di attivazione nei canali frontali nella gamma”\n",
    "\n",
    "Le convoluzioni possono spostarsi sia tra frequenze che tra canali, quindi può apprendere interazioni spaziali.\n",
    "\n",
    "👁️‍🗨️ Visualizzazione (semplificata)\n",
    "Tua:\n",
    "scss\n",
    "Copia\n",
    "Modifica\n",
    "Canale Fz →  [ delta ]  \n",
    "              [ theta ]  \n",
    "              [ alpha ]  \n",
    "              [ beta  ]  \n",
    "              [ gamma ]\n",
    "\n",
    "Canale Cz →  [ delta ]  \n",
    "              [ theta ]  \n",
    "              ...\n",
    "(61 ripetizioni)\n",
    "Mia:\n",
    "matlab\n",
    "Copia\n",
    "Modifica\n",
    "        C1   C2   C3   ...  C61\n",
    "delta   ■    □    ■    ...  □\n",
    "theta   □    ■    □    ...  ■\n",
    "alpha   ■    ■    ■    ...  □\n",
    "beta    □    □    ■    ...  ■\n",
    "gamma   ■    □    □    ...  □\n",
    "Una griglia piena, dove ogni pixel rappresenta la potenza in una frequenza per un canale.\n",
    "\n",
    "✅ Conclusione\n",
    "Hai descritto benissimo:\n",
    "\n",
    "Versione\tInterpretazione\tVantaggio principale\n",
    "Tua (permute + unsqueeze)\t61 immagini verticali (1 per canale)\tFocus dettagliato su pattern per canale\n",
    "Classica (unsqueeze(1))\tUna sola immagine 2D F×C\tCattura pattern e relazioni tra canali\n",
    "\n",
    "Se vuoi fare un passo in più, puoi anche combinare le due visioni con multi-branch CNN (una per canale, una per immagine globale). \n",
    "Fammi sapere se vuoi un esempio anche per questo!\n",
    "\n",
    "****************** ****************** ****************** ****************** ****************** ****************** ******************\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        #🔁 Prima:\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        #x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        #x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "        \n",
    "        #✅ Ora:\n",
    "        #Siccome i dati arrivano come (B, 45, 61) — cioè frequenze × canali, non serve permutare. Ti basta:\n",
    "        \n",
    "        # Aggiungiamo una dimensione per il canale \"immagine\"\n",
    "        x = x.unsqueeze(1)  # → (B, 1, 45, 61)\n",
    "            \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = self.act_fns[0](x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = self.act_fns[1](x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = self.act_fns[2](x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "       \n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "VERSIONE CONVOLUZIONE 3D PURA e CONVOLUZIONI SEPARABILI 19 LUGLIO 2025\n",
    "\n",
    "\n",
    "Due versioni dell’architettura:\n",
    "\n",
    "CNN3D_LSTM_FC: usa nn.Conv3d per eseguire una vera convoluzione 3D sui cinque depth (bande di frequenza), \n",
    "mantenendo il resto del flusso identico.\n",
    "\n",
    "SeparableCNN2D_LSTM_FC: applica in sequenza una convoluzione depthwise (gruppi = canali) e una pointwise (1×1) \n",
    "per fondere i cinque canali in modo efficiente.\n",
    "\n",
    "Entrambe le classi si integrano con il tuo blocco LSTM e il classificatore come nella versione originale.\n",
    "\n",
    "\n",
    "\n",
    "Per ottenere un Grad‑CAM “3D” su ciascuna delle 5 bande (cioè un volume 9×9×5) \n",
    "invece di schiacciare tutto in una mappa 9×9, bisogna:\n",
    "\n",
    "Non appiattire la dimensione di profondità (“depth” = bande) con cam.mean(dim=1).\n",
    "\n",
    "Calcolare i pesi medi dei gradienti solo su altezza e larghezza, non su depth, in modo da preservare D=5.\n",
    "\n",
    "Upsample (solo) le due dimensioni spaziali H×W, lasciando inalterata la profondità D.\n",
    "\n",
    "(Opzionale) \n",
    "\n",
    "Se il tuo primo Conv3d usa un kernel di profondità pari all’intera profondità d’ingresso, \n",
    "quella informazione viene compressa in D=1!\n",
    "\n",
    "Se vuoi davvero avere D=5 in uscita, devi cambiare conv1 in:\n",
    "\n",
    "\n",
    "# ❌ kernel_size=(5,3,3), padding=(0,1,1) → D_out = 1\n",
    "self.conv1 = nn.Conv3d(1, 32, kernel_size=(3,3,3), padding=(1,1,1))\n",
    "così la profondità si conserva da 5→5.\n",
    "\n",
    "\n",
    "\n",
    "1) Perché in conv1 useremo padding=(1,1,1) e negli altri layer padding=(0,1,1)\n",
    "Obiettivo: mantenere la profondità (numero di bande, D = 5) costante lungo tutta la rete.\n",
    "\n",
    "In conv1, abbiamo scelto kernel_size=(3,3,3) perché vogliamo che il filtro “scorra” su tutti e tre gli assi (D,H,W).\n",
    "\n",
    "Con kernel_depth=3, per avere\n",
    "\n",
    "𝐷out = (𝐷in + 2⋅𝑃 depth − 𝐾 depth)/ 𝑆 + 1 = 5\n",
    "\n",
    "Da qui (1,1,1) per (depth, height, width).\n",
    "\n",
    "Negli altri layer 3D (conv2a, conv2b, conv3) il kernel depth = 1 (kernel_size=(1,3,3)), \n",
    "quindi la profondità non cambia se mettiamo padding_depth=0 con padding (0,1,1) nel layer conv2 e conv3\n",
    "\n",
    "In altre parole, su quell’asse non serve alcun padding:\n",
    "\n",
    "se P dept = 0 allora diventa infatti\n",
    "\n",
    "𝐷out = (𝐷in + 2⋅0 − 𝐾 depth)/ 𝑆 + 1 = 5\n",
    "\n",
    "2⋅0\n",
    "\n",
    "\n",
    "Non è che la tua rete “CNN3D_LSTM_FC” sia sbagliata in senso assoluto, \n",
    "ma — proprio a causa di quel primo Conv3d con kernel_size=(5,3,3) e padding=(0,1,1) — \n",
    "\n",
    "stai automaticamente comprimendo tutte e 5 le bande nella singola fetta di profondità:\n",
    "\n",
    "\n",
    "self.conv1 = nn.Conv3d(\n",
    "    in_channels=1, out_channels=32,\n",
    "    kernel_size=(5, 3, 3),  # → D_out = (5 − 5 + 2·0)/1 + 1 = 1\n",
    "    padding=(0, 1, 1)\n",
    ")\n",
    "Quindi il tuo tensore (B, 1, 5, 9, 9) diventa (B, 32, 1, 9, 9): la dimensione depth (5) si riduce a 1 subito.\n",
    "\n",
    "Se invece vuoi davvero preservare le 5 “fette” come vera terza dimensione spaziale, hai due possibili correzioni:\n",
    "\n",
    "Usare un kernel 3×3×3 (o 1×3×3) in conv1, in modo da non “abbracciare” tutta la profondità d’ingresso:\n",
    "\n",
    "\n",
    "- self.conv1 = nn.Conv3d(1, 32, kernel_size=(5, 3, 3), padding=(0, 1, 1))\n",
    "+ # preserva D: depth out = depth in = 5\n",
    "+ self.conv1 = nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "\n",
    "Oppure, se vuoi mantenere le bande completamente indipendenti in questo layer,\n",
    "\n",
    "\n",
    "self.conv1 = nn.Conv3d(1, 32, kernel_size=(1, 3, 3), padding=(0, 1, 1))\n",
    "che trasforma (B,1,5,9,9) → (B,32,5,9,9).\n",
    "\n",
    "Lasciare com’è, sapendo però che la rete “fonderà” le 5 bande in un’unica mappa di profondità: \n",
    "non è un bug, è una scelta architetturale.\n",
    "\n",
    "La SeparableCNN2D_LSTM_FC\n",
    "Quella architettura non comprime mai le bande all’interno di un’unica fetta, perché:\n",
    "\n",
    "Le bande diventano canali di un Conv2d depth‑wise:\n",
    "\n",
    "\n",
    "x = x.permute(0, 3, 1, 2)  # (B,5,9,9)\n",
    "self.dw_conv1 = nn.Conv2d(5, 5, kernel_size=3, padding=1, groups=5)\n",
    "Ogni “fetta” (banda) resta separata fino al pointwise e agli strati successivi.\n",
    "\n",
    "Quindi se il tuo obiettivo è avere un’uscita per banda (e poi poter plottare un Grad‑CAM 2D per ciascuna),\n",
    "la SeparableCNN2D è già configurata correttamente.\n",
    "\n",
    "Se invece vuoi un Grad‑CAM “volumetrico” 3D (5×9×9) direttamente dal modello 3D puro,\n",
    "la vera modifica necessaria è solo sul primo Conv3d, come mostrato sopra.\n",
    "\n",
    "Fammi sapere quale dei due setup stai usando e ti aiuto a integrare il Grad‑CAM 3D di conseguenza!\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Ecco l’implementazione completa di CNN3D_LSTM_FC (“approccio sequenza di profondità”) in cui:\n",
    "\n",
    "mantieni il tuo primo blocco 3D con kernel (3,3,3) e padding (1,1,1), quindi D rimane 5 fino alla fine;\n",
    "\n",
    "riduci le spatial singleton dims (H=1,W=1) e trasformi la depth D=5 in una sequenza di lunghezza 5;\n",
    "\n",
    "imposti l’input_size=128 nell’LSTM (feature per time‑step = 128);\n",
    "\n",
    "\n",
    "Con questa versione:\n",
    "\n",
    "la sequenza per l’LSTM ha lunghezza D=5;\n",
    "\n",
    "ogni passo ha 128 feature, esattamente input_size=128;\n",
    "\n",
    "non servono trucchi di reshape su scala globale.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN3D_LSTM_FC(nn.Module):\n",
    "    \"\"\"\n",
    "    Version with pure 3D convolutions treating the 5 frequency bands\n",
    "    as a sequence (depth) for the LSTM.\n",
    "    Input: Tensor of shape (B, 9, 9, 5) --> reshaped to (B, 1, 5, 9, 9)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, dropout=0.5, hidden_size=64, use_lstm=True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        # --- Block 1 (3D) ---\n",
    "        self.conv1   = nn.Conv3d(1,  32, kernel_size=(3,3,3), padding=(1,1,1))\n",
    "        self.bn1     = nn.BatchNorm3d(32)\n",
    "        self.pool3d  = nn.MaxPool3d((1,2,2))  # non tocca D\n",
    "\n",
    "        # --- Block 2 (3D Residual) ---\n",
    "        self.res_conv3d = nn.Conv3d(32, 64, kernel_size=1, bias=False)\n",
    "        self.res_bn3d   = nn.BatchNorm3d(64)\n",
    "        self.conv2a     = nn.Conv3d(32, 64, kernel_size=(1,3,3), padding=(0,1,1))\n",
    "        self.bn2a       = nn.BatchNorm3d(64)\n",
    "        self.conv2b     = nn.Conv3d(64, 64, kernel_size=(1,3,3), padding=(0,1,1))\n",
    "        self.bn2b       = nn.BatchNorm3d(64)\n",
    "\n",
    "        # --- Block 3 (3D) ---\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(1,3,3), padding=(0,1,1))\n",
    "        self.bn3   = nn.BatchNorm3d(128)\n",
    "\n",
    "        # LSTM o FC finale\n",
    "        if self.use_lstm:\n",
    "            # input_size = feature_dim per time‑step = 128\n",
    "            self.lstm       = nn.LSTM(input_size=128,\n",
    "                                      hidden_size=self.hidden_size,\n",
    "                                      num_layers=1,\n",
    "                                      batch_first=True)\n",
    "            self.classifier = nn.LazyLinear(num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.LazyLinear(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 9, 9, 5)\n",
    "        if x.ndim == 4:\n",
    "            # -> (B,1,D=5,H=9,W=9)\n",
    "            x = x.permute(0, 3, 1, 2).unsqueeze(1)\n",
    "\n",
    "        # --- Block 1 ---\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B,32,5,9,9)\n",
    "        x = self.pool3d(x)                   # (B,32,5,4,4)\n",
    "\n",
    "        # --- Block 2 (Residual) ---\n",
    "        res = self.res_bn3d(self.res_conv3d(x))  # (B,64,5,4,4)\n",
    "        x   = F.relu(self.conv2a(x))             # (B,64,5,4,4)\n",
    "        x   = self.bn2b(self.conv2b(x))          # (B,64,5,4,4)\n",
    "        x   = F.relu(x + res)                    # (B,64,5,4,4)\n",
    "        x   = self.pool3d(x)                     # (B,64,5,2,2)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        x = F.relu(self.bn3(self.conv3(x)))      # (B,128,5,2,2)\n",
    "        x = self.pool3d(x)                       # (B,128,5,1,1)\n",
    "\n",
    "        # Stampa delle dimensioni prima di passare al classifier\n",
    "        #print(f\"Dimensioni prima del classifier: {x.shape}\")\n",
    "\n",
    "        if self.use_lstm:\n",
    "            # x: (B,128,5,1,1)\n",
    "            # -> squeeze spatial dims → (B,128,5)\n",
    "            x = x.squeeze(-1).squeeze(-1)\n",
    "            # -> permute per batch_first → (B, seq_len=5, feat=128)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.dropout(x)\n",
    "            out, _ = self.lstm(x)               # out: (B,5,hidden_size)\n",
    "            last    = out[:, -1, :]             # prendo l’ultimo time-step\n",
    "            logits  = self.classifier(last)     # (B, num_classes)\n",
    "        else:\n",
    "            # x: (B,128,5,1,1) → flatten → (B,128)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            logits = self.classifier(self.dropout(x))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    \n",
    "\n",
    "class SeparableCNN2D_LSTM_FC(nn.Module):\n",
    "    \"\"\"\n",
    "    Version with depthwise + pointwise separable convolutions\n",
    "    across the 5 channels.\n",
    "    Input: Tensor of shape (B, 9, 9, 5) -> (B,5,9,9)\n",
    "    \n",
    "    \n",
    "    groups=5 → impone che ogni canale venga convoluto indipendentemente dagli altri → depthwise ✅\n",
    "\n",
    "    kernel_size=1 → combina i 5 canali in un nuovo spazio di 32 feature maps → pointwise ✅\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, dropout=0.5, hidden_size=64, use_lstm=True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        # --- Block 1 separabile ---\n",
    "        self.dw_conv1 = nn.Conv2d(5, 5, kernel_size=3, padding=1, groups=5)\n",
    "        self.pw_conv1 = nn.Conv2d(5, 32, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # --- Block 2 (residuo) ---\n",
    "        self.res_conv = nn.Conv2d(32, 64, kernel_size=1, bias=False)\n",
    "        self.res_bn = nn.BatchNorm2d(64)\n",
    "        self.bn2a = nn.BatchNorm2d(32)\n",
    "        self.conv2a = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- Block 3 ---\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=128 * 5, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=128 * 1,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.classifier = nn.Linear(self.hidden_size, num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)  # -> (B,5,9,9)\n",
    "\n",
    "        x = F.relu(self.dw_conv1(x))\n",
    "        x = F.relu(self.bn1(self.pw_conv1(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        res = self.res_bn(self.res_conv(x))\n",
    "        x = F.relu(self.conv2a(self.bn2a(x)))\n",
    "        x = self.bn2b(self.conv2b(x))\n",
    "        x = F.relu(x + res)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)  # (B,128,1,1)\n",
    "\n",
    "        if self.use_lstm:\n",
    "            x = x.permute(0, 2, 1, 3).reshape(x.size(0), 1, -1)  # (B,1,128)\n",
    "            out, _ = self.lstm(self.dropout(x))\n",
    "            last = out[:, -1, :]\n",
    "            logits = self.classifier(last)\n",
    "        else:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            logits = self.classifier(self.dropout(x))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48661c0-b7ee-4162-870f-ad9be2acaff7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **TRAINING (NON DEVI ESEGUIRLA VAI DIRETTAMENTE AL TESTING!)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d07febf5-640c-4de4-8ef2-eecda0dbcafe",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                            DESCRIZIONE\n",
    "\n",
    "La funzione pbar.set_description() che stai utilizzando all'interno del ciclo for epoch in pbar:\n",
    "gestisce già il printing delle metriche di loss e accuracy sia per il training che per la validazione.\n",
    "\n",
    "Il metodo set_description() aggiorna dinamicamente la barra di avanzamento di tqdm con il testo specificato,\n",
    "quindi non è strettamente necessario stampare MANUALMENTE i risultati con print() subito dopo. \n",
    "\n",
    "Tuttavia, se desideri avere un log persistente nella console, i print() possono ancora essere utili.\n",
    "\n",
    "\n",
    "Cosa succede nella tua implementazione:\n",
    "\n",
    "1) Barra di progresso (tqdm)\n",
    "\n",
    "La barra viene aggiornata a ogni epoca con le seguenti metriche:\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "\n",
    "    Questo aggiorna dinamicamente la barra di progresso, mostrando le informazioni in tempo reale.\n",
    "\n",
    "2) Output dettagliato (print)\n",
    "\n",
    "Dopo ogni epoca stampi i risultati in formato leggibile, con caratteri in grassetto per evidenziare l'epoca corrente:\n",
    "\n",
    "\n",
    "    print(f\"\\033[1mEpoch {epoch+1}/{n_epochs}\\033[0m \")\n",
    "    print(f\"Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}\")\n",
    "    print(f\"Train Accuracy: {accuracy_train:.4f}, Val Accuracy: {accuracy_val:.4f}\\n\")\n",
    "    \n",
    "    Questo serve a mantenere un log più dettagliato e leggibile.\n",
    "\n",
    "\n",
    "Cosa potresti fare:\n",
    "\n",
    "- Se preferisci evitare stampe duplicate e affidarti solo alla barra di avanzamento, \n",
    "puoi semplicemente rimuovere i print() finali e lasciare solo pbar.set_description().\n",
    "\n",
    "- Se invece vuoi mantenere i print(), potresti usarli solo per il salvataggio in un file di log, per esempio:\n",
    "\n",
    "\n",
    "    with open(\"training_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\\n\")\n",
    "    \n",
    "\n",
    "Considerazioni generali:\n",
    "\n",
    "Efficienza: La barra di progresso è utile per un'osservazione veloce senza affollare l'output della console.\n",
    "Tracciabilità: I print() aiutano per tenere traccia di eventuali problemi e risultati storici.\n",
    "Leggibilità: Se il tuo ambiente di esecuzione è un notebook Jupyter, la barra di progresso di tqdm è spesso sufficiente e più pulita.\n",
    "\n",
    "\n",
    "N.B. SCHEDULER\n",
    "\n",
    "    Scheduler per il Learning Rate:\n",
    "    \n",
    "    torch.optim.lr_scheduler.LRScheduler provides several methods to adjust the learning rate based on the number of epochs.\n",
    "    \n",
    "    Among the strategies of learning rate scheduling there is one:\n",
    "    \n",
    "    1) torch.optim.lr_scheduler.ReduceLROnPlateau which allows dynamic learning rate reducing based on some validation measurements.\n",
    "    \n",
    "    Nel caso tu stia usando Adam, che è già un ottimizzatore con un adattamento automatico \n",
    "    dei tassi di apprendimento per i vari parametri (grazie al termine \"momentum\"), \n",
    "    \n",
    "    l'uso di uno scheduler può comunque essere utile per adattare ulteriormente il learning rate durante l'allenamento,\n",
    "    migliorando le performance finali o il comportamento di convergenza.\n",
    "    \n",
    "    ReduceLROnPlateau non esclude Adam. In effetti, puoi usare Adam come ottimizzatore insieme a ReduceLROnPlateau \n",
    "    per modificare dinamicamente il learning rate durante l'allenamento\n",
    "    \n",
    "    ReduceLROnPlateau non dipende dal tipo di ottimizzatore che stai utilizzando;\n",
    "    piuttosto, si basa sulla metriche di valutazione (ad esempio la loss di validazione) \n",
    "    per decidere quando e quanto ridurre il learning rate (funziona con qualsiasi ottimizzatore, inclusi Adam, SGD, RMSprop, e altri).\n",
    "\n",
    "    L'idea di ReduceLROnPlateau è che se la metrica che monitori non sta migliorando \n",
    "    (o non migliora abbastanza), \n",
    "    allora il learning rate viene ridotto per tentare di fare progressi con un passo più piccolo. \n",
    "    Questo approccio è utile, specialmente quando il modello non sta migliorando \n",
    "    e si potrebbe beneficiare di un apprendimento più fine\n",
    "\n",
    "\n",
    "\n",
    "IO:\n",
    "\n",
    "\n",
    "Ok voglio ragionare meglio su 'factor', ossia, hai detto che se factor=0.1, \n",
    "il learning rate verrà ridotto del 90% ogni volta che il scheduler interviene.\n",
    "Quindi, se il learning rate iniziale è 0.001, dopo un intervento sarà 0.0001.\n",
    "\n",
    "ma se volessi invece fargli fare dei passi più piccoli, \n",
    "ossia della metà di quelli che mi hai descritto tu? dovrei imporre factor a 0.05..? quindi il passaggio sarebbe da 0.001 a quanto?\n",
    "\n",
    "La mia idea è che, quando nota che la val loss non scende, \n",
    "il learning rate viene sì abbassato magari, ma in maniera più graduale, \n",
    "perché l'idea è che solo in una certa fase la val loss si sia incastrata in minimo locale, è giusto come ragionamento?\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Per includere il plot dei risultati di training (come un'immagine) nel dizionario, \n",
    "possiamo utilizzare un buffer in memoria (invece di salvare direttamente il file su disco)\n",
    "per ottenere un'immagine in formato PNG. Questo ti consente di salvare il grafico come dati binari e\n",
    "successivamente accedere all'immagine in memoria per visualizzarla quando ne hai bisogno.\n",
    "\n",
    "Passi per implementarlo:\n",
    "Creare il plot e salvarlo in un buffer di memoria (utilizzando BytesIO di io).\n",
    "Salvare i dati dell'immagine nel dizionario.\n",
    "Quando ne hai bisogno, recuperare i dati dall'immagine dal dizionario e visualizzarla.\n",
    "\n",
    "1️⃣ Modifica della funzione plot_training_results per salvare il plot in memoria:\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "2️⃣ Salvare il plot nei risultati del training (dizionario results):\n",
    "Nel momento in cui vuoi salvare il plot dei risultati di training, puoi farlo come segue:\n",
    "    \n",
    "# Esegui il training\n",
    "plot_data = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "# Salviamo i dati binari dell'immagine nel dizionario\n",
    "results = {\n",
    "    'training_results_plot': plot_data,\n",
    "    # altri risultati del training\n",
    "}\n",
    "\n",
    "3️⃣ Recuperare e visualizzare il plot dal dizionario:\n",
    "Quando vuoi visualizzare l'immagine dal dizionario, puoi farlo recuperando \n",
    "i dati binari dell'immagine e usando PIL per convertirla in un'immagine visualizzabile con matplotlib\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Recuperiamo i dati dell'immagine dal dizionario\n",
    "plot_data = results['training_results_plot']\n",
    "\n",
    "# Convertire i dati binari in immagine PIL\n",
    "img = Image.open(io.BytesIO(plot_data))\n",
    "\n",
    "# Visualizzare l'immagine\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Nascondere gli assi\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "#def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2):\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "   \n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='min')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    \n",
    "    # https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "    \n",
    "    '''Inizializza il scheduler ReduceLROnPlateau'''\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "    \n",
    "    \n",
    "    # Dizionario per memorizzare le performance di training\n",
    "    training_performances = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_precision\": [],\n",
    "        \"train_recall\": [],\n",
    "        \"train_f1_score\": [],\n",
    "        \"train_auc\": []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "\n",
    "            #Calculate the Validation Loss\n",
    "\n",
    "            #remember: since we use CrossEntropyLoss (IN PYTORCH) we DO NOT need\n",
    "            #to do any ONE HOT ENCODING between y_pred and y_train (AS IN TENSORFLOW!) \n",
    "            \n",
    "            #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "            \n",
    "             # Qui inserisci i print di controllo\n",
    "            #print(\"y_pred shape:\", y_pred.shape)\n",
    "            #print(\"y shape:\", y.view(-1).shape)\n",
    "            #print(\"y dtype:\", y.dtype)\n",
    "        \n",
    "        \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            \n",
    "            #Perform Backpropagation\n",
    "\n",
    "            #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "            #Well, at every step the gradients will accumulate with every backprop,\n",
    "            #so to prevent 'compounding', we need to reset the stored gradient for EACH NEW EPOCH!\n",
    "\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "    \n",
    "        # Alla fine del ciclo di training (oppure!) quando si attiva l'early stopping, memorizza solo l'ultimo valore\n",
    "        if epoch == n_epochs - 1 or early_stopping.early_stop:\n",
    "            training_performances = {\n",
    "                \"train_loss\": [round(loss_train_history[-1], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[-1], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        '''\n",
    "        L'early stopping interrompe il training in due possibilità, \n",
    "        a seconda della metrica monitorata \n",
    "        \n",
    "        1) Se la loss di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "        Se vuoi monitorare la loss (val_loss), allora:\n",
    "\n",
    "        a) Devi impostare mode='min' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando la loss di validazione diminuisce.\n",
    "        c) Controlli l'early stopping usando val_loss.\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(val_loss)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - -         \n",
    "        Quindi, se imposto a 'min' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore loss \n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        2) Se l' accuracy di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "         Se vuoi monitorare l'accuracy (accuracy_val)\n",
    "        \n",
    "        a) Devi impostare mode='max' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando l'accuratezza aumenta\n",
    "        c) Controlli l'early stopping usando accuracy_val.\n",
    "    \n",
    "        if accuracy_val > max_val_acc:  # Oppure usa val_loss < min_val_loss per monitorare la loss\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(accuracy_val)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping att\n",
    "        \n",
    "        \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        Quindi, se imposto a 'max' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore accuracy\n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        Quale scegliere?\n",
    "\n",
    "        Se il tuo obiettivo è minimizzare l'errore, usa la loss (opzione 1).\n",
    "        Se vuoi massimizzare la performance del modello, usa l'accuracy (opzione 2).\n",
    "        Visto che vuoi salvare il modello in base alla best accuracy, la scelta corretta è opzione 2 con mode='max'. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima.\n",
    "        \n",
    "        \n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "        \n",
    "        \n",
    "        # Controllo per l'early stopping basato sulla loss di validazione\n",
    "        early_stopping(accuracy_val)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            #print(\"Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "            \n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "        \n",
    "        #print(f\"\\033[1mEpoch {epoch+1}/{n_epochs}\\033[0m \")\n",
    "        #print(f\"Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}\")\n",
    "        #print(f\"Train Accuracy: {accuracy_train:.4f}, Val Accuracy: {accuracy_val:.4f}\\n\")\n",
    "        \n",
    "        '''Chiamata al scheduler per ridurre il learning rate'''\n",
    "        #scheduler.step(np.mean(loss_tmp_val))  # Aggiorna il learning rate basato sulla validation loss\n",
    "        \n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": training_performances,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908531f6-f0f4-4327-ae6e-cc755dc11ed8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **VERSIONE PRE- WEIGHT AND BIASES (W&B)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f3f5d1e-9bcf-4667-886e-f2d948ce4a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''UFFICIALE - VERSIONE PRE- WEIGHT AND BIASES'''\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    #Questa sarebbe la migliore accuratezza ottenuta sul validation set\n",
    "    #in base alla quale viene preso il modello migliore!\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    best_epoch = 0  # Epoca con la migliore validazione\n",
    "    \n",
    "    best_metrics = {} # Dizionario con le metriche del migliore modello nel set di validazione\n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L'early stopping interrompe il training in due possibilità, \n",
    "        a seconda della metrica monitorata \n",
    "        \n",
    "        1) Se la loss di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "        Se vuoi monitorare la loss (val_loss), allora:\n",
    "\n",
    "        a) Devi impostare mode='min' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando la loss di validazione diminuisce.\n",
    "        c) Controlli l'early stopping usando val_loss.\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(val_loss)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - -         \n",
    "        Quindi, se imposto a 'min' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore loss \n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        2) Se l' accuracy di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "         Se vuoi monitorare l'accuracy (accuracy_val)\n",
    "        \n",
    "        a) Devi impostare mode='max' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando l'accuratezza aumenta\n",
    "        c) Controlli l'early stopping usando accuracy_val.\n",
    "    \n",
    "        if accuracy_val > max_val_acc:  # Oppure usa val_loss < min_val_loss per monitorare la loss\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(accuracy_val)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping att\n",
    "        \n",
    "        \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        Quindi, se imposto a 'max' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore accuracy\n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        Quale scegliere?\n",
    "\n",
    "        Se il tuo obiettivo è minimizzare l'errore, usa la loss (opzione 1).\n",
    "        Se vuoi massimizzare la performance del modello, usa l'accuracy (opzione 2).\n",
    "        Visto che vuoi salvare il modello in base alla best accuracy, la scelta corretta è opzione 2 con mode='max'. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima\n",
    "        \n",
    "        # Controllo della miglior validazione\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_metrics = {\n",
    "                \"train_loss\": [round(loss_train_history[best_epoch], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[best_epoch], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "            best_model = cp.deepcopy(model)  # Salvo il miglior modello\n",
    "\n",
    "        # Controllo Early Stopping\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch}, recupero il modello dell'epoca {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "        \n",
    "        #print(f\"\\033[1mEpoch {epoch+1}/{n_epochs}\\033[0m \")\n",
    "        #print(f\"Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}\")\n",
    "        #print(f\"Train Accuracy: {accuracy_train:.4f}, Val Accuracy: {accuracy_val:.4f}\\n\")\n",
    "        \n",
    "        '''Chiamata al scheduler per ridurre il learning rate'''\n",
    "        #scheduler.step(np.mean(loss_tmp_val))  # Aggiorna il learning rate basato sulla validation loss\n",
    "        \n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcae537-d2cc-42d2-8949-c1f560f92bb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###### **VERSIONE POST- WEIGHT AND BIASES (W&B)**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dddbf42-058b-4e3d-9401-9a30211c4e7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''UFFICIALE - VERSIONE POST- WEIGHT AND BIASES CON COMMENTI'''\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        :param patience: Numero di epoche da attendere prima di interrompere il training se non c'è miglioramento\n",
    "        \n",
    "        Esempio: il training si interromperà se non si osserva un miglioramento per (N = 5) epoche consecutive.\n",
    "        \n",
    "        :param min_delta: Variazione minima richiesta per considerare un miglioramento\n",
    "        \n",
    "        definisce il miglioramento minimo richiesto per essere considerato significativo. \n",
    "        Se il miglioramento è inferiore a min_delta, non viene considerato un vero miglioramento.\n",
    "        \n",
    "        Il parametro min_delta in una configurazione di early stopping indica \n",
    "        la minima variazione del valore di una metrica \n",
    "        (ad esempio, la perdita o l'accuratezza) \n",
    "        che deve verificarsi tra un'epoca e la successiva \n",
    "        per continuare l'allenamento. \n",
    "        \n",
    "        In genere, il valore di min_delta dipende dal tipo di modello e dai dati specifici, \n",
    "        ma di solito si trova in un intervallo tra 0.001 e 0.01.\n",
    "    \n",
    "            - Se stai cercando di evitare che l'allenamento si fermi troppo presto,\n",
    "            puoi impostare un valore più basso per min_delta (come 0.001), \n",
    "            - Se vuoi essere più conservativo e permettere fluttuazioni nei valori della metrica,\n",
    "            un valore più alto (come 0.01) potrebbe essere appropriato.\n",
    "\n",
    "        Un buon punto di partenza potrebbe essere 0.001, e poi fare dei test per capire quale valore funziona meglio\n",
    "        nel tuo caso specifico!\n",
    "        \n",
    "        :param mode: 'min' per monitorare la loss (minimizzazione), 'max' per l'accuracy (massimizzazione)\n",
    "        \n",
    "        'max' → ottimizza metriche da massimizzare (es. accuracy, F1-score, AUC).\n",
    "        'min' → ottimizza metriche da minimizzare (es. loss).\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    #Questa sarebbe la migliore accuratezza ottenuta sul validation set\n",
    "    #in base alla quale viene preso il modello migliore!\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    best_epoch = 0  # Epoca con la migliore validazione\n",
    "    \n",
    "    best_metrics = {} # Dizionario con le metriche del migliore modello nel set di validazione\n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L'early stopping interrompe il training in due possibilità, \n",
    "        a seconda della metrica monitorata \n",
    "        \n",
    "        1) Se la loss di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "        Se vuoi monitorare la loss (val_loss), allora:\n",
    "\n",
    "        a) Devi impostare mode='min' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando la loss di validazione diminuisce.\n",
    "        c) Controlli l'early stopping usando val_loss.\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(val_loss)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping attivato!\")\n",
    "            break\n",
    "            \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - -         \n",
    "        Quindi, se imposto a 'min' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore loss \n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        2) Se l' accuracy di validazione non migliora per N epoche (evitando overfitting).\n",
    "        \n",
    "         Se vuoi monitorare l'accuracy (accuracy_val)\n",
    "        \n",
    "        a) Devi impostare mode='max' nell'early stopping.\n",
    "        b) Salvi il modello migliore quando l'accuratezza aumenta\n",
    "        c) Controlli l'early stopping usando accuracy_val.\n",
    "    \n",
    "        if accuracy_val > max_val_acc:  # Oppure usa val_loss < min_val_loss per monitorare la loss\n",
    "            max_val_acc = accuracy_val\n",
    "            best_model = cp.deepcopy(model)\n",
    "\n",
    "        # Controllo per Early Stopping\n",
    "        early_stopping(accuracy_val)  # Oppure early_stopping(val_loss) se vuoi monitorare la loss\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"🛑 Early stopping att\n",
    "        \n",
    "        \n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        Quindi, se imposto a 'max' il mode dell'early stopping allora\n",
    "        vuol dire che lui mi salverà le performance del modello con la migliore migliore accuracy\n",
    "        che non è migliorata per oltre 10 epoche del valore di delta impostato\n",
    "        — - -  — - -  — - -  — - -  — - - — - -  — - -  — - -  — - -  — - - \n",
    "        \n",
    "        Quale scegliere?\n",
    "\n",
    "        Se il tuo obiettivo è minimizzare l'errore, usa la loss (opzione 1).\n",
    "        Se vuoi massimizzare la performance del modello, usa l'accuracy (opzione 2).\n",
    "        Visto che vuoi salvare il modello in base alla best accuracy, la scelta corretta è opzione 2 con mode='max'. \n",
    "\n",
    "        '''\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima\n",
    "        \n",
    "        # Controllo della miglior validazione\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_metrics = {\n",
    "                \"train_loss\": [round(loss_train_history[best_epoch], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[best_epoch], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "            best_model = cp.deepcopy(model)  # Salvo il miglior modello\n",
    "\n",
    "        # Controllo Early Stopping\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch}, recupero il modello dell'epoca {best_epoch}\")\n",
    "            break\n",
    "\n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "        \n",
    "\n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69b3db-65f8-4f63-aaed-7b0f656cc0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''UFFICIALE - VERSIONE POST- WEIGHT AND BIASES SENZA COMMENTI'''\n",
    "\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience = 10, min_delta = 0.001, mode = 'max'):\n",
    "        \n",
    "            \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.best_score = None # Tiene traccia del miglior punteggio osservato\n",
    "        self.counter = 0 # Conta quante epoche consecutive non migliorano\n",
    "        self.early_stop = False # Flag che indica se attivare l'early stopping\n",
    "        \n",
    "        #Ogni volta che si chiama la classe con early_stopping(current_score), controlla se il modello sta migliorando o meno.\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        \n",
    "        #Caso 1: Prima iterazione (best_score ancora None)\n",
    "        #→ Se non esiste ancora un miglior punteggio, lo inizializza con il primo valore ricevuto.\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            \n",
    "        #Caso 2: Il modello migliora\n",
    "        #→ Se il valore migliora di almeno min_delta, aggiorna best_score e resetta il contatore.\n",
    "\n",
    "        elif (self.mode == 'min' and current_score < self.best_score - self.min_delta) or \\\n",
    "             (self.mode == 'max' and current_score > self.best_score + self.min_delta):\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0  # Reset contatore se migliora\n",
    "            \n",
    "        #Caso 3: Il modello NON migliora\n",
    "        \n",
    "        #→ Se il valore non migliora, incrementa il contatore.\n",
    "        #→ Se il contatore raggiunge patience, imposta early_stop = True, segnalando che il training deve essere interrotto.\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1  # Incrementa se non migliora\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"🛑 Early stopping attivato! Nessun miglioramento per {self.patience} epoche consecutive.\")\n",
    "                self.early_stop = True\n",
    "                \n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    # Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    #plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    '''\n",
    "    \n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "\n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "\n",
    "def training(model, dataset_train_loader, dataset_val_loader, optimizer, criterion, n_epochs = 100, patience = 10):\n",
    "    \n",
    "    # Sposta il modello sulla GPU (se disponibile)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    #Setta il modello in fase di training\n",
    "    model.train()\n",
    "    \n",
    "    # Storico delle metriche per ogni epoca\n",
    "    loss_train_history = []  # History of Training loss\n",
    "    loss_val_history = []    # History of Validation loss\n",
    "    accuracy_train_history = []  # History of Training Accuracy\n",
    "    accuracy_val_history = []    # History of Validation Accuracy\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "    \n",
    "    # Liste per le metriche di valutazione (precision, recall, F1, AUC)\n",
    "    precision_train_history = []\n",
    "    recall_train_history = []\n",
    "    f1_train_history = []\n",
    "    auc_train_history = []\n",
    "    \n",
    "    #Questa sarebbe la migliore accuratezza ottenuta sul validation set\n",
    "    #in base alla quale viene preso il modello migliore!\n",
    "    \n",
    "    max_val_acc = 0\n",
    "    best_model = None\n",
    "    \n",
    "    best_epoch = 0  # Epoca con la migliore validazione\n",
    "    \n",
    "    best_metrics = {} # Dizionario con le metriche del migliore modello nel set di validazione\n",
    "    \n",
    "    # Variabili per memorizzare le etichette vere e predette per l'intero training\n",
    "    y_true_train_list = []\n",
    "    y_pred_train_list = []\n",
    "    \n",
    "    \n",
    "    pbar = tqdm(range(n_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        #Create a list for temporary monitoring of train loss and accuracy at each epoch\n",
    "        train_loss_tmp = [] \n",
    "        correct_train = 0 \n",
    "        \n",
    "        \n",
    "        #'''STARTING OF THE TRAINING PHASE'''\n",
    "        \n",
    "        #Iterating for every batch inside dataset_train_loader\n",
    "        for x, y in dataset_train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            #Run forward pass through my network and get a prediction\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            train_loss = criterion(y_pred, y.view(-1))\n",
    "            optimizer.zero_grad() #so essentially finding where gradients is 0\n",
    "                                  #we're looking for minimum's there\n",
    "\n",
    "            train_loss.backward() #performing the backprop step\n",
    "            optimizer.step() #update the model's hyperparameters based off of the step\n",
    "        \n",
    "            train_loss_tmp.append(train_loss.item()) #append the loss at each epoch in the temporary train loss list inside each epoch\n",
    "            \n",
    "            # Calculate the Accuracy Score during the Training Phase\n",
    "                \n",
    "            #qui il \"_,\"\n",
    "            _, predicted_train = torch.max(y_pred, 1)\n",
    "            correct_train += (predicted_train == y).sum().item()\n",
    "            \n",
    "            # Aggiungere le etichette vere e quelle predette alla lista\n",
    "            y_true_train_list.extend(y.cpu().numpy())\n",
    "            y_pred_train_list.extend(predicted_train.cpu().numpy())\n",
    "        \n",
    "        # Save the results of training set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        loss_train_history.append(np.mean(train_loss_tmp))\n",
    "        accuracy_train = correct_train / len(dataset_train_loader.dataset)\n",
    "        accuracy_train_history.append(accuracy_train)\n",
    "        \n",
    "        # Calcolare precision, recall, F1-score e AUC durante il training\n",
    "        precision_train = precision_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        recall_train = recall_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        f1_train = f1_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        auc_train = roc_auc_score(y_true_train_list, y_pred_train_list, average='weighted')\n",
    "        \n",
    "        precision_train_history.append(precision_train)\n",
    "        recall_train_history.append(recall_train)\n",
    "        f1_train_history.append(f1_train)\n",
    "        auc_train_history.append(auc_train)\n",
    "        \n",
    "        # '''STARTING OF THE VALIDATION PHASE'''\n",
    "        \n",
    "        #Setta il modello in fase di validation\n",
    "        #model.eval() \n",
    "        \n",
    "        loss_tmp_val = []  #create a list for temporary val list at each epoch\n",
    "        correct_val = 0\n",
    "        \n",
    "        y_true_list = []\n",
    "        y_pred_list = []\n",
    "        \n",
    "        #Here we disable gradient computation for the validation phase!\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for x, y in dataset_val_loader:\n",
    "                \n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                #Run forward pass through my network and get a prediction\n",
    "                y_pred = model(x)\n",
    "\n",
    "                #Calculate Validation Loss\n",
    "\n",
    "                #remember: since we use CrossEntropyLoss we DO NOT need\n",
    "                #to do any ONE HOT ENCODING between y_pred and y_train \n",
    "                \n",
    "                #loss = criterion(y_pred.to(device), y.view(-1).to(device))\n",
    "                \n",
    "                val_loss = criterion(y_pred, y.view(-1))\n",
    "\n",
    "                #Perform Backpropagation\n",
    "\n",
    "                #HOW TO ADJUST THE VALUES (weights and biases)?\n",
    "                #well, at every step the gradients will accumulate with every backprop,\n",
    "                #so to prevent 'compounding', we need to reset the stored gradient for each new epoch!\n",
    "\n",
    "                loss_tmp_val.append(val_loss.item()) #append the loss at each epoch in the temporary val loss list inside each epoch \n",
    "                \n",
    "                # Calculate the Accuracy Score during the Validation Phase\n",
    "                _, predicted_val = torch.max(y_pred, 1)\n",
    "                correct_val += (predicted_val == y).sum().item()\n",
    "                \n",
    "                # Aggiungi le etichette e le predizioni per la confusion matrix\n",
    "                y_true_list.extend(y.cpu().numpy())\n",
    "                y_pred_list.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "                \n",
    "        # Save the results of validation set for every epoch\n",
    "        \n",
    "        #i.e., append the results in the whole train loss history list outside the cycle of each epoch \n",
    "        \n",
    "        loss_val_history.append(np.mean(loss_tmp_val)) \n",
    "        accuracy_val = correct_val / len(dataset_val_loader.dataset)\n",
    "        accuracy_val_history.append(accuracy_val)\n",
    "        \n",
    "        #L'early stopping deve essere basato sulla val accuracy,\n",
    "        #ma quando il training si interrompe, \n",
    "        #dobbiamo salvare le migliori performance ottenute sul training in corrispondenza dell'epoca in cui\n",
    "        #la val accuracy era massima\n",
    "        \n",
    "        # Controllo della miglior validazione\n",
    "        if accuracy_val > max_val_acc:\n",
    "            max_val_acc = accuracy_val\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_metrics = {\n",
    "                \"train_loss\": [round(loss_train_history[best_epoch], 4)],\n",
    "                \"train_accuracy\": [round(accuracy_train_history[best_epoch], 4)],\n",
    "                \"train_precision\": [round(precision_train, 4)],\n",
    "                \"train_recall\": [round(recall_train, 4)],\n",
    "                \"train_f1_score\": [round(f1_train, 4)],\n",
    "                \"train_auc\": [round(auc_train, 4)]\n",
    "            }\n",
    "            best_model = cp.deepcopy(model)  # Salvo il miglior modello\n",
    "\n",
    "        # Controllo Early Stopping\n",
    "        early_stopping(accuracy_val)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"⚠️ Early stopping attivato all'epoca \\033[1m{epoch}\\033[0m, recupero il modello dell'epoca \\033[1m{best_epoch}\\033[0m\")\n",
    "            break\n",
    "\n",
    "        # Update of the progress bar\n",
    "        pbar.set_description(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {loss_train_history[-1]:.4f}, Val Loss: {loss_val_history[-1]:.4f}, Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n",
    "\n",
    "        # Calculate the confusion matrix and the classification report after all epochs in the Validation Phase\n",
    "        conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "        class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_train\": train_loader.batch_size,\n",
    "        \"batch_size_val\": val_loader.batch_size,\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "        \"n_epochs\": n_epochs\n",
    "    }\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "    \"optimizer\": str(optimizer),\n",
    "    \"loss_function\": str(criterion),\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "   }\n",
    "\n",
    "    \n",
    "    # Plot dei risultati\n",
    "    #plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history, exp_cond_1, exp_cond_2)\n",
    "    training_plot = plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history)\n",
    "\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \"best_model\": best_model,\n",
    "        \"confusion_matrix_val\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,\n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "\n",
    "    return train_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce82f3-9008-434e-be98-f020b79a2998",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **TESTING**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b96458a-c064-4458-82ce-1a88b44073a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "TESTING FUNCTION: CORRETTA ANCHE PER IL GRAD-CAM\n",
    "\n",
    "SUCCESSIVAMENTE, DENTRO AL FOR LOOP DEL TRAINING E TESTING, \n",
    "SI RICHIAMA LA FUNZIONE DIRETTAMENTE DI \n",
    "\n",
    "1) compute_gradcam_figure, LA QUALE AL SUO INTERNO PRESENTA GIÀ \n",
    "TUTTO QUELLO CHE SERVE PER CALCOLARE IL GRADCAM, DI MODO CHE VADA A \n",
    "\n",
    "Selezionare esempi rappresentativi per ciascuna classe.\n",
    "Calcolare le mappe GradCAM e gli overlay.\n",
    "Creare una figura con le heatmap e le sovrapposizioni, completa di titoli esplicativi.\n",
    "Restituire un'immagine (buffer) pronta per essere salvata\n",
    "\n",
    "SUCCESSIVAMENTE, QUINDI, IL PROCEDIMENTO DIVENTA COME SEGUE:\n",
    "\n",
    "1) Si esegue il TESTING, per ottenere le metriche e salvare i risultati (senza GradCAM)\n",
    "\n",
    "2) Nel loop principale di TRAINING & TESTING, se il modello è CNN2D, allora \n",
    "\n",
    " - richiama la funzione 'compute_gradcam_figure', la quale va a\n",
    "    - calcolare le mappe di attivazione e successivamente creo le immagini che gli ho chiesto\n",
    "    - passa l'immagine ottenuta da GradCAM alla funzione 'save_performance_results', la quale va a \n",
    "        - salvare i risultati di test ottenuti dalla funzione di 'testing'\n",
    "        - salvare l'immagine risultatante del GradCAM e la sovrapposizione del GradCAM sullo spettrogramma originale della classe risultante\n",
    "        \n",
    "        \n",
    "Questo approccio garantisce chiarezza e separa la parte di performance (testing) dalla parte di explainability (GradCAM).\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def testing(results, test_loader, criterion):\n",
    "    \n",
    "    # Recupera il miglior modello ottenuto durante la validazione\n",
    "    model = results['best_model']\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()  # Imposta il modello in modalità valutazione\n",
    "\n",
    "    y_true_list = []  # Lista per salvare le etichette reali\n",
    "    y_pred_list = []  # Lista per salvare le previsioni del modello\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    test_performances = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"test_precision\": [],\n",
    "        \"test_recall\": [],\n",
    "        \"test_f1_score\": [],\n",
    "        \"test_auc\": []\n",
    "    }\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "        \n",
    "        for inputs, labels in pbar:\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Ottenere le predizioni del modello\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calcolare la loss\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "            # Memorizzare predizioni ed etichette vere\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred_list.extend(predicted.cpu().numpy())\n",
    "            y_true_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Aggiornare il numero di predizioni corrette\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            pbar.set_description(f\"Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "    # Calcolare l'accuratezza complessiva\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    # Calcolare precision, recall, F1-score, AUC durante il testing\n",
    "    precision_test = precision_score(y_true_list, y_pred_list, average='weighted')\n",
    "    recall_test = recall_score(y_true_list, y_pred_list, average='weighted')\n",
    "    f1_test = f1_score(y_true_list, y_pred_list, average='weighted')\n",
    "    auc_test = roc_auc_score(y_true_list, y_pred_list, average='weighted')  # Assicurati che il problema sia binario o multi-class\n",
    "\n",
    "    # Aggiungere questi valori nel dizionario delle performance (arrotondando a 4 decimali)\n",
    "    test_performances[\"test_loss\"].append(round(total_loss / len(test_loader), 4))  # Media della loss\n",
    "    test_performances[\"test_accuracy\"].append(round(accuracy, 4))\n",
    "    test_performances[\"test_precision\"].append(round(precision_test, 4))\n",
    "    test_performances[\"test_recall\"].append(round(recall_test, 4))\n",
    "    test_performances[\"test_f1_score\"].append(round(f1_test, 4))\n",
    "    test_performances[\"test_auc\"].append(round(auc_test, 4))\n",
    "    \n",
    "    # Creare la confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "    \n",
    "    # Stampare classification report\n",
    "    class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "    # Visualizzare la confusion matrix\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    #sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    #plt.title(\"Confusion Matrix\")\n",
    "    #plt.xlabel(\"Predicted\")\n",
    "    #plt.ylabel(\"True\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    #buf = io.BytesIO()\n",
    "    #plt.savefig(buf, format='png')\n",
    "    #buf.seek(0)\n",
    "    #conf_matrix_image_data = buf.getvalue()\n",
    "    #buf.close()\n",
    "    \n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    buf = io.BytesIO()\n",
    "    #plt.figure(figsize=(8, 6))  # Nuova figura per evitare sovrapposizioni\n",
    "    #sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    #plt.title(\"Confusion Matrix\")\n",
    "    #plt.xlabel(\"Predicted\")\n",
    "    #plt.ylabel(\"True\")\n",
    "    #plt.savefig(buf, format='png')  # Salva l'immagine nel buffer\n",
    "    buf.seek(0)  # Torna all'inizio del buffer\n",
    "    conf_matrix_image_data = buf.getvalue()  # Ottieni l'immagine in formato binario\n",
    "    buf.close()  # Chiudi il buffer\n",
    "\n",
    "    # Mostra la confusion matrix (opzionale)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    model_config = {\n",
    "        \"model_architecture\": str(model),\n",
    "        \"batch_size_test\": test_loader.batch_size,\n",
    "    }\n",
    "\n",
    "    # Dizionario degli iper-parametri\n",
    "    hyperparams = {\n",
    "        \"optimizer\": str(optimizer),\n",
    "        \"loss_function\": str(criterion),\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Restituisci i risultati come dizionario\n",
    "    test_results = {\n",
    "        \"test_performances\": test_performances,  # Aggiungi il dizionario delle performance\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"model_configuration\": model_config,\n",
    "        \"hyperparameters\": hyperparams,  # Aggiunti i due nuovi dizionari\n",
    "        \"confusion_matrix_image\": conf_matrix_image_data,  # Aggiunta l'immagine della confusion matrix\n",
    "    }\n",
    "        \n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1226b530-70b7-4af4-a072-9e92036be53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TESTING FUNCTION: CORRETTA ANCHE PER IL GRAD-CAM\n",
    "\n",
    "SUCCESSIVAMENTE, DENTRO AL FOR LOOP DEL TRAINING E TESTING, \n",
    "SI RICHIAMA LA FUNZIONE DIRETTAMENTE DI \n",
    "\n",
    "1) compute_gradcam_figure, LA QUALE AL SUO INTERNO PRESENTA GIÀ \n",
    "TUTTO QUELLO CHE SERVE PER CALCOLARE IL GRADCAM, DI MODO CHE VADA A \n",
    "\n",
    "Selezionare esempi rappresentativi per ciascuna classe.\n",
    "Calcolare le mappe GradCAM e gli overlay.\n",
    "Creare una figura con le heatmap e le sovrapposizioni, completa di titoli esplicativi.\n",
    "Restituire un'immagine (buffer) pronta per essere salvata\n",
    "\n",
    "SUCCESSIVAMENTE, QUINDI, IL PROCEDIMENTO DIVENTA COME SEGUE:\n",
    "\n",
    "1) Si esegue il TESTING, per ottenere le metriche e salvare i risultati (senza GradCAM)\n",
    "\n",
    "2) Nel loop principale di TRAINING & TESTING, se il modello è CNN2D, allora \n",
    "\n",
    " - richiama la funzione 'compute_gradcam_figure', la quale va a\n",
    "    - calcolare le mappe di attivazione e successivamente creo le immagini che gli ho chiesto\n",
    "    - passa l'immagine ottenuta da GradCAM alla funzione 'save_performance_results', la quale va a \n",
    "        - salvare i risultati di test ottenuti dalla funzione di 'testing'\n",
    "        - salvare l'immagine risultatante del GradCAM e la sovrapposizione del GradCAM sullo spettrogramma originale della classe risultante\n",
    "        \n",
    "        \n",
    "Questo approccio garantisce chiarezza e separa la parte di performance (testing) dalla parte di explainability (GradCAM).\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def testing(results, test_loader, criterion):\n",
    "    \n",
    "    # Recupera il miglior modello ottenuto durante la validazione\n",
    "    model = results['best_model']\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()  # Imposta il modello in modalità valutazione\n",
    "\n",
    "    y_true_list = []  # Lista per salvare le etichette reali\n",
    "    y_pred_list = []  # Lista per salvare le previsioni del modello\n",
    "    \n",
    "    '''AGGIUNTA NUOVA PER CALCOLO AUC-ROC'''\n",
    "    y_score_list = []   # <— Lista per salvare gli score per le probabilità della classe positiva (per auc-roc!)\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    test_performances = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"test_precision\": [],\n",
    "        \"test_recall\": [],\n",
    "        \"test_f1_score\": [],\n",
    "        \"test_auc\": []\n",
    "    }\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "        \n",
    "        for inputs, labels in pbar:\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Ottenere le predizioni del modello\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            '''AGGIUNTA NUOVA PER CALCOLO AUC-ROC'''\n",
    "            # aggiungi queste due righe\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            y_score_list.extend(probs[:,1].cpu().numpy())\n",
    "\n",
    "            # Calcolare la loss\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "            # Memorizzare predizioni ed etichette vere\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred_list.extend(predicted.cpu().numpy())\n",
    "            y_true_list.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Aggiornare il numero di predizioni corrette\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            pbar.set_description(f\"Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "    # Calcolare l'accuratezza complessiva\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    # Calcolare precision, recall, F1-score, AUC durante il testing\n",
    "    precision_test = precision_score(y_true_list, y_pred_list, average='weighted')\n",
    "    recall_test = recall_score(y_true_list, y_pred_list, average='weighted')\n",
    "    f1_test = f1_score(y_true_list, y_pred_list, average='weighted')\n",
    "    \n",
    "    '''OLD VERSION'''\n",
    "    #auc_test = roc_auc_score(y_true_list, y_pred_list, average='weighted')  # Assicurati che il problema sia binario o multi-class\n",
    "    \n",
    "    '''AGGIUNTA NUOVA PER CALCOLO AUC-ROC\n",
    "    \n",
    "    In questo modo l’roc_auc_score calcola l’area sotto tutta la curva ROC (tutte le soglie), \n",
    "    invece di valutare un solo punto corrispondente alla soglia 0.5\n",
    "    '''\n",
    "    \n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "    auc_test = roc_auc_score(y_true_list, y_score_list)\n",
    "\n",
    "\n",
    "    # Aggiungere questi valori nel dizionario delle performance (arrotondando a 4 decimali)\n",
    "    test_performances[\"test_loss\"].append(round(total_loss / len(test_loader), 4))  # Media della loss\n",
    "    test_performances[\"test_accuracy\"].append(round(accuracy, 4))\n",
    "    test_performances[\"test_precision\"].append(round(precision_test, 4))\n",
    "    test_performances[\"test_recall\"].append(round(recall_test, 4))\n",
    "    test_performances[\"test_f1_score\"].append(round(f1_test, 4))\n",
    "    test_performances[\"test_auc\"].append(round(auc_test, 4))\n",
    "    \n",
    "    # Creare la confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_list, y_pred_list)\n",
    "    \n",
    "    # Stampare classification report\n",
    "    class_report = classification_report(y_true_list, y_pred_list)\n",
    "\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "    # Visualizzare la confusion matrix\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    #sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    #plt.title(\"Confusion Matrix\")\n",
    "    #plt.xlabel(\"Predicted\")\n",
    "    #plt.ylabel(\"True\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    #buf = io.BytesIO()\n",
    "    #plt.savefig(buf, format='png')\n",
    "    #buf.seek(0)\n",
    "    #conf_matrix_image_data = buf.getvalue()\n",
    "    #buf.close()\n",
    "    \n",
    "    \n",
    "    # Salviamo l'immagine della confusion matrix in un buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.figure(figsize=(8, 6))  # Nuova figura per evitare sovrapposizioni\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(buf, format='png')  # Salva l'immagine nel buffer\n",
    "    buf.seek(0)  # Torna all'inizio del buffer\n",
    "    conf_matrix_image_data = buf.getvalue()  # Ottieni l'immagine in formato binario\n",
    "    buf.close()  # Chiudi il buffer\n",
    "\n",
    "    # Mostra la confusion matrix (opzionale)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Salvataggio della configurazione del modello e iper-parametri\n",
    "    '''COMMENTATO'''\n",
    "    #model_config = {\n",
    "        #\"model_architecture\": str(model),\n",
    "        #\"batch_size_test\": test_loader.batch_size,\n",
    "    #}\n",
    "    \n",
    "    '''COMMENTATO'''\n",
    "    # Dizionario degli iper-parametri\n",
    "    #hyperparams = {\n",
    "        #\"optimizer\": str(optimizer),\n",
    "        #\"loss_function\": str(criterion),\n",
    "        #\"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    #}\n",
    "\n",
    "    \n",
    "    '''COMMENTATO'''\n",
    "    # Restituisci i risultati come dizionario\n",
    "    #test_results = {\n",
    "        #\"test_performances\": test_performances,  # Aggiungi il dizionario delle performance\n",
    "        #\"confusion_matrix\": conf_matrix,\n",
    "        #\"classification_report\": class_report,\n",
    "        #\"model_configuration\": model_config,\n",
    "        #\"hyperparameters\": hyperparams,  # Aggiunti i due nuovi dizionari\n",
    "        #\"confusion_matrix_image\": conf_matrix_image_data,  # Aggiunta l'immagine della confusion matrix\n",
    "    #}\n",
    "    \n",
    "    \n",
    "    # Restituisci i risultati come dizionario\n",
    "    test_results = {\n",
    "        \"test_performances\": test_performances,  # Aggiungi il dizionario delle performance\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "        \"classification_report\": class_report,\n",
    "        \"confusion_matrix_image\": conf_matrix_image_data,  # Aggiunta l'immagine della confusion matrix\n",
    "    }   \n",
    "        \n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4475e21-e706-4102-98ea-870b754c69e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **CREAZIONE CLASSE GRADCAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2eb0072-b2ff-45dc-89a5-cabd4f877e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### **CREAZIONE CLASSE GRADCAM**\n",
    "\n",
    "'''\n",
    "Creazione della classe GradCAM\n",
    "\n",
    "-----1. Costruttore (init)-----\n",
    "\n",
    "Cosa fa:\n",
    "\n",
    "Salva il modello e il layer target (ad esempio, l'ultimo strato convoluzionale) su cui calcolare le mappe di attivazione.\n",
    "\n",
    "A) Inizializza due variabili, \n",
    "\n",
    "1) self.activations e 2) self.gradients, che verranno usate per memorizzare rispettivamente \n",
    "1) le attivazioni (feature maps) e 2) i gradienti di quel layer\n",
    "\n",
    "B) Registra due hook sul target_layer:\n",
    "\n",
    "1) Forward Hook: Quando il modello effettua la forward pass, viene eseguito save_activation per salvare le attivazioni\n",
    "2) Backward Hook: Durante la backward pass, save_gradient viene chiamato per salvare i gradienti\n",
    "\n",
    "\n",
    "-----2. Hook per Salvare Attivazioni e Gradienti-----\n",
    "\n",
    "B) Save Activation\n",
    "\n",
    "def save_activation(self, module, input, output):\n",
    "    self.activations = output.detach()\n",
    "\n",
    "Cosa fa:\n",
    "\n",
    "Quando viene eseguita la forward pass sul target_layer, questo hook cattura l'output (le attivazioni) del layer.\n",
    "Usa detach() per ottenere una copia dei dati senza il tracking dei gradienti, in modo da non interferire con la retropropagazione.\n",
    "\n",
    "C) Save Gradient\n",
    "\n",
    "def save_gradient(self, module, grad_input, grad_output):\n",
    "    self.gradients = grad_output[0].detach()\n",
    "\n",
    "\n",
    "Cosa fa:\n",
    "\n",
    "Durante la backward pass, questo hook cattura i gradienti che fluiscono attraverso il target_layer.\n",
    "grad_output è una tupla; solitamente il primo elemento contiene i gradienti utili. \n",
    "\n",
    "Anche qui si usa detach() per isolare i dati dai grafi di calcolo.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Registra hook per catturare attivazioni e gradienti\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257404c0-5768-4fdb-9960-6c583c7da130",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **DRAFT IMPLEMENTATIONS OF GRADCAM COMPUTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7217a-5909-4121-97aa-a2cb01d38846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### INITIAL IMPLEMENTATIONS OF GRADCAM\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Registra hook per catturare attivazioni e gradienti\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        \n",
    "1) Funzione generate_cam (interna alla classe GradCAM)\n",
    "\n",
    "    La differenza chiave tra i due approcci è proprio la selezione dell'input su cui viene calcolata la Grad-CAM. Ti riassumo le due opzioni:\n",
    "\n",
    "    1️⃣ Approccio attuale (generate_cam)\n",
    "    Viene passato un singolo input_tensor, e il Grad-CAM viene calcolato su di esso.\n",
    "    Se target_class non è specificata, viene selezionata la classe predetta dal modello per quell'input.\n",
    "    Il calcolo del Grad-CAM si basa su una backward pass del gradiente rispetto alla classe target.\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        # Effettua la forward pass\n",
    "        output = self.model(input_tensor)\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        # Azzeramento dei gradienti\n",
    "        self.model.zero_grad()\n",
    "        # Calcola il gradiente per la classe target\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "\n",
    "        # Calcola i pesi come media dei gradienti su width e height\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        # Somma pesata delle attivazioni\n",
    "        cam = torch.sum(weights * self.activations, dim=1)\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalizza la mappa\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Upsample alla dimensione dell'immagine di input\n",
    "        cam = F.interpolate(cam.unsqueeze(1), size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        return cam\n",
    "    \n",
    "    \n",
    "2) Funzione compute_gradcam_figure (esterna alla classe GradCAM)   \n",
    "    \n",
    "2️⃣ Alternativa proposta (compute_gradcam_figure)\n",
    "Seleziona esplicitamente un esempio per ciascuna classe (0 e 1) iterando sul test_loader.\n",
    "Questo garantisce che il Grad-CAM sia calcolato su esempi rappresentativi di entrambe le classi.\n",
    "La visualizzazione finale confronta le heatmap delle due classi, sovrapponendole agli spettrogrammi.\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "    I titoli della figura vengono personalizzati con exp_cond, data_type, category_subject.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assumiamo che il modello sia CNN2D e che il layer target sia model.conv3\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "    # Dizionari per salvare il campione per ogni classe\n",
    "    samples = {}      # Salveremo il sample input per ogni classe\n",
    "    labels_found = {} # Per tenere traccia delle etichette già trovate\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int not in labels_found:\n",
    "                samples[label_int] = inputs[i].unsqueeze(0)  # salva come tensore 4D\n",
    "                labels_found[label_int] = True\n",
    "            if 0 in labels_found and 1 in labels_found:\n",
    "                break\n",
    "        if 0 in labels_found and 1 in labels_found:\n",
    "            break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    if 0 not in samples or 1 not in samples:\n",
    "        print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "        return None\n",
    "\n",
    "    # Per ciascun campione, calcola GradCAM\n",
    "    cams = {}\n",
    "    overlays = {}\n",
    "    for cls in [0, 1]:\n",
    "        sample_input = samples[cls]\n",
    "        sample_input.requires_grad = True  # Abilita gradiente per il campione\n",
    "        cam = gradcam.generate_cam(sample_input)\n",
    "        cams[cls] = cam\n",
    "\n",
    "        # Converti il sample in immagine numpy per la visualizzazione\n",
    "        img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        # Normalizza l'immagine in scala 0-255\n",
    "        img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "        # Applica la heatmap\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        # Sovrapponi la heatmap all'immagine originale\n",
    "        overlay = cv2.addWeighted(img_norm, 0.6, heatmap, 0.4, 0)\n",
    "        overlays[cls] = overlay\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Titolo per la prima riga\n",
    "    title_row1 = f\"Grad-CAM mapping of experimental condition {exp_cond}, EEG {data_type}, Subject {category_subject}\"\n",
    "    # Titolo per la seconda riga\n",
    "    title_row2 = f\"Grad-CAM mapping superimposition over EEG Spectrogram of experimental condition {exp_cond}, Subject {category_subject}\"\n",
    "    \n",
    "    # Prima riga: solo le heatmap\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        axs[0, j].imshow(cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB))\n",
    "        axs[0, j].set_title(f\"Class {cls} Heatmap\")\n",
    "        axs[0, j].axis('off')\n",
    "    axs[0, 0].set_ylabel(title_row1, fontsize=10)\n",
    "    \n",
    "    # Seconda riga: overlay della heatmap sullo spettrogramma originale\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        axs[1, j].imshow(overlays[cls])\n",
    "        axs[1, j].set_title(f\"Class {cls} Overlay\")\n",
    "        axs[1, j].axis('off')\n",
    "    axs[1, 0].set_ylabel(title_row2, fontsize=10)\n",
    "    \n",
    "    # Ottimizza la disposizione della figura\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image\n",
    "\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742a3eb-9a0a-4614-9912-9da386f7cd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''SOLUZIONE ? INTEGRARE LA 2) IN 1)\n",
    "\n",
    "\n",
    "🛠️ Cosa conviene fare?\n",
    "Se il tuo obiettivo è sempre confrontare le attivazioni per entrambe le classi, \n",
    "allora conviene integrare compute_gradcam_figure dentro la classe GradCAM e rimuovere generate_cam come metodo separato.\n",
    "\n",
    "📌 Quindi suggerirei di fare così:\n",
    "\n",
    "Rendere compute_gradcam_figure un metodo della classe GradCAM.\n",
    "Rimuovere generate_cam, perché il calcolo della CAM viene già eseguito all'interno del loop che seleziona i campioni.\n",
    "Mantenere la logica che seleziona i campioni da entrambe le classi, perché è più robusta rispetto a calcolare la CAM su un singolo input arbitrario.\n",
    "⚠️ Attenzione a una cosa però!\n",
    "Il metodo generate_cam fa un passaggio importante che non è presente in compute_gradcam_figure:\n",
    "\n",
    "python\n",
    "Copia\n",
    "Modifica\n",
    "weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "cam = torch.sum(weights * self.activations, dim=1)\n",
    "cam = F.relu(cam)\n",
    "🔹 Bisogna assicurarsi che questa logica venga mantenuta nel nuovo metodo!\n",
    "Attualmente compute_gradcam_figure chiama gradcam.generate_cam(sample_input), quindi se generate_cam viene eliminato, questa parte va spostata nel nuovo metodo.\n",
    "\n",
    "📌 In sintesi, cosa farei\n",
    "✅ Modificare la classe GradCAM e aggiungere direttamente compute_gradcam_figure.\n",
    "✅ Eliminare generate_cam, ma mantenere la sua logica di calcolo della CAM.\n",
    "✅ Garantire che il calcolo dei pesi e della CAM sia integrato nel nuovo metodo.\n",
    "✅ Mantenere la selezione di un campione per ciascuna classe, per una migliore interpretabilità.\n",
    "\n",
    "\n",
    "\n",
    "Ha senso integrare compute_gradcam_figure direttamente come metodo della classe GradCAM ed eliminare generate_cam, perché:\n",
    "\n",
    "Selezione più rappresentativa dei campioni\n",
    "\n",
    "Il metodo compute_gradcam_figure assicura che vengano selezionati esempi di entrambe le classi (0 e 1), cosa che generate_cam non fa.\n",
    "Questo approccio fornisce una migliore interpretabilità della Grad-CAM confrontando diverse classi.\n",
    "Chiarezza e modularità\n",
    "\n",
    "generate_cam è attualmente chiamato da compute_gradcam_figure, ma possiamo integrare direttamente la logica dentro GradCAM.\n",
    "Questo evita la duplicazione del codice e rende più chiaro il flusso.\n",
    "Ottimizzazione del calcolo\n",
    "\n",
    "La pipeline di compute_gradcam_figure gestisce direttamente la forward pass e il calcolo del gradiente per entrambi i campioni in un'unica operazione, evitando di dover chiamare generate_cam separatamente.\n",
    "Prossimi passi:\n",
    "Spostiamo compute_gradcam_figure dentro GradCAM come metodo della classe.\n",
    "Eliminiamo generate_cam e integriamo direttamente la logica di forward pass e backward pass dentro compute_gradcam_figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2141d9d1-f830-49d8-9c71-035283ef58f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **FINAL IMPLEMENTATION OF GRADCAM COMPUTATION**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5a692f6-0431-4daa-9edd-65149f865258",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "PRE- FINAL VERSION WITH STILL SOME EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int not in labels_found:\n",
    "                samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                labels_found[label_int] = True\n",
    "            if 0 in labels_found and 1 in labels_found:\n",
    "                break\n",
    "        if 0 in labels_found and 1 in labels_found:\n",
    "            break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    if 0 not in samples or 1 not in samples:\n",
    "        print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "        return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        sample_input = samples[cls]\n",
    "        sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "        # Esegui forward pass per ottenere l'output del modello\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        # Se non viene specificata una classe target, seleziona quella predetta\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        # Azzeramento dei gradienti e backward pass per la classe target\n",
    "        # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "        model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        \n",
    "        #Qui si calcola la mappa CAM:\n",
    "\n",
    "        #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "        #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "        #Si applica ReLU per eliminare i valori negativi.\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "        \n",
    "        #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "        #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "        weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "        cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "        \n",
    "        # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        #Passaggio 5: Normalizzazione e upsampling\n",
    "        \n",
    "        #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "        #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "        # -------------------------------\n",
    "        # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Normalizza la mappa\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Upsample alla dimensione dell'immagine di input\n",
    "        cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cams[cls] = cam\n",
    "        \n",
    "        \n",
    "        #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "        \n",
    "        #L'immagine originale viene convertita in un array numpy.\n",
    "        #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "        #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "        #🔹 Esempio pratico:\n",
    "        #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 6: Creazione dell'Overlay\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "        # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "        \n",
    "        # Prepara l'immagine originale per la visualizzazione\n",
    "        img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Normalizza l'immagine in scala 0-255\n",
    "        img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "        \n",
    "        # Applica la heatmap usando OpenCV\n",
    "        #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "        # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "        \n",
    "        '''\n",
    "        OLD VERSIONS\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        '''\n",
    "        \n",
    "        '''COMMENTATO'''\n",
    "        #heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "        \n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Sovrapponi la heatmap all'immagine originale\n",
    "        # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "        \n",
    "        '''\n",
    "        OLD VERSIONS\n",
    "        overlay = cv2.addWeighted(img_norm, 0.6, heatmap, 0.4, 0)\n",
    "        overlays[cls] = overlay\n",
    "        '''\n",
    "        \n",
    "        overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "        overlays[cls] = overlay\n",
    "    \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    '''\n",
    "    OLD VERSION\n",
    "    # Titoli personalizzati per le righe\n",
    "    title_row1 = f\"Grad-CAM mapping of experimental condition {exp_cond}, EEG {data_type}, Subject {category_subject}\"\n",
    "    title_row2 = f\"Grad-CAM mapping superimposition over EEG Spectrogram of experimental condition {exp_cond}, Subject {category_subject}\"\n",
    "    \n",
    "    # Prima riga: visualizza solo le heatmap\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        # Visualizza la heatmap applicata sul colore (questo step può essere ripetuto, oppure si visualizza la mappa grezza)\\n\n",
    "        axs[0, j].imshow(cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB))\n",
    "        axs[0, j].set_title(f\"Class {cls} Heatmap\")\n",
    "        axs[0, j].axis('off')\n",
    "    axs[0, 0].set_ylabel(title_row1, fontsize=10)\n",
    "    \n",
    "    # Seconda riga: visualizza l'overlay della heatmap sullo spettrogramma originale\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        axs[1, j].imshow(overlays[cls])\n",
    "        axs[1, j].set_title(f\"Class {cls} Overlay\")\n",
    "        axs[1, j].axis('off')\n",
    "    axs[1, 0].set_ylabel(title_row2, fontsize=10)\n",
    "    \n",
    "    # Ottimizza la disposizione della figura\n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    plt.suptitle(f\"Grad-CAM Mapping for {exp_cond} - EEG {data_type} - Subject {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 25], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"{condition_names[cls]} Heatmap\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 25], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"{condition_names[cls]} Overlay\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83ad4790-8bbf-4cf2-88b2-fc78d62f6bc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "FINAL VERSION WITH ULTIMATED EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int not in labels_found:\n",
    "                samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                labels_found[label_int] = True\n",
    "            if 0 in labels_found and 1 in labels_found:\n",
    "                break\n",
    "        if 0 in labels_found and 1 in labels_found:\n",
    "            break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    if 0 not in samples or 1 not in samples:\n",
    "        print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "        return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        sample_input = samples[cls]\n",
    "        sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "        # Esegui forward pass per ottenere l'output del modello\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        # Se non viene specificata una classe target, seleziona quella predetta\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        # Azzeramento dei gradienti e backward pass per la classe target\n",
    "        # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "        model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        \n",
    "        #Qui si calcola la mappa CAM:\n",
    "\n",
    "        #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "        #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "        #Si applica ReLU per eliminare i valori negativi.\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "        \n",
    "        #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "        #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "        weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "        cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "        \n",
    "        # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        #Passaggio 5: Normalizzazione e upsampling\n",
    "        \n",
    "        #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "        #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "        # -------------------------------\n",
    "        # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Normalizza la mappa\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        # Upsample alla dimensione dell'immagine di input\n",
    "        cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cams[cls] = cam\n",
    "        \n",
    "        \n",
    "        #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "        \n",
    "        #L'immagine originale viene convertita in un array numpy.\n",
    "        #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "        #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "        #🔹 Esempio pratico:\n",
    "        #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 6: Creazione dell'Overlay\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "        # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "        \n",
    "        # Prepara l'immagine originale per la visualizzazione\n",
    "        img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Normalizza l'immagine in scala 0-255\n",
    "        img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "        \n",
    "        # Applica la heatmap usando OpenCV\n",
    "        #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "        # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "        \n",
    "        '''\n",
    "        Il processo è lo stesso di quello descritto per le cam:\n",
    "        \n",
    "        I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "        Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "        La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "        '''\n",
    "\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Sovrapponi la heatmap all'immagine originale\n",
    "        # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Overlay troppo sfocato e colori discordanti\n",
    "        Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "        Differenza di colormap e blending:\n",
    "        L'overlay viene creato con una combinazione di due immagini: \n",
    "            1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "            2) la heatmap\n",
    "            \n",
    "        Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "        \n",
    "        Suggerimenti:\n",
    "        \n",
    "        a) Modifica i pesi in cv2.addWeighted:\n",
    "        Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "        \n",
    "        b) Uniforma il formato dell'immagine originale:\n",
    "        Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "        considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "        \n",
    "        c) Usa lo stesso colormap: \n",
    "        Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "        usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "        \n",
    "        '''\n",
    "        overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "        #overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "        overlays[cls] = overlay\n",
    "    \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping - Experimental Condition: {exp_cond} - Subject: {category_subject}\", fontsize=12)\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG trial Spectrogram\\nExperimental Condition: {exp_cond} - Subject: {category_subject}\",\n",
    "    #fontsize=10,\n",
    "    #y=0.95  # Puoi regolare la posizione verticale se necessario\n",
    "    #)\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051cb5de-3005-4e1f-af97-81b04d740508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **FINAL IMPLEMENTATION OF GRADCAM COMPUTATION: FREQUENCIES X ELECTRODES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde2223-ff7a-4ea7-a806-f5555090d751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N.B.\n",
    "\n",
    "Certo, ti spiego nel dettaglio il motivo per cui la modifica che hai fatto con il codice sample_input.permute(0, 2, 1).unsqueeze(3) è importante.\n",
    "\n",
    "Contesto: Formato dei dati di input nel modello\n",
    "Nel caso delle reti neurali convoluzionali 2D (CNN2D), l'input che viene passato al modello deve avere una forma specifica affinché il modello possa elaborarlo correttamente. La forma tipica dell'input per un modello CNN2D è:\n",
    "\n",
    "arduino\n",
    "Copia\n",
    "Modifica\n",
    "(batch_size, num_channels, height, width)\n",
    "batch_size: Numero di campioni nel batch (ad esempio, 32 immagini per batch).\n",
    "\n",
    "num_channels: Numero di canali dell'immagine (ad esempio, 1 per immagini in bianco e nero, 3 per immagini RGB).\n",
    "\n",
    "height: Altezza dell'immagine.\n",
    "\n",
    "width: Larghezza dell'immagine.\n",
    "\n",
    "Nel tuo caso specifico, stai lavorando con dati EEG che sono organizzati in spettrogrammi, dove ogni esempio ha la forma (batch_size, num_channels, height, width).\n",
    "\n",
    "Problema\n",
    "Nel codice che hai mostrato prima, il formato originale dell'input potrebbe non essere quello previsto dal modello. Supponiamo che i dati siano in un formato come questo:\n",
    "\n",
    "css\n",
    "Copia\n",
    "Modifica\n",
    "(batch_size, height, width)  # (batch, 45, 61) in questo caso, senza i canali espliciti\n",
    "Qui:\n",
    "\n",
    "batch_size è il numero di campioni nel batch.\n",
    "\n",
    "height e width rappresentano la dimensione spaziale dei dati EEG (ad esempio, frequenze e tempo nello spettrogramma).\n",
    "\n",
    "Tuttavia, il modello CNN2D si aspetta che l'input abbia 4 dimensioni, ovvero (batch_size, num_channels, height, width).\n",
    "\n",
    "Se i tuoi dati sono di forma (batch_size, height, width), ciò significa che manca la dimensione per i canali (in pratica, il modello non sa come trattare i tuoi dati se non ha l'informazione sui canali). In effetti, il modello si aspetta un tensore di 4 dimensioni: un canale per ciascun dato.\n",
    "\n",
    "Soluzione: Perché devi fare sample_input.permute(0, 2, 1).unsqueeze(3)?\n",
    "La funzione permute(0, 2, 1) serve a cambiare l'ordine delle dimensioni. Se inizialmente hai:\n",
    "\n",
    "arduino\n",
    "Copia\n",
    "Modifica\n",
    "(batch_size, height, width)  # (batch, 45, 61)\n",
    "La chiamata permute(0, 2, 1) cambia l'ordine delle dimensioni, facendo diventare l'input:\n",
    "\n",
    "arduino\n",
    "Copia\n",
    "Modifica\n",
    "(batch_size, width, height)  # (batch, 61, 45)\n",
    "Questo è utile perché il modello può aspettarsi che la dimensione dell'input per la larghezza venga prima della dimensione dell'altezza (ad esempio, come nel caso delle immagini 2D tradizionali).\n",
    "\n",
    "Poi, la chiamata a unsqueeze(3) aggiunge una dimensione extra per il canale (che è la quarta dimensione). In pratica, stai dicendo che ogni campione ha un solo canale (perché il dato originale è un’immagine in scala di grigi, e non RGB). La forma finale diventa:\n",
    "\n",
    "arduino\n",
    "Copia\n",
    "Modifica\n",
    "(batch_size, width, height, 1)  # (batch, 61, 45, 1)\n",
    "In questo modo, il tuo input è pronto per essere passato a un modello CNN2D, che ora può trattarlo come se fosse un'immagine in scala di grigi (un solo canale).\n",
    "\n",
    "Sintesi\n",
    "La ragione per cui devi fare questa modifica è che il modello CNN2D richiede che l'input abbia una forma di 4 dimensioni, ovvero (batch_size, num_channels, height, width). Poiché i tuoi dati erano inizialmente di forma (batch_size, height, width), è necessario trasformarli in modo che includano un canale (se il dato è in scala di grigi) e per adattarli all'input del modello.\n",
    "\n",
    "Con sample_input.permute(0, 2, 1).unsqueeze(3), ottieni il formato giusto, che consente al modello di elaborare correttamente i tuoi dati EEG.\n",
    "                                                                                                              \n",
    "                                                                                                              "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6597a54-c431-4761-b5ce-62d7bfee0572",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                    \n",
    "                                                                    OLD VERSION SOLO 1 ESEMPIO\n",
    "\n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "FINAL VERSION WITH ULTIMATED EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ecco una versione modificata della funzione che tiene conto che:\n",
    "\n",
    "L'input originale ha forma (batch, frequenze, canali) con frequenze = 45 e canali = 61.\n",
    "\n",
    "Dopo il preprocessing nella rete (permute e unsqueeze) il modello lavora con tensori di forma (batch, 61, 45, 1), cioè:\n",
    "\n",
    "asse dei canali = 61 (che ora costituirà l’asse x della visualizzazione),\n",
    "\n",
    "asse delle frequenze = 45 (che sarà l’asse y).\n",
    "\n",
    "Nella visualizzazione degli overlay imposteremo l’extent su 0,61,0,45 (oltre a ruotare l’immagine per far sì che l’asse y rappresenti le frequenze).\n",
    "\n",
    "In aggiunta, ti fornisco uno spunto su come estrarre i nomi dei canali da una tripletta di file in formato BrainVision usando MNE, \n",
    "in modo da poterli usare per etichettare l’asse x.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device, channel_names = None):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    #target_layer = model.conv3\n",
    "    \n",
    "    target_layer = model.layers[-1][0]\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int not in labels_found:\n",
    "                samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                labels_found[label_int] = True\n",
    "            if 0 in labels_found and 1 in labels_found:\n",
    "                break\n",
    "        if 0 in labels_found and 1 in labels_found:\n",
    "            break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    if 0 not in samples or 1 not in samples:\n",
    "        print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "        return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        sample_input = samples[cls]\n",
    "        sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "        \n",
    "        print(f\"\\033[1mSHAPE OF SAMPLE_INPUT: {sample_input.shape}\\033[0m\")\n",
    "        \n",
    "        # Assicurati che sample_input sia nel formato atteso dal modello (come nel forward)\n",
    "        #if sample_input.dim() == 3:  # Supponiamo che abbia shape (batch, 45, 61)\n",
    "        #    sample_input = sample_input.permute(0, 2, 1).unsqueeze(3)  # Ora (batch, 61, 45, 1)\n",
    "\n",
    "\n",
    "        # Esegui forward pass per ottenere l'output del modello\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        # Se non viene specificata una classe target, seleziona quella predetta\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        # Azzeramento dei gradienti e backward pass per la classe target\n",
    "        # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "        model.zero_grad()\n",
    "        target = output[0, target_class]\n",
    "        target.backward()\n",
    "        \n",
    "        #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        \n",
    "        #Qui si calcola la mappa CAM:\n",
    "\n",
    "        #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "        #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "        #Si applica ReLU per eliminare i valori negativi.\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "        \n",
    "        #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "        #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "        weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "        cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "        \n",
    "        # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        #Passaggio 5: Normalizzazione e upsampling\n",
    "        \n",
    "        #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "        #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "        \n",
    "        #🔹 Esempio pratico:\n",
    "        #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "        # -------------------------------\n",
    "        # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Normalizza la mappa\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        \n",
    "        '''\n",
    "        # Upsample alla dimensione dell'immagine di input: usa la shape originale degli input convoluzionali (frequenze, larghezza)\n",
    "        \n",
    "        \n",
    "        Nel tuo caso, il dato originale che esce dal test_loader è di forma (1,45,61), dove \n",
    "        - 45 rappresenta le frequenze \n",
    "        - 61 i canali\n",
    "        \n",
    "        Nel forward del modello:\n",
    "\n",
    "        Il modello parte da un input 3D  (batch,45,61)\n",
    "        lo trasforma con 'permute' in (batch, 61, 45)\n",
    "        e poi aggiunge una dimensione con unsqueeze(3) per ottenere (batch, 61, 45, 1)\n",
    "        \n",
    "        Quindi il modello lavora internamente su un \"immagine\" con dimensioni spaziali (45,1)\n",
    "        (dove 61 è il numero di canali, non le dimensioni spaziali).\n",
    "        \n",
    "        \n",
    "        Per il calcolo della GradCAM:\n",
    "        \n",
    "        L'obiettivo è quello di upsamplare la mappa CAM per poterla sovrapporre al dato originale (lo spettrogramma), che ha forma \n",
    "        (45,61)\n",
    "        \n",
    "        Se usi sample_input.shape[2:] su un tensore di forma (1,45, 61) otterrai (61,),\n",
    "        cioè solo la seconda dimensione! \n",
    "        Mentre ciò che ti serve è una tupla di due valori: le frequenze e i canali, cioè (45,61).\n",
    "        \n",
    "        Quindi, per F.interpolate devi usare come target size la tupla\n",
    "        \n",
    "        (sample_input.shape[1], sample_input.shape[2]), che, per il tuo dato, è (45, 61)\n",
    "        \n",
    "        \n",
    "        Quindi, modifica la chiamata a F.interpolate in questo modo:\n",
    "        \n",
    "            target_size = (sample_input.shape[1], sample_input.shape[2])  # (45, 61)\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size=target_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "        In questo modo, l'upsampling della mappa CAM avverrà alla dimensione corretta per poterla sovrapporre al dato originale, che è (45,61).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        target_size = (sample_input.shape[1], sample_input.shape[2])\n",
    "        cam = F.interpolate(cam.unsqueeze(1), size = target_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cams[cls] = cam\n",
    "        \n",
    "        \n",
    "        #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "        \n",
    "        #L'immagine originale viene convertita in un array numpy.\n",
    "        #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "        #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "        #🔹 Esempio pratico:\n",
    "        #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Passaggio 6: Creazione dell'Overlay\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, 1)\n",
    "        # Dopo squeeze, otteniamo (61, 45). Poiché vogliamo:\n",
    "        # - asse x: canali (61)\n",
    "        # - asse y: frequenze (45)\n",
    "        # invertiamo le dimensioni per ottenere (45, 61)\n",
    "        \n",
    "        # Prepara l'immagine originale per la visualizzazione\n",
    "        #img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        img = sample_input.squeeze().cpu().detach().numpy().transpose()  # ora (45, 61)\n",
    "        \n",
    "        # Normalizza l'immagine in scala 0-255\n",
    "        '''\n",
    "                                                    SOLUZIONE\n",
    "        img_norm: Questa è l'immagine originale in scala di grigi, che ha 2 dimensioni (ad esempio, shape (45, 61)).\n",
    "        \n",
    "        1) ANDIAMO A CONVERTIRE img_norm da scala di grigi (2D) a un'immagine a 3 canali (RGB):\n",
    "        \n",
    "        img_norm_color = cv2.cvtColor(img_norm, cv2.COLOR_GRAY2RGB)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "    \n",
    "        img_norm_color = cv2.cvtColor(img_norm, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        \n",
    "        # Applica la heatmap usando OpenCV\n",
    "        #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "        # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "        \n",
    "        '''\n",
    "        Il processo è lo stesso di quello descritto per le cam:\n",
    "        \n",
    "        I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "        Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "        La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "                                            SOLUZIONE\n",
    "        heatmap: Questa è l'immagine a colori, che ha 3 dimensioni (ad esempio, shape (45, 61, 3)\n",
    "        '''\n",
    "        \n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        '''NOTA BENE: \n",
    "        print(f\"img_norm_color shape: {img_norm_color.shape}\")\n",
    "        print(f\"heatmap shape: {heatmap.shape}\")\n",
    "        \n",
    "        img_norm_color shape: (61, 45, 3)\n",
    "        heatmap shape: (45, 61, 3)\n",
    "\n",
    "        QUINDI ANDIAMO AD INVERTIRE LE SHAPES DI img_norm_color_shape PER FARLE CORRISPONDERE A QUELLE DI heatmap \n",
    "        '''\n",
    "        \n",
    "        # Inverti le due prime dimensioni di img_norm_color\n",
    "        # Ridimensiona una delle immagini per farla corrispondere\n",
    "        # if img_norm_color.shape != heatmap.shape:\n",
    "        # img_norm_color = cv2.resize(img_norm_color, (heatmap.shape[1], heatmap.shape[0]))\n",
    "        \n",
    "        img_norm_color = img_norm_color.transpose(1, 0, 2)\n",
    "        \n",
    "        \n",
    "        # Sovrapponi la heatmap all'immagine originale\n",
    "        # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Overlay troppo sfocato e colori discordanti\n",
    "        Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "        Differenza di colormap e blending:\n",
    "        L'overlay viene creato con una combinazione di due immagini: \n",
    "            1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "            2) la heatmap\n",
    "            \n",
    "        Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "        \n",
    "        Suggerimenti:\n",
    "        \n",
    "        a) Modifica i pesi in cv2.addWeighted:\n",
    "        Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "        \n",
    "        b) Uniforma il formato dell'immagine originale:\n",
    "        Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "        considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "        \n",
    "        c) Usa lo stesso colormap: \n",
    "        Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "        usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "                                                    SOLUZIONE \n",
    "        overlay: Questo è il risultato finale che sovrappone la heatmap sull'immagine originale. \n",
    "        La soluzione proposta suggerisce di usare cv2.addWeighted\n",
    "        per combinare img_norm_color (l'immagine in scala di grigi convertita a 3 canali) e heatmap.\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        2) ANDIAMO A combinare img_norm_color (l'immagine ora a 3 canali) con la heatmap usando cv2.addWeighted:\n",
    "        \n",
    "        overlay = cv2.addWeighted(img_norm_color, 0.4, heatmap, 0.6, 0)\n",
    "        '''\n",
    "       \n",
    "        overlay = cv2.addWeighted(img_norm_color, 0.4, heatmap, 0.6, 0)\n",
    "        \n",
    "        #overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "        #overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        3) ANDIAMO ad assegnare l'overlay al dizionario overlays (che sta memorizzando gli overlay per ciascuna classe):\n",
    "        overlays[cls] = overlay\n",
    "        '''\n",
    "        \n",
    "        overlays[cls] = overlay\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping - Experimental Condition: {exp_cond} - Subject: {category_subject}\", fontsize=12)\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG trial Spectrogram\\nExperimental Condition: {exp_cond} - Subject: {category_subject}\",\n",
    "    #fontsize=10,\n",
    "    #y=0.95  # Puoi regolare la posizione verticale se necessario\n",
    "    #)\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Impostiamo l'estensione: x da 0 a 61 (canali) e y da 0 a 45 (frequenze)\n",
    "    extent = [0, 61, 0, 45]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questo codice visualizza solo le heatmap di Grad-CAM (senza l'overlay), utilizzando il colormap \"INFERNO\" \n",
    "    (o un altro che preferisci) e applica l'inversione verticale (np.flipud()) per ottenere la giusta visualizzazione (se necessario).\n",
    "    '''\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        \n",
    "        #axs[0, j].imshow(cam_img, extent=extent, aspect='auto')\n",
    "        #axs[0, j].set_title(f\"Grad-CAM for {condition_names[cls]}\", fontsize=12)\n",
    "        #axs[0, j].axis('off')\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        # Invertiamo verticalmente per avere le frequenze in ordine crescente (se necessario)\n",
    "        \n",
    "        #cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        #axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].imshow(cam_img, extent = extent, aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questo codice visualizza ANCHE l'overlay e le heatmap di Grad-CAM,  utilizzando il colormap \"INFERNO\" \n",
    "    (o un altro che preferisci) e applica l'inversione verticale (np.flipud()) per ottenere la giusta visualizzazione (se necessario).\n",
    "    \n",
    "    \n",
    "    Per ridurre la grandezza della stringa dei canali all'interno dell'asse x (EEG Channels), puoi intervenire su vari aspetti della visualizzazione, come la dimensione del font, l'orientamento e, se necessario, l'abbreviazione o il ridimensionamento dei nomi dei canali.\n",
    "\n",
    "    1. Ridurre la dimensione del font\n",
    "    Puoi facilmente ridurre la grandezza del testo delle etichette sugli assi xticks utilizzando l'argomento fontsize dentro set_xticklabels.\n",
    "\n",
    "    Ecco un esempio di come applicarlo:\n",
    "    \n",
    "        axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    Puoi provare a modificare la dimensione di fontsize fino a trovare quella più adatta per visualizzare i nomi dei canali \n",
    "    senza che risultino troppo grandi o sovrapposti.\n",
    "\n",
    "    2. Abbreviare i nomi dei canali\n",
    "    Se i nomi dei canali sono troppo lunghi e non si adattano all'asse x, puoi abbreviarli. \n",
    "    Ad esempio, puoi creare una lista di nomi abbreviati (ad esempio, \"Fz\" al posto di \"Frontal Z\") e usarla per l'asse x.\n",
    "\n",
    "    Esempio:\n",
    "\n",
    "    # Creare abbreviazioni per i canali\n",
    "    abbreviated_channel_names = [name[:3] for name in channel_names]\n",
    "\n",
    "    # Impostare le etichette abbreviate\n",
    "    axs[1, j].set_xticklabels(abbreviated_channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    In questo caso, i nomi dei canali verranno abbreviati ai primi 3 caratteri di ogni nome.\n",
    "\n",
    "    3. Aumentare lo spazio tra le etichette (se necessario)\n",
    "    Se le etichette sono ancora troppo vicine, puoi anche aumentare la distanza tra le etichette usando set_xticks:\n",
    "\n",
    "    axs[1, j].set_xticks(np.arange(0.5, extent[1], 2))  # Spaziatura maggiore\n",
    "    In questo modo, le etichette saranno meno affollate sull'asse x.\n",
    "        '''\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        #axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        axs[1, j].imshow(overlay_img, extent=extent, aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"EEG Channels\", fontsize=10)\n",
    "        \n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        #axs[1, j].axis('on')\n",
    "        \n",
    "        '''\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            axs[1, j].set_xticks(np.arange(0.3, extent[1], 2))\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize = 6)\n",
    "        '''\n",
    "        \n",
    "        # Calcola le posizioni in modo che il numero di tick corrisponda al numero di canali\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            num_channels = len(channel_names)\n",
    "            ticks = np.linspace(0.5, extent[1] - 1, num_channels)  # crea num_channels posizioni equidistanti\n",
    "\n",
    "            # Imposta i tick e le etichette\n",
    "            axs[1, j].set_xticks(ticks)\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1aa35-77e3-4620-bf93-d580f4b80fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "FINAL VERSION WITH ULTIMATED EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ecco una versione modificata della funzione che tiene conto che:\n",
    "\n",
    "L'input originale ha forma (batch, frequenze, canali) con frequenze = 45 e canali = 61.\n",
    "\n",
    "Dopo il preprocessing nella rete (permute e unsqueeze) il modello lavora con tensori di forma (batch, 61, 45, 1), cioè:\n",
    "\n",
    "asse dei canali = 61 (che ora costituirà l’asse x della visualizzazione),\n",
    "\n",
    "asse delle frequenze = 45 (che sarà l’asse y).\n",
    "\n",
    "Nella visualizzazione degli overlay imposteremo l’extent su 0,61,0,45 (oltre a ruotare l’immagine per far sì che l’asse y rappresenti le frequenze).\n",
    "\n",
    "In aggiunta, ti fornisco uno spunto su come estrarre i nomi dei canali da una tripletta di file in formato BrainVision usando MNE, \n",
    "in modo da poterli usare per etichettare l’asse x.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device, channel_names = None):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    #target_layer = model.conv3\n",
    "    \n",
    "    target_layer = model.layers[-1][0]\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    #samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    #labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "\n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "            \n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #if label_int not in labels_found:\n",
    "                #samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                #labels_found[label_int] = True\n",
    "            #if 0 in labels_found and 1 in labels_found:\n",
    "            #    break\n",
    "        #if 0 in labels_found and 1 in labels_found:\n",
    "        #    break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    #if 0 not in samples or 1 not in samples:\n",
    "    #    print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "    #    return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    #cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    #overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "\n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        for sample_input in samples[cls]:\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''    \n",
    "            #sample_input = samples[cls]\n",
    "\n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            #print(f\"\\033[1mSHAPE OF SAMPLE_INPUT: {sample_input.shape}\\033[0m\")\n",
    "\n",
    "            # Assicurati che sample_input sia nel formato atteso dal modello (come nel forward)\n",
    "            #if sample_input.dim() == 3:  # Supponiamo che abbia shape (batch, 45, 61)\n",
    "            #    sample_input = sample_input.permute(0, 2, 1).unsqueeze(3)  # Ora (batch, 61, 45, 1)\n",
    "\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "\n",
    "            #Passaggio 5: Normalizzazione e upsampling\n",
    "\n",
    "            #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "            #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "            # ---------------------------\n",
    "\n",
    "            # Normalizza la mappa\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "            '''\n",
    "            # Upsample alla dimensione dell'immagine di input: usa la shape originale degli input convoluzionali (frequenze, larghezza)\n",
    "\n",
    "\n",
    "            Nel tuo caso, il dato originale che esce dal test_loader è di forma (1,45,61), dove \n",
    "            - 45 rappresenta le frequenze \n",
    "            - 61 i canali\n",
    "\n",
    "            Nel forward del modello:\n",
    "\n",
    "            Il modello parte da un input 3D  (batch,45,61)\n",
    "            lo trasforma con 'permute' in (batch, 61, 45)\n",
    "            e poi aggiunge una dimensione con unsqueeze(3) per ottenere (batch, 61, 45, 1)\n",
    "\n",
    "            Quindi il modello lavora internamente su un \"immagine\" con dimensioni spaziali (45,1)\n",
    "            (dove 61 è il numero di canali, non le dimensioni spaziali).\n",
    "\n",
    "\n",
    "            Per il calcolo della GradCAM:\n",
    "\n",
    "            L'obiettivo è quello di upsamplare la mappa CAM per poterla sovrapporre al dato originale (lo spettrogramma), che ha forma \n",
    "            (45,61)\n",
    "\n",
    "            Se usi sample_input.shape[2:] su un tensore di forma (1,45, 61) otterrai (61,),\n",
    "            cioè solo la seconda dimensione! \n",
    "            Mentre ciò che ti serve è una tupla di due valori: le frequenze e i canali, cioè (45,61).\n",
    "\n",
    "            Quindi, per F.interpolate devi usare come target size la tupla\n",
    "\n",
    "            (sample_input.shape[1], sample_input.shape[2]), che, per il tuo dato, è (45, 61)\n",
    "\n",
    "\n",
    "            Quindi, modifica la chiamata a F.interpolate in questo modo:\n",
    "\n",
    "                target_size = (sample_input.shape[1], sample_input.shape[2])  # (45, 61)\n",
    "                cam = F.interpolate(cam.unsqueeze(1), size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            In questo modo, l'upsampling della mappa CAM avverrà alla dimensione corretta per poterla sovrapporre al dato originale, che è (45,61).\n",
    "            '''\n",
    "\n",
    "\n",
    "            #cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            target_size = (sample_input.shape[1], sample_input.shape[2])\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size = target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            cam = cam.squeeze().cpu().numpy()\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #cams[cls] = cam\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa alla lista per la classe\n",
    "            cams_list[cls].append(cam)\n",
    "\n",
    "\n",
    "            #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "\n",
    "            #L'immagine originale viene convertita in un array numpy.\n",
    "            #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "            #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 6: Creazione dell'Overlay\n",
    "            # -------------------------------\n",
    "\n",
    "            # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, 1)\n",
    "            # Dopo squeeze, otteniamo (61, 45). Poiché vogliamo:\n",
    "            # - asse x: canali (61)\n",
    "            # - asse y: frequenze (45)\n",
    "            # invertiamo le dimensioni per ottenere (45, 61)\n",
    "\n",
    "            # Prepara l'immagine originale per la visualizzazione\n",
    "            #img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "            img = sample_input.squeeze().cpu().detach().numpy().transpose()  # ora (45, 61)\n",
    "\n",
    "            # Normalizza l'immagine in scala 0-255\n",
    "            '''\n",
    "                                                        SOLUZIONE\n",
    "            img_norm: Questa è l'immagine originale in scala di grigi, che ha 2 dimensioni (ad esempio, shape (45, 61)).\n",
    "\n",
    "            1) ANDIAMO A CONVERTIRE img_norm da scala di grigi (2D) a un'immagine a 3 canali (RGB):\n",
    "\n",
    "            img_norm_color = cv2.cvtColor(img_norm, cv2.COLOR_GRAY2RGB)\n",
    "            '''\n",
    "\n",
    "\n",
    "            img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "            img_norm_color = cv2.cvtColor(img_norm, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "            # Applica la heatmap usando OpenCV\n",
    "            #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "            # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "\n",
    "            '''\n",
    "            Il processo è lo stesso di quello descritto per le cam:\n",
    "\n",
    "            I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "            Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "            La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "                                                SOLUZIONE\n",
    "            heatmap: Questa è l'immagine a colori, che ha 3 dimensioni (ad esempio, shape (45, 61, 3)\n",
    "            '''\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            '''NOTA BENE: \n",
    "            print(f\"img_norm_color shape: {img_norm_color.shape}\")\n",
    "            print(f\"heatmap shape: {heatmap.shape}\")\n",
    "\n",
    "            img_norm_color shape: (61, 45, 3)\n",
    "            heatmap shape: (45, 61, 3)\n",
    "\n",
    "            QUINDI ANDIAMO AD INVERTIRE LE SHAPES DI img_norm_color_shape PER FARLE CORRISPONDERE A QUELLE DI heatmap \n",
    "            '''\n",
    "\n",
    "            # Inverti le due prime dimensioni di img_norm_color\n",
    "            # Ridimensiona una delle immagini per farla corrispondere\n",
    "            # if img_norm_color.shape != heatmap.shape:\n",
    "            # img_norm_color = cv2.resize(img_norm_color, (heatmap.shape[1], heatmap.shape[0]))\n",
    "\n",
    "            img_norm_color = img_norm_color.transpose(1, 0, 2)\n",
    "\n",
    "\n",
    "            # Sovrapponi la heatmap all'immagine originale\n",
    "            # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "\n",
    "\n",
    "            '''\n",
    "            Overlay troppo sfocato e colori discordanti\n",
    "            Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "            Differenza di colormap e blending:\n",
    "            L'overlay viene creato con una combinazione di due immagini: \n",
    "                1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "                2) la heatmap\n",
    "\n",
    "            Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "\n",
    "            Suggerimenti:\n",
    "\n",
    "            a) Modifica i pesi in cv2.addWeighted:\n",
    "            Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "\n",
    "            b) Uniforma il formato dell'immagine originale:\n",
    "            Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "            considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "\n",
    "            c) Usa lo stesso colormap: \n",
    "            Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "            usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "                                                        SOLUZIONE \n",
    "            overlay: Questo è il risultato finale che sovrappone la heatmap sull'immagine originale. \n",
    "            La soluzione proposta suggerisce di usare cv2.addWeighted\n",
    "            per combinare img_norm_color (l'immagine in scala di grigi convertita a 3 canali) e heatmap.\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "            2) ANDIAMO A combinare img_norm_color (l'immagine ora a 3 canali) con la heatmap usando cv2.addWeighted:\n",
    "\n",
    "            overlay = cv2.addWeighted(img_norm_color, 0.4, heatmap, 0.6, 0)\n",
    "            '''\n",
    "\n",
    "            overlay = cv2.addWeighted(img_norm_color, 0.4, heatmap, 0.6, 0)\n",
    "\n",
    "            #overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "            #overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "\n",
    "            '''\n",
    "            3) ANDIAMO ad assegnare l'overlay al dizionario overlays (che sta memorizzando gli overlay per ciascuna classe):\n",
    "            overlays[cls] = overlay\n",
    "            '''\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #overlays[cls] = overlay\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi l'overlay alla lista per la classe\n",
    "            overlays_list[cls].append(overlay)\n",
    "    \n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    mean_cams = {}\n",
    "    mean_overlays = {}    \n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "        mean_overlays[cls] = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "        \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping - Experimental Condition: {exp_cond} - Subject: {category_subject}\", fontsize=12)\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG trial Spectrogram\\nExperimental Condition: {exp_cond} - Subject: {category_subject}\",\n",
    "    #fontsize=10,\n",
    "    #y=0.95  # Puoi regolare la posizione verticale se necessario\n",
    "    #)\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Impostiamo l'estensione: x da 0 a 61 (canali) e y da 0 a 45 (frequenze)\n",
    "    extent = [0, 61, 0, 45]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questo codice visualizza solo le heatmap di Grad-CAM (senza l'overlay), utilizzando il colormap \"INFERNO\" \n",
    "    (o un altro che preferisci) e applica l'inversione verticale (np.flipud()) per ottenere la giusta visualizzazione (se necessario).\n",
    "    '''\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''SOLO UN ESEMPIO'''\n",
    "        #cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        \n",
    "        #axs[0, j].imshow(cam_img, extent=extent, aspect='auto')\n",
    "        #axs[0, j].set_title(f\"Grad-CAM for {condition_names[cls]}\", fontsize=12)\n",
    "        #axs[0, j].axis('off')\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        # Invertiamo verticalmente per avere le frequenze in ordine crescente (se necessario)\n",
    "        \n",
    "        #cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        #axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].imshow(cam_img, extent = extent, aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questo codice visualizza ANCHE l'overlay e le heatmap di Grad-CAM,  utilizzando il colormap \"INFERNO\" \n",
    "    (o un altro che preferisci) e applica l'inversione verticale (np.flipud()) per ottenere la giusta visualizzazione (se necessario).\n",
    "    \n",
    "    \n",
    "    Per ridurre la grandezza della stringa dei canali all'interno dell'asse x (EEG Channels), puoi intervenire su vari aspetti della visualizzazione, come la dimensione del font, l'orientamento e, se necessario, l'abbreviazione o il ridimensionamento dei nomi dei canali.\n",
    "\n",
    "    1. Ridurre la dimensione del font\n",
    "    Puoi facilmente ridurre la grandezza del testo delle etichette sugli assi xticks utilizzando l'argomento fontsize dentro set_xticklabels.\n",
    "\n",
    "    Ecco un esempio di come applicarlo:\n",
    "    \n",
    "        axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    Puoi provare a modificare la dimensione di fontsize fino a trovare quella più adatta per visualizzare i nomi dei canali \n",
    "    senza che risultino troppo grandi o sovrapposti.\n",
    "\n",
    "    2. Abbreviare i nomi dei canali\n",
    "    Se i nomi dei canali sono troppo lunghi e non si adattano all'asse x, puoi abbreviarli. \n",
    "    Ad esempio, puoi creare una lista di nomi abbreviati (ad esempio, \"Fz\" al posto di \"Frontal Z\") e usarla per l'asse x.\n",
    "\n",
    "    Esempio:\n",
    "\n",
    "    # Creare abbreviazioni per i canali\n",
    "    abbreviated_channel_names = [name[:3] for name in channel_names]\n",
    "\n",
    "    # Impostare le etichette abbreviate\n",
    "    axs[1, j].set_xticklabels(abbreviated_channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    In questo caso, i nomi dei canali verranno abbreviati ai primi 3 caratteri di ogni nome.\n",
    "\n",
    "    3. Aumentare lo spazio tra le etichette (se necessario)\n",
    "    Se le etichette sono ancora troppo vicine, puoi anche aumentare la distanza tra le etichette usando set_xticks:\n",
    "\n",
    "    axs[1, j].set_xticks(np.arange(0.5, extent[1], 2))  # Spaziatura maggiore\n",
    "    In questo modo, le etichette saranno meno affollate sull'asse x.\n",
    "        '''\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        '''SOLO UN ESEMPIO'''\n",
    "        #overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        overlay_img = np.flipud(mean_overlays[cls])\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        #axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        axs[1, j].imshow(overlay_img, extent=extent, aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"EEG Channels\", fontsize=10)\n",
    "        \n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        #axs[1, j].axis('on')\n",
    "        \n",
    "        '''\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            axs[1, j].set_xticks(np.arange(0.3, extent[1], 2))\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize = 6)\n",
    "        '''\n",
    "        \n",
    "        # Calcola le posizioni in modo che il numero di tick corrisponda al numero di canali\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            num_channels = len(channel_names)\n",
    "            ticks = np.linspace(0.5, extent[1] - 1, num_channels)  # crea num_channels posizioni equidistanti\n",
    "\n",
    "            # Imposta i tick e le etichette\n",
    "            axs[1, j].set_xticks(ticks)\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcdea9c-d0f5-40a4-b28d-fdab94d0bf8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **FINAL IMPLEMENTATION OF GRADCAM COMPUTATION: FREQUENCIES X ELECTRODES PER EEG STATS**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c28bc203-76d4-40a5-8d35-600cd1976873",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "                                                                    VERSION FREQUENCY x TIME\n",
    "\n",
    "NEW VERSION - CON NUOVO CONTRASTO?\n",
    "\n",
    "Riassunto delle modifiche e dove sono state inserite:\n",
    "\n",
    "1) Normalizzazione robusta basata sui percentili (nuovo Passaggio 7.1):\n",
    "\n",
    "    Inserita subito dopo il ciclo che calcola mean_cams e mean_overlays.\n",
    "\n",
    "    Per ciascuna classe, viene calcolato il vettore dei pixel (flatten), vengono ricavati i quantili\n",
    "    (5° e 95° percentile), viene eseguito il clipping e poi la normalizzazione.\n",
    "\n",
    "\n",
    "2) Istogramma della distribuzione dei valori (nuovo Passaggio 7.2):\n",
    "\n",
    "    Nella figura finale, ho creato una terza riga di subplot in cui viene plottato l'istogramma dei valori della heatmap media\n",
    "    (prima della normalizzazione robusta) per ciascuna classe.\n",
    "\n",
    "\n",
    "3) Figura finale:\n",
    "\n",
    "    La figura finale ora ha 3 righe e 2 colonne:\n",
    "\n",
    "        Riga 1: Heatmap CAM normalizzate.\n",
    "        Riga 2: Overlay (CAM sovrapposta allo spettrogramma).\n",
    "        Riga 3: Istogramma della distribuzione dei valori della CAM.\n",
    "        Riga 4: Media dello Spettrogramma (Raw) rispetto ai Trial della Stessa Classe \n",
    "\n",
    "Questo codice aggiornato implementa tutti i passaggi che hai descritto. Fammi sapere se hai ulteriori domande o se necessiti di altre modifiche!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "      \n",
    "      - Riga 3: Istogramma della distribuzione dei valori della heatmap media.\n",
    "      - Riga 4: Spettrogramma medio (raw) per i trial di ciascuna classe.\n",
    "      \n",
    "      \n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    target_layer = model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            \n",
    "            # Aggiungi il campione alla lista della classe corrispondente\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "\n",
    "    \n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        for sample_input in samples[cls]:\n",
    "        \n",
    "        #sample_input = samples[cls]\n",
    "        \n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "\n",
    "            #Passaggio 5: Normalizzazione e upsampling\n",
    "\n",
    "            #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "            #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "            # ---------------------------\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            Sì, la normalizzazione viene eseguita nel Passaggio 5: Normalizzazione e upsampling della CAM, con questa parte di codice:\n",
    "\n",
    "                # Normalizza la mappa\n",
    "                cam = cam - cam.min()\n",
    "                cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            Cosa fa questa normalizzazione?\n",
    "            \n",
    "            1) Sottrazione del minimo:\n",
    "\n",
    "                Porta i valori minimi della CAM a zero.\n",
    "                Se cam.min() fosse -0.3, sottraendo cam.min() tutti i valori si traslano in modo che il minimo sia 0.\n",
    "\n",
    "            2) Divisione per il massimo:\n",
    "\n",
    "                Scala i valori in modo che il massimo diventi 1.\n",
    "                Aggiungere 1e-8 nel denominatore previene problemi di divisione per zero.\n",
    "\n",
    "            💡 Risultato → Dopo questa operazione, cam avrà valori normalizzati tra 0 e 1.\n",
    "            '''\n",
    "            \n",
    "            # Normalizza la mappa\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            #print(f\"Valori minimi e massimi dopo la normalizzazione: min={cam.min()}, max={cam.max()}\")\n",
    "            assert 0 <= cam.min() and cam.max() <= 1, \"Errore: i valori della CAM non sono nel range [0,1]!\"\n",
    "\n",
    "            # Upsample alla dimensione dell'immagine di input\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "            cam = cam.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa alla lista per la classe\n",
    "            cams_list[cls].append(cam)\n",
    "\n",
    "\n",
    "            #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "\n",
    "            #L'immagine originale viene convertita in un array numpy.\n",
    "            #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "            #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 6: Creazione dell'Overlay\n",
    "            # -------------------------------\n",
    "\n",
    "            # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, tempo)\n",
    "            # dopo squeeze si ottiene (canali, frequenze, tempo). Per visualizzare come immagine color, trasformiamo in (frequenze, tempo, canali).\n",
    "\n",
    "            # Prepara l'immagine originale per la visualizzazione\n",
    "            img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "            # Normalizza l'immagine in scala 0-255\n",
    "            img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "            # Applica la heatmap usando OpenCV\n",
    "            #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "            # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "\n",
    "            '''\n",
    "            Il processo è lo stesso di quello descritto per le cam:\n",
    "\n",
    "            I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "            Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "            La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "            '''\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Sovrapponi la heatmap all'immagine originale\n",
    "            # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "\n",
    "\n",
    "            '''\n",
    "            Overlay troppo sfocato e colori discordanti\n",
    "            Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "            Differenza di colormap e blending:\n",
    "            L'overlay viene creato con una combinazione di due immagini: \n",
    "                1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "                2) la heatmap\n",
    "\n",
    "            Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "\n",
    "            Suggerimenti:\n",
    "\n",
    "            a) Modifica i pesi in cv2.addWeighted:\n",
    "            Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "\n",
    "            b) Uniforma il formato dell'immagine originale:\n",
    "            Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "            considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "\n",
    "            c) Usa lo stesso colormap: \n",
    "            Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "            usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "\n",
    "            '''\n",
    "            overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi l'overlay alla lista per la classe\n",
    "            overlays_list[cls].append(overlay)\n",
    "    \n",
    "    \n",
    "    mean_cams = {}\n",
    "    mean_overlays = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "        mean_overlays[cls] = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    NEW VERSION\n",
    "    '''\n",
    "    \n",
    "    # =======================================================\n",
    "    # Nuovo Passaggio 7.1: Normalizzazione basata sui percentili\n",
    "    # Per ogni classe, restringiamo il range della heatmap media\n",
    "    # per enfatizzare il contrasto nelle regioni più frequenti.\n",
    "    # =======================================================\n",
    "    normalized_mean_cams = {}\n",
    "    hist_data = {}  # Per salvare i dati dell'istogramma per ciascuna classe\n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        # Estrai tutti i pixel in un vettore\n",
    "        heatmap_flat = mean_cams[cls].flatten()\n",
    "        \n",
    "        # Calcola i quantili (ad esempio 5° e 95° percentile)\n",
    "        vmin = np.percentile(heatmap_flat, 5)\n",
    "        vmax = np.percentile(heatmap_flat, 95)\n",
    "        \n",
    "        # Clipping dei valori per limitare gli outlier\n",
    "        heatmap_clipped = np.clip(mean_cams[cls], vmin, vmax)\n",
    "        \n",
    "        # Normalizza tra 0 e 1 usando i valori clipped\n",
    "        heatmap_normalized = (heatmap_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "        normalized_mean_cams[cls] = heatmap_normalized\n",
    "        \n",
    "        # Salva i dati per l'istogramma (per visualizzare la distribuzione)\n",
    "        hist_data[cls] = heatmap_flat\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    OLD VERSION\n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "    #Istogramma della distribuzione dei valori della heatmap media per ciascuna classe.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    # - Terza Riga : L'istogramma della distribuzione dei valori della heatmap media per ciascuna classe.\n",
    "    \n",
    "    #fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    NEW VERSION\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # =======================================================\n",
    "    # Nuovo Passaggio 7.2: Calcolo dello spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Cosa fa questo codice?\n",
    "    ✅ Calcola lo spettrogramma medio per ogni classe (0 e 1) prendendo i trials da samples e facendo la media sulla prima dimensione (batch).\n",
    "    ✅ Plotta lo spettrogramma medio nella quarta riga del grafico finale, con una colonna per ogni classe.\n",
    "    ✅ Usa una colormap jet per una migliore visualizzazione.\n",
    "    ✅ Evita errori: Se una classe non ha dati, non plotta nulla per quella colonna.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Sì, l'errore indica che stai cercando di convertire un tensore PyTorch che richiede il calcolo del gradiente\n",
    "    #in un array NumPy direttamente con .numpy(), cosa che non è permessa.\n",
    "    \n",
    "    \n",
    "    mean_raw_spectrograms = {}\n",
    "    for cls in [0, 1]:\n",
    "        if len(samples[cls]) > 0:\n",
    "            # Stacka tutti i trials per la classe e calcola la media sul batch (dimensione 0)\n",
    "            \n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().cpu().numpy()\n",
    "            '''\n",
    "            🔍 Perché dovrebbe funzionare?\n",
    "            .detach(): Disattiva il tracciamento del gradiente (rende il tensore statico, senza dipendenze dalla computational graph di PyTorch).\n",
    "            .cpu(): Porta il tensore sulla CPU (necessario per numpy()).\n",
    "            .numpy(): Converte il tensore in un array NumPy.\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            1) Calcolo della media degli spettrogrammi (rimozione dei canali)\n",
    "            L'errore \"Invalid shape (3, 26, 11)\" con questa riga commentata sopra ☝️\n",
    "            \n",
    "            #mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).squeeze().detach().cpu().numpy()\n",
    "             \n",
    "            indica che l'array finale ha 3 canali in più (la prima dimensione) che non ti aspetti. \n",
    "            I tuoi dati originali hanno la forma:\n",
    "\n",
    "            (trials, canali, frequenze, tempo)\n",
    "\n",
    "            Se vuoi ottenere una rappresentazione media dello spettrogramma per tutti i trial di una classe, mediando anche sui canali, \n",
    "            allora devi calcolare la media lungo la dimensione dei trial e quella dei canali.\n",
    "            \n",
    "            Dovresti fare:\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=(0, 1)).detach().cpu().numpy()\n",
    "            \n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "            SPIEGAZIONE:\n",
    "\n",
    "            torch.cat(samples[cls], dim=0)\n",
    "            => Concatena tutti i trial per quella classe lungo la dimensione 0, ottenendo un tensore con forma:\n",
    "            (num_trials, canali, frequenze, tempo).\n",
    "\n",
    "            .mean(dim=(0, 1))\n",
    "            => Calcola la media prima lungo la dimensione dei trial (dim=0) e poi lungo quella dei canali (dim=1) in un'unica operazione, \n",
    "            ottenendo un tensore di forma (frequenze, tempo).\n",
    "\n",
    "            .detach().cpu().numpy()\n",
    "            => Rimuove il tracking del gradiente, sposta il tensore sulla CPU e lo converte in un array NumPy, pronto per imshow.\n",
    "\n",
    "            Questo ti darà l'array 2D (frequenze × tempo) che imshow si aspetta.\n",
    "\n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "\n",
    "            '''\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim =(0, 1)).detach().cpu().numpy()\n",
    "        else:\n",
    "            mean_raw_spectrograms[cls] = None\n",
    "    \n",
    "    \n",
    "    # =======================================================\n",
    "    # Passaggio 7.3: Creazione della figura finale\n",
    "    # Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Heatmap CAM normalizzata (basata sui percentili) per ciascuna classe\n",
    "    #  - Riga 2: Overlay (CAM + spettrogramma) per ciascuna classe\n",
    "    #  - Riga 3: Istogramma della distribuzione dei valori della heatmap media per ciascuna classe\n",
    "    #  - Riga 4: Spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Output atteso\n",
    "    \n",
    "    Ora avrai una figura con 4 righe di subplot: \n",
    "    1️⃣ Heatmap Grad-CAM media per classe\n",
    "    2️⃣ Grad-CAM normalizzato\n",
    "    3️⃣ Istogramma dei valori della heatmap (pre-normalizzazione)\n",
    "    4️⃣ Spettrogramma medio per ogni classe (la nuova riga che hai chiesto!)\n",
    "    '''\n",
    "    \n",
    "    # Creiamo una figura con 4 righe e 2 colonne\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Overlay over EEG Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()  # Regola automaticamente la spaziatura globale\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM) normalizzate\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        #cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * normalized_mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Mean Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "        \n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi (CAM + spettrogramma)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        overlay_img = np.flipud(mean_overlays[cls])\n",
    "        \n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        axs[1, j].axis('on')\n",
    "        \n",
    "    \n",
    "        '''\n",
    "\n",
    "        Istogramma della distribuzione dei valori della heatmap media (prima della normalizzazione robusta) per ciascuna classe\n",
    "\n",
    "        1) Cosa rappresenta l'istogramma?\n",
    "\n",
    "        L'istogramma mostra la distribuzione dei valori della heatmap media (Grad-CAM) per ogni classe.\n",
    "        In altre parole, stai visualizzando quante volte certi valori della heatmap compaiono nella distribuzione per quella classe.\n",
    "\n",
    "        2) Cosa intendo con \"prima della normalizzazione robusta\"?\n",
    "\n",
    "        Nel tuo codice, la heatmap media è stata calcolata per ciascuna classe.\n",
    "        Successivamente, hai applicato una normalizzazione robusta per rendere i valori più comparabili tra classi diverse.\n",
    "        Tuttavia, l'istogramma che stai plottando rappresenta i valori della heatmap prima che questa normalizzazione venga applicata.\n",
    "        Questo ti permette di vedere la distribuzione originale dei valori senza alterazioni dovute alla normalizzazione.\n",
    "\n",
    "\n",
    "        3) Come si collega alla figura finale?\n",
    "\n",
    "        Hai creato una terza riga di subplot (axs[2, j]) in cui plotti questo istogramma per ogni classe.\n",
    "        Il titolo che hai scelto chiarisce bene il significato della visualizzazione, specificando che si tratta dell'istogramma dei valori Grad-CAM \n",
    "        PRIMA della normalizzazione, per ogni classe.\n",
    "        La tua modifica al titolo è chiara e aiuta a evitare ambiguità.\n",
    "        \n",
    "        Per enfatizzare ancora di più il concetto, si potrebbe aggiungere \"(Raw Values)\" nel titolo, ad esempio:\n",
    "        \n",
    "        axs[2, j].set_title(f\"Histogram of Mean Grad-CAM Values (Raw) for Class {condition_names[cls]}\", fontsize=12)\n",
    "        oppure\n",
    "        axs[2, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) Before Normalization\\nClass{condition_names[cls]}\", fontsize=12)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "    # Terza 3: Visualizzazione degli istogrammi della distribuzione dei valori\n",
    "    #for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        #hist, bins = np.histogram(hist_data[cls], bins=50)\n",
    "        #axs[2, j].plot(bins[:-1], hist, color='blue')\n",
    "        #axs[2, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) for Class Before Normalization {condition_names[cls]}\", fontsize=12)\n",
    "        #axs[2, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        #axs[2, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "        \n",
    "    # Terza 3: Visualizzazione degli istogrammi della distribuzione dei valori\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        axs[2, j].hist(hist_data[cls], bins= 'auto', color='blue', edgecolor='black')\n",
    "        axs[2, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[2, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        axs[2, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "    \n",
    "        '''\n",
    "\n",
    "        Spiegazione delle modifiche aggiunte:\n",
    "\n",
    "        1) Calcolo dello spettrogramma medio raw:\n",
    "        \n",
    "        Dopo aver raccolto i campioni nel dizionario samples, viene creato il dizionario mean_raw_spectrograms.\n",
    "        Per ogni classe, i tensori vengono concatenati lungo la dimensione batch e si calcola la media sul batch (dim=0).\n",
    "        Il risultato viene convertito in un array NumPy per il plotting.\n",
    "\n",
    "        2) Aggiornamento della figura finale:\n",
    "        La figura viene creata con 4 righe e 2 colonne.\n",
    "        La quarta riga (axs[3, j]) visualizza lo spettrogramma medio per ogni classe usando imshow, con la colormap 'jet' (puoi modificarla se preferisci).\n",
    "        Vengono aggiunti titoli, etichette e una barra dei colori.\n",
    "\n",
    "        '''\n",
    "    \n",
    "    # Quarta 4: Spettrogramma medio (raw) per ciascuna classe\n",
    "    \n",
    "    # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        if mean_raw_spectrograms[cls] is not None:\n",
    "            im = axs[3, j].imshow(mean_raw_spectrograms[cls], extent=[0, 1000, 0, 26],aspect='auto', cmap='jet', origin='lower')\n",
    "            axs[3, j].set_title(f\"Mean Raw Spectrogram for Class {condition_names[cls]}\", fontsize=12)\n",
    "            axs[3, j].set_xlabel(\"Time (mms)\", fontsize=10)\n",
    "            axs[3, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "            fig.colorbar(im, ax=axs[3, j])\n",
    "            axs[3, j].axis('on')\n",
    "        else:\n",
    "            axs[3, j].axis(\"off\")\n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b505287a-8b5a-4617-97ca-1943d6c40042",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "                                                                    VERSION FREQUENCY x CHANNELS\n",
    "                                                                        \n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "FINAL VERSION WITH ULTIMATED EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ecco una versione modificata della funzione che tiene conto che:\n",
    "\n",
    "L'input originale ha forma (batch, frequenze, canali) con frequenze = 45 e canali = 61.\n",
    "\n",
    "Dopo il preprocessing nella rete (permute e unsqueeze) il modello lavora con tensori di forma (batch, 61, 45, 1), cioè:\n",
    "\n",
    "asse dei canali = 61 (che ora costituirà l’asse x della visualizzazione),\n",
    "\n",
    "asse delle frequenze = 45 (che sarà l’asse y).\n",
    "\n",
    "Nella visualizzazione degli overlay imposteremo l’extent su 0,61,0,45 (oltre a ruotare l’immagine per far sì che l’asse y rappresenti le frequenze).\n",
    "\n",
    "In aggiunta, ti fornisco uno spunto su come estrarre i nomi dei canali da una tripletta di file in formato BrainVision usando MNE, \n",
    "in modo da poterli usare per etichettare l’asse x.\n",
    "\n",
    "\n",
    "******\n",
    "\n",
    "NEW VERSION - CON NUOVO CONTRASTO\n",
    "\n",
    "Riassunto delle modifiche e dove sono state inserite:\n",
    "\n",
    "1) Normalizzazione robusta basata sui percentili (nuovo Passaggio 7.1):\n",
    "\n",
    "    Inserita subito dopo il ciclo che calcola mean_cams e mean_overlays.\n",
    "\n",
    "    Per ciascuna classe, viene calcolato il vettore dei pixel (flatten), vengono ricavati i quantili\n",
    "    (5° e 95° percentile), viene eseguito il clipping e poi la normalizzazione.\n",
    "\n",
    "\n",
    "2) Istogramma della distribuzione dei valori (nuovo Passaggio 7.2):\n",
    "\n",
    "    Nella figura finale, ho creato una terza riga di subplot in cui viene plottato l'istogramma dei valori della heatmap media\n",
    "    (prima della normalizzazione robusta) per ciascuna classe.\n",
    "\n",
    "\n",
    "3) Figura finale:\n",
    "\n",
    "    La figura finale ora ha 3 righe e 2 colonne:\n",
    "\n",
    "        Riga 1: Heatmap CAM normalizzate.\n",
    "        Riga 2: Overlay (CAM sovrapposta allo spettrogramma).\n",
    "        Riga 3: Istogramma della distribuzione dei valori della CAM.\n",
    "        Riga 4: Media dello Spettrogramma (Raw) rispetto ai Trial della Stessa Classe \n",
    "\n",
    "Questo codice aggiornato implementa tutti i passaggi che hai descritto. Fammi sapere se hai ulteriori domande o se necessiti di altre modifiche!\n",
    "\n",
    "******\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi.\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale.\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device, channel_names = None):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1.\n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "    - Riga 3: Istogramma della distribuzione dei valori della heatmap media.\n",
    "      - Riga 4: Spettrogramma medio (raw) per i trial di ciascuna classe.\n",
    "      \n",
    "      \n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    #target_layer = model.conv3\n",
    "    \n",
    "    target_layer = model.layers[-1][0]\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    #samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    #labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "\n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "            \n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #if label_int not in labels_found:\n",
    "                #samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                #labels_found[label_int] = True\n",
    "            #if 0 in labels_found and 1 in labels_found:\n",
    "            #    break\n",
    "        #if 0 in labels_found and 1 in labels_found:\n",
    "        #    break\n",
    "\n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    #if 0 not in samples or 1 not in samples:\n",
    "    #    print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "    #    return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    #cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    #overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "\n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        for sample_input in samples[cls]:\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''    \n",
    "            #sample_input = samples[cls]\n",
    "\n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            #print(f\"\\033[1mSHAPE OF SAMPLE_INPUT: {sample_input.shape}\\033[0m\")\n",
    "\n",
    "            # Assicurati che sample_input sia nel formato atteso dal modello (come nel forward)\n",
    "            #if sample_input.dim() == 3:  # Supponiamo che abbia shape (batch, 45, 61)\n",
    "            #    sample_input = sample_input.permute(0, 2, 1).unsqueeze(3)  # Ora (batch, 61, 45, 1)\n",
    "\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "\n",
    "            #Passaggio 5: Normalizzazione e upsampling\n",
    "\n",
    "            #La mappa CAM viene normalizzata tra 0 e 1.\n",
    "            #Viene ridimensionata (upsampling) per adattarsi alla dimensione originale dell'immagine\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 5: Normalizzazione e upsampling della CAM\n",
    "            # ---------------------------\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            Sì, la normalizzazione viene eseguita nel Passaggio 5: Normalizzazione e upsampling della CAM, con questa parte di codice:\n",
    "\n",
    "                # Normalizza la mappa\n",
    "                cam = cam - cam.min()\n",
    "                cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            Cosa fa questa normalizzazione?\n",
    "            \n",
    "            1) Sottrazione del minimo:\n",
    "\n",
    "                Porta i valori minimi della CAM a zero.\n",
    "                Se cam.min() fosse -0.3, sottraendo cam.min() tutti i valori si traslano in modo che il minimo sia 0.\n",
    "\n",
    "            2) Divisione per il massimo:\n",
    "\n",
    "                Scala i valori in modo che il massimo diventi 1.\n",
    "                Aggiungere 1e-8 nel denominatore previene problemi di divisione per zero.\n",
    "\n",
    "            💡 Risultato → Dopo questa operazione, cam avrà valori normalizzati tra 0 e 1.\n",
    "            '''\n",
    "            \n",
    "            # Normalizza la mappa\n",
    "            cam = cam - cam.min()\n",
    "            cam = cam / (cam.max() + 1e-8)\n",
    "            \n",
    "            #print(f\"Valori minimi e massimi dopo la normalizzazione: min={cam.min()}, max={cam.max()}\")\n",
    "            assert 0 <= cam.min() and cam.max() <= 1, \"Errore: i valori della CAM non sono nel range [0,1]!\"\n",
    "\n",
    "            '''\n",
    "            # Upsample alla dimensione dell'immagine di input: usa la shape originale degli input convoluzionali (frequenze, larghezza)\n",
    "\n",
    "\n",
    "            Nel tuo caso, il dato originale che esce dal test_loader è di forma (1,45,61), dove \n",
    "            - 45 rappresenta le frequenze \n",
    "            - 61 i canali\n",
    "\n",
    "            Nel forward del modello:\n",
    "\n",
    "            Il modello parte da un input 3D  (batch,45,61)\n",
    "            lo trasforma con 'permute' in (batch, 61, 45)\n",
    "            e poi aggiunge una dimensione con unsqueeze(3) per ottenere (batch, 61, 45, 1)\n",
    "\n",
    "            Quindi il modello lavora internamente su un \"immagine\" con dimensioni spaziali (45,1)\n",
    "            (dove 61 è il numero di canali, non le dimensioni spaziali).\n",
    "\n",
    "\n",
    "            Per il calcolo della GradCAM:\n",
    "\n",
    "            L'obiettivo è quello di upsamplare la mappa CAM per poterla sovrapporre al dato originale (lo spettrogramma), che ha forma \n",
    "            (45,61)\n",
    "\n",
    "            Se usi sample_input.shape[2:] su un tensore di forma (1,45, 61) otterrai (61,),\n",
    "            cioè solo la seconda dimensione! \n",
    "            Mentre ciò che ti serve è una tupla di due valori: le frequenze e i canali, cioè (45,61).\n",
    "\n",
    "            Quindi, per F.interpolate devi usare come target size la tupla\n",
    "\n",
    "            (sample_input.shape[1], sample_input.shape[2]), che, per il tuo dato, è (45, 61)\n",
    "\n",
    "\n",
    "            Quindi, modifica la chiamata a F.interpolate in questo modo:\n",
    "\n",
    "                target_size = (sample_input.shape[1], sample_input.shape[2])  # (45, 61)\n",
    "                cam = F.interpolate(cam.unsqueeze(1), size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            In questo modo, l'upsampling della mappa CAM avverrà alla dimensione corretta per poterla sovrapporre al dato originale, che è (45,61).\n",
    "            '''\n",
    "\n",
    "\n",
    "            #cam = F.interpolate(cam.unsqueeze(1), size=sample_input.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            target_size = (sample_input.shape[1], sample_input.shape[2])\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size = target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            cam = cam.squeeze().cpu().numpy()\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #cams[cls] = cam\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa alla lista per la classe\n",
    "            cams_list[cls].append(cam)\n",
    "\n",
    "\n",
    "            #Passaggio 6: Creazione dell’overlay Grad-CAM\n",
    "\n",
    "            #L'immagine originale viene convertita in un array numpy.\n",
    "            #La mappa CAM viene colorata con COLORMAP_JET.\n",
    "            #Si sovrappone l'heatmap all'immagine originale.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se il CAM ha valori alti in alcune regioni, il colormap evidenzierà in rosso le aree più attivate.\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 6: Creazione dell'Overlay\n",
    "            # -------------------------------\n",
    "\n",
    "            # Converte l'immagine originale in numpy; considerando che l'input è (batch, canali, frequenze, 1)\n",
    "            # Dopo squeeze, otteniamo (61, 45). Poiché vogliamo:\n",
    "            # - asse x: canali (61)\n",
    "            # - asse y: frequenze (45)\n",
    "            # invertiamo le dimensioni per ottenere (45, 61)\n",
    "\n",
    "            # Prepara l'immagine originale per la visualizzazione\n",
    "            #img = sample_input.squeeze().cpu().detach().numpy().transpose(1, 2, 0)\n",
    "            img = sample_input.squeeze().cpu().detach().numpy().transpose()  # ora (45, 61)\n",
    "\n",
    "            # Normalizza l'immagine in scala 0-255\n",
    "            '''\n",
    "                                                        SOLUZIONE\n",
    "            img_norm: Questa è l'immagine originale in scala di grigi, che ha 2 dimensioni (ad esempio, shape (45, 61)).\n",
    "\n",
    "            1) ANDIAMO A CONVERTIRE img_norm da scala di grigi (2D) a un'immagine a 3 canali (RGB):\n",
    "\n",
    "            img_norm_color = cv2.cvtColor(img_norm, cv2.COLOR_GRAY2RGB)\n",
    "            '''\n",
    "\n",
    "\n",
    "            img_norm = np.uint8(255 * (img - img.min()) / (img.max() - img.min()))\n",
    "\n",
    "            img_norm_color = cv2.cvtColor(img_norm, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "            # Applica la heatmap usando OpenCV\n",
    "            #Per l'Overlay possiamo scegliere un colormap alternativo,\n",
    "            # ad esempio COLORMAP_HOT o COLORMAP_INFERNO, per contrastare lo spettrogramma originale\n",
    "\n",
    "            '''\n",
    "            Il processo è lo stesso di quello descritto per le cam:\n",
    "\n",
    "            I valori del CAM (normalizzati) vengono scalati a 255 e convertiti in un'immagine in scala di grigi.\n",
    "            Il colormap INFERNO viene applicato per ottenere una rappresentazione colorata (dove i valori elevati diventano in genere rossi/gialli).\n",
    "            La conversione BGR→RGB assicura una visualizzazione corretta\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "                                                SOLUZIONE\n",
    "            heatmap: Questa è l'immagine a colori, che ha 3 dimensioni (ad esempio, shape (45, 61, 3)\n",
    "            '''\n",
    "\n",
    "            heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
    "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            '''NOTA BENE: \n",
    "            print(f\"img_norm_color shape: {img_norm_color.shape}\")\n",
    "            print(f\"heatmap shape: {heatmap.shape}\")\n",
    "\n",
    "            img_norm_color shape: (61, 45, 3)\n",
    "            heatmap shape: (45, 61, 3)\n",
    "\n",
    "            QUINDI ANDIAMO AD INVERTIRE LE SHAPES DI img_norm_color_shape PER FARLE CORRISPONDERE A QUELLE DI heatmap \n",
    "            '''\n",
    "\n",
    "            # Inverti le due prime dimensioni di img_norm_color\n",
    "            # Ridimensiona una delle immagini per farla corrispondere\n",
    "            # if img_norm_color.shape != heatmap.shape:\n",
    "            # img_norm_color = cv2.resize(img_norm_color, (heatmap.shape[1], heatmap.shape[0]))\n",
    "\n",
    "            img_norm_color = img_norm_color.transpose(1, 0, 2)\n",
    "\n",
    "\n",
    "            # Sovrapponi la heatmap all'immagine originale\n",
    "            # Crea l'overlay: scegliendo pesi diversi per ottenere un contrasto chiaro\n",
    "\n",
    "\n",
    "            '''\n",
    "            Overlay troppo sfocato e colori discordanti\n",
    "            Il problema che descrivi (overlay con toni azzurri/turchesi anziché il rosso della heatmap) può derivare da:\n",
    "\n",
    "            Differenza di colormap e blending:\n",
    "            L'overlay viene creato con una combinazione di due immagini: \n",
    "                1)lo spettrogramma originale (che potrebbe avere un proprio mapping di colori) e\n",
    "                2) la heatmap\n",
    "\n",
    "            Se il bilanciamento (i pesi) è 0.5-0.5, l'influenza dello spettrogramma può \"modificare\" i colori della heatmap.\n",
    "\n",
    "            Suggerimenti:\n",
    "\n",
    "            a) Modifica i pesi in cv2.addWeighted:\n",
    "            Ad esempio, prova con 0.3 per l'immagine originale e 0.7 per la heatmap, in modo che il colore della heatmap (ad es. il rosso) prevalga.\n",
    "\n",
    "            b) Uniforma il formato dell'immagine originale:\n",
    "            Se lo spettrogramma originale è in scala di grigi o usa un colormap diverso,\n",
    "            considera di convertirlo in un'immagine in scala di grigi a 8 bit prima di creare l'overlay.\n",
    "\n",
    "            c) Usa lo stesso colormap: \n",
    "            Se vuoi che l'overlay abbia colori simili a quelli della heatmap, \n",
    "            usa lo stesso colormap (qui COLORMAP_INFERNO) per entrambe e regola il blending.\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "                                                        SOLUZIONE \n",
    "            overlay: Questo è il risultato finale che sovrappone la heatmap sull'immagine originale. \n",
    "            La soluzione proposta suggerisce di usare cv2.addWeighted\n",
    "            per combinare img_norm_color (l'immagine in scala di grigi convertita a 3 canali) e heatmap.\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "            2) ANDIAMO A combinare img_norm_color (l'immagine ora a 3 canali) con la heatmap usando cv2.addWeighted:\n",
    "\n",
    "            overlay = cv2.addWeighted(img_norm_color, 0.4, heatmap, 0.6, 0)\n",
    "            '''\n",
    "\n",
    "            overlay = cv2.addWeighted(img_norm_color, 0.4, heatmap, 0.6, 0)\n",
    "\n",
    "            #overlay = cv2.addWeighted(img_norm, 0.4, heatmap, 0.6, 0)\n",
    "            #overlay = cv2.addWeighted(img_norm, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "\n",
    "            '''\n",
    "            3) ANDIAMO ad assegnare l'overlay al dizionario overlays (che sta memorizzando gli overlay per ciascuna classe):\n",
    "            overlays[cls] = overlay\n",
    "            '''\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #overlays[cls] = overlay\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi l'overlay alla lista per la classe\n",
    "            overlays_list[cls].append(overlay)\n",
    "    \n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    mean_cams = {}\n",
    "    mean_overlays = {}    \n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "        mean_overlays[cls] = np.mean(np.array(overlays_list[cls]), axis=0).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    NEW VERSION\n",
    "    '''\n",
    "    \n",
    "    # =======================================================\n",
    "    # Nuovo Passaggio 7.1: Normalizzazione basata sui percentili\n",
    "    # Per ogni classe, restringiamo il range della heatmap media\n",
    "    # per enfatizzare il contrasto nelle regioni più frequenti.\n",
    "    # =======================================================\n",
    "    normalized_mean_cams = {}\n",
    "    hist_data = {}  # Per salvare i dati dell'istogramma per ciascuna classe\n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        # Estrai tutti i pixel in un vettore\n",
    "        heatmap_flat = mean_cams[cls].flatten()\n",
    "        \n",
    "        '''\n",
    "        L'idea è enfatizzare le regioni in cui i valori della heatmap sono più frequenti, \n",
    "        riducendo l'influenza degli outlier, e normalizzare in modo adattivo alla forma della distribuzione\n",
    "        \n",
    "        1. Mediana ± margine percentile (centrato sulla mediana)\n",
    "        \n",
    "        Calcoli la mediana e costruisci un range percentile attorno a essa (es. ±20%)\n",
    "        \n",
    "        Pro: Robusto, adatto a distribuzioni skewed o simmetriche\t\n",
    "        Contro: Se la mediana non coincide con la modalità (valore più frequente), potresti perdere le aree più “dense”\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Calcola la mediana\n",
    "        median = np.median(heatmap_flat)\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Percentile della mediana\n",
    "        median_percentile = np.sum(heatmap_flat < median) / len(heatmap_flat) * 100\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Margine personalizzabile\n",
    "        margin = 20  \n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Definisci range percentile dinamico\n",
    "        lower_p = max(0, median_percentile - margin)\n",
    "        upper_p = min(100, median_percentile + margin)\n",
    "    \n",
    "        # Calcola i quantili (ad esempio 30° e 70° percentile)\n",
    "        #vmin = np.percentile(heatmap_flat, 30)\n",
    "        #vmax = np.percentile(heatmap_flat, 70)\n",
    "        \n",
    "        #AGGIUNTA\n",
    "        # Calcola limiti reali\n",
    "        vmin = np.percentile(heatmap_flat, lower_p)\n",
    "        vmax = np.percentile(heatmap_flat, upper_p)\n",
    "\n",
    "        # Clipping dei valori per limitare gli outlier\n",
    "        heatmap_clipped = np.clip(mean_cams[cls], vmin, vmax)\n",
    "\n",
    "        \n",
    "        # Normalizza tra 0 e 1 usando i valori clipped\n",
    "        heatmap_normalized = (heatmap_clipped - vmin) / (vmax - vmin + 1e-8)\n",
    "        normalized_mean_cams[cls] = heatmap_normalized\n",
    "        \n",
    "        # Salva i dati per l'istogramma (per visualizzare la distribuzione)\n",
    "        hist_data[cls] = heatmap_flat\n",
    "        \n",
    "        #print(f\"\\n[Classe \\033[1m{cls}\\033[0m] Mediana: \\033[1m{median:.4f}\\033[0m | vmin ({lower_p:.1f}°): {vmin:.4f} | vmax ({upper_p:.1f}°): {vmax:.4f}\\n\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    OLD VERSION\n",
    "    \n",
    "    #Passaggio 7: Creazione della figura finale\n",
    "    \n",
    "    #La prima riga mostra solo le heatmap Grad-CAM.\n",
    "    #La seconda riga mostra le heatmap sovrapposte agli spettrogrammi.\n",
    "\n",
    "    # Crea la figura con due righe e due colonne\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 7: Creazione della figura finale\n",
    "    # -------------------------------\n",
    "    # Creiamo una figura con 2 righe e 2 colonne:\n",
    "    # - Prima riga: le heatmap CAM (da 0 a 1) per ciascuna condizione.\n",
    "    # - Seconda riga: l'overlay (CAM + spettrogramma) per ciascuna condizione, con etichette per gli assi.\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Imposta un titolo generale per la figura\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping - Experimental Condition: {exp_cond} - Subject: {category_subject}\", fontsize=12)\n",
    "    \n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG trial Spectrogram\\nExperimental Condition: {exp_cond} - Subject: {category_subject}\",\n",
    "    #fontsize=10,\n",
    "    #y=0.95  # Puoi regolare la posizione verticale se necessario\n",
    "    #)\n",
    "    \n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Impostiamo l'estensione: x da 0 a 61 (canali) e y da 0 a 45 (frequenze)\n",
    "    extent = [0, 61, 0, 45]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    NEW VERSION\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # =======================================================\n",
    "    # Nuovo Passaggio 7.2: Calcolo dello spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Cosa fa questo codice?\n",
    "    ✅ Calcola lo spettrogramma medio per ogni classe (0 e 1) prendendo i trials da samples e facendo la media sulla prima dimensione (batch).\n",
    "    ✅ Plotta lo spettrogramma medio nella quarta riga del grafico finale, con una colonna per ogni classe.\n",
    "    ✅ Usa una colormap jet per una migliore visualizzazione.\n",
    "    ✅ Evita errori: Se una classe non ha dati, non plotta nulla per quella colonna.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Sì, l'errore indica che stai cercando di convertire un tensore PyTorch che richiede il calcolo del gradiente\n",
    "    #in un array NumPy direttamente con .numpy(), cosa che non è permessa.\n",
    "    \n",
    "    \n",
    "    mean_raw_spectrograms = {}\n",
    "    for cls in [0, 1]:\n",
    "        if len(samples[cls]) > 0:\n",
    "            # Stacka tutti i trials per la classe e calcola la media sul batch (dimensione 0)\n",
    "            \n",
    "            '''\n",
    "            Nel tuo caso l'input ha la forma:\n",
    "\n",
    "                (batch, 45, 61), dove:\n",
    "\n",
    "                45 rappresenta le frequenze (asse y)\n",
    "                61 rappresenta i canali (asse x)\n",
    "\n",
    "            Se vuoi visualizzare lo \"spettrogramma medio\" per ogni classe come un'immagine 2D (45 × 61), \n",
    "            devi mediare solo sul batch (cioè, sui trial) e lasciare intatte le dimensioni di frequenza e canali. \n",
    "            \n",
    "            Aver mediato anche sui canali (usando mean(dim=(0,1))) porterebbe a un vettore 1D (di forma (61,)), mentre quello che ti serve è una matrice 2D.\n",
    "\n",
    "            Quindi, la riga corretta per calcolare lo spettrogramma medio è:\n",
    "\n",
    "                mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).detach().cpu().numpy()\n",
    "            \n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "            SPIEGAZIONE\n",
    "            \n",
    "            In questo modo:\n",
    "\n",
    "                torch.cat(samples[cls], dim=0) concatena tutti i trial per quella classe, ottenendo un tensore di forma (num_trials, 45, 61).\n",
    "                .mean(dim=0) calcola la media lungo la dimensione del batch, restituendo un tensore di forma (45, 61).\n",
    "                Infine, .detach().cpu().numpy() converte il risultato in un array NumPy, adatto per imshow.\n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "\n",
    "            '''\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim = 0).detach().cpu().numpy()\n",
    "        else:\n",
    "            mean_raw_spectrograms[cls] = None\n",
    "            \n",
    "    \n",
    "    # =======================================================\n",
    "    # Passaggio 7.3: Creazione della figura finale\n",
    "    # Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Heatmap CAM normalizzata (basata sui percentili) per ciascuna classe\n",
    "    #  - Riga 2: Overlay (CAM + spettrogramma) per ciascuna classe\n",
    "    #  - Riga 3: Istogramma della distribuzione dei valori della heatmap media per ciascuna classe\n",
    "    #  - Riga 4: Spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Output atteso\n",
    "    \n",
    "    Ora avrai una figura con 4 righe di subplot: \n",
    "    1️⃣ Heatmap Grad-CAM media per classe\n",
    "    2️⃣ Grad-CAM normalizzato\n",
    "    3️⃣ Istogramma dei valori della heatmap (pre-normalizzazione)\n",
    "    4️⃣ Spettrogramma medio per ogni classe (la nuova riga che hai chiesto!)\n",
    "    '''\n",
    "    \n",
    "    # Creiamo una figura con 4 righe e 2 colonne\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()  # Regola automaticamente la spaziatura globale\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "    # Impostiamo l'estensione: x da 0 a 61 (canali) e y da 0 a 45 (frequenze)\n",
    "    extent = [0, 61, 0, 45]\n",
    "    \n",
    "    '''\n",
    "    Questo codice visualizza solo le heatmap di Grad-CAM (senza l'overlay), utilizzando il colormap \"INFERNO\" \n",
    "    (o un altro che preferisci) e applica l'inversione verticale (np.flipud()) per ottenere la giusta visualizzazione (se necessario).\n",
    "    '''\n",
    "    \n",
    "    # Prima riga: Visualizza solo le heatmap (CAM)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Qui usiamo il colormap INFERNO per la CAM, ma puoi modificare se preferisci\n",
    "        \n",
    "        '''\n",
    "        np.uint8(255 * cams[cls]):\n",
    "        La mappa CAM (calcolata e normalizzata) ha valori compresi tra 0 e 1.\n",
    "        Moltiplicando per 255 e convertendo in uint8, ottieni un'immagine in scala di grigi a 8 bit (0-255).\n",
    "        \n",
    "        cv2.applyColorMap(..., cv2.COLORMAP_INFERNO):\n",
    "        Applica il colormap INFERNO che trasforma la scala di grigi in un'immagine a colori, \n",
    "        dove i valori bassi saranno scuri e quelli alti appariranno in toni caldi (ad es. giallo/rosso).\n",
    "        \n",
    "        cv2.cvtColor(..., cv2.COLOR_BGR2RGB):\n",
    "        OpenCV usa il formato BGR per impostazione predefinita. \n",
    "        Convertire in RGB assicura che l'immagine venga visualizzata correttamente (matplotlib si aspetta RGB).\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''SOLO UN ESEMPIO'''\n",
    "        #cam_img = cv2.applyColorMap(np.uint8(255 * cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        cam_img = cv2.applyColorMap(np.uint8(255 * mean_cams[cls]), cv2.COLORMAP_INFERNO)\n",
    "        cam_img = cv2.cvtColor(cam_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        \n",
    "        #axs[0, j].imshow(cam_img, extent=extent, aspect='auto')\n",
    "        #axs[0, j].set_title(f\"Grad-CAM for {condition_names[cls]}\", fontsize=12)\n",
    "        #axs[0, j].axis('off')\n",
    "        \n",
    "        '''QUI AGGIUNGIAMO L'INVERSIONE DEGLI ASSI'''\n",
    "        # Se necessario, inverti gli assi per ottenere la visualizzazione desiderata\n",
    "        # Invertiamo verticalmente per avere le frequenze in ordine crescente (se necessario)\n",
    "        \n",
    "        #cam_img = np.flipud(cam_img)  # Inverte verticalmente\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[0, j].imshow(cam_img)\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        #axs[0, j].imshow(cam_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        \n",
    "        axs[0, j].imshow(cam_img, extent = extent, aspect='auto')\n",
    "        \n",
    "        axs[0, j].set_title(f\"Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].axis('off')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questo codice visualizza ANCHE l'overlay e le heatmap di Grad-CAM,  utilizzando il colormap \"INFERNO\" \n",
    "    (o un altro che preferisci) e applica l'inversione verticale (np.flipud()) per ottenere la giusta visualizzazione (se necessario).\n",
    "    \n",
    "    \n",
    "    Per ridurre la grandezza della stringa dei canali all'interno dell'asse x (EEG Channels), puoi intervenire su vari aspetti della visualizzazione, come la dimensione del font, l'orientamento e, se necessario, l'abbreviazione o il ridimensionamento dei nomi dei canali.\n",
    "\n",
    "    1. Ridurre la dimensione del font\n",
    "    Puoi facilmente ridurre la grandezza del testo delle etichette sugli assi xticks utilizzando l'argomento fontsize dentro set_xticklabels.\n",
    "\n",
    "    Ecco un esempio di come applicarlo:\n",
    "    \n",
    "        axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    Puoi provare a modificare la dimensione di fontsize fino a trovare quella più adatta per visualizzare i nomi dei canali \n",
    "    senza che risultino troppo grandi o sovrapposti.\n",
    "\n",
    "    2. Abbreviare i nomi dei canali\n",
    "    Se i nomi dei canali sono troppo lunghi e non si adattano all'asse x, puoi abbreviarli. \n",
    "    Ad esempio, puoi creare una lista di nomi abbreviati (ad esempio, \"Fz\" al posto di \"Frontal Z\") e usarla per l'asse x.\n",
    "\n",
    "    Esempio:\n",
    "\n",
    "    # Creare abbreviazioni per i canali\n",
    "    abbreviated_channel_names = [name[:3] for name in channel_names]\n",
    "\n",
    "    # Impostare le etichette abbreviate\n",
    "    axs[1, j].set_xticklabels(abbreviated_channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    In questo caso, i nomi dei canali verranno abbreviati ai primi 3 caratteri di ogni nome.\n",
    "\n",
    "    3. Aumentare lo spazio tra le etichette (se necessario)\n",
    "    Se le etichette sono ancora troppo vicine, puoi anche aumentare la distanza tra le etichette usando set_xticks:\n",
    "\n",
    "    axs[1, j].set_xticks(np.arange(0.5, extent[1], 2))  # Spaziatura maggiore\n",
    "    In questo modo, le etichette saranno meno affollate sull'asse x.\n",
    "        '''\n",
    "    \n",
    "    # Seconda riga: Visualizza gli overlay con etichette degli assi\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        '''COMMENTATO PER L'OVERLAY SOLO RAPPRESENTARE L'ASSE DEL TEMPO IN FORMATO DI MILLISECONDI E NON DI FINESTRE STFT'''\n",
    "        #axs[1, j].imshow(overlays[cls])\n",
    "        \n",
    "        # Qui, se vuoi che l'asse y (frequenze) venga ordinato in modo crescente,\n",
    "        # puoi anche invertire l'immagine verticalmente, se non è già corretto.\n",
    "        \n",
    "        '''SOLO UN ESEMPIO'''\n",
    "        #overlay_img = np.flipud(overlays[cls])\n",
    "        \n",
    "        '''CON MEDIA'''\n",
    "        overlay_img = np.flipud(mean_overlays[cls])\n",
    "        \n",
    "        # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "        #axs[1, j].imshow(overlay_img, extent=[0, 1000, 0, 26], aspect='auto')\n",
    "        axs[1, j].imshow(overlay_img, extent=extent, aspect='auto')\n",
    "        \n",
    "        axs[1, j].set_title(f\"Overlay of Grad-CAM Heatmap for Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[1, j].set_xlabel(\"EEG Channels\", fontsize=10)\n",
    "        \n",
    "        axs[1, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "        #axs[1, j].axis('on')\n",
    "        \n",
    "        '''\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            axs[1, j].set_xticks(np.arange(0.3, extent[1], 2))\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize = 6)\n",
    "        '''\n",
    "        \n",
    "        # Calcola le posizioni in modo che il numero di tick corrisponda al numero di canali\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            num_channels = len(channel_names)\n",
    "            ticks = np.linspace(0.5, extent[1] - 1, num_channels)  # crea num_channels posizioni equidistanti\n",
    "\n",
    "            # Imposta i tick e le etichette\n",
    "            axs[1, j].set_xticks(ticks)\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "    \n",
    "   \n",
    "    # Terza 3: Visualizzazione degli istogrammi della distribuzione dei valori\n",
    "        \n",
    "    '''\n",
    "    Questi valori rappresentano la distribuzione delle attivazioni prima della procedura di normalizzazione che viene applicata per amplificare il contrasto, \n",
    "    quindi sì, si tratta di valori di attivazione della mappa Grad-CAM.\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Istogramma dei valori prima della normalizzazione: Stai visualizzando un istogramma di questi valori medi, \n",
    "                                                           prima che vengano normalizzati o clippati \n",
    "                                                           (secondo il processo di normalizzazione basato sui percentili che hai descritto). \n",
    "                                                           Questo ti dà una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti prima che tu applichi il filtro per migliorare\n",
    "                                                           il contrasto nelle aree di interesse.\n",
    "    \n",
    "    \n",
    "    Perché \"Grad-CAM value\" può creare confusione:\n",
    "    Il termine \"Grad-CAM value\" potrebbe sembrare che faccia riferimento direttamente ai valori generati dalla mappa Grad-CAM finale. \n",
    "    Ma in realtà, i valori che stai trattando sono le attivazioni mediate e clippate, che formano la heatmap. \n",
    "    L'istogramma che stai tracciando rappresenta la distribuzione delle attivazioni prima della normalizzazione.\n",
    "\n",
    "    Riepilogo\n",
    "    Quindi, questi valori sono attivazioni pesate per ciascun pixel della mappa Grad-CAM, e mediati per classe. \n",
    "    Il processo di normalizzazione che segue (basato sui percentili) serve a enfatizzare il contrasto in modo da focalizzarsi sulle aree più significative \n",
    "    per la previsione.\n",
    "\n",
    "    Per rispondere alla tua domanda: sì, è corretto dire che stai visualizzando la distribuzione delle attivazioni pesate prima della normalizzazione\n",
    "    per migliorare il contrasto, ma è meglio riferirsi a questi valori come valori di attivazione della mappa Grad-CAM o valori della heatmap Grad-CAM, \n",
    "    piuttosto che \"Grad-CAM value\" che potrebbe risultare ambiguo.\n",
    "\n",
    "    Se vuoi, puoi anche aggiungere una nota nella visualizzazione dell'istogramma che chiarisca il processo:\n",
    "    \n",
    "    axs[2, j].set_title(f\"Histogram of Heatmap Activation Values (Raw, before Normalization) Class {condition_names[cls]}\", fontsize=12)\n",
    "    oppure\n",
    "    axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "    \n",
    "    è molto chiara e corretta!\n",
    "    Indica perfettamente che stai visualizzando l'istogramma dei valori di attivazione medi della heatmap, \n",
    "    senza fare confusione sul fatto che si tratti di valori medi per ciascuna classe.\n",
    "    \n",
    "    In sintesi, questa frase comunica in modo preciso che stai mostrando la distribuzione delle attivazioni mediate dalla mappa Grad-CAM\n",
    "    per una specifica classe. Quindi sì, va benissimo!\n",
    "\n",
    "    '''\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        axs[2, j].hist(hist_data[cls], bins= 'auto', color='blue', edgecolor='black')\n",
    "        #axs[2, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[2, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        axs[2, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "    \n",
    "        '''\n",
    "\n",
    "        Spiegazione delle modifiche aggiunte:\n",
    "\n",
    "        1) Calcolo dello spettrogramma medio raw:\n",
    "        \n",
    "        Dopo aver raccolto i campioni nel dizionario samples, viene creato il dizionario mean_raw_spectrograms.\n",
    "        Per ogni classe, i tensori vengono concatenati lungo la dimensione batch e si calcola la media sul batch (dim=0).\n",
    "        Il risultato viene convertito in un array NumPy per il plotting.\n",
    "\n",
    "        2) Aggiornamento della figura finale:\n",
    "        La figura viene creata con 4 righe e 2 colonne.\n",
    "        La quarta riga (axs[3, j]) visualizza lo spettrogramma medio per ogni classe usando imshow, con la colormap 'jet' (puoi modificarla se preferisci).\n",
    "        Vengono aggiunti titoli, etichette e una barra dei colori.\n",
    "\n",
    "        '''\n",
    "    \n",
    "    # Quarta 4: Spettrogramma medio (raw) per ciascuna classe\n",
    "    \n",
    "    # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        if mean_raw_spectrograms[cls] is not None:\n",
    "            im = axs[3, j].imshow(mean_raw_spectrograms[cls], extent=extent,aspect='auto', cmap='jet', origin='lower')\n",
    "            axs[3, j].set_title(f\"Mean Raw Spectrogram for Class {condition_names[cls]}\", fontsize=12)\n",
    "            axs[3, j].set_xlabel(\"EEG Channels\", fontsize=10)\n",
    "            axs[3, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "            fig.colorbar(im, ax=axs[3, j])\n",
    "            axs[3, j].axis('on')\n",
    "            \n",
    "            # Calcola le posizioni in modo che il numero di tick corrisponda al numero di canali\n",
    "            # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "            if channel_names is not None and len(channel_names) == extent[1]:\n",
    "                num_channels = len(channel_names)\n",
    "                ticks = np.linspace(0.5, extent[1] - 1, num_channels)  # crea num_channels posizioni equidistanti\n",
    "\n",
    "                # Imposta i tick e le etichette\n",
    "                axs[3, j].set_xticks(ticks)\n",
    "                axs[3, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "        else:\n",
    "            axs[3, j].axis(\"off\")\n",
    "            \n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f786d5-393a-479a-8a89-cfb7090102e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "PER CNN2D\n",
    "\n",
    "\n",
    "                                                                        NEW VERSION 27/06/2025\n",
    "                                                                        \n",
    "                                                                  \n",
    "                                                                    VERSION FREQUENCY x CHANNELS\n",
    "                                                                    \n",
    "                                                                                VERSIONE C\n",
    "                                                                            \n",
    "                                                                    \n",
    "                                                                    NORMALIZZAZIONE SOLO SU SCALA DI COLORI \n",
    "                                                                    PER HEATMAP MEDIA DEL GRADCAM \n",
    "                                                                \n",
    "                                                                    SCALA LOGARITMICA PER SPETTOGRAMMA\n",
    "                                                                        CONGIUNTA PER DUE CLASSI\n",
    "                                                                    \n",
    "                                                                    ATTENZIONE CHE VIENE FATTA IMPORTATA \n",
    "                                                                        MA PICCOLA MODIFICA,\n",
    "                                                                        \n",
    "                                                                    DATO CHE ORA LA RETE SAREBBE FIXED \n",
    "                                                                    E NON PIU' DINAMICA COME PRIMA!\n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                                            PER LA CNN 2D \n",
    "                                                                    \n",
    "-    # -------------------------------\n",
    "-    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "-    # -------------------------------\n",
    "-    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "-    #target_layer = model.conv3\n",
    "-    \n",
    "-    target_layer = model.layers[-1][0]\n",
    "-    gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "+    # -------------------------------\n",
    "+    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "+    # -------------------------------\n",
    "+    # Qui prendiamo direttamente il conv3 della tua CNN:\n",
    "+    target_layer = model.conv3\n",
    "+    gradcam = GradCAM(model, target_layer)\n",
    "                                                                        \n",
    "                                                                        \n",
    "                                                                        \n",
    "Creazione della funzione per generare le immagini associate alla GradCAM compution\n",
    "\n",
    "FINAL VERSION WITH ULTIMATED EDITING PHASES\n",
    "\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "1) Selezione dei Campioni:\n",
    "La funzione itera sul test_loader e salva il primo campione trovato per ciascuna delle due classi (0 e 1).\n",
    "\n",
    "2) Calcolo GradCAM per ciascun campione:\n",
    "\n",
    "Per ogni campione, si abilita il gradiente e si esegue la forward pass.\n",
    "Viene scelto il target (se non specificato, quello predetto) e si esegue la backward pass per calcolare i gradienti.\n",
    "\n",
    "- I pesi vengono calcolati come la media dei gradienti lungo le dimensioni spaziali (dim=(2,3)) e usati per eseguire una somma pesata sulle attivazioni.\n",
    "- La mappa risultante viene passata attraverso una ReLU, normalizzata e upsampled per avere la stessa dimensione dell’input.\n",
    "\n",
    "Creazione degli Overlay:\n",
    "Viene normalizzata l’immagine originale e viene applicata una heatmap (usando OpenCV), quindi l’overlay viene ottenuto con cv2.addWeighted.\n",
    "\n",
    "Costruzione della Figura:\n",
    "Viene creata una figura con due righe e due colonne:\n",
    "\n",
    "- La prima riga mostra le heatmap per ciascuna classe.\n",
    "- La seconda riga mostra le sovrapposizioni (overlay) tra heatmap e spettrogramma originale.\n",
    "\n",
    "I titoli sono personalizzati in base a exp_cond, data_type e category_subject.\n",
    "\n",
    "Questa struttura mantiene tutta la logica necessaria (incluso il calcolo dei pesi) e la rende simile alla versione precedente,\n",
    "con la differenza che il calcolo della CAM viene eseguito per campioni rappresentativi di entrambe le classi. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ecco una versione modificata della funzione che tiene conto che:\n",
    "\n",
    "L'input originale ha forma (batch, frequenze, canali) con frequenze = 45 e canali = 61.\n",
    "\n",
    "Dopo il preprocessing nella rete (permute e unsqueeze) il modello lavora con tensori di forma (batch, 61, 45, 1), cioè:\n",
    "\n",
    "asse dei canali = 61 (che ora costituirà l’asse x della visualizzazione),\n",
    "\n",
    "asse delle frequenze = 45 (che sarà l’asse y).\n",
    "\n",
    "Nella visualizzazione degli overlay imposteremo l’extent su 0,61,0,45 (oltre a ruotare l’immagine per far sì che l’asse y rappresenti le frequenze).\n",
    "\n",
    "In aggiunta, ti fornisco uno spunto su come estrarre i nomi dei canali da una tripletta di file in formato BrainVision usando MNE, \n",
    "in modo da poterli usare per etichettare l’asse x.\n",
    "\n",
    "\n",
    "******\n",
    "\n",
    "    #  - Riga 1: Istogramma della distribuzione dei valori della heatmap media RAW per ciascuna classe \n",
    "    #            rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 2: GradCAM medio della distribuzione dei valori della heatmap media per ogni classe, \n",
    "    #            a seguito della NORMALIZZAZIONE rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 3: Spettrogramma medio (RAW) rispetto ai Trial della Stessa Classe, su range logaritmico \n",
    "\n",
    "******\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "\n",
    "#La funzione compute_gradcam_figure serve a calcolare e visualizzare \n",
    "#le mappe di attivazione Grad-CAM per un modello CNN2D, applicandole a spettrogrammi EEG. \n",
    "\n",
    "#In particolare, seleziona un campione per ciascuna classe (0 e 1), calcola la Grad-CAM e costruisce una figura con:\n",
    "\n",
    "#Prima riga → Heatmap della Grad-CAM per entrambe le classi\n",
    "#Seconda riga → Heatmap sovrapposta allo spettrogramma originale\n",
    "#Questa visualizzazione aiuta a interpretare su quali parti dell'immagine il modello si sta concentrando per prendere decisioni.\n",
    "\n",
    "\n",
    "#Questa funzione aiuta a visualizzare le regioni attivate dalla rete CNN su immagini di spettrogrammi EEG,\n",
    "#evidenziando le aree più importanti per la classificazione.\n",
    "\n",
    "#🔹 Esempio finale:\n",
    "#La figura risultante avrà due righe:\n",
    "\n",
    "#Heatmap puro della Grad-CAM.\n",
    "#Heatmap sovrapposta allo spettrogramma EEG originale.\n",
    "\n",
    "\n",
    "'''RICORDATI: aggiunto parametro TEST_LOADER_RAW per i plots della POTENZA SPETTRALE MEDIA PER BANDA (i.e., test_loader_raw)'''\n",
    "\n",
    "def compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device, channel_names = None):\n",
    "    \"\"\"\n",
    "    Per il modello CNN2D, seleziona un campione per ciascuna classe (0 e 1),\n",
    "    calcola la GradCAM e costruisce una figura con:\n",
    "    \n",
    "      - Riga 1: Heatmap per classe 0 e classe 1\n",
    "      \n",
    "      - Riga 2: Sovrapposizione della heatmap sullo spettrogramma originale.\n",
    "      \n",
    "      - Riga 3: Istogramma della distribuzione dei valori della heatmap media per ogni classe, \n",
    "                prima della normalizzazione centrata sulla mediana (o sul picco) della distribuzione? \n",
    "      \n",
    "      - Riga 4: Δ-GradCAM della distribuzione dei valori della heatmap media per ogni classe, \n",
    "                a seguito della normalizzazione centrata sulla mediana (o sul picco) della distribuzione? \n",
    "                \n",
    "      - Riga 5: Spettrogramma medio (raw) per i trial di ciascuna classe.\n",
    "      \n",
    "      \n",
    "      \n",
    "    I titoli e le etichette degli assi sono personalizzati:\n",
    "    \n",
    "    - L'asse x rappresenta il tempo (ms) e l'asse y le frequenze (Hz) (solo per la riga overlay)    \n",
    "    - I titoli dei subplot usano i nomi delle condizioni estratte automaticamente da 'exp_cond'\n",
    "        (assumendo che exp_cond sia del tipo \"th_resp_vs_pt_resp\"), data_type e category_subject\n",
    "    \n",
    "    Il calcolo della CAM include il passaggio:\n",
    "       weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "       cam = torch.sum(weights * activations, dim=1)\n",
    "       cam = F.relu(cam)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    \n",
    "    #Qui si definisce quale layer convoluzionale sarà usato per la Grad-CAM.\n",
    "    #In questo caso, conv3 è il terzo layer convoluzionale del modello model.\n",
    "    \n",
    "    #Grad-CAM calcola la mappa di attivazione basandosi sulle feature generate da questo livello.\n",
    "    \n",
    "    #🔹 Esempio:Se model.conv3 è un layer convoluzionale con 128 feature map,\n",
    "    #la Grad-CAM genererà una mappa di attivazione basata su queste 128 feature.)\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # Passaggio 1: Impostazione del layer target e istanziazione di GradCAM\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Imposta il layer target (ad esempio conv3) e crea un'istanza di GradCAM\n",
    "    \n",
    "    '''SCOMMENTATO QUESTA RIGA'''\n",
    "    target_layer = model.conv3\n",
    "    \n",
    "    '''COMMENTATO QUESTA RIGA'''\n",
    "    #target_layer = model.layers[-1][0]\n",
    "    \n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Estrai i nomi delle condizioni separando exp_cond (es: \"th_resp_vs_pt_resp\")\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    #Passaggio 2: Selezione di un campione per ogni classe\n",
    "    \n",
    "    #Qui la funzione cerca almeno un campione per ciascuna delle due classi (0 e 1) nel test_loader.\n",
    "    \n",
    "    #🔹 Esempio pratico:\n",
    "    #Se il batch contiene:\n",
    "        \n",
    "    #labels = [1, 0, 1, 0, 1]  \n",
    "    #inputs.shape = (5, 1, 64, 64)  # 5 immagini 64x64 in scala di grigi\n",
    "    \n",
    "    #Il codice estrae:\n",
    "\n",
    "    #samples[0] = inputs[1] (il primo esempio della classe 0)\n",
    "    #samples[1] = inputs[0] (il primo esempio della classe 1)\n",
    "    #Se il test_loader non contiene entrambe le classi, la funzione stampa un messaggio di errore e termina.\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 2: Selezione dei campioni per ciascuna classe\n",
    "    # -------------------------------\n",
    "    \n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Dizionari per salvare un campione per ciascuna classe\n",
    "    #samples = {}      # Qui salveremo il sample input per ogni classe \n",
    "    #labels_found = {} # Per tracciare se abbiamo già trovato un esempio per ciascuna classe di etichette\n",
    "    \n",
    "    '''CON MEDIA'''\n",
    "    \n",
    "    #Ora che ogni classe ha una sua chiave nel dizionario samples, non c'è più bisogno di usare labels_found \n",
    "    #per verificare la presenza di entrambe le classi.\n",
    "    #In precedenza, stavi iterando nel test_loader e verificando la presenza di almeno un esempio per entrambe le classi (0 e 1),\n",
    "    #ma ora i dati vengono direttamente organizzati nel dizionario in base alla loro classe. Quindi, se la classe non esiste nel dataset,\n",
    "    #semplicemente non avrà una chiave nel dizionario samples.\n",
    "    #Il controllo finale if 0 not in samples or 1 not in samples: è ancora necessario per assicurarsi che entrambe le classi siano presenti.\n",
    "    #Se manca una classe, possiamo ancora uscire con un messaggio di errore.\n",
    "    \n",
    "\n",
    "    # Dizionari per salvare tutti i campioni per ciascuna classe\n",
    "    \n",
    "    \n",
    "    '''DATI ORIGINALI DEL TEST LOADER'''\n",
    "    samples = {0: [], 1: []}\n",
    "\n",
    "    # Itera sul test_loader fino a trovare gli esempi per ciascuna classe (0 e 1)\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "            \n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #if label_int not in labels_found:\n",
    "                #samples[label_int] = inputs[i].unsqueeze(0)  # Salva come tensore 4D\n",
    "                #labels_found[label_int] = True\n",
    "            #if 0 in labels_found and 1 in labels_found:\n",
    "            #    break\n",
    "        #if 0 in labels_found and 1 in labels_found:\n",
    "        #    break\n",
    "        \n",
    "    \n",
    "    '''TEST_LOADER RAW (DATI NON STANDARDIZZATI)'''\n",
    "    samples_raw = {0: [], 1: []}\n",
    "    \n",
    "    for inputs, labels in test_loader_raw:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples_raw:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                samples_raw[label_int].append(inputs[i].unsqueeze(0))\n",
    "    \n",
    "    \n",
    "    # Se non troviamo entrambi gli esempi, esci con un messaggio\n",
    "    #if 0 not in samples or 1 not in samples:\n",
    "    #    print(\"Non sono stati trovati esempi per entrambe le classi nel test_loader.\")\n",
    "    #    return None\n",
    "\n",
    "    #Passaggio 3: Calcolo della Grad-CAM\n",
    "    \n",
    "    # Qui il codice:\n",
    "\n",
    "    #Passa l'input al modello per ottenere le predizioni.\n",
    "    #Identifica la classe predetta (target_class).\n",
    "    #Fa il backpropagation per calcolare i gradienti rispetto alla classe target.\n",
    "\n",
    "    #🔹 Esempio pratico:\n",
    "    #Se output = [0.3, 0.7], il modello predice la classe 1, quindi target_class = 1 e il backpropagation calcola il gradiente rispetto a questa classe.\n",
    "    \n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 3: Calcolo della Grad-CAM per ciascun campione\n",
    "    # -------------------------------\n",
    "    \n",
    "    '''SOLO UN ESEMPIO'''\n",
    "    # Per ciascun campione, calcola la GradCAM\n",
    "    #cams = {} # Qui salveremo la mappa CAM per ogni classe\n",
    "    #overlays = {} # Qui salveremo l'overlay (CAM + spettrogramma)\n",
    "    \n",
    "    '''\n",
    "    L'errore si verifica perché ora la variabile samples[cls] è una lista di tensori (cioè, più campioni) e non un singolo tensore. \n",
    "    Di conseguenza, cercando di eseguire samples[cls].requires_grad ottieni l'errore (dato che la lista non ha l'attributo requires_grad).\n",
    "    Per risolvere il problema devi iterare sui singoli campioni all'interno della lista per ciascuna classe. Ad esempio, sostituisci questo blocco:\n",
    "    \n",
    "    In questo modo, per ogni classe iteri su ciascun campione, calcoli la Grad-CAM e l'overlay, e li accumuli nelle rispettive liste \n",
    "    (cams_list e overlays_list). Successivamente potrai calcolare la media per ciascuna classe e utilizzarla per la visualizzazione.\n",
    "    Con questa modifica non otterrai più l'errore e la logica sarà coerente con l'obiettivo di aggregare i risultati su più campioni.\n",
    "    '''\n",
    "\n",
    "    '''CON MEDIA'''\n",
    "    cams_list = {0: [], 1: []}\n",
    "    overlays_list = {0: [], 1: []}\n",
    "\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        for sample_input in samples[cls]:\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''    \n",
    "            #sample_input = samples[cls]\n",
    "\n",
    "            sample_input.requires_grad = True  # Abilita il gradiente per il campione\n",
    "\n",
    "            #print(f\"\\033[1mSHAPE OF SAMPLE_INPUT: {sample_input.shape}\\033[0m\")\n",
    "\n",
    "            # Assicurati che sample_input sia nel formato atteso dal modello (come nel forward)\n",
    "            #if sample_input.dim() == 3:  # Supponiamo che abbia shape (batch, 45, 61)\n",
    "            #    sample_input = sample_input.permute(0, 2, 1).unsqueeze(3)  # Ora (batch, 61, 45, 1)\n",
    "\n",
    "\n",
    "            # Esegui forward pass per ottenere l'output del modello\n",
    "            output = model(sample_input)\n",
    "\n",
    "            # Se non viene specificata una classe target, seleziona quella predetta\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "\n",
    "            # Azzeramento dei gradienti e backward pass per la classe target\n",
    "            # Azzera i gradienti e fai backpropagation rispetto al punteggio della target_class\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "\n",
    "            #Passaggio 4: Computazione della mappa Grad-CAM\n",
    "\n",
    "            #Qui si calcola la mappa CAM:\n",
    "\n",
    "            #I pesi Grad-CAM sono la media dei gradienti lungo height & width.\n",
    "            #La mappa CAM è la somma pesata delle attivazioni del layer target.\n",
    "            #Si applica ReLU per eliminare i valori negativi.\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se abbiamo 128 feature map in conv3, il calcolo sarà:\n",
    "\n",
    "            #weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)  # (batch, 128, 1, 1)\n",
    "            #cam = torch.sum(weights * gradcam.activations, dim=1)  # (batch, height, width)\n",
    "\n",
    "            # -------------------------------\n",
    "            # Passaggio 4: Computazione della mappa Grad-CAM\n",
    "            # -------------------------------\n",
    "\n",
    "            # Calcola i pesi: media dei gradienti lungo le dimensioni spaziali (height e width)\n",
    "            weights = torch.mean(gradcam.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calcola la mappa CAM: somma pesata delle attivazioni\n",
    "            cam = torch.sum(weights * gradcam.activations, dim=1)\n",
    "\n",
    "            # Calcola la CAM: applica ReLU per eliminare i valori negativi\n",
    "            cam = F.relu(cam)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            TUTTO IL PASSAGGIO DELLO STEP 5 \n",
    "            \n",
    "            OSSIA NORMALIZZAZIONE i.e.,  NEL SENSO DI RISCALATURA NEL RANGE 0-1 + UPSAMPLING \n",
    "            \n",
    "            (CHE SERVIVA PER UNIFORMARE I VALORI E ADATTARSI ALLA DIMENSIONE DELLA IMMAGINE ORIGINALE \n",
    "            PER VEDERE UN SOLO ESEMPIO DELLA CLASSE RISPETTO ALLA MAPPA DI ATTIVAZIONE E ALL'OVERLAY\n",
    "            DEL GRADCAM RISPETTO ALLA IMMAGINE ORIGINALE)\n",
    "\n",
    "            #🔹 Esempio pratico:\n",
    "            #Se cam ha dimensione 16x16 e l'immagine originale è 64x64, viene interpolata per adattarsi.\n",
    "\n",
    "            \n",
    "            NON SERVE PIU', AD ECCEZIONE DI QUESTE RIGHE CHE ORA TI RIMETTO QUI SOTTO!'''\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            \n",
    "            ✅ Cosa fa correttamente questo codice:\n",
    "            \n",
    "            Estrae i campioni da test_loader separandoli in samples[0] e samples[1].\n",
    "            \n",
    "            Per ogni campione di ogni classe:\n",
    "            \n",
    "            Calcola la Grad-CAM raw (senza riscaling),\n",
    "            La interpola per adattarla alla dimensione originale (n_freq, n_time)\n",
    "            Applica ReLU per tenere solo le attivazioni positive (come da standard Grad-CAM)\n",
    "            La converte in NumPy e la salva in cams_list[cls].\n",
    "            \n",
    "            Alla fine, fa la media delle CAM raw per ciascuna classe:\n",
    "            \n",
    "            mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "            \n",
    "            🔍 Stato attuale del dato\n",
    "            \n",
    "            cams_list[cls] → lista di array cam 2D non normalizzati, uno per ogni trial.\n",
    "            mean_cams[cls] → array 2D (frequenza × tempo), media dei trial per ciascuna classe.\n",
    "            \n",
    "            La normalizzazione Z-score congiunta la farai dopo, sulla base di mean_cams.\n",
    "            \n",
    "            '''\n",
    "\n",
    "            target_size = (sample_input.shape[1], sample_input.shape[2])\n",
    "            cam = F.interpolate(cam.unsqueeze(1), size = target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "            \n",
    "            # squeeze \n",
    "            cam = cam.squeeze()                 # tensor 2D\n",
    "            \n",
    "            \n",
    "            # Infine sposti su CPU e passi a numpy\n",
    "            cam = cam.cpu().numpy()\n",
    "\n",
    "            '''SOLO UN ESEMPIO'''\n",
    "            #cams[cls] = cam\n",
    "\n",
    "            '''CON MEDIA'''\n",
    "            # Aggiungi la mappa del singolo esempio alla lista per la classe (per poi dopo farci la media dentro mean_cams!)\n",
    "            cams_list[cls].append(cam)\n",
    "    \n",
    "    # ============================================================\n",
    "    # Calcolo dello heatmap media dei valori (raw) per ciascuna classe\n",
    "    # ============================================================\n",
    "    \n",
    "    mean_cams = {}\n",
    "    \n",
    "    for cls in [0, 1]:\n",
    "        mean_cams[cls] = np.mean(np.array(cams_list[cls]), axis=0)\n",
    "\n",
    "\n",
    "    # =======================================================\n",
    "    # Calcolo dello spettrogramma medio (raw) per ciascuna classe\n",
    "    # =======================================================\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Cosa fa questo codice?\n",
    "    ✅ Calcola lo spettrogramma medio per ogni classe (0 e 1) prendendo i trials da samples e facendo la media sulla prima dimensione (batch).\n",
    "    ✅ Plotta lo spettrogramma medio nella quarta riga del grafico finale, con una colonna per ogni classe.\n",
    "    ✅ Usa una colormap jet per una migliore visualizzazione.\n",
    "    ✅ Evita errori: Se una classe non ha dati, non plotta nulla per quella colonna.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Sì, l'errore indica che stai cercando di convertire un tensore PyTorch che richiede il calcolo del gradiente\n",
    "    #in un array NumPy direttamente con .numpy(), cosa che non è permessa.\n",
    "    \n",
    "    \n",
    "    mean_raw_spectrograms = {}\n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        '''ATTENZIONE CHE QUESTO DIVENTA samples_raw'''\n",
    "        if len(samples_raw[cls]) > 0:\n",
    "            # Stacka tutti i trials per la classe e calcola la media sul batch (dimensione 0)\n",
    "            \n",
    "            mean_raw_spectrograms[cls] = torch.cat(samples_raw[cls], dim=0).mean(dim = 0).detach().cpu().numpy()\n",
    "        else:\n",
    "            mean_raw_spectrograms[cls] = None\n",
    "            \n",
    "            '''\n",
    "            Nel tuo caso l'input ha la forma:\n",
    "\n",
    "                (batch, 45, 61), dove:\n",
    "\n",
    "                45 rappresenta le frequenze (asse y)\n",
    "                61 rappresenta i canali (asse x)\n",
    "\n",
    "            Se vuoi visualizzare lo \"spettrogramma medio\" per ogni classe come un'immagine 2D (45 × 61), \n",
    "            devi mediare solo sul batch (cioè, sui trial) e lasciare intatte le dimensioni di frequenza e canali. \n",
    "            \n",
    "            Aver mediato anche sui canali (usando mean(dim=(0,1))) porterebbe a un vettore 1D (di forma (61,)), mentre quello che ti serve è una matrice 2D.\n",
    "\n",
    "            Quindi, la riga corretta per calcolare lo spettrogramma medio è:\n",
    "\n",
    "                mean_raw_spectrograms[cls] = torch.cat(samples[cls], dim=0).mean(dim=0).detach().cpu().numpy()\n",
    "            \n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "            SPIEGAZIONE\n",
    "            \n",
    "            In questo modo:\n",
    "\n",
    "                torch.cat(samples[cls], dim=0) concatena tutti i trial per quella classe, ottenendo un tensore di forma (num_trials, 45, 61).\n",
    "                .mean(dim=0) calcola la media lungo la dimensione del batch, restituendo un tensore di forma (45, 61).\n",
    "                Infine, .detach().cpu().numpy() converte il risultato in un array NumPy, adatto per imshow.\n",
    "            ************ ************ ************ ************ ************ ************ ************ ************ ************ ************\n",
    "\n",
    "            '''\n",
    "            \n",
    "    '''\n",
    "    # =======================================================\n",
    "    # Passaggio Finale: Creazione della figura finale\n",
    "    # Ora la figura ha 3 righe:\n",
    "    \n",
    "    #  - Riga 1: Istogramma della distribuzione dei valori della heatmap media per ciascuna classe \n",
    "    #            normalizzata rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 2: GradCAM medio della distribuzione dei valori della heatmap media per ogni classe, \n",
    "    #            a seguito della normalizzazione rispetto alla distribuzione congiunta!\n",
    "    \n",
    "    #  - Riga 3: Spettrogramma medio (raw) rispetto ai Trial della Stessa Classe, su range logaritmico \n",
    "    # =======================================================\n",
    "    \n",
    "    \n",
    "    Quando devo plottare l'istogramma dei valori di ogni heatmap media solamente (riga 3), \n",
    "    devo plottarli in base alla normalizzazione rispetto alla distribuzione congiunta.\n",
    "    \n",
    "    Quindi, devo plottarli in base al range minimo e massimo della intera distribuzione congiunta, quando è stata normalizzata!\n",
    "    Di conseguenza devo fare\n",
    "    \n",
    "    1) Prendere la Media delle CAM per ogni classe (già fatto)\n",
    "    2) Costruzione distribuzione congiunta raw\n",
    "    3) Calcolo Media e Deviazione Standard della Distribuzione Congiunta\n",
    "    4) Normalizzazione Z-score della Distribuzione Congiunta\n",
    "    \n",
    "    5) Prendo il range minimo e massimo della Distribuzione Congiunta Normalizzata\n",
    "    \n",
    "    Ossia, il range minimo e massimo su cui plottare entrambe le heatmap medie normalizzate in base alla distribuzione congiunta,\n",
    "    dovrà essere rispetto alla distribuzione congiunta a seguito della normalizzazione.\n",
    "    \n",
    "    Quindi, dovrei ricreare un'altra variabile che contiene i valori normalizzati di entrambe le distribuzioni assieme,\n",
    "    ossia una cosa del tipo\n",
    "    \n",
    "    normalized_all_vals = np.concatenate([normalized_mean_cams[0].flatten(), normalized_mean_cams[1].flatten()])\n",
    "    \n",
    "    e da questa prendere il minimo ed il massimo!\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Creiamo una figura con 4 righe e 2 colonne\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Overlay over EEG Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    #fig, axs = plt.subplots(4, 2, figsize=(12, 20))\n",
    "    #plt.suptitle(f\"Grad-CAM Mapping and Resulting Overlay over EEG Trial Spectrogram\\nExperimental Conditions: {exp_cond} - Subject: {category_subject}\", fontsize=15)\n",
    "    \n",
    "    # Creiamo una figura con 3 righe e 2 colonne\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    plt.suptitle(f\"Grad-CAM Mapping over EEG Trials\\nExperimental Conditions: {exp_cond}\", fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()  # Regola automaticamente la spaziatura globale\n",
    "    plt.subplots_adjust(hspace = 0.5, wspace = 0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Impostiamo l'estensione: x da 0 a 61 (canali) e y da 0 a 45 (frequenze)\n",
    "    extent = [0, 61, 0, 45]\n",
    "    \n",
    "    #extent = [0, 64, 0, 45]\n",
    "    \n",
    "    \n",
    "   \n",
    "    # PLOT RIGA 1: Visualizzazione degli istogrammi della distribuzione dei valori delle heatmap medie RAW\n",
    "    # RISPETTO ALLA DISTRIBUZIONE CONGIUNTA\n",
    "        \n",
    "    '''\n",
    "    Questi valori rappresentano la distribuzione delle attivazioni di entrambe le classi, \n",
    "    rispetto alla DISTRIBUZIONE CONGIUNTA\n",
    "    \n",
    "    Quindi si tratta di valori di attivazione della mappa Grad-CAM media rispetto alla distribuzione congiunta!\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Istogramma dei valori raw medi di ogni classe (su distribuzione congiunta!): Stai visualizzando un istogramma di questi valori medi, \n",
    "                                                           sulla distribuzione congiunta, ossia\n",
    "                                                           \n",
    "                                                           - prendo i valori (raw)delle heatmap media di entrambe le classi\n",
    "                                                           - calcolare la distribuzione congiunta dei valori (all_vals = ...)\n",
    "                                                           - ottengo quindi la nuova distribuzione congiunta dalle heatmap medie di entrambe le classi\n",
    "                                                          \n",
    "                                                           - calcolo minimo e massimo a seguito della normalizzazione (?) e non prima\n",
    "                                                           - faccio i plot di entrambe delle heatmap medie raw,\n",
    "                                                             ma rispetto a distribuzione congiunta\n",
    "                                                             \n",
    "                                                           \n",
    "                                                           Questo darebbe una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti tra le 2 classi (che ora son confrontabili!)\n",
    "                                                           a livello RAW!\n",
    "                                                        \n",
    "    \n",
    "    N.B. PER IL NOME DEL TITOLO DEL PLOT\n",
    "    \n",
    "    Perché \"Grad-CAM value\" può creare confusione:\n",
    "    Il termine \"Grad-CAM value\" potrebbe sembrare che faccia riferimento direttamente ai valori generati dalla mappa Grad-CAM finale. \n",
    "    Ma in realtà, i valori che stai trattando sono le attivazioni mediate e clippate, che formano la heatmap. \n",
    "    L'istogramma che stai tracciando rappresenta la distribuzione delle attivazioni prima della normalizzazione.\n",
    "\n",
    "    Riepilogo\n",
    "    Quindi, questi valori sono attivazioni pesate per ciascun pixel della mappa Grad-CAM, e mediati per classe. \n",
    "    Il processo di normalizzazione che segue (basato sui percentili) serve a enfatizzare il contrasto in modo da focalizzarsi sulle aree più significative \n",
    "    per la previsione.\n",
    "\n",
    "    Per rispondere alla tua domanda: sì, è corretto dire che stai visualizzando la distribuzione delle attivazioni pesate prima della normalizzazione\n",
    "    per migliorare il contrasto, ma è meglio riferirsi a questi valori come valori di attivazione della mappa Grad-CAM o valori della heatmap Grad-CAM, \n",
    "    piuttosto che \"Grad-CAM value\" che potrebbe risultare ambiguo.\n",
    "\n",
    "    Se vuoi, puoi anche aggiungere una nota nella visualizzazione dell'istogramma che chiarisca il processo:\n",
    "    \n",
    "    axs[2, j].set_title(f\"Histogram of Heatmap Activation Values (Raw, before Normalization) Class {condition_names[cls]}\", fontsize=12)\n",
    "    oppure\n",
    "    axs[2, j].set_title(f\"Histogram of Mean Heatmap Activation Values - Class {condition_names[cls]}\", fontsize=12)\n",
    "    \n",
    "    è molto chiara e corretta!\n",
    "    Indica perfettamente che stai visualizzando l'istogramma dei valori di attivazione medi della heatmap, \n",
    "    senza fare confusione sul fatto che si tratti di valori medi per ciascuna classe.\n",
    "    \n",
    "    In sintesi, questa frase comunica in modo preciso che stai mostrando la distribuzione delle attivazioni mediate dalla mappa Grad-CAM\n",
    "    per una specifica classe. Quindi sì, va benissimo!\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #PER PLOT RIGA 1 \n",
    "    \n",
    "    # Creo la distribuzione congiunta dei valori di ogni heatmap media RAW delle due classi, srotolando i valori di entrambe\n",
    "    all_vals_raw = np.concatenate([mean_cams[0].flatten(), mean_cams[1].flatten()])\n",
    "    \n",
    "    # Il range minimo e massimo su cui plottare entrambe le heatmap medie raw in base alla distribuzione congiunta (riga 3)\n",
    "    # dovrà essere rispetto alla distribuzione congiunta raw\n",
    "    \n",
    "    vmin_raw = all_vals_raw.min()\n",
    "    vmax_raw = all_vals_raw.max()\n",
    "    \n",
    "    \n",
    "    # Prima riga: Visualizza l'istogramma della heatmap media rispetto alla distribuzione congiunta!\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        # Calcola l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "        axs[0, j].hist(mean_cams[cls].flatten(), bins= 'auto', color='blue', edgecolor='black')\n",
    "        #axs[0, j].set_title(f\"Histogram of Mean Grad-CAM values (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].set_title(f\"Histogram of Mean Heatmap Activation Values (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        axs[0, j].set_xlabel(\"Grad-CAM value\", fontsize=10)\n",
    "        axs[0, j].set_ylabel(\"Frequency\", fontsize=10)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # PLOT RIGA 2: Visualizzazione dei valori delle heatmap medie delle due classi\n",
    "    # RISPETTO ALLA DISTRIBUZIONE CONGIUNTA, SU CUI VIENE FATTA LA NORMALIZZAZIONE\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Questi valori rappresentano le heatmap medie delle attivazioni di entrambe le classi, \n",
    "    rispetto alla DISTRIBUZIONE CONGIUNTA, SU CUI VIENE FATTA LA NORMALIZZAZIONE\n",
    "    \n",
    "    Quindi si tratta di valori di attivazione della mappa Grad-CAM media rispetto alla distribuzione congiunta NORMALIZZATA\n",
    "\n",
    "    Per chiarire meglio il processo:\n",
    "\n",
    "        1) Valori di attivazione: Quando si calcola la Grad-CAM, ottieni una mappa di attivazione per ciascun pixel. \n",
    "                               Questa mappa mostra quanto ciascun pixel contribuisce alla decisione del modello.\n",
    "                               Questi valori di attivazione sono pesati in base ai gradienti della classe di interesse.\n",
    "\n",
    "        2) Mediati per classe: Nel tuo caso, stai calcolando la media di queste attivazioni per ogni classe (ad esempio, classe 0 e classe 1). \n",
    "                            Questo processo permette di ottenere una rappresentazione complessiva di come la rete percepisce l'importanza di ogni pixel \n",
    "                            rispetto alla classe.\n",
    "\n",
    "        3) Calcolo la distribuzione congiunta dei valori raw medi di ogni classe (su distribuzione congiunta!): \n",
    "        Stai visualizzando un istogramma di questi valori medi, sulla DISTRIBUZIONE CONGIUNTA, ossia\n",
    "                                                           \n",
    "                                                           - prendo i valori (raw)delle heatmap media di entrambe le classi\n",
    "                                                           - calcolare la distribuzione congiunta dei valori (all_vals = ...)\n",
    "                                                           - ottengo quindi la nuova distribuzione congiunta dalle heatmap medie di entrambe le classi\n",
    "                                                           \n",
    "                                                           - calcolo media e deviazione standard delle distribuzione congiunta\n",
    "                                                           - faccio la normalizzazione della distribuzione congiunta\n",
    "                                                           \n",
    "                                                           - calcolo minimo e massimo a seguito della normalizzazione e non prima\n",
    "                                                             della distribuzione congiunta normalizzata\n",
    "                                                           \n",
    "                                                           - faccio i plot di entrambe delle heatmap medie normalizzate,\n",
    "                                                             ma rispetto alla distribuzione congiunta\n",
    "                                                             \n",
    "                                                           \n",
    "                                                           Questo darebbe una visione della distribuzione delle attivazioni,\n",
    "                                                           per capire come i valori siano distribuiti tra le 2 classi (che ora son confrontabili!)\n",
    "                                                           a livello NORMALIZZATO!\n",
    "                                                        \n",
    "    '''\n",
    "    \n",
    "    '''SOPRA ABBIAMO CREATO --> all_vals_raw'''\n",
    "    \n",
    "    # Creo la distribuzione congiunta dei valori di ogni heatmap media RAW delle due classi, srotolando i valori di entrambe\n",
    "    #all_vals_raw = np.concatenate([mean_cams[0].flatten(), mean_cams[1].flatten()])\n",
    "    \n",
    "    #Calcolo media e deviazione standard della distribuzione congiunta dei valori (raw) delle heatmap medie di entrambe le classi \n",
    "    #joint_mean = np.mean(all_vals_raw)\n",
    "    #joint_std = np.std(all_vals_raw)\n",
    "    \n",
    "    # Normalizzazione Z-score della distribuzione congiunta\n",
    "    #normalized_mean_cams = {}\n",
    "    \n",
    "    #for cls in [0, 1]:\n",
    "        #normalized_mean_cams[cls] = (mean_cams[cls] - joint_mean) / joint_std\n",
    "\n",
    "    # Il range minimo e massimo su cui plottare entrambe le heatmap medie normalizzate in base alla distribuzione congiunta (riga 3)\n",
    "    # dovrà essere rispetto alla distribuzione congiunta a seguito della normalizzazione\n",
    "    \n",
    "    #normalized_all_vals = np.concatenate([normalized_mean_cams[0].flatten(), normalized_mean_cams[1].flatten()])\n",
    "    \n",
    "    #vmin_normalized = normalized_all_vals.min()\n",
    "    #vmax_normalized = normalized_all_vals.max()\n",
    "    \n",
    "    vmin_normalized = all_vals_raw.min()\n",
    "    vmax_normalized = all_vals_raw.max()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # Opzione: normalizzazione robusta con percentili\n",
    "    vmin_normalized, vmax_normalized = np.percentile(all_vals_raw, [5, 95])\n",
    "    '''\n",
    "\n",
    "    # Seconda riga: Mean heatmap di ogni classe normalizzata a partire dalla distribuzione congiunta ( = di entrambe le classi)\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "    \n",
    "        \n",
    "        im = axs[1, j].imshow(\n",
    "            mean_cams[cls],\n",
    "            #normalized_mean_cams[cls], #QUI LA RENDO IN 2D, NON IN 1D COME PRIMA\n",
    "            #delta,\n",
    "            #cmap='seismic',\n",
    "            cmap='RdYlBu_r',\n",
    "            vmin= vmin_normalized, vmax= vmax_normalized,\n",
    "            #extent=[0, 64, 0, 45],\n",
    "            extent=[0, 61, 0, 45],\n",
    "            aspect='auto',\n",
    "            origin='lower'\n",
    "        )\n",
    "    \n",
    "        \n",
    "        # → calcola 6 tick equi-spaziati\n",
    "        ticks = np.linspace(vmin_normalized, vmax_normalized, 6)  \n",
    "        \n",
    "        cbar = fig.colorbar(\n",
    "            im,\n",
    "            ax=axs[1, j],\n",
    "            orientation='horizontal',\n",
    "            pad=0.12,\n",
    "            ticks=ticks)\n",
    "        \n",
    "        cbar.ax.set_xticklabels([f\"{t:.4f}\" for t in ticks])\n",
    "\n",
    "        axs[1, j].set_title(f\"Mean Grad-CAM Heatmap (Raw) - Class {condition_names[cls]}\", fontsize=12)\n",
    "        \n",
    "        '''QUESTA NON CONSENTE DEFINIZIONE ASSI!'''\n",
    "        #axs[1, j].axis('off')\n",
    "        \n",
    "        axs[1, j].axis('on') \n",
    "        axs[1,j].set_xlabel(\"EEG Channels\")\n",
    "        axs[1,j].set_ylabel(\"Frequency (Hz)\")\n",
    "        \n",
    "        # Calcola le posizioni in modo che il numero di tick corrisponda al numero di canali\n",
    "        # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "        if channel_names is not None and len(channel_names) == extent[1]:\n",
    "            num_channels = len(channel_names)\n",
    "            ticks = np.linspace(0.5, extent[1] - 1, num_channels)  # crea num_channels posizioni equidistanti\n",
    "\n",
    "            # Imposta i tick e le etichette\n",
    "            axs[1, j].set_xticks(ticks)\n",
    "            axs[1, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "        else:\n",
    "            axs[1, j].axis(\"off\")\n",
    "        \n",
    "    print(f\"\\033[1mRange heatmap raw globale (vmin_raw, vmax_raw): {vmin_normalized}, {vmax_normalized}\\033[0m\")\n",
    "    \n",
    "    # PLOT RIGA 3: Spettrogramma medio (raw) per ciascuna classe log-scaled\n",
    "    \n",
    "    '''\n",
    "    Spiegazione delle modifiche aggiunte:\n",
    "\n",
    "    1) Calcolo dello spettrogramma medio raw:\n",
    "\n",
    "    Dopo aver raccolto i campioni nel dizionario samples, viene creato il dizionario mean_raw_spectrograms.\n",
    "    Per ogni classe, i tensori vengono concatenati lungo la dimensione batch e si calcola la media sul batch (dim=0).\n",
    "    \n",
    "    Poi, però, ogni spettogramma medio deve congiunto in una distribuzione in modo da plottare poi il valore dello spettrogramma  \n",
    "    rispetto al minimo ed al massimo della distribuzione congiunta dello spettrogramma medio di entrambe le classi! \n",
    "    \n",
    "    Il risultato viene convertito in un array NumPy per il plotting.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Calcolo della distribuzione congiunta degli spettrogrammi medi delle due classi! \n",
    "    #all_vals_raw_samples = np.concatenate([mean_raw_spectrograms[0].flatten(), mean_raw_spectrograms[1].flatten()])\n",
    "    \n",
    "    '''SE VOLESSI RESTRINGERE TRA 5° e 95° PERCENTILE'''\n",
    "    #low_raw, high_raw = np.percentile(all_vals_raw, [5, 95])\n",
    "    #half_width_raw = max(abs(low_raw), abs(high_raw))   \n",
    "    #vmin_raw, vmax_raw = -half_width_raw, +half_width_raw\n",
    "    \n",
    "    '''ALTRIMENTI, TENGO TUTTO IL RANGE, DAL MINIMO AL MASSIMO'''\n",
    "    \n",
    "    #Ora qui prendo il miimo e massimo a partire dalla distribuzione congiunta!\n",
    "    #vmin_raw_samples, vmax_raw_samples = all_vals_raw_samples.min(), all_vals_raw_samples.max()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1) Qual è la differenza tra prima e ora?\n",
    "    \n",
    "    Prima calcolavo, dentro il for cls in [0,1], un nuovo vmin_raw_samples e vmax_raw_samples separatamente per ciascuna classe.\n",
    "    Di conseguenza ogni subplot sulla riga 3 aveva la sua scala di colori, rendendo impossibile un confronto diretto visivo \n",
    "    fra le due condizioni.\n",
    "    \n",
    "    Ora invece calcolerai una sola volta il log-power medio di entrambe le classi, ne ricavi un unico array congiunto,\n",
    "    quindi ne estrai un solo vmin e vmax. Questo ti garantisce che entrambi i subplot della riga 3 useranno la stessa scala di colori.\n",
    "\n",
    "\n",
    "    Per far sì che tutte e due le condizioni usino lo stesso minimo e massimo, sposto la raccolta dei limiti fuori dal ciclo,\n",
    "    usando la distribuzione congiunta dei log-power di entrambe le classi\n",
    "    \n",
    "    vmin_raw_samples e vmax_raw_samples li calcoli una volta sola, su tutti i valori logaritmici concatenati.\n",
    "    Entrambe le mappe usano esattamente lo stesso range, così le barre dei colori saranno allineate.\n",
    "    \n",
    "    Con questa modifica:\n",
    "\n",
    "    log_mean_power contiene già i valori in scala logaritmica.\n",
    "    vmin_raw_samples e vmax_raw_samples sono condivisi fra entrambe le colonne.\n",
    "    Ogni subplot userà la stessa “barretta” di colore, quindi potrai confrontare direttamente “deep blues” e “reds” delle due condizioni.\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # 1. Calcola i log-power medi per ciascuna classe\n",
    "    log_mean_power = {\n",
    "        cls: np.log1p(mean_raw_spectrograms[cls])\n",
    "        for cls in [0,1]\n",
    "    }\n",
    "\n",
    "    # 2. Raccogli TUTTI i valori in un unico array\n",
    "    all_log_vals = np.concatenate([\n",
    "        log_mean_power[0].flatten(),\n",
    "        log_mean_power[1].flatten()\n",
    "    ])\n",
    "\n",
    "    # 3. Estrai un unico vmin/vmax condiviso\n",
    "    vmin_raw_samples = all_log_vals.min()\n",
    "    vmax_raw_samples = all_log_vals.max()\n",
    "    \n",
    "    \n",
    "    # Se conosci i limiti temporali e di frequenza, puoi usare l'argomento extent\n",
    "    for j, cls in enumerate([0, 1]):\n",
    "        \n",
    "        #if mean_raw_spectrograms[cls] is not None:\n",
    "        if log_mean_power[cls] is not None:    \n",
    "            \n",
    "            #Trasformo in scala logaritmica i miei dati EEG sulla spettro medio di ogni classe\n",
    "            #mean_raw_spectrograms[cls] = np.log1p(mean_raw_spectrograms[cls])\n",
    "            \n",
    "            # Calcolo della distribuzione congiunta degli spettrogrammi medi delle due classi! \n",
    "            #all_vals_raw_samples = np.concatenate([mean_raw_spectrograms[0].flatten(), mean_raw_spectrograms[1].flatten()])\n",
    "            \n",
    "            #Ora qui prendo il miimo e massimo a partire dalla distribuzione congiunta!\n",
    "            #vmin_raw_samples, vmax_raw_samples = all_vals_raw_samples.min(), all_vals_raw_samples.max()\n",
    "    \n",
    "            \n",
    "            im = axs[2, j].imshow(log_mean_power[cls],\n",
    "                                  #mean_raw_spectrograms[cls], \n",
    "                                  extent= extent,\n",
    "                                  aspect='auto', \n",
    "                                  cmap='jet', \n",
    "                                  vmin = vmin_raw_samples, vmax = vmax_raw_samples,\n",
    "                                  origin='lower')\n",
    "            \n",
    "            axs[2, j].set_title(f\"Log-Scaled Mean Raw Spectrogram - Class {condition_names[cls]}\", fontsize=12)\n",
    "            axs[2, j].set_xlabel(\"EEG Channels\", fontsize=10)\n",
    "            axs[2, j].set_ylabel(\"Frequency (Hz)\", fontsize=10)\n",
    "            \n",
    "        \n",
    "            '''\n",
    "            ATTENZIONE QUI CHE C'ERA UN GRAVE ERRORE\n",
    "            \n",
    "            --> fig.colorbar(im, ax=axs[3, j]) \n",
    "            \n",
    "            #Qui la Color Bar Verticale sarebbe \n",
    "            #scala dello spettrogramma raw, finita per sbaglio sul Δ-GradCAM perché hai scritto ax=axs[3,j] invece di ax=axs[4,j].\n",
    "            \n",
    "            \n",
    "            La barra VERTICALE (CHE DOVEVA STAR NELLA 5° RIGA!!!!) della color bar accanto alla heatmap ti sta mostrando\n",
    "            \n",
    "            i VALORI ASSOLUTI della Grad-CAM (nel tuo caso non normalizzati, quindi scala di milioni --> variabile hist_data\n",
    "            ossia l'istogramma dei valori della heatmap media (prima della normalizzazione robusta)\n",
    "            '''\n",
    "    \n",
    "            fig.colorbar(im, ax=axs[2, j])\n",
    "            \n",
    "            axs[2, j].axis('on')\n",
    "            \n",
    "            # Calcola le posizioni in modo che il numero di tick corrisponda al numero di canali\n",
    "            # Se sono disponibili i nomi dei canali, impostiamo le xticks:\n",
    "            if channel_names is not None and len(channel_names) == extent[1]:\n",
    "                num_channels = len(channel_names)\n",
    "                ticks = np.linspace(0.5, extent[1] - 1, num_channels)  # crea num_channels posizioni equidistanti\n",
    "\n",
    "                # Imposta i tick e le etichette\n",
    "                axs[2, j].set_xticks(ticks)\n",
    "                axs[2, j].set_xticklabels(channel_names, rotation=90, fontsize=6)\n",
    "        else:\n",
    "            axs[2, j].axis(\"off\")\n",
    "            \n",
    "            \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ef5f5-b792-449e-a127-c2e6fcf1108b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                                                        NEW VERSION 26/07/2025\n",
    "                                                                        \n",
    "                                                                \n",
    "                                                                        \n",
    "                                                                  \n",
    "                                                                    VERSION FREQUENCY x CHANNELS\n",
    "                                                                            \n",
    "                                                                     ****** PER GRID 2D! ******\n",
    "                                                                      ****** MULTI BAND******\n",
    "                                                                      \n",
    "                                                                      PER CONVOLUZIONE 3D (PURA)\n",
    "                                                                              +\n",
    "                                                                      PER CONVOLUZIONI SEPARABILI\n",
    "                                                                      \n",
    "                                                              \n",
    "                                                                CON VALORI MEAN GRADCAM e MEAN RAW POWER\n",
    "                                                                            SU STESSA SCALA\n",
    "                                                                PER OGNI CLASSE E BANDA DI FREQUENZA!\n",
    "                                                                \n",
    "                                                                        ^^^^^SENZA COMMENTI^^^^^\n",
    "                                                                        ^^^^^            ^^^^^\n",
    "                                                                        \n",
    "                                                                        \n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "SINTESI DELLA FUNZIONE check_negative_residuals\n",
    "\n",
    "\n",
    "| Blocco                          | Scopo                                                                                                                                                                                                                                                                                                                                                                                                                                         | Perché serve                                                                                                                                                                                                                                                                                                                                                                                   |\n",
    "| ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **1. eps / “dynamic tol”**      | Calcola una soglia *dinamica* sotto la quale i valori negativi sono quasi certamente puro rumore numerico.<br><br>`python<br>eps32 = np.finfo(np.float32).eps  # ≈ 1.19e‑7<br>dynamic_tol = -eps32 * max(arr.max(), 1.0)`                                                                                                                                                                                                                     | *machine‑epsilon* (ε) è l’errore di arrotondamento massimo relativo per `float32` vicino a 1.<br>Moltiplicandolo per il massimo **positivo** trovato nella mappa definiamo una “fascia di tolleranza” proporzionale alla scala reale del dato. Tutto ciò che cade **sotto** `dynamic_tol` è troppo piccolo perché sia fisico — è, con ottima probabilità, soltanto rumore di rappresentazione. |\n",
    "| **2. Negativi “significativi”** | Conta e, se ce ne sono, stampa quante celle sono < `dynamic_tol`, quanto è il minimo osservato e la tolleranza stessa.                                                                                                                                                                                                                                                                                                                        | Ti permette di capire a colpo d’occhio se il preprocessing ha generato valori negativi (che non dovrebbero esistere in potenza) che **superano** il rumore ammesso.                                                                                                                                                                                                                            |\n",
    "| **3. Log‑hint**                 | Se *non* ci sono negativi significativi, calcola il **dynamic range** della mappa (max/min **> 0**) e decide se suggerire la scala log.<br><br>`python<br>positive = arr[arr > 0]<br>if positive.size == 0:<br>    ratio = np.inf          # tutto zero<br>else:<br>    min_pos = positive.min()<br>    max_pos = positive.max()<br>    ratio   = max_pos / max(min_pos, 1e‑12)<br>note = \"LOG consigliato\" if ratio > 1e3 else \"lineare ok\"` | – Ignoriamo gli zeri: la scala log non li supporta.<br>– Aggiungiamo un “cuscinetto” di `1e‑12` per evitare div/0.<br>– Se il range è > 10³ (tre ordini di grandezza ≃ “la potenza massima è **> 1000 ×** la minima”), la lettura lineare diventa poco informativa → meglio log10.                                                                                                             |\n",
    "\n",
    "\n",
    "\n",
    "In sintesi\n",
    "\n",
    "Prima parte → caccia ai negativi “numeric‑noise” tramite ε.\n",
    "\n",
    "Seconda parte → valuta solo i valori positivi e suggerisce log‑scale quando il dynamic‑range supera ~3 decadi (≈ 10³).\n",
    "\n",
    "\n",
    "Quando il codice passa in log‑scale?\n",
    "Raccogli tutte le mappe (di entrambe le classi e di tutte le bande)\n",
    "\n",
    "\n",
    "all_mean_pow = np.concatenate([...])\n",
    "\n",
    "Filtra i positivi e trova vmin_pow (con un 10 % di margine per non “appiattire” il minimo nella color‑bar)\n",
    "\n",
    "positive_vals = all_mean_pow[all_mean_pow > 0]\n",
    "vmin_pow = positive_vals.min() * 0.9 if positive_vals.size else 1e‑12\n",
    "vmax_pow = all_mean_pow.max()\n",
    "\n",
    "Decidi\n",
    "\n",
    "use_log = vmax_pow / max(vmin_pow, 1e‑12) > 1e3\n",
    "Se la potenza massima è > 1000 × la minima positiva, usare LogNorm.\n",
    "\n",
    "Come si spiega “tre ordini di grandezza”?\n",
    "\n",
    "“Il massimo è mille volte il minimo”.\n",
    "“Dynamic‑range di 3 decadi”.\n",
    "Oppure “max/min > 10³”.\n",
    "\n",
    "\n",
    "\n",
    "Riassunto finale\n",
    "ε: misura il rumore di quantizzazione, ti dice quando un (piccolo) negativo è solo un effetto di arrotondamento.\n",
    "\n",
    "Dynamic‑range: se la banda ha valori reali che variano più di 10³ ×, la scala log10 rende le differenze leggibili senza “schiacciare” i dettagli bassi.\n",
    "\n",
    "La funzione: un’unica utility per\n",
    "\n",
    "diagnosticare residui numerici,\n",
    "\n",
    "suggerire in automatico la rappresentazione (lineare / log) più sensata per i tuoi plot di potenza.\n",
    "\n",
    "Così il flusso diventa:\n",
    "\n",
    "\n",
    "check_negative_residuals(...)   # → log “SCALA LOG consigliata”\n",
    "↓\n",
    "use_log = True                  # ratio > 1e3\n",
    "↓\n",
    "plot con LogNorm + LogLocator   # color‑bar pulita, dettagli visibili\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Perché vedi ratio = inf\n",
    "* Nelle bande Beta e Gamma la tua mappa media è tutta a zero (o comunque tutti i valori ≤ ε).\n",
    "* Con soli zeri il vettore positive = arr[arr > 0] è vuoto, quindi lo tratto come “dynamic‑range infinito” per evitare la divisione per 0.\n",
    "* In realtà non hai un’«escursione infinita»; semplicemente non hai segnale in quelle bande → la scala log non aggiungerebbe nulla.\n",
    "\n",
    "Se vuoi evitare quel suggerimento “falso‑positivo”, basta cambiare la logica così (l’ho già indicato ma lo riscrivo compatto):\n",
    "\n",
    "positive = arr[arr > 0]\n",
    "if positive.size < 2:          # 0 o 1 valore positivo → niente dinamica utile\n",
    "    ratio = 0                  # forza il consiglio a “lineare”\n",
    "else:\n",
    "    ratio = positive.max() / max(positive.min(), 1e-12)\n",
    "    \n",
    "    \n",
    "Linear vs log: quale usare davvero?\n",
    "Banda\tDynamic‑range (≈ max/min)\tScala consigliabile\n",
    "Delta ‑ Theta\t3‑11 ×\tLineare: già leggibile.\n",
    "Alpha\t12 ×\tAncora lineare (o log, ma non cambia molto).\n",
    "Beta – Gamma\t0 (tutti zeri)\tLog inutile: non c’è potenza da mostrare.\n",
    "\n",
    "Di conseguenza:\n",
    "\n",
    "Mantieni la scala lineare globale come nel tuo blocco finale.\n",
    "\n",
    "Se in altri dataset vedrai rapporti > 1 000 con almeno 2 valori positivi, allora attiva la parte LogNorm.\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def check_negative_residuals(band_names, tensor_dict, tag, log_hint=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    • band_names   : lista di stringhe  (lunghezza = n_bands)\n",
    "    • tensor_dict  : {cls: [np.ndarray(H,W), … n_bands]} --> dict  {cls: [np.ndarray(H,W), … 5 bande]}\n",
    "    • tag          : prefisso stampato nel log --> string visualizzato nel log\n",
    "    • log_hint     : se True mostra il rapporto max/min ⇒ aiuta a decidere\n",
    "                     se usare la scala log nei plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #https://numpy.org/doc/2.1/reference/generated/numpy.finfo.html\n",
    "    eps32 = np.finfo(np.float32).eps        # 1.19e‑7\n",
    "    for cls in [0, 1]:\n",
    "        for b, b_name in enumerate(band_names):\n",
    "            arr   = tensor_dict[cls][b]\n",
    "            \n",
    "            # Soglia dinamica = −eps * valore_massimo_della_mappa\n",
    "            \n",
    "            #dynamic_tol = -eps32 * arr.max()      # tolleranza dinamica\n",
    "            \n",
    "            dynamic_tol = -eps32 * max(arr.max(), 1.0)   # evita max==0\n",
    "            \n",
    "            neg_mask  = arr < dynamic_tol # “negativi significativi”\n",
    "            \n",
    "            min_v, max_v = float(arr.min()), float(arr.max())\n",
    "            \n",
    "            if np.any(neg_mask):\n",
    "                print(f\"Valori sotto la soglia per classe e banda:\\n\")\n",
    "                \n",
    "                #forma\tcosa fa\tquando differisce\n",
    "                #np.count_nonzero(neg_mask)\tconverte il bool‑array in int (True→1, False→0) e somma gli 1\tè sempre un intero Python\n",
    "                \n",
    "                #neg_mask.sum()\tchiama il metodo .sum() dell’ndarray; \n",
    "                #per tipo bool fa esattamente la stessa somma di sopra\trestituisce uno numpy.int_ (stesso valore, differente tipo)\n",
    "                \n",
    "                n_neg   = np.count_nonzero(neg_mask)\n",
    "                \n",
    "                min_val = arr.min()\n",
    "                \n",
    "                print(f\"[{tag} {b_name}] class={cls}  band={b_name:<6}  \"\n",
    "                      f\"neg={n_neg}  min={min_val:.3e}  tol={dynamic_tol:.3e}\")\n",
    "            else:\n",
    "                print(f\"Nessun valore sotto la soglia per classe {cls} e banda {b_name}\\n\")\n",
    "                print(f\"Definisco il range minimo e massimo per classe {cls} e banda{b_name} :\\n\")\n",
    "                \n",
    "                # ‑‑ opzionale: suggerimento scala log\n",
    "                if log_hint:                    \n",
    "                    #if max_v == 0 or min_v == 0:\n",
    "                    positive = arr[arr > 0]           # considera solo i valori > 0\n",
    "                    \n",
    "                    #Se il primo valore della potenza è proprio 0:\n",
    "                    #per evitare problemi di NaN lo impongo ad infinito\n",
    "                    if positive.size == 0:\n",
    "                        ratio = float('inf')          # tutto zero → range “infinito”\n",
    "                    \n",
    "                    #Se il primo valore della potenza è proprio 0:\n",
    "                    #per evitare problemi di NaN lo impongo ad infinito\n",
    "                    else:\n",
    "                        min_pos = positive.min()\n",
    "                        max_pos = positive.max()\n",
    "                        ratio   = max_pos / max(min_pos, 1e-12)\n",
    "                    note = f\"\\033[1mSCALA LOG per plots consigliata\\033[0m\" if ratio > 1e3 else f\"\\033[1mSCALA LINEARE per plots consigliata ok\\033[0m\"\n",
    "                    print(f\"        dynamic‑range ≈ {ratio:8.1f}  → {note}\")\n",
    "            print()\n",
    "                \n",
    "                \n",
    "                \n",
    "import torch.nn as nn\n",
    "\n",
    "def model_has_cudnn_rnn(model):\n",
    "    \"\"\"Ritorna True se il modello usa LSTM/GRU/RNN supportati da CuDNN.\"\"\"\n",
    "    return any(isinstance(m, (nn.LSTM, nn.GRU, nn.RNN)) for m in model.modules())\n",
    "\n",
    "\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import (LogLocator, LogFormatterMathtext,\n",
    "                               ScalarFormatter)\n",
    "\n",
    "\n",
    "'''RICORDATI: aggiunto parametro TEST_LOADER_RAW per i plots della POTENZA SPETTRALE MEDIA PER BANDA (i.e., test_loader_raw)'''\n",
    "def compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device, channel_names=None, debug = False):\n",
    "    \n",
    "    \n",
    "    '''SOLO PER I MODELLI OTTIMIZZATI CON ANCHE LA LSTM'''\n",
    "    \n",
    "    #Solo i modelli con LSTM entrano in questo giro; gli altri non cambiano di stato.\n",
    "    #Con questa sequenza:\n",
    "    #non ottieni più l’errore “cudnn RNN backward…”;\n",
    "    #la rete “si comporta” come in eval (Dropout off, BN congelato) mentre calcoli le CAM;\n",
    "    #l’ambiente di chiamata (il tuo loop di testing) riceve il modello esattamente nello stato in cui l’aveva passato alla funzione compute_gradcam_figure\n",
    "    \n",
    "\n",
    "    ### Perché serve model.train() anche se la CAM è presa prima della LSTM\n",
    "    \n",
    "    #Il backward, per arrivare dal loss (o dal logit scelto) fino al tuo layer conv3, deve comunque attraversare l’LSTM che sta più avanti nella rete.\n",
    "    #Le implementazioni CuDNN degli RNN (LSTM/GRU) alzano un’eccezione se provi a chiamare tensor.backward() mentre il modulo è in modalità eval().\n",
    "    #RuntimeError: cudnn RNN backward can only be called in training mode\n",
    "    #Quindi, anche se la CAM è calcolata su conv3, devi mettere l’intero modello in train() per il tempo del backward.\n",
    "    #condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    ### Che cos’è model.training\n",
    "    \n",
    "    #model.training è un semplice flag booleano (impostato da nn.Module.train() / nn.Module.eval()), ereditato da tutti i sotto‑moduli.\n",
    "    #Con was_training = model.training ricordi in che stato era il modello (quasi sempre False, cioè eval, nel tuo flusso)\n",
    "    #per poterlo ripristinare dopo.\n",
    "    \n",
    "    #Facendo così\n",
    "    \n",
    "    #for m in model.modules():\n",
    "    #if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n",
    "                      #nn.Dropout, nn.Dropout2d, nn.Dropout3d)):\n",
    "        #if m.training:         # cioè erano in train\n",
    "            #m.eval()\n",
    "            #frozen_layers.append(m)\n",
    "    \n",
    "    #Li sposti in eval uno per uno, senza toccare il resto della rete che deve restare in train() per far funzionare CuDNN‑RNN.\n",
    "    \n",
    "    \n",
    "    ### Perché, a fine blocco, servono due ripristini\n",
    "    \n",
    "    #1) Riattivo i BatchNorm / Dropout che avevo forzato in eval:\n",
    "    \n",
    "    #for m in frozen_layers:\n",
    "        #m.train()              # torna come prima\n",
    "    \n",
    "    #2) Riporto l’intero modello nello stato in cui si trovava prima del Grad‑CAM:\n",
    "    \n",
    "    #model.train(was_training)  # se era eval() torna eval, altrimenti resta train\n",
    "    \n",
    "    #Se non facessi il punto 1, lasceresti quei moduli permanentemente in eval anche quando, più tardi, \n",
    "    #rientri in training (per esempio in un fine‑tuning).\n",
    "    #Se non facessi il punto 2, lasceresti tutto il modello in train → dropout attivo, BN che accumula statistiche, ecc.\n",
    "\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    # ❶ — se serve, abilito temporaneamente la modalità train per il modello ottimizzato che aveva ANCHE la LSTM... \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    \n",
    "    needs_train_mode = model_has_cudnn_rnn(model)\n",
    "    \n",
    "    if needs_train_mode:\n",
    "        was_training = model.training      # salvo lo stato\n",
    "        model.train()                      # abilito backward su CuDNN‑RNN\n",
    "        \n",
    "        # ➊ salvo lo stato di OGNI BN/Dropout\n",
    "        \n",
    "        saved = [(m, m.training) for m in model.modules()\n",
    "             if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n",
    "                               nn.Dropout, nn.Dropout2d, nn.Dropout3d))]\n",
    "        \n",
    "        model.train()                              # abilita backward su CuDNN‑RNN\n",
    "        \n",
    "        # ➋ congelo in ogni layer della rete gli strati di BatchNorm e Dropout\n",
    "        for m, _ in saved:\n",
    "            m.eval()\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # ❷ — QUI sotto metti tutto il tuo codice Grad‑CAM\n",
    "    #      (forward, backward, costruzione delle mappe, plot, …)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # … il tuo lunghissimo corpo della funzione rimane invariato …\n",
    "    # → al momento di fare backward NON avrà più l’eccezione\n",
    "    #   “cudnn RNN backward can only be called in training mode”\n",
    "\n",
    "    \n",
    "    target_layer = model.conv2b #model.conv3\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Determina il target layer in base al tipo di modello\n",
    "    #if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        #target_layer = model.dw_conv1  # Per il modello separabile 2D\n",
    "    #else:\n",
    "        #target_layer = model.conv3  # Per il modello CNN3D\n",
    "        \n",
    "\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    # ✅ Raccogli TUTTI i campioni per ciascuna classe\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    \n",
    "    \n",
    "    #PER IL CASO CONV 3D\n",
    "    \n",
    "    # Ogni mio sample è 3D, perché infatti è fatto per convoluzione 3d pura o convoluzioni separabili, \n",
    "    # Quindi ha shape  (B, C, D, H, W), dove:  \n",
    "    \n",
    "    #B = batch (in questo caso, per ogni singolo esempio quindi sarà 1 -> singolo esempio alla volta)\n",
    "    #C = feature maps/canali (numero di feature maps estratte dalla convoluzione, o meglio anche noti come  canali convoluzionali)\n",
    "    #D = depth (la dimensione di profondità del mio tensore --> 5 ossia, la potenza spettrale ad ogni banda di frequenza - i.e.,  delta, theta, alfa, beta e gamma)\n",
    "    #H = height (altezza, prima dimensione SPAZIALE del mio tensore i.e., altezza griglia, canali EEG) \n",
    "    #W = width (larghezza, seconda dimensione SPAZIALE del mio tensore i.e., larghezza griglia, canali EEG)\n",
    "    \n",
    "    \n",
    "    '''SHAPE DEI DATI ORIGINALE SAREBBE (B, 9, 9, 5)'''\n",
    "    samples = {0: [], 1: []}\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                '''OSSIA QUI DIVENTA (1, 9, 9, 5)'''\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "    \n",
    "    '''TEST_LOADER RAW (B, 9, 9, 5)'''\n",
    "    samples_raw = {0: [], 1: []}\n",
    "    for inputs, labels in test_loader_raw:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples_raw:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                '''OSSIA QUI DIVENTA (1, 9, 9, 5)'''\n",
    "                samples_raw[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "    # ============================================================\n",
    "    # Calcolo delle Grad-CAM per ogni singola banda di frequenza\n",
    "    # ============================================================\n",
    "    \n",
    "    n_bands = 5  # numero di canali/bande di frequenza\n",
    "    band_names = ['Delta (δ)', 'Theta (Θ)', 'Alpha (α)', 'Beta (β) ', 'Gamma (γ)']  \n",
    "    \n",
    "    \n",
    "    '''STRUTTURE DATI PER CONV3D'''\n",
    "    #✅ Struttura per il GradCAM 3D, ossia qui raccolgo il GradCAM 3D di ogni esempio\n",
    "    # quindi la mappa di attivazione che identifica l'attivazioni più rilevanti per la classificazione di una esemplare di una certa classe\n",
    "    # sia spazialmente (height and width, ossia le dimensioni spaziali del mio tensore)\n",
    "    # sia frequenzialmente (depth), ossia le attivazioni più rilevanti in base alla banda di frequenza\n",
    "    \n",
    "    global_cams_3d = {0: [], 1: []} # shape (D, 9, 9)\n",
    "    \n",
    "    #Poi qui abbiamo: \n",
    "\n",
    "    # ✅ Struttura: classe → banda → immagini raw di input filtrate per singola banda (senza passare dal modello)\n",
    "    #tutte le mappe di potenza per la classe cls nella banda b-esima\n",
    "    \n",
    "    #La struttura per salvare invece lo potenza spettrale media per ogni relativa banda \n",
    "    raw_power_per_band_3d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    #La struttura che salverà la \"fetta\" del gradcam3D, ossia dove plotto solo la fetta della banda a partire dal global_cam_3d\n",
    "    #Ossia per ogni esempio di una specifica classe, prenderò la mappa di attivazione spazialmente più rilevante, in base alla specifica banda di frequenza indagata\n",
    "\n",
    "    #✅ Struttura: classe → banda → lista CAM\n",
    "    #cams_per_band_3d[cls][banda]: la slice D-esima (ossia la slice frequenziale) della mappa GradCAM per ogni campione di classe cls.\n",
    "    \n",
    "    cams_per_band_3d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    '''STRUTTURE DATI PER CONV SEPARABLE'''\n",
    "\n",
    "    \n",
    "    global_cams_2d = {0: [], 1: []} \n",
    "    \n",
    "    raw_power_per_band_2d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    cams_per_band_2d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Da qui in giù, vado a eseguire i passaggi essenziali per:\n",
    "    \n",
    "    1) Calcolare il GradCAM 3D per ogni esempio,\n",
    "    2) Isolare le fette (slice) per banda,\n",
    "    3) Raccogliere le immagini di potenza raw per ogni banda,\n",
    "    4) Calcolare le medie per classe e banda.\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        for sample_input, sample_input_raw in zip(samples[cls], samples_raw[cls]):\n",
    "            \n",
    "            # 1) Preparo il sample per il calcolo\n",
    "            sample_input = sample_input.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            Devo far squeeze perché essendo prelevati dal test loader, li avevo già resi con un 1 davanti ossia qui sopra\n",
    "\n",
    "            samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "            \n",
    "            perché io, è vero che prendo i dati dal test_loader che son in formato batch (batch, D, H, W)\n",
    "            ma siccome poi prelevo ogni singolo esempio per metterli dentro a \"samples\", allora quella dimensione di batch si leva nel dizionario \"samples\".\n",
    "            \n",
    "            Per cui, quando voglio prendermi il singolo esempio per salvarmelo dentro a \"raw_vol\" (ossia la potenza spettrale del volume proprio di ogni esempio),\n",
    "            allora devo ri-assegnare ad ogni esempio la dimensione del batch (ossia 1, ossia il singolo esempio)\n",
    "            quando li salvo dentro samples. \n",
    "            \n",
    "            Ed infatti, quindi quell' \".unsqueeze(0)\" --> samples[label_int].append(inputs[i].unsqueeze(0)) serve proprio a questo...\n",
    "\n",
    "            Quando poi, devo prendere il singolo esempio da salvare, nei termini di potenza spettrale di ogni esempio (come volume 3D), \n",
    "            allora devo rifare .squeeze(), per togliere nuovamente la dimensione del batch (ossia 1, ossia il singolo esempio)\n",
    "            perché la rappresentazione del singolo esempio è, appunto, composta da, le sole dimensioni che costituiscono proprio il singolo esempio,\n",
    "            ossia 9x9x5 -->  ossia la griglia 3D !\n",
    "\n",
    "            E quindi, per ogni esempio, mi salvo la griglia 3d, ossia\n",
    "            \"raw_vol = sample_input.detach().cpu().numpy().squeeze()   # → (9, 9, 5)\"\n",
    "            \n",
    "            E poi, la divido però per banda \n",
    "            \n",
    "            \"for b in range(n_bands):\n",
    "                raw_power_per_band_3d[cls][b].append(raw_vol[:, :, b])\n",
    "            \"\n",
    "            '''\n",
    "            \n",
    "            # 2) Subito qui prendo la potenza raw del volume 9×9×5, senza passare dal modello!\n",
    "            #raw_vol[:, :, b] è una mappa 2D (9, 9) della potenza spettrale per la banda b per ogni singolo esempio\n",
    "            \n",
    "            raw_vol = sample_input_raw.detach().cpu().numpy().squeeze()     # → (9, 9, 5)\n",
    "            \n",
    "            '''CASTING IN FLOAT64'''\n",
    "            #raw_vol = sample_input.detach().cpu().numpy().squeeze().astype(np.float64)         # 🔹 cast a float64   # → (9, 9, 5)\n",
    "            \n",
    "            for b in range(n_bands):\n",
    "                \n",
    "                \n",
    "                # Determina in base al tipo di modello i dati e le shape da salvare:\n",
    "                \n",
    "                #if isinstance(model, CNN3D_LSTM_FC):\n",
    "                #if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "                \n",
    "                    #target_layer = model.dw_conv1  # Per il modello separabile 2D\n",
    "                #else:\n",
    "                    #target_layer = model.conv3  # Per il modello CNN3D\n",
    "                    \n",
    "                if isinstance(model, CNN3D_LSTM_FC):\n",
    "                    \n",
    "                    #raw_power_per_band_3d invece raccoglie TUTTE le mappe 2D (9, 9) di ogni singolo esempio che conterrà la potenza spettrale alla stessa banda b \n",
    "                    #Lista di mappe di potenza 2D (una per trial) per la relativa banda\n",
    "\n",
    "                    raw_power_per_band_3d[cls][b].append(raw_vol[:, :, b])\n",
    "                    \n",
    "                elif isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "                    raw_power_per_band_2d[cls][b].append(raw_vol[:,:,b])\n",
    "\n",
    "            \n",
    "            # 2) Esegui il forward pass\n",
    "            output = model(sample_input)\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "            \n",
    "            # 3) Esegui il backward pass\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "            \n",
    "            # 4) Preleva attivazioni e gradienti\n",
    "            activ = gradcam.activations   # shape può essere 5D (B,C,D,H,W) per CNN3D o 4D (B, C, H, W) per CNN Separable\n",
    "            grads = gradcam.gradients # shape può essere 5D (B,C,D,H,W) per CNN3D o 4D (B, C, H, W) per CNN Separable\n",
    "            \n",
    "            #Nel caso 3D dovrebbe essere\n",
    "\n",
    "            #Media dei gradienti solo su H,W → (B,C,D,1,1)\n",
    "            #w3d = torch.mean(grads, dim=(3, 4))\n",
    "\n",
    "            # b) Sommo sui canali → (B,D,H,W)\n",
    "            #cam3d = F.relu(torch.sum(w3d * activ, dim=1))\n",
    "            \n",
    "            #e così la shape finale sarebbe 3D con (B,C,D)\n",
    "            \n",
    "            #✔️ w3d è correttamente calcolato per ogni (B, C, D, 1, 1)\n",
    "            #✔️ La somma su dim=1 aggrega le feature maps con pesi per ogni banda\n",
    "            #✔️ ReLU rimuove componenti negative\n",
    "            \n",
    "            '''\n",
    "            Nel caso della CNN3D calcoli una mappa Grad-CAM 3D \"globale\" direttamente dal layer convoluzionale 3D, ottenendo attivazioni di shape \n",
    "            (B,C,D,H,W) e quindi una CAM volumetrica per ogni trial\n",
    "            \n",
    "            CNN3D → calcoli una CAM 3D da attivazioni (B,C,D,H,W), una volta sola per ogni esempio.\n",
    "\n",
    "            '''\n",
    "            \n",
    "            if activ.ndim == 5:  # Caso per modello CNN3D pura \n",
    "                \n",
    "                # 3D Volumetric Grad-CAM\n",
    "                \n",
    "                # a) Media dei gradienti solo su H,W → (B,C,D,1,1)\n",
    "                w3d = torch.mean(grads, dim=(3, 4), keepdim=True)\n",
    "\n",
    "                # b) Sommo sui canali (feature maps) → (B,D,H,W)\n",
    "                cam3d = F.relu(torch.sum(w3d * activ, dim=1))\n",
    "                \n",
    "                # c) Upsample H×W, mantenendo D intatto\n",
    "                B, D, H, W = cam3d.shape\n",
    "                cam_flat = cam3d.view(B*D, 1, H, W)\n",
    "                cam_up   = F.interpolate(cam_flat,\n",
    "                                         size=(9, 9),\n",
    "                                         mode='bilinear',\n",
    "                                         align_corners=False)\n",
    "                \n",
    "                cam_vol  = cam_up.view(B, D, 9, 9).cpu().numpy()\n",
    "                \n",
    "                # d) Prendi ogni batch-item\n",
    "                \n",
    "                '''\n",
    "                Quindi qui ottengo che:\n",
    "            \n",
    "                1) appendo a global_cams_3d che cosa qui? il gradcam 3D ossia la mappa di attivazione di volume,\n",
    "                ossia OGNI esempio (volumetrico) per ogni classe (ossia 9x9x5 ancora, di OGNI esempio)\n",
    "                \n",
    "                Quindi semplicemente anziché rendere il dato come 'batch, D, H, W'.. siccome prendiamo ogni esempio UNO ALLA VOLTA\n",
    "                è inutile mantenere la dimensione batch (che sarebbe sempre 1, perché parliamo di ogni esempio, uno alla volta)\n",
    "                \n",
    "                ossia anziché fare \n",
    "                \n",
    "                global_cams_3d[cls].append(cam_vol)\n",
    "                \n",
    "                faccio\n",
    "                \n",
    "                global_cams_3d[cls].append(cam_vol[0])\n",
    "                \n",
    "                \n",
    "                E quindi, mi salvo per OGNI esempio direttamente la mappa cam 3d, per ogni banda, direttamente\n",
    "                ossia ogni esempio sarà costituito da 3 dimensioni (D, H, W) anziché dire \n",
    "                \n",
    "                \"Ogni dato (ossia ogni esempio) è composto da (batch, D, H, W) \n",
    "                se tanto il batch = 1 (perché il batch è il singolo esempio ogni volta)\n",
    "                \n",
    "                e quindi significherebbe aggiungere una dimensione (quella del batch) che in realtà è inutile, \n",
    "                perché si riferisce all'esempio stesso di già!\n",
    "                \n",
    "                Quindi:\n",
    "                👉 cam_vol[0] estrae la CAM 3D senza la dimensione \"batch\", che è inutile in quel contesto\n",
    "                👉 Serve per poter fare medie e slicing banda per banda correttamente dopo lo stack\n",
    "                👉 Questo rende compatibile il risultato finale con imshow (che accetta solo 2D o 3D RGB)\n",
    "       \n",
    "                2) appendo anche l'esempio volumetrico a cams_per_band_3d, MA GIA' suddiviso per banda! (per cui diventa 2d là dentro! 9x9)\n",
    "\n",
    "                '''\n",
    "                \n",
    "                global_cams_3d[cls].append(cam_vol[0])\n",
    "                \n",
    "                for b in range(n_bands):\n",
    "                    cams_per_band_3d[cls][b].append(cam_vol[0,b])\n",
    "            \n",
    "                '''\n",
    "                Nel caso della SeparableCNN2D, invece, il layer convoluzionale è 2D e riceve in input \n",
    "                (B,5,9,9), cioè con le bande di frequenza come canali (non come profondità). \n",
    "\n",
    "                Questo significa che non puoi ottenere direttamente una CAM 3D nello stesso modo, \n",
    "                ma si può ottenere una CAM 2D per ogni banda, \n",
    "                \"mascherando\" l’input attivando una banda alla volta\n",
    "\n",
    "                SeparableCNN2D → non hai accesso diretto a una \"profondità\" come in GradCAM 3D, quindi:\n",
    "\n",
    "                Simuli la profondità attivando una banda alla volta.\n",
    "\n",
    "                Ottieni una CAM 2D per ogni slice (banda), iterando sulle bande.\n",
    "\n",
    "                Questo approccio ti consente di costruire comunque strutture 3D:\n",
    "                cams_per_band_2d[cls][b] con b = 0...4 contiene \n",
    "                le CAM 2D relative alla banda b, ricostruendo idealmente la distribuzione tridimensionale\n",
    "                '''\n",
    "            \n",
    "            elif activ.ndim == 4: #Caso per il modello Conv Separabili \n",
    "                \n",
    "                # 1) Preparo il sample per il calcolo\n",
    "                sample_input = sample_input.clone().detach().requires_grad_(True) # sample_input: (1, 9, 9, 5)\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                il mio sample input ora è sempre 4D, ma a differenza di prima, io sto trattando in questo caso \n",
    "                le bande come CANALI, e non come DEPTH (della convoluzione 3d pura!)\n",
    "                \n",
    "                Come prima, devo togliere la dimensione del batch ...\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                # 2) Subito qui prendo la potenza raw del volume 9×9×5, senza passare dal modello!\n",
    "                #raw_vol[:, :, b] è una mappa 2D (9, 9) della potenza spettrale per la banda b per ogni singolo esempio\n",
    "                \n",
    "                raw_vol = sample_input_raw.detach().cpu().numpy().squeeze()     # → (9, 9, 5)\n",
    "                \n",
    "                '''CASTING IN FLOAT64'''\n",
    "                #raw_vol = sample_input.detach().cpu().numpy().squeeze().astype(np.float64)         # 🔹 cast a float64   # → (9, 9, 5)\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                cams_per_band_2d e raw_power_per_band_2d son dentro al loop di MASKING, \n",
    "                perché entrambe si riferiscono alla banda e quindi devo essere inserite dentro al loop di masking...\n",
    "                \n",
    "                --> raw_power_per_band_2d[cls][b] e cams_per_band_2d[cls][b] sono dentro il loop (for b in ...) \n",
    "\n",
    "\n",
    "                \n",
    "                mentre global_cams_2d è FUORI da quel loop, perché raccoglie tutti gli esempi delle gradcam, \n",
    "                considerando però le mappe di attivazione di ogni banda singolarmente \n",
    "                e le aggrega per avere una visualizzazione dell'impatto complessivo di ogni singola banda sulla decisione del modello,\n",
    "                facendo vedere dove son maggiormente concentrate le attivazioni a livello spaziale TRA le bande ( = considerando TUTTE le bande assieme!)\n",
    "                \n",
    "                --> global_cams_2d[cls] sta dopo quel for b, raccogliendo una sola mappa 2D “complessiva” per trial\n",
    "                \n",
    "                In questo modo:\n",
    "\n",
    "                raw_power_per_band_2d e cams_per_band_2d catturano tutti i trial per banda.\n",
    "\n",
    "                global_cams_2d cattura un’unica mappa per trial, che poi aggregherò in global_mean_cams_2d per ottenere la heatmap 2D “globale”\n",
    "                che comprende tutte le bande insieme\n",
    "\n",
    "                '''\n",
    "                \n",
    "                for b in range(n_bands):\n",
    "                    \n",
    "                    #raw_power_per_band_2d invece raccoglie TUTTE le mappe 2D (9, 9) di ogni singolo esempio che conterrà la potenza spettrale alla stessa banda b \n",
    "                    #Lista di mappe di potenza 2D (una per trial) per la relativa banda\n",
    "                    raw_power_per_band_2d[cls][b].append(raw_vol[:, :, b])\n",
    "                    \n",
    "                    # ✅ Creo un input mascherato con **solo** la banda b attiva\n",
    "                    masked = np.zeros_like(raw_vol)  # (9, 9, 5)\n",
    "                    \n",
    "                    masked[:, :, b] = raw_vol[:, :, b]  # attiva solo la banda b\n",
    "                    \n",
    "                    #Qui lo prepari in formato 4D, come vorrebbe il modello Conv Separable\n",
    "                    masked_tensor = torch.tensor(masked).unsqueeze(0).to(device)  # (1, 9, 9, 5)\n",
    "                    \n",
    "                    # Preparo il sample\n",
    "                    masked_tensor.requires_grad_(True)\n",
    "                    \n",
    "                    # Forward + backward\n",
    "                    output = model(masked_tensor)\n",
    "                    target_class = output.argmax(dim=1).item()\n",
    "                    model.zero_grad()\n",
    "                    \n",
    "                    target = output[0, target_class]\n",
    "                    target.backward()\n",
    "\n",
    "                    activ = gradcam.activations   # (B, C, H, W)\n",
    "                    grads = gradcam.gradients     # (B, C, H, W)\n",
    "                    \n",
    "                    # Calcolo CAM 2D (come standard GradCAM)\n",
    "                    w2d = torch.mean(grads, dim=(2, 3), keepdim=True)  # (B, C, 1, 1)\n",
    "                    cam = F.relu(torch.sum(w2d * activ, dim=1))        # (B, H, W) --> # (1, H, W)\n",
    "                    \n",
    "                    #Riporto la shape con .unsqueeze(1) a 4D per fare interpolation\n",
    "                    cam_up = F.interpolate(cam.unsqueeze(1), size=(9, 9), mode='bilinear', align_corners=False)\n",
    "                    \n",
    "                    #Riporto la shape con .squeeze(1) a 3D per salvare i dati\n",
    "                    cam_2d = cam_up.squeeze(1).cpu().numpy()  # (B, 9, 9)\n",
    "\n",
    "                    # ✅ Aggiungo la mappa CAM alla banda corrispondente\n",
    "                    cams_per_band_2d[cls][b].append(cam_2d[0])  # prende il CAM per il sample corrente (9x9)\n",
    "                \n",
    "                #Qui “ricompatti” i 5 CAM 2D in un volume e poi medii, per ottenere una mappa 2D complessiva che tenga insieme l’informazione su tutte le bande.\n",
    "                #Durante il loop, dopo il masking e il calcolo di cam_2d, fai:\n",
    "                # cam_2d ha shape (1,9,9) → [0] è la matrice 9×9\n",
    "                global_cams_2d[cls].append(cam_2d[0])\n",
    "            else:\n",
    "                raise RuntimeError(f\"activ.ndim inatteso: {activ.ndim}\")\n",
    "                \n",
    "                \n",
    "    '''\n",
    "    CASO MODELLO CON 3D PURO, dovrei fare: \n",
    "\n",
    "    1) per global_cams_3d vado ad ottenere una media, ossia una global_mean_cams_3d, che riassume il contributo GLOBALE della gradcam 3D aggregata\n",
    "    all'interno dell'intero volume 3D (che poi al massimo si può scorporare vedendo per ogni banda successivamante)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if isinstance(model, CNN3D_LSTM_FC):\n",
    "        \n",
    "        #1) global_cams_3d → media “globale” del volume 3D Grad‑CAM\n",
    "        # media sul numero di esempi per ogni classe → ottieni un array (D, H, W)\n",
    "        global_mean_cams_3d = {\n",
    "            cls: np.mean(np.stack(global_cams_3d[cls]), axis=0)  # da [ (1,D,H,W), … ] a (D,H,W)\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "\n",
    "\n",
    "        '''\n",
    "        2) poi, dentro a raw_power_per_band_3d (siccome è già suddivsa ogni potenza spettrale in 2D di ogni esempio, per ogni classe e per ogni banda !)\n",
    "        ottenere una media sulla potenza spettrale dei gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_raw_power_per_band_3d)\n",
    "        '''\n",
    "\n",
    "        #2) raw_power_per_band_3d → media della potenza raw per banda\n",
    "        #Hai già raccolto, per ogni cls e per ogni banda b, tutte le mappe 2D raw_power_per_band_3d[cls][b] (una per trial).\n",
    "        #La media diventa:\n",
    "\n",
    "        mean_raw_power_per_band_3d = {\n",
    "            cls: [ np.mean(np.stack(raw_power_per_band_3d[cls][b]), axis=0)\n",
    "                   for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "\n",
    "        # risultato: mean_raw_power_per_band_3d[cls][b] è (H,W)\n",
    "\n",
    "        '''\n",
    "        3) dentro a cams_per_band_3d, (siccome è già suddivsa ogni gradcam in 2D di ogni esempio, per ogni classe e per ogni banda !) \n",
    "        ottenere una media sulle gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_cam_3d_per_band) \n",
    "\n",
    "        '''\n",
    "\n",
    "        #3) cams_per_band_3d → media della Grad‑CAM per banda\n",
    "        #Analogamente hai raccolto tutte le slice 2D di Grad‑CAM in cams_per_band_3d[cls][b].\n",
    "        #La media diventa:\n",
    "\n",
    "        mean_cams_per_band_3d = {\n",
    "            cls: [ np.mean(np.stack(cams_per_band_3d[cls][b]), axis=0)\n",
    "                   for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        # mean_cam_3d_per_band[cls][b] ha shape (H,W)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Con queste tre strutture (global_mean_cams_3d, mean_raw_power_per_band_3d, mean_cam_3d_per_band) puoi\n",
    "\n",
    "        riga 1–2: istogrammi di mean_cam_3d_per_band[cls][b]\n",
    "\n",
    "        riga 3–4: heatmap di mean_cam_3d_per_band[cls][b]\n",
    "\n",
    "        riga 5–6: heatmap di mean_raw_power_per_band_3d[cls][b]\n",
    "\n",
    "        riga 7–8: slice di global_mean_cams_3d[cls] per ogni b\n",
    "\n",
    "        '''\n",
    "    \n",
    "    elif isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "    \n",
    "        '''\n",
    "        CASO MODELLO CONV SEPARABLE, dovrei fare: \n",
    "\n",
    "        1) per global_cams_2d vado ad ottenere una media, ossia una global_mean_cams_2d, che riassume il contributo GLOBALE della gradcam 2D aggregata\n",
    "        all'interno di TUTTE LE BANDE ASSIME (che mi dovrebbe dare quindi per OGNI CLASSE un plot unico, \n",
    "        e non come il global_mean_cams_3d, dove dovrei vedere in quel caso, invece, la stessa mappa di “rilevanza complessiva”, MA distribuita lungo la profondità,\n",
    "        ossia tra le bande e quindi potrei vedere se effettivamente io abbia una banda che è specificatamente più attiva di altre COMPLESSIVAMENTE...\n",
    "\n",
    "        Per la SeparableCNN2D ricostruisci un Grad‑CAM “3D” artificiale facendo 5 Grad‑CAM 2D una per ogni banda\n",
    "        '''\n",
    "\n",
    "        #global_cams_2d\n",
    "        #Qui “ricompatti” i 5 CAM 2D in un volume e poi medii, per ottenere una mappa 2D complessiva che tenga insieme l’informazione su tutte le bande.\n",
    "        #Durante il loop, dopo il masking e il calcolo di cam_2d, fai:\n",
    "\n",
    "        # cam_2d ha shape (1,9,9) → [0] è la matrice 9×9\n",
    "        #global_cams_2d[cls].append(cam_2d[0])\n",
    "\n",
    "        global_mean_cams_2d = {\n",
    "            cls: np.mean(np.stack(global_cams_2d[cls]), axis=0)\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        # global_mean_cams_2d[cls] shape = (9,9)\n",
    "\n",
    "        '''\n",
    "        2) poi, dentro a raw_power_per_band_2d (siccome è già suddivsa ogni potenza spettrale in 2D di ogni esempio, per ogni classe e per ogni banda !)\n",
    "        ottenere una media sulla potenza spettrale dei gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_raw_power_per_band_2d)\n",
    "        '''\n",
    "\n",
    "        #raw_power_per_band_2d\n",
    "        #Hai già in raw_power_per_band_2d[cls][b] tutte le mappe 2D di potenza (9×9) per trial, per ciascuna banda b.\n",
    "        #La media finale:\n",
    "\n",
    "\n",
    "        mean_raw_power_per_band_2d = {\n",
    "            cls: [ np.mean(np.stack(raw_power_per_band_2d[cls][b]), axis=0)\n",
    "                  for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "\n",
    "        # mean_raw_power_per_band_2d[cls][b] shape = (9,9)\n",
    "\n",
    "\n",
    "        '''\n",
    "        3) dentro a cams_per_band_2d, (siccome è già suddivsa ogni gradcam in 2D di ogni esempio, per ogni classe e per ogni banda !) \n",
    "        ottenere una media sulle gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_cam_2d_per_band) \n",
    "\n",
    "        '''\n",
    "\n",
    "        #cams_per_band_2d\n",
    "        #Durante il masking loop appendi in cams_per_band_2d[cls][b] il CAM 2D (9×9) di ogni trial.\n",
    "        #La media finale:\n",
    "\n",
    "        mean_cams_per_band_2d = {\n",
    "            cls: [ np.mean(np.stack(cams_per_band_2d[cls][b]), axis=0)\n",
    "                   for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        # mean_cam_2d_per_band[cls][b] shape = (9,9)\n",
    "\n",
    "        '''\n",
    "        Con queste tre strutture —\n",
    "\n",
    "        mean_raw_power_per_band_2d (5 mappe 9×9),\n",
    "\n",
    "        mean_cam_2d_per_band (5 mappe 9×9),\n",
    "\n",
    "        global_mean_cams_2d (1 mappa 9×9) —\n",
    "\n",
    "        puoi costruire esattamente le stesse righe di plot che avevi per il caso 3D, solo che al posto di “slice” del volume userai le CAM 2D mascherate.\n",
    "\n",
    "\n",
    "        '''\n",
    "    \n",
    "    # Preleva la struttura corretta in base al modello\n",
    "    \n",
    "    if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        mean_cams_per_band = mean_cams_per_band_2d\n",
    "        mean_raw_power_per_band = mean_raw_power_per_band_2d\n",
    "        global_mean_cams = global_mean_cams_2d\n",
    "    \n",
    "    else:  # Caso per modello CNN3D\n",
    "        mean_cams_per_band = mean_cams_per_band_3d\n",
    "        mean_raw_power_per_band = mean_raw_power_per_band_3d\n",
    "        global_mean_cams = global_mean_cams_3d\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # prima di salvare la figura, solo se richiesto vedi i valori delle potenze medie per banda e condizione sperimentale...\n",
    "    \n",
    "    #\"tag_names=[f\"{model.__class__.__name__} power\"]\")\n",
    "    \n",
    "    if debug:\n",
    "        if isinstance(model, CNN3D_LSTM_FC):\n",
    "            model_tag = f\"{model.__class__.__name__} power\"\n",
    "            check_negative_residuals(band_names,\n",
    "                                     mean_raw_power_per_band_3d,\n",
    "                                     model_tag)\n",
    "        else:\n",
    "            model_tag = f\"{model.__class__.__name__} power\"\n",
    "            check_negative_residuals(band_names,\n",
    "                                     mean_raw_power_per_band_2d,\n",
    "                                     model_tag)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Crea la figura dinamicamente in base al modello\n",
    "    if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        fig, axs = plt.subplots(8, 5, figsize=(24, 30))  # 2 righe per 5 colonne per modello 2D\n",
    "    else:\n",
    "        fig, axs = plt.subplots(8, 5, figsize=(24, 30))  # 5 righe per 2 colonne per modello 3D\n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "    title = (\n",
    "        f\"Grad-CAM Mapping over EEG Trials – Experimental Conditions: {exp_cond}\\n\\n\"\n",
    "        \"Row 1-2: Histogram of Mean Grad-CAM raw values for each class and frequency band\\n\"\n",
    "        \"Row 3-4: Normalized Mean Grad-CAM heatmaps for Class 0 (top) and Class 1 (bottom)\\n\"\n",
    "        \"Row 5-6: Raw power maps for Class 0 (top) and Class 1 (bottom)\\n\"\n",
    "        \"Row 7-8: Global CAM per Class\"\n",
    "        )\n",
    "    \n",
    "    plt.suptitle(title, fontsize=15)\n",
    "\n",
    "    # Spaziatura verticale per evitare sovrapposizione\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    plt.subplots_adjust(hspace = 0.7, wspace = 0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "    #PER PLOT RIGA 1-2 \n",
    "    \n",
    "    from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "    # Crea un formatter per notazione scientifica\n",
    "    sci_formatter = ScalarFormatter(useMathText=True)\n",
    "    \n",
    "    #Questa chiamata serve a forzare il range entro cui usare la notazione scientifica\n",
    "    #Se non metti questo limite, il comportamento può variare leggermente a seconda della scala dei dati —\n",
    "    #a volte sarà decimale (0.0001), altre volte esponenziale (1e-4), e potrebbe non essere uniforme tra subplot.\n",
    "    \n",
    "    sci_formatter.set_powerlimits((-3, 3))  # usa 1e-xxx se valori sono piccoli\n",
    "\n",
    "    for b, b_name in enumerate(band_names):\n",
    "    \n",
    "        for j, cls in enumerate([0, 1]):\n",
    "            \n",
    "            # Calcola l'istogramma dei valori della heatmap media\n",
    "            # rispetto alle 2 classi in base alla banda di frequenza isolata\n",
    "            \n",
    "            ax = axs[0, b] if cls == 0 else axs[1, b]\n",
    "            \n",
    "            ax.hist(mean_cams_per_band[cls][b].flatten(), bins='auto', color='blue', edgecolor='black')\n",
    "            ax.set_title(f\"{b_name} - Class {condition_names[cls]}\", fontsize=10)\n",
    "            ax.set_xlabel(\"Grad-CAM Value\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            \n",
    "            # ✅ Format tick con notazione scientifica\n",
    "            ax.xaxis.set_major_formatter(sci_formatter)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PER PLOT RIGA 3-4 \n",
    "    \n",
    "    '''\n",
    "    Concateno tutte le mean-CAM (cls 0+1, tutte le bande) in un unico array\n",
    "    in modo da confrontare le Gradcam tra classi e bande tra di loro! \n",
    "    '''\n",
    "    \n",
    "    all_mean_cams = np.concatenate([\n",
    "        mean_cams_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        mean_cams_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    vmin_cam = all_mean_cams.min()\n",
    "    vmax_cam = all_mean_cams.max()\n",
    "    \n",
    "    \n",
    "    for b, band in enumerate(band_names):                                 \n",
    "        \n",
    "        for j, cls in enumerate([0, 1]):\n",
    "            \n",
    "            ax = axs[2, b] if cls == 0 else axs[3, b]\n",
    "            \n",
    "            cam = mean_cams_per_band[cls][b] \n",
    "            \n",
    "            # Controlla se la forma è corretta per l'input di imshow\n",
    "            assert cam.ndim == 2, f\"Expected 2D array, got {cam.ndim}D array\"\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                cam,\n",
    "                cmap = 'RdYlBu_r',\n",
    "                vmin = vmin_cam, \n",
    "                vmax = vmax_cam,\n",
    "                aspect = 'equal',\n",
    "                origin = 'upper'\n",
    "            )\n",
    "            \n",
    "            ticks = np.linspace(vmin_cam, vmax_cam, 6)\n",
    "\n",
    "            cbar = fig.colorbar(\n",
    "                im, ax=ax, orientation='horizontal', pad=0.12, ticks=ticks, format='%.1e')\n",
    "            \n",
    "            cbar.set_ticks(ticks)\n",
    "            cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "\n",
    "            ax.set_title(\n",
    "                f\"{band} - Class {condition_names[cls]}\",\n",
    "                fontsize=10\n",
    "            )\n",
    "\n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(\n",
    "                        x, y, name,\n",
    "                        ha='center', va='center',\n",
    "                        fontsize=6, color='black', weight='bold'\n",
    "                    )\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PER PLOT RIGA 5-6 (SCALA LINEARE)\n",
    "    \n",
    "    from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "    sci = ScalarFormatter(useMathText=True)\n",
    "    sci.set_powerlimits((-2, 2))          # forza 1eX fuori dall’intervallo 1e‑2 … 1e2\n",
    "\n",
    "    \n",
    "    #Concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    \n",
    "    \n",
    "    # concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    all_mean_pow = np.concatenate([\n",
    "        mean_raw_power_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        mean_raw_power_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    vmin_pow = all_mean_pow.min()\n",
    "    vmax_pow = all_mean_pow.max()\n",
    "    \n",
    "    \n",
    "    ticks = np.linspace(vmin_pow, vmax_pow, 6)\n",
    "    \n",
    "    # Riga 3: Mappa della potenza media rispetto a distribuzione congiunta (su ciascuna banda e classe)\n",
    "    for b, band in enumerate(band_names):\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            ax = axs[4, b] if cls == 0 else axs[5, b]\n",
    "            \n",
    "            power = mean_raw_power_per_band[cls][b] \n",
    "            \n",
    "            im = ax.imshow(\n",
    "                power, \n",
    "                cmap='jet',\n",
    "                vmin= vmin_pow,\n",
    "                vmax= vmax_pow,\n",
    "                aspect='equal',\n",
    "                origin='upper'\n",
    "            )\n",
    "            \n",
    "            #ticks = np.linspace(vmin_pow, vmax_pow, 6)\n",
    "            \n",
    "            cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.12, ticks = ticks, format= sci)#format='%.1e')\n",
    "            \n",
    "            #cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            #cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "            \n",
    "            cbar.ax.xaxis.set_major_formatter(sci)   # <-- solo formatter\n",
    "            cbar.ax.tick_params(labelsize=6)\n",
    "            \n",
    "            ax.set_title(f\"{band} Power - Class {condition_names[cls]}\", fontsize=10)\n",
    "            \n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name, ha='center', va='center', fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #PER PLOT RIGA 5-6 (SCALA LOGARITMICA)\n",
    "        \n",
    "    \n",
    "    # ----- 1. calcola vmin_pow / vmax_pow -----\n",
    "    \n",
    "    #Concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    \n",
    "    \n",
    "    # concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    all_mean_pow = np.concatenate([\n",
    "        mean_raw_power_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        mean_raw_power_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    vmin_pow = all_mean_pow.min()\n",
    "    vmax_pow = all_mean_pow.max()\n",
    "    \n",
    "    #Filtra solo i valori strettamente > 0 (la scala log non accetta zeri o negativi).\n",
    "    #Perché? Se l’intero array fosse ≤ 0 (caso patologico) avremmo positive.size == 0.\n",
    "    positive = all_mean_pow[all_mean_pow > 0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #In pratica: stiamo abbassando il bordo inferiore del colormap di un 10 % rispetto al minimo positivo reale, \n",
    "    #così quei pixel non finiscono “incollati” al limite della color‑bar. Se preferisci usare un altro margine (5 %, 1 %) basta cambiare 0.9 in 0.95, 0.99, ecc.\n",
    "    #Se invece vuoi proprio che vada esattamente sul minimo, puoi togliere *0.9 (ma occhio ai warning di Matplotlib)\n",
    "    \n",
    "    #Il resto del blocco:\n",
    "\n",
    "    #Calcola vmax_pow dal massimo globale.\n",
    "    #Decide automaticamente use_log se il dynamic‑range supera 10³.\n",
    "    #Imposta una sola logica di plotting: quando use_log è True usa LogNorm, LogLocator e LogFormatterMathtext; altrimenti scala lineare + ScalarFormatter.\n",
    "\n",
    "    #I titoli aggiungono “(log10)” solo quando serve.\n",
    "    #Nota: quando use_log è True, passiamo vmin/vmax tramite LogNorm; quando è False, li passiamo direttamente a imshow con i parametri vmin=…, vmax=….\n",
    "    #Così la stessa funzione disegna correttamente entrambe le situazioni senza dover duplicare codice.\n",
    "    \n",
    "    \n",
    "    #1. Se esistono valori positivi, prende il più piccolo e lo moltiplica per 0.9 (−10 %).\n",
    "    # Obiettivo: Creare un piccolo margine: il vero minimo non cade esattamente sul bordo inferiore della scala log, evitando clip / warning.\n",
    "    \n",
    "    #2. Se non esistono, imposta un fallback sicur0\n",
    "    # Obiettivo: Garantire che vmin_pow > 0 in ogni caso (requisito di LogNorm).\n",
    "    \n",
    "    #vmin_pow = positive.min()*0.9 if positive.size else 1e-12\n",
    "    #vmin_pow = positive.min() if positive.size else 1e-12\n",
    "    \n",
    "    vmin_pow = positive.min()\n",
    "    \n",
    "    vmax_pow = all_mean_pow.max()\n",
    "\n",
    "    use_log  = vmax_pow / max(vmin_pow, 1e-12) > 1e3   # o il flag suggest_log\n",
    "\n",
    "    if use_log:\n",
    "        norm      = LogNorm(vmin=vmin_pow, vmax=vmax_pow)\n",
    "        locator   = LogLocator(base=10.0)\n",
    "        formatter = LogFormatterMathtext(base=10.0)\n",
    "    else:\n",
    "        norm      = None\n",
    "        locator   = None\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_powerlimits((-2, 2))         # 1e‑2 – 1e2 lineare\n",
    "\n",
    "    # ----- 2. plot -----\n",
    "    for b, band in enumerate(band_names):\n",
    "        for cls in (0, 1):\n",
    "            ax   = axs[4, b] if cls == 0 else axs[5, b]\n",
    "            pow_ = mean_raw_power_per_band[cls][b]\n",
    "\n",
    "            im = ax.imshow(pow_, cmap='jet', norm=norm,\n",
    "                           vmin=None if use_log else vmin_pow,\n",
    "                           vmax=None if use_log else vmax_pow,\n",
    "                           aspect='equal', origin='upper')\n",
    "\n",
    "            cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.12)\n",
    "            \n",
    "            if locator is not None:\n",
    "                cbar.locator   = locator\n",
    "            cbar.formatter = formatter\n",
    "            cbar.update_ticks()\n",
    "            cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "            scale = \"(log10)\" if use_log else \"\"\n",
    "            ax.set_title(f\"{band} Power {scale} – Class {condition_names[cls]}\",\n",
    "                         fontsize=10)\n",
    "            \n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name, ha='center', va='center', fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PER PLOT RIGA 7-8\n",
    "    \n",
    "    '''\n",
    "    Vorrei solo verificare allora l'ultima riga 7-8 per la differenza tra i due modelli, perchè: \n",
    "    \n",
    "    \n",
    "    1) Nel caso del modello 3d puro, ho ancora una fetta rappresentata, ossia\n",
    "\n",
    "    per global_cams_3d vado ad ottenere una media, ossia una global_mean_cams_3d, che riassume il contributo GLOBALE della gradcam 3D aggregata\n",
    "    all'interno dell'intero volume 3D (che poi al massimo si può scorporare vedendo per ogni banda successivamante) \n",
    "\n",
    "    ed è quello che vorrei fare per il modello Conv3D puro...\n",
    "\n",
    "    2) per il modello Conv Separabili invece, \n",
    "\n",
    "    per global_cams_2d vado ad ottenere una media, ossia una global_mean_cams_2d, che riassume il contributo GLOBALE della gradcam 2D aggregata\n",
    "    all'interno di TUTTE LE BANDE ASSIEME (che mi dovrebbe dare quindi per OGNI CLASSE un plot unico, \n",
    "    \n",
    "    e non come il global_mean_cams_3d, dove dovrei vedere in quel caso, invece, la stessa mappa di “rilevanza complessiva”, MA distribuita lungo la profondità,\n",
    "    ossia tra le bande e quindi potrei vedere se effettivamente io abbia una banda che è specificatamente più attiva di altre COMPLESSIVAMENTE ...\n",
    "\n",
    "    devo verificare che per queste righe 7-8, a seconda del modello, il codice sia corretto, in base a come so che \n",
    "\n",
    "    global_mean_cams_3d e global_mean_cams_2d sono in realtà adesso ossia \n",
    "    \n",
    "    global_mean_cams_3d[cls]\t(5, 9, 9)\tvolume medio 3D\n",
    "    global_mean_cams_2d[cls]\t(9, 9)\theatmap 2D “globale” su tutte le bande\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Costruiamo la distribuzione congiunta della media del singolo input multi-canale per ogni classe\n",
    "    '''\n",
    "    \n",
    "    all_global_mean_cams = np.concatenate([global_mean_cams[0].flatten(), global_mean_cams[1].flatten()])\n",
    "    \n",
    "    global_vmin_cam = all_global_mean_cams.min()\n",
    "    global_vmax_cam = all_global_mean_cams.max()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    In sintesi:\n",
    "\n",
    "    CNN3D: global_mean_cams_3d[cls] è già shape (5,9,9), quindi fai subito mat2d = global_mean_cams_3d[cls][b]\n",
    "    SeparableCNN2D: global_mean_cams_2d[cls] è shape (9,9), e la metti in axs[6, cls]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #'''Global CAM 2D: una mappa per classe, entrambe su riga 6'''\n",
    "    \n",
    "    if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        \n",
    "        #mean_cams_per_band = mean_cams_per_band_2d\n",
    "        #mean_raw_power_per_band = mean_raw_power_per_band_2d\n",
    "        global_mean_cams = global_mean_cams_2d\n",
    "        \n",
    "        # Global 2D: una sola heatmap per classe\n",
    "        for cls in [0, 1]:\n",
    "            ax = axs[6, cls]\n",
    "            mat2d = global_mean_cams[cls]  # (9,9)\n",
    "            im = ax.imshow(mat2d,\n",
    "                           cmap='RdYlBu_r',\n",
    "                           vmin=global_vmin_cam,\n",
    "                           vmax=global_vmax_cam,\n",
    "                           aspect='equal',\n",
    "                           origin='upper')\n",
    "            ticks = np.linspace(global_vmin_cam, global_vmax_cam, 6)\n",
    "            cbar = fig.colorbar(im, ax=ax,\n",
    "                                orientation='horizontal',\n",
    "                                pad=0.12,\n",
    "                                ticks=ticks,\n",
    "                                format='%.1e')\n",
    "            cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "            ax.set_title(f\"Global CAM 2D – Class {condition_names[cls]}\", fontsize=10)\n",
    "\n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([]);  ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name,\n",
    "                            ha='center', va='center',\n",
    "                            fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        # spegni i subplot vuoti\n",
    "        for col in range(2, n_bands):\n",
    "            axs[6, col].axis(\"off\")\n",
    "        for col in range(n_bands):\n",
    "            axs[7, col].axis(\"off\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Global 3D: una heatmap per banda e per classe\n",
    "        #mean_cams_per_band = mean_cams_per_band_3d\n",
    "        #mean_raw_power_per_band = mean_raw_power_per_band_3d\n",
    "        global_mean_cams = global_mean_cams_3d\n",
    "        \n",
    "        for b, band in enumerate(band_names):\n",
    "            \n",
    "            #for cls in [0, 1]:\n",
    "            for j, cls in enumerate([0, 1]):\n",
    "                \n",
    "                #ax = axs[6 + cls, b]  # cls==0→riga6, cls==1→riga7\n",
    "                \n",
    "                ax = axs[6, b] if cls == 0 else axs[7, b]\n",
    "                \n",
    "                vol3d = global_mean_cams[cls]     # (5,9,9) --> perché? \n",
    "                                                  # Perché sopra è stato fatto 'global_cams_3d[cls].append(cam_vol[0])'\n",
    "                                                  # Quindi ogni dato non era più fatto da (B, D, W, H) dove B = 1 (ossia l'esempio stesso)\n",
    "                                                  # Per cui dopo in global_mean_cams_3d quando ho fatto la media, ho ottenuto una rappresentazione MEDIA\n",
    "                                                  # del gradcam 3D, PER OGNI BANDA. Quindi, quando prelevo la SINGOLA BANDA, basta che faccio lo 'slicing' ossia\n",
    "                                                  # mat2d = vol3d[b]  --> da (5,9,9) diventa --> (9,9)\n",
    "                \n",
    "                mat2d = vol3d[b]                  # slice b → (9,9)\n",
    "                \n",
    "                im = ax.imshow(mat2d,\n",
    "                               cmap='RdYlBu_r',\n",
    "                               vmin=global_vmin_cam,\n",
    "                               vmax=global_vmax_cam,\n",
    "                               aspect='equal',\n",
    "                               origin='upper')\n",
    "                ticks = np.linspace(global_vmin_cam, global_vmax_cam, 6)\n",
    "                cbar = fig.colorbar(im, ax=ax,\n",
    "                                    orientation='horizontal',\n",
    "                                    pad=0.12,\n",
    "                                    ticks=ticks,\n",
    "                                    format='%.1e')\n",
    "                cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "                cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "                ax.set_title(f\"{band} Global CAM 3D – Class {condition_names[cls]}\", fontsize=10)\n",
    "\n",
    "                if channel_names is not None:\n",
    "                    ax.set_xticks([]);  ax.set_yticks([])\n",
    "                    for name, (y, x) in channel_names.items():\n",
    "                        ax.text(x, y, name,\n",
    "                                ha='center', va='center',\n",
    "                                fontsize=6, color='black', weight='bold')\n",
    "                else:\n",
    "                    ax.axis(\"off\")\n",
    "                    \n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    # ❸ — Ripristino allo stato precedente il modello ottimizzato trovato migliore, che aveva incluso anche layer LSTM\n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    \n",
    "    if needs_train_mode:\n",
    "        # ➌ ripristino layer singoli (i.e., riporto BN/Dropout dove stavano in eval mode)\n",
    "        for m, old_flag in saved:\n",
    "            m.train(old_flag)\n",
    "        # ➍ ripristino lo stato globale del modello (di nuovo ad .eval())\n",
    "        # i.e.,  come era stato passato in input alla funzione compute_gradcam_figure a partire 'load_best_run_results'!\n",
    "        \n",
    "        #Così simuli l’eval (Dropout off, BN congelato) pur essendo in train() per soddisfare CuDNN‑RNN.\n",
    "        model.train(was_training)\n",
    "        \n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "                                \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dde430d9-5459-4405-9242-c67e022fb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                                                        NEW VERSION 17/11/2025\n",
    "                                                                        \n",
    "                                                                \n",
    "                                                                        \n",
    "                                                                  \n",
    "                                                                    VERSION FREQUENCY x CHANNELS\n",
    "                                                                            \n",
    "                                                                     ****** PER GRID 2D! ******\n",
    "                                                                      ****** MULTI BAND******\n",
    "                                                                      \n",
    "                                                                      PER CONVOLUZIONE 3D (PURA)\n",
    "                                                                              +\n",
    "                                                                      PER CONVOLUZIONI SEPARABILI\n",
    "                                                                      \n",
    "                                                              \n",
    "                                                                CON VALORI MEAN GRADCAM e MEAN RAW POWER\n",
    "                                                                            SU STESSA SCALA\n",
    "                                                                PER OGNI CLASSE E BANDA DI FREQUENZA!\n",
    "                                                                \n",
    "                                                                        ^^^^^SENZA COMMENTI^^^^^\n",
    "                                                                        ^^^^^            ^^^^^\n",
    "                                                                        \n",
    "                                                                    SENZA ADOZIONE DELLA MASCHERA \n",
    "                                                                PER INDICARE LE POSIZIONI DELLA GRIGLIA REALI    \n",
    "                                                            \n",
    "                                                            1) STIMOLA A VERIFICARE CHE LA RETE DISTINGUA TRA \n",
    "                                                        \n",
    "                                                        COORDINATE ELETTRODICHE REALI VS FITTIZIE (SPAZI VUOTI GRIGLIA)\n",
    "                                                            \n",
    "                                                            2) CONFERMA IN MODO DATA-DRIVEN LA RILEVANZA NEUROFISIOLOGICA\n",
    "                                                            DEL FENOMENO IPOTIZZATO\n",
    "                                                            \n",
    "\n",
    "OLTRETUTTO\n",
    "\n",
    "1) senza mask vedevi hotspot che escono fuori dalla “sagoma” degli elettrodi;\n",
    "\n",
    "2) con la mask, l’informazione si appiattisce / cambia abbastanza \n",
    "→ questo ti dice che il modello 2D sta effettivamente usando anche le celle fittizie / bordi / padding come feature.\n",
    "Mascherando a posteriori, tu forzi la visualizzazione dentro la sagoma, \n",
    "ma non stai più mostrando fedelmente dove il modello guarda.\n",
    "\n",
    "\n",
    "Quindi la tua intuizione:\n",
    "\n",
    "“non imporre una maschera aiuta a capire se davvero il modello discrimina tra posizioni reali e non reali”\n",
    "\n",
    "è giusta al 100%. La mask è solo un filtro di visualizzazione, non cambia il modello: se la applichi \n",
    "puoi rendere la mappa “più neuro-plausibile”, ma rischi di nascondere il fatto che\n",
    "la Separable CNN2D sta facendo cose un po’ spurie nella parte fittizia della griglia.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "SINTESI DELLA FUNZIONE check_negative_residuals\n",
    "\n",
    "\n",
    "| Blocco                          | Scopo                                                                                                                                                                                                                                                                                                                                                                                                                                         | Perché serve                                                                                                                                                                                                                                                                                                                                                                                   |\n",
    "| ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **1. eps / “dynamic tol”**      | Calcola una soglia *dinamica* sotto la quale i valori negativi sono quasi certamente puro rumore numerico.<br><br>`python<br>eps32 = np.finfo(np.float32).eps  # ≈ 1.19e‑7<br>dynamic_tol = -eps32 * max(arr.max(), 1.0)`                                                                                                                                                                                                                     | *machine‑epsilon* (ε) è l’errore di arrotondamento massimo relativo per `float32` vicino a 1.<br>Moltiplicandolo per il massimo **positivo** trovato nella mappa definiamo una “fascia di tolleranza” proporzionale alla scala reale del dato. Tutto ciò che cade **sotto** `dynamic_tol` è troppo piccolo perché sia fisico — è, con ottima probabilità, soltanto rumore di rappresentazione. |\n",
    "| **2. Negativi “significativi”** | Conta e, se ce ne sono, stampa quante celle sono < `dynamic_tol`, quanto è il minimo osservato e la tolleranza stessa.                                                                                                                                                                                                                                                                                                                        | Ti permette di capire a colpo d’occhio se il preprocessing ha generato valori negativi (che non dovrebbero esistere in potenza) che **superano** il rumore ammesso.                                                                                                                                                                                                                            |\n",
    "| **3. Log‑hint**                 | Se *non* ci sono negativi significativi, calcola il **dynamic range** della mappa (max/min **> 0**) e decide se suggerire la scala log.<br><br>`python<br>positive = arr[arr > 0]<br>if positive.size == 0:<br>    ratio = np.inf          # tutto zero<br>else:<br>    min_pos = positive.min()<br>    max_pos = positive.max()<br>    ratio   = max_pos / max(min_pos, 1e‑12)<br>note = \"LOG consigliato\" if ratio > 1e3 else \"lineare ok\"` | – Ignoriamo gli zeri: la scala log non li supporta.<br>– Aggiungiamo un “cuscinetto” di `1e‑12` per evitare div/0.<br>– Se il range è > 10³ (tre ordini di grandezza ≃ “la potenza massima è **> 1000 ×** la minima”), la lettura lineare diventa poco informativa → meglio log10.                                                                                                             |\n",
    "\n",
    "\n",
    "\n",
    "In sintesi\n",
    "\n",
    "Prima parte → caccia ai negativi “numeric‑noise” tramite ε.\n",
    "\n",
    "Seconda parte → valuta solo i valori positivi e suggerisce log‑scale quando il dynamic‑range supera ~3 decadi (≈ 10³).\n",
    "\n",
    "\n",
    "Quando il codice passa in log‑scale?\n",
    "Raccogli tutte le mappe (di entrambe le classi e di tutte le bande)\n",
    "\n",
    "\n",
    "all_mean_pow = np.concatenate([...])\n",
    "\n",
    "Filtra i positivi e trova vmin_pow (con un 10 % di margine per non “appiattire” il minimo nella color‑bar)\n",
    "\n",
    "positive_vals = all_mean_pow[all_mean_pow > 0]\n",
    "vmin_pow = positive_vals.min() * 0.9 if positive_vals.size else 1e‑12\n",
    "vmax_pow = all_mean_pow.max()\n",
    "\n",
    "Decidi\n",
    "\n",
    "use_log = vmax_pow / max(vmin_pow, 1e‑12) > 1e3\n",
    "Se la potenza massima è > 1000 × la minima positiva, usare LogNorm.\n",
    "\n",
    "Come si spiega “tre ordini di grandezza”?\n",
    "\n",
    "“Il massimo è mille volte il minimo”.\n",
    "“Dynamic‑range di 3 decadi”.\n",
    "Oppure “max/min > 10³”.\n",
    "\n",
    "\n",
    "\n",
    "Riassunto finale\n",
    "ε: misura il rumore di quantizzazione, ti dice quando un (piccolo) negativo è solo un effetto di arrotondamento.\n",
    "\n",
    "Dynamic‑range: se la banda ha valori reali che variano più di 10³ ×, la scala log10 rende le differenze leggibili senza “schiacciare” i dettagli bassi.\n",
    "\n",
    "La funzione: un’unica utility per\n",
    "\n",
    "diagnosticare residui numerici,\n",
    "\n",
    "suggerire in automatico la rappresentazione (lineare / log) più sensata per i tuoi plot di potenza.\n",
    "\n",
    "Così il flusso diventa:\n",
    "\n",
    "\n",
    "check_negative_residuals(...)   # → log “SCALA LOG consigliata”\n",
    "↓\n",
    "use_log = True                  # ratio > 1e3\n",
    "↓\n",
    "plot con LogNorm + LogLocator   # color‑bar pulita, dettagli visibili\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Perché vedi ratio = inf\n",
    "* Nelle bande Beta e Gamma la tua mappa media è tutta a zero (o comunque tutti i valori ≤ ε).\n",
    "* Con soli zeri il vettore positive = arr[arr > 0] è vuoto, quindi lo tratto come “dynamic‑range infinito” per evitare la divisione per 0.\n",
    "* In realtà non hai un’«escursione infinita»; semplicemente non hai segnale in quelle bande → la scala log non aggiungerebbe nulla.\n",
    "\n",
    "Se vuoi evitare quel suggerimento “falso‑positivo”, basta cambiare la logica così (l’ho già indicato ma lo riscrivo compatto):\n",
    "\n",
    "positive = arr[arr > 0]\n",
    "if positive.size < 2:          # 0 o 1 valore positivo → niente dinamica utile\n",
    "    ratio = 0                  # forza il consiglio a “lineare”\n",
    "else:\n",
    "    ratio = positive.max() / max(positive.min(), 1e-12)\n",
    "    \n",
    "    \n",
    "Linear vs log: quale usare davvero?\n",
    "Banda\tDynamic‑range (≈ max/min)\tScala consigliabile\n",
    "Delta ‑ Theta\t3‑11 ×\tLineare: già leggibile.\n",
    "Alpha\t12 ×\tAncora lineare (o log, ma non cambia molto).\n",
    "Beta – Gamma\t0 (tutti zeri)\tLog inutile: non c’è potenza da mostrare.\n",
    "\n",
    "Di conseguenza:\n",
    "\n",
    "Mantieni la scala lineare globale come nel tuo blocco finale.\n",
    "\n",
    "Se in altri dataset vedrai rapporti > 1 000 con almeno 2 valori positivi, allora attiva la parte LogNorm.\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def check_negative_residuals(band_names, tensor_dict, tag, log_hint=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    • band_names   : lista di stringhe  (lunghezza = n_bands)\n",
    "    • tensor_dict  : {cls: [np.ndarray(H,W), … n_bands]} --> dict  {cls: [np.ndarray(H,W), … 5 bande]}\n",
    "    • tag          : prefisso stampato nel log --> string visualizzato nel log\n",
    "    • log_hint     : se True mostra il rapporto max/min ⇒ aiuta a decidere\n",
    "                     se usare la scala log nei plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #https://numpy.org/doc/2.1/reference/generated/numpy.finfo.html\n",
    "    eps32 = np.finfo(np.float32).eps        # 1.19e‑7\n",
    "    for cls in [0, 1]:\n",
    "        for b, b_name in enumerate(band_names):\n",
    "            arr   = tensor_dict[cls][b]\n",
    "            \n",
    "            # Soglia dinamica = −eps * valore_massimo_della_mappa\n",
    "            \n",
    "            #dynamic_tol = -eps32 * arr.max()      # tolleranza dinamica\n",
    "            \n",
    "            dynamic_tol = -eps32 * max(arr.max(), 1.0)   # evita max==0\n",
    "            \n",
    "            neg_mask  = arr < dynamic_tol # “negativi significativi”\n",
    "            \n",
    "            min_v, max_v = float(arr.min()), float(arr.max())\n",
    "            \n",
    "            if np.any(neg_mask):\n",
    "                print(f\"Valori sotto la soglia per classe e banda:\\n\")\n",
    "                \n",
    "                #forma\tcosa fa\tquando differisce\n",
    "                #np.count_nonzero(neg_mask)\tconverte il bool‑array in int (True→1, False→0) e somma gli 1\tè sempre un intero Python\n",
    "                \n",
    "                #neg_mask.sum()\tchiama il metodo .sum() dell’ndarray; \n",
    "                #per tipo bool fa esattamente la stessa somma di sopra\trestituisce uno numpy.int_ (stesso valore, differente tipo)\n",
    "                \n",
    "                n_neg   = np.count_nonzero(neg_mask)\n",
    "                \n",
    "                min_val = arr.min()\n",
    "                \n",
    "                print(f\"[{tag} {b_name}] class={cls}  band={b_name:<6}  \"\n",
    "                      f\"neg={n_neg}  min={min_val:.3e}  tol={dynamic_tol:.3e}\")\n",
    "            else:\n",
    "                print(f\"Nessun valore sotto la soglia per classe {cls} e banda {b_name}\\n\")\n",
    "                print(f\"Definisco il range minimo e massimo per classe {cls} e banda{b_name} :\\n\")\n",
    "                \n",
    "                # ‑‑ opzionale: suggerimento scala log\n",
    "                if log_hint:                    \n",
    "                    #if max_v == 0 or min_v == 0:\n",
    "                    positive = arr[arr > 0]           # considera solo i valori > 0\n",
    "                    \n",
    "                    #Se il primo valore della potenza è proprio 0:\n",
    "                    #per evitare problemi di NaN lo impongo ad infinito\n",
    "                    if positive.size == 0:\n",
    "                        ratio = float('inf')          # tutto zero → range “infinito”\n",
    "                    \n",
    "                    #Se il primo valore della potenza è proprio 0:\n",
    "                    #per evitare problemi di NaN lo impongo ad infinito\n",
    "                    else:\n",
    "                        min_pos = positive.min()\n",
    "                        max_pos = positive.max()\n",
    "                        ratio   = max_pos / max(min_pos, 1e-12)\n",
    "                    note = f\"\\033[1mSCALA LOG per plots consigliata\\033[0m\" if ratio > 1e3 else f\"\\033[1mSCALA LINEARE per plots consigliata ok\\033[0m\"\n",
    "                    print(f\"        dynamic‑range ≈ {ratio:8.1f}  → {note}\")\n",
    "            print()\n",
    "                \n",
    "                \n",
    "                \n",
    "import torch.nn as nn\n",
    "\n",
    "def model_has_cudnn_rnn(model):\n",
    "    \"\"\"Ritorna True se il modello usa LSTM/GRU/RNN supportati da CuDNN.\"\"\"\n",
    "    return any(isinstance(m, (nn.LSTM, nn.GRU, nn.RNN)) for m in model.modules())\n",
    "\n",
    "\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import (LogLocator, LogFormatterMathtext,\n",
    "                               ScalarFormatter)\n",
    "\n",
    "\n",
    "'''RICORDATI: aggiunto parametro TEST_LOADER_RAW per i plots della POTENZA SPETTRALE MEDIA PER BANDA (i.e., test_loader_raw)'''\n",
    "def compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device, channel_names=None, debug = False):\n",
    "    \n",
    "    \n",
    "    '''SOLO PER I MODELLI OTTIMIZZATI CON ANCHE LA LSTM'''\n",
    "    \n",
    "    #Solo i modelli con LSTM entrano in questo giro; gli altri non cambiano di stato.\n",
    "    #Con questa sequenza:\n",
    "    #non ottieni più l’errore “cudnn RNN backward…”;\n",
    "    #la rete “si comporta” come in eval (Dropout off, BN congelato) mentre calcoli le CAM;\n",
    "    #l’ambiente di chiamata (il tuo loop di testing) riceve il modello esattamente nello stato in cui l’aveva passato alla funzione compute_gradcam_figure\n",
    "    \n",
    "\n",
    "    ### Perché serve model.train() anche se la CAM è presa prima della LSTM\n",
    "    \n",
    "    #Il backward, per arrivare dal loss (o dal logit scelto) fino al tuo layer conv3, deve comunque attraversare l’LSTM che sta più avanti nella rete.\n",
    "    #Le implementazioni CuDNN degli RNN (LSTM/GRU) alzano un’eccezione se provi a chiamare tensor.backward() mentre il modulo è in modalità eval().\n",
    "    #RuntimeError: cudnn RNN backward can only be called in training mode\n",
    "    #Quindi, anche se la CAM è calcolata su conv3, devi mettere l’intero modello in train() per il tempo del backward.\n",
    "    #condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    \n",
    "    ### Che cos’è model.training\n",
    "    \n",
    "    #model.training è un semplice flag booleano (impostato da nn.Module.train() / nn.Module.eval()), ereditato da tutti i sotto‑moduli.\n",
    "    #Con was_training = model.training ricordi in che stato era il modello (quasi sempre False, cioè eval, nel tuo flusso)\n",
    "    #per poterlo ripristinare dopo.\n",
    "    \n",
    "    #Facendo così\n",
    "    \n",
    "    #for m in model.modules():\n",
    "    #if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n",
    "                      #nn.Dropout, nn.Dropout2d, nn.Dropout3d)):\n",
    "        #if m.training:         # cioè erano in train\n",
    "            #m.eval()\n",
    "            #frozen_layers.append(m)\n",
    "    \n",
    "    #Li sposti in eval uno per uno, senza toccare il resto della rete che deve restare in train() per far funzionare CuDNN‑RNN.\n",
    "    \n",
    "    \n",
    "    ### Perché, a fine blocco, servono due ripristini\n",
    "    \n",
    "    #1) Riattivo i BatchNorm / Dropout che avevo forzato in eval:\n",
    "    \n",
    "    #for m in frozen_layers:\n",
    "        #m.train()              # torna come prima\n",
    "    \n",
    "    #2) Riporto l’intero modello nello stato in cui si trovava prima del Grad‑CAM:\n",
    "    \n",
    "    #model.train(was_training)  # se era eval() torna eval, altrimenti resta train\n",
    "    \n",
    "    #Se non facessi il punto 1, lasceresti quei moduli permanentemente in eval anche quando, più tardi, \n",
    "    #rientri in training (per esempio in un fine‑tuning).\n",
    "    #Se non facessi il punto 2, lasceresti tutto il modello in train → dropout attivo, BN che accumula statistiche, ecc.\n",
    "\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    # ❶ — se serve, abilito temporaneamente la modalità train per il modello ottimizzato che aveva ANCHE la LSTM... \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    \n",
    "    needs_train_mode = model_has_cudnn_rnn(model)\n",
    "    \n",
    "    if needs_train_mode:\n",
    "        was_training = model.training      # salvo lo stato\n",
    "        model.train()                      # abilito backward su CuDNN‑RNN\n",
    "        \n",
    "        # ➊ salvo lo stato di OGNI BN/Dropout\n",
    "        \n",
    "        saved = [(m, m.training) for m in model.modules()\n",
    "             if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n",
    "                               nn.Dropout, nn.Dropout2d, nn.Dropout3d))]\n",
    "        \n",
    "        model.train()                              # abilita backward su CuDNN‑RNN\n",
    "        \n",
    "        # ➋ congelo in ogni layer della rete gli strati di BatchNorm e Dropout\n",
    "        for m, _ in saved:\n",
    "            m.eval()\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    # ❷ — QUI sotto metti tutto il tuo codice Grad‑CAM\n",
    "    #      (forward, backward, costruzione delle mappe, plot, …)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # … il tuo lunghissimo corpo della funzione rimane invariato …\n",
    "    # → al momento di fare backward NON avrà più l’eccezione\n",
    "    #   “cudnn RNN backward can only be called in training mode”\n",
    "\n",
    "    \n",
    "    '''SE VUOI USARE STESSO LAYER PER GRADCAM IN ENTRAMBE ARCHITETTURE'''\n",
    "    target_layer = model.conv2b #model.conv3\n",
    "    \n",
    "    \n",
    "    '''SE VUOI USARE DIVERSI LAYER PER GRADCAM NELLE 2 ARCHITETTURE'''\n",
    "    #if isinstance(model, CNN3D_LSTM_FC):\n",
    "        #target_layer = model.conv2b\n",
    "        \n",
    "    #elif isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        #target_layer = model.pw_conv1  # feature map 9x9, allineata 1:1 alla griglia\n",
    "    \n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Determina il target layer in base al tipo di modello\n",
    "    #if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        #target_layer = model.dw_conv1  # Per il modello separabile 2D\n",
    "    #else:\n",
    "        #target_layer = model.conv3  # Per il modello CNN3D\n",
    "        \n",
    "\n",
    "    '''OLD APPROACH'''\n",
    "    #condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    '''NEW APPROACH'''\n",
    "    condition_names = exp_cond.split(\"_vs_\") if \"_vs_\" in exp_cond else [\"Class 0\", \"Class 1\"]\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Mapping etichette condizioni per i TITOLI dei plot\n",
    "    # -------------------------------\n",
    "    \n",
    "    label_map = {\n",
    "        \"th_resp\": \"obs_resp\",\n",
    "        \"pt_resp\": \"rec_resp\",\n",
    "    }\n",
    "    \n",
    "    def remap_condition_label(s: str) -> str:\n",
    "        for old, new in label_map.items():\n",
    "            s = s.replace(old, new)\n",
    "        return s\n",
    "    \n",
    "    # Rimappa i nomi delle condizioni usati nei titoli dei subplot\n",
    "    condition_names = [remap_condition_label(x) for x in condition_names]\n",
    "    \n",
    "    # Rimappa anche la stringa mostrata nel titolo principale (suptitle)\n",
    "    exp_cond_display = remap_condition_label(exp_cond)\n",
    "    \n",
    "    \n",
    "    # ✅ Raccogli TUTTI i campioni per ciascuna classe\n",
    "    # Itera sul test_loader fino a trovare almeno un esempio per ciascuna classe (0 e 1)\n",
    "    \n",
    "    \n",
    "    #PER IL CASO CONV 3D\n",
    "    \n",
    "    # Ogni mio sample è 3D, perché infatti è fatto per convoluzione 3d pura o convoluzioni separabili, \n",
    "    # Quindi ha shape  (B, C, D, H, W), dove:  \n",
    "    \n",
    "    #B = batch (in questo caso, per ogni singolo esempio quindi sarà 1 -> singolo esempio alla volta)\n",
    "    #C = feature maps/canali (numero di feature maps estratte dalla convoluzione, o meglio anche noti come  canali convoluzionali)\n",
    "    #D = depth (la dimensione di profondità del mio tensore --> 5 ossia, la potenza spettrale ad ogni banda di frequenza - i.e.,  delta, theta, alfa, beta e gamma)\n",
    "    #H = height (altezza, prima dimensione SPAZIALE del mio tensore i.e., altezza griglia, canali EEG) \n",
    "    #W = width (larghezza, seconda dimensione SPAZIALE del mio tensore i.e., larghezza griglia, canali EEG)\n",
    "    \n",
    "    \n",
    "    '''SHAPE DEI DATI ORIGINALE SAREBBE (B, 9, 9, 5)'''\n",
    "    samples = {0: [], 1: []}\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                '''OSSIA QUI DIVENTA (1, 9, 9, 5)'''\n",
    "                samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "    \n",
    "    '''TEST_LOADER RAW (B, 9, 9, 5)'''\n",
    "    samples_raw = {0: [], 1: []}\n",
    "    for inputs, labels in test_loader_raw:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        for i, label in enumerate(labels):\n",
    "            label_int = int(label.item())\n",
    "            if label_int in samples_raw:  # Assumendo solo classi 0 e 1\n",
    "                \n",
    "                '''OSSIA QUI DIVENTA (1, 9, 9, 5)'''\n",
    "                samples_raw[label_int].append(inputs[i].unsqueeze(0))\n",
    "                \n",
    "    # ============================================================\n",
    "    # Calcolo delle Grad-CAM per ogni singola banda di frequenza\n",
    "    # ============================================================\n",
    "    \n",
    "    n_bands = 5  # numero di canali/bande di frequenza\n",
    "    band_names = ['Delta (δ)', 'Theta (Θ)', 'Alpha (α)', 'Beta (β) ', 'Gamma (γ)']  \n",
    "    \n",
    "    \n",
    "    '''STRUTTURE DATI PER CONV3D'''\n",
    "    #✅ Struttura per il GradCAM 3D, ossia qui raccolgo il GradCAM 3D di ogni esempio\n",
    "    # quindi la mappa di attivazione che identifica l'attivazioni più rilevanti per la classificazione di una esemplare di una certa classe\n",
    "    # sia spazialmente (height and width, ossia le dimensioni spaziali del mio tensore)\n",
    "    # sia frequenzialmente (depth), ossia le attivazioni più rilevanti in base alla banda di frequenza\n",
    "    \n",
    "    global_cams_3d = {0: [], 1: []} # shape (D, 9, 9)\n",
    "    \n",
    "    #Poi qui abbiamo: \n",
    "\n",
    "    # ✅ Struttura: classe → banda → immagini raw di input filtrate per singola banda (senza passare dal modello)\n",
    "    #tutte le mappe di potenza per la classe cls nella banda b-esima\n",
    "    \n",
    "    #La struttura per salvare invece lo potenza spettrale media per ogni relativa banda \n",
    "    raw_power_per_band_3d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    #La struttura che salverà la \"fetta\" del gradcam3D, ossia dove plotto solo la fetta della banda a partire dal global_cam_3d\n",
    "    #Ossia per ogni esempio di una specifica classe, prenderò la mappa di attivazione spazialmente più rilevante, in base alla specifica banda di frequenza indagata\n",
    "\n",
    "    #✅ Struttura: classe → banda → lista CAM\n",
    "    #cams_per_band_3d[cls][banda]: la slice D-esima (ossia la slice frequenziale) della mappa GradCAM per ogni campione di classe cls.\n",
    "    \n",
    "    cams_per_band_3d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    '''STRUTTURE DATI PER CONV SEPARABLE'''\n",
    "\n",
    "    \n",
    "    global_cams_2d = {0: [], 1: []} \n",
    "    \n",
    "    raw_power_per_band_2d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    cams_per_band_2d = {0: [[] for _ in range(n_bands)], 1: [[] for _ in range(n_bands)]}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Da qui in giù, vado a eseguire i passaggi essenziali per:\n",
    "    \n",
    "    1) Calcolare il GradCAM 3D per ogni esempio,\n",
    "    2) Isolare le fette (slice) per banda,\n",
    "    3) Raccogliere le immagini di potenza raw per ogni banda,\n",
    "    4) Calcolare le medie per classe e banda.\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    for cls in [0, 1]:\n",
    "        \n",
    "        for sample_input, sample_input_raw in zip(samples[cls], samples_raw[cls]):\n",
    "            \n",
    "            # 1) Preparo il sample per il calcolo\n",
    "            sample_input = sample_input.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            Devo far squeeze perché essendo prelevati dal test loader, li avevo già resi con un 1 davanti ossia qui sopra\n",
    "\n",
    "            samples[label_int].append(inputs[i].unsqueeze(0))\n",
    "            \n",
    "            perché io, è vero che prendo i dati dal test_loader che son in formato batch (batch, D, H, W)\n",
    "            ma siccome poi prelevo ogni singolo esempio per metterli dentro a \"samples\", allora quella dimensione di batch si leva nel dizionario \"samples\".\n",
    "            \n",
    "            Per cui, quando voglio prendermi il singolo esempio per salvarmelo dentro a \"raw_vol\" (ossia la potenza spettrale del volume proprio di ogni esempio),\n",
    "            allora devo ri-assegnare ad ogni esempio la dimensione del batch (ossia 1, ossia il singolo esempio)\n",
    "            quando li salvo dentro samples. \n",
    "            \n",
    "            Ed infatti, quindi quell' \".unsqueeze(0)\" --> samples[label_int].append(inputs[i].unsqueeze(0)) serve proprio a questo...\n",
    "\n",
    "            Quando poi, devo prendere il singolo esempio da salvare, nei termini di potenza spettrale di ogni esempio (come volume 3D), \n",
    "            allora devo rifare .squeeze(), per togliere nuovamente la dimensione del batch (ossia 1, ossia il singolo esempio)\n",
    "            perché la rappresentazione del singolo esempio è, appunto, composta da, le sole dimensioni che costituiscono proprio il singolo esempio,\n",
    "            ossia 9x9x5 -->  ossia la griglia 3D !\n",
    "\n",
    "            E quindi, per ogni esempio, mi salvo la griglia 3d, ossia\n",
    "            \"raw_vol = sample_input.detach().cpu().numpy().squeeze()   # → (9, 9, 5)\"\n",
    "            \n",
    "            E poi, la divido però per banda \n",
    "            \n",
    "            \"for b in range(n_bands):\n",
    "                raw_power_per_band_3d[cls][b].append(raw_vol[:, :, b])\n",
    "            \"\n",
    "            '''\n",
    "            \n",
    "            # 2) Subito qui prendo la potenza raw del volume 9×9×5, senza passare dal modello!\n",
    "            #raw_vol[:, :, b] è una mappa 2D (9, 9) della potenza spettrale per la banda b per ogni singolo esempio\n",
    "            \n",
    "            raw_vol = sample_input_raw.detach().cpu().numpy().squeeze()     # → (9, 9, 5)\n",
    "            \n",
    "            '''CASTING IN FLOAT64'''\n",
    "            #raw_vol = sample_input.detach().cpu().numpy().squeeze().astype(np.float64)         # 🔹 cast a float64   # → (9, 9, 5)\n",
    "            \n",
    "            for b in range(n_bands):\n",
    "                \n",
    "                \n",
    "                # Determina in base al tipo di modello i dati e le shape da salvare:\n",
    "                \n",
    "                #if isinstance(model, CNN3D_LSTM_FC):\n",
    "                #if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "                \n",
    "                    #target_layer = model.dw_conv1  # Per il modello separabile 2D\n",
    "                #else:\n",
    "                    #target_layer = model.conv3  # Per il modello CNN3D\n",
    "                    \n",
    "                if isinstance(model, CNN3D_LSTM_FC):\n",
    "                    \n",
    "                    #raw_power_per_band_3d invece raccoglie TUTTE le mappe 2D (9, 9) di ogni singolo esempio che conterrà la potenza spettrale alla stessa banda b \n",
    "                    #Lista di mappe di potenza 2D (una per trial) per la relativa banda\n",
    "\n",
    "                    raw_power_per_band_3d[cls][b].append(raw_vol[:, :, b])\n",
    "                    \n",
    "                elif isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "                    raw_power_per_band_2d[cls][b].append(raw_vol[:,:,b])\n",
    "\n",
    "            \n",
    "            # 2) Esegui il forward pass\n",
    "            output = model(sample_input)\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "            \n",
    "            # 3) Esegui il backward pass\n",
    "            model.zero_grad()\n",
    "            target = output[0, target_class]\n",
    "            target.backward()\n",
    "            \n",
    "            # 4) Preleva attivazioni e gradienti\n",
    "            activ = gradcam.activations   # shape può essere 5D (B,C,D,H,W) per CNN3D o 4D (B, C, H, W) per CNN Separable\n",
    "            grads = gradcam.gradients # shape può essere 5D (B,C,D,H,W) per CNN3D o 4D (B, C, H, W) per CNN Separable\n",
    "            \n",
    "            #Nel caso 3D dovrebbe essere\n",
    "\n",
    "            #Media dei gradienti solo su H,W → (B,C,D,1,1)\n",
    "            #w3d = torch.mean(grads, dim=(3, 4))\n",
    "\n",
    "            # b) Sommo sui canali → (B,D,H,W)\n",
    "            #cam3d = F.relu(torch.sum(w3d * activ, dim=1))\n",
    "            \n",
    "            #e così la shape finale sarebbe 3D con (B,C,D)\n",
    "            \n",
    "            #✔️ w3d è correttamente calcolato per ogni (B, C, D, 1, 1)\n",
    "            #✔️ La somma su dim=1 aggrega le feature maps con pesi per ogni banda\n",
    "            #✔️ ReLU rimuove componenti negative\n",
    "            \n",
    "            '''\n",
    "            Nel caso della CNN3D calcoli una mappa Grad-CAM 3D \"globale\" direttamente dal layer convoluzionale 3D, ottenendo attivazioni di shape \n",
    "            (B,C,D,H,W) e quindi una CAM volumetrica per ogni trial\n",
    "            \n",
    "            CNN3D → calcoli una CAM 3D da attivazioni (B,C,D,H,W), una volta sola per ogni esempio.\n",
    "\n",
    "            '''\n",
    "            \n",
    "            if activ.ndim == 5:  # Caso per modello CNN3D pura \n",
    "                \n",
    "                # 3D Volumetric Grad-CAM\n",
    "                \n",
    "                # a) Media dei gradienti solo su H,W → (B,C,D,1,1)\n",
    "                w3d = torch.mean(grads, dim=(3, 4), keepdim=True)\n",
    "\n",
    "                # b) Sommo sui canali (feature maps) → (B,D,H,W)\n",
    "                cam3d = F.relu(torch.sum(w3d * activ, dim=1))\n",
    "                \n",
    "                # c) Upsample H×W, mantenendo D intatto\n",
    "                B, D, H, W = cam3d.shape\n",
    "                cam_flat = cam3d.view(B*D, 1, H, W)\n",
    "                cam_up   = F.interpolate(cam_flat,\n",
    "                                         size=(9, 9),\n",
    "                                         mode='bilinear',\n",
    "                                         align_corners=False)\n",
    "                \n",
    "                cam_vol  = cam_up.view(B, D, 9, 9).cpu().numpy()\n",
    "                \n",
    "                # d) Prendi ogni batch-item\n",
    "                \n",
    "                '''\n",
    "                Quindi qui ottengo che:\n",
    "            \n",
    "                1) appendo a global_cams_3d che cosa qui? il gradcam 3D ossia la mappa di attivazione di volume,\n",
    "                ossia OGNI esempio (volumetrico) per ogni classe (ossia 9x9x5 ancora, di OGNI esempio)\n",
    "                \n",
    "                Quindi semplicemente anziché rendere il dato come 'batch, D, H, W'.. siccome prendiamo ogni esempio UNO ALLA VOLTA\n",
    "                è inutile mantenere la dimensione batch (che sarebbe sempre 1, perché parliamo di ogni esempio, uno alla volta)\n",
    "                \n",
    "                ossia anziché fare \n",
    "                \n",
    "                global_cams_3d[cls].append(cam_vol)\n",
    "                \n",
    "                faccio\n",
    "                \n",
    "                global_cams_3d[cls].append(cam_vol[0])\n",
    "                \n",
    "                \n",
    "                E quindi, mi salvo per OGNI esempio direttamente la mappa cam 3d, per ogni banda, direttamente\n",
    "                ossia ogni esempio sarà costituito da 3 dimensioni (D, H, W) anziché dire \n",
    "                \n",
    "                \"Ogni dato (ossia ogni esempio) è composto da (batch, D, H, W) \n",
    "                se tanto il batch = 1 (perché il batch è il singolo esempio ogni volta)\n",
    "                \n",
    "                e quindi significherebbe aggiungere una dimensione (quella del batch) che in realtà è inutile, \n",
    "                perché si riferisce all'esempio stesso di già!\n",
    "                \n",
    "                Quindi:\n",
    "                👉 cam_vol[0] estrae la CAM 3D senza la dimensione \"batch\", che è inutile in quel contesto\n",
    "                👉 Serve per poter fare medie e slicing banda per banda correttamente dopo lo stack\n",
    "                👉 Questo rende compatibile il risultato finale con imshow (che accetta solo 2D o 3D RGB)\n",
    "       \n",
    "                2) appendo anche l'esempio volumetrico a cams_per_band_3d, MA GIA' suddiviso per banda! (per cui diventa 2d là dentro! 9x9)\n",
    "\n",
    "                '''\n",
    "                \n",
    "                global_cams_3d[cls].append(cam_vol[0])\n",
    "                \n",
    "                for b in range(n_bands):\n",
    "                    cams_per_band_3d[cls][b].append(cam_vol[0,b])\n",
    "            \n",
    "                '''\n",
    "                Nel caso della SeparableCNN2D, invece, il layer convoluzionale è 2D e riceve in input \n",
    "                (B,5,9,9), cioè con le bande di frequenza come canali (non come profondità). \n",
    "\n",
    "                Questo significa che non puoi ottenere direttamente una CAM 3D nello stesso modo, \n",
    "                ma si può ottenere una CAM 2D per ogni banda, \n",
    "                \"mascherando\" l’input attivando una banda alla volta\n",
    "\n",
    "                SeparableCNN2D → non hai accesso diretto a una \"profondità\" come in GradCAM 3D, quindi:\n",
    "\n",
    "                Simuli la profondità attivando una banda alla volta.\n",
    "\n",
    "                Ottieni una CAM 2D per ogni slice (banda), iterando sulle bande.\n",
    "\n",
    "                Questo approccio ti consente di costruire comunque strutture 3D:\n",
    "                cams_per_band_2d[cls][b] con b = 0...4 contiene \n",
    "                le CAM 2D relative alla banda b, ricostruendo idealmente la distribuzione tridimensionale\n",
    "                '''\n",
    "            \n",
    "            elif activ.ndim == 4: #Caso per il modello Conv Separabili \n",
    "                \n",
    "                # 1) Preparo il sample per il calcolo\n",
    "                sample_input = sample_input.clone().detach().requires_grad_(True) # sample_input: (1, 9, 9, 5)\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                il mio sample input ora è sempre 4D, ma a differenza di prima, io sto trattando in questo caso \n",
    "                le bande come CANALI, e non come DEPTH (della convoluzione 3d pura!)\n",
    "                \n",
    "                Come prima, devo togliere la dimensione del batch ...\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                # 2) Subito qui prendo la potenza raw del volume 9×9×5, senza passare dal modello!\n",
    "                #raw_vol[:, :, b] è una mappa 2D (9, 9) della potenza spettrale per la banda b per ogni singolo esempio\n",
    "                \n",
    "                raw_vol = sample_input_raw.detach().cpu().numpy().squeeze()     # → (9, 9, 5)\n",
    "                \n",
    "                '''CASTING IN FLOAT64'''\n",
    "                #raw_vol = sample_input.detach().cpu().numpy().squeeze().astype(np.float64)         # 🔹 cast a float64   # → (9, 9, 5)\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                cams_per_band_2d e raw_power_per_band_2d son dentro al loop di MASKING, \n",
    "                perché entrambe si riferiscono alla banda e quindi devo essere inserite dentro al loop di masking...\n",
    "                \n",
    "                --> raw_power_per_band_2d[cls][b] e cams_per_band_2d[cls][b] sono dentro il loop (for b in ...) \n",
    "\n",
    "\n",
    "                \n",
    "                mentre global_cams_2d è FUORI da quel loop, perché raccoglie tutti gli esempi delle gradcam, \n",
    "                considerando però le mappe di attivazione di ogni banda singolarmente \n",
    "                e le aggrega per avere una visualizzazione dell'impatto complessivo di ogni singola banda sulla decisione del modello,\n",
    "                facendo vedere dove son maggiormente concentrate le attivazioni a livello spaziale TRA le bande ( = considerando TUTTE le bande assieme!)\n",
    "                \n",
    "                --> global_cams_2d[cls] sta dopo quel for b, raccogliendo una sola mappa 2D “complessiva” per trial\n",
    "                \n",
    "                In questo modo:\n",
    "\n",
    "                raw_power_per_band_2d e cams_per_band_2d catturano tutti i trial per banda.\n",
    "\n",
    "                global_cams_2d cattura un’unica mappa per trial, che poi aggregherò in global_mean_cams_2d per ottenere la heatmap 2D “globale”\n",
    "                che comprende tutte le bande insieme\n",
    "\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                per_band_cams = []  # 👈 CAM di questo trial, una per banda\n",
    "                \n",
    "                \n",
    "                for b in range(n_bands):\n",
    "                    \n",
    "                    #raw_power_per_band_2d invece raccoglie TUTTE le mappe 2D (9, 9) di ogni singolo esempio che conterrà la potenza spettrale alla stessa banda b \n",
    "                    #Lista di mappe di potenza 2D (una per trial) per la relativa banda\n",
    "                    raw_power_per_band_2d[cls][b].append(raw_vol[:, :, b])\n",
    "                    \n",
    "                    # ✅ Creo un input mascherato con **solo** la banda b attiva\n",
    "                    masked = np.zeros_like(raw_vol)  # (9, 9, 5)\n",
    "                    \n",
    "                    masked[:, :, b] = raw_vol[:, :, b]  # attiva solo la banda b\n",
    "                    \n",
    "                    #Qui lo prepari in formato 4D, come vorrebbe il modello Conv Separable\n",
    "                    masked_tensor = torch.tensor(masked).unsqueeze(0).to(device)  # (1, 9, 9, 5)\n",
    "                    \n",
    "                    # Preparo il sample\n",
    "                    masked_tensor.requires_grad_(True)\n",
    "                    \n",
    "                    # Forward + backward\n",
    "                    output = model(masked_tensor)\n",
    "                    target_class = output.argmax(dim=1).item()\n",
    "                    model.zero_grad()\n",
    "                    \n",
    "                    target = output[0, target_class]\n",
    "                    target.backward()\n",
    "\n",
    "                    activ = gradcam.activations   # (B, C, H, W)\n",
    "                    grads = gradcam.gradients     # (B, C, H, W)\n",
    "                    \n",
    "                    # Calcolo CAM 2D (come standard GradCAM)\n",
    "                    w2d = torch.mean(grads, dim=(2, 3), keepdim=True)  # (B, C, 1, 1)\n",
    "                    cam = F.relu(torch.sum(w2d * activ, dim=1))        # (B, H, W) --> # (1, H, W)\n",
    "                    \n",
    "                    # ---- stesso nome in entrambi i casi ----\n",
    "                    \n",
    "                    B, H, W = cam.shape\n",
    "                    \n",
    "                    '''\n",
    "                    NEL CASO LAYER SIA DIVERSO DA pw_conv1, ALLORA FACCIO UPSAMPLING\n",
    "                    Cosa cambia in pratica:\n",
    "\n",
    "                    Se il target_layer è pw_conv1 (feature map 32×9×9):\n",
    "\n",
    "                    H, W = 9, 9 → salta l’interpolate, quindi nessuna distorsione spaziale.\n",
    "\n",
    "                    Se un domani cambi target_layer a un layer dopo un pool (es. conv2b con mappe 4×4):\n",
    "\n",
    "                    H, W != 9 → scatta l’upsampling e tutto continua a funzionare come prima.\n",
    "\n",
    "                    Il ramo 3D (activ.ndim == 5) non lo tocchi, lì l’upsampling è ancora necessario (4×4 → 9×9).\n",
    "\n",
    "                    Con questa modifica il caso SeparableCNN2D è “pulito” e allineato alla griglia elettrodica 1:1\n",
    "                    '''\n",
    "                    \n",
    "                    if (H, W) != (9, 9):\n",
    "                    \n",
    "                        #Riporto la shape con .unsqueeze(1) a 4D per fare interpolation e alla fine di nuovo in 3d\n",
    "                        cam = F.interpolate(cam.unsqueeze(1), size=(9, 9), mode='bilinear', align_corners=False).squeeze(1)\n",
    "                    \n",
    "                    \n",
    "                    # caso pw_conv1: la mappa è già 9x9 → nessun upsampling\n",
    "                    \n",
    "                    #Riporto la shape con .squeeze(1) a 3D per salvare i dati\n",
    "                    #cam_2d = cam_up.squeeze(1).cpu().numpy()  # (B, 9, 9)\n",
    "                    \n",
    "                    cam_2d = cam.detach().cpu().numpy()      # (B,9,9)\n",
    "\n",
    "                    # ✅ Aggiungo la mappa CAM alla banda corrispondente\n",
    "                    cams_per_band_2d[cls][b].append(cam_2d[0])  # prende il CAM per il sample corrente (9x9)\n",
    "                    \n",
    "                    # Conteggio della CAM corrente, per questo trial\n",
    "                    per_band_cams.append(cam_2d[0])                   \n",
    "                    \n",
    "                #Qui “ricompatti” i 5 CAM 2D in un volume e poi medii, per ottenere una mappa 2D complessiva che tenga insieme l’informazione su tutte le bande.\n",
    "                #Durante il loop, dopo il masking e il calcolo di cam_2d, fai:\n",
    "                # cam_2d ha shape (1,9,9) → [0] è la matrice 9×9\n",
    "                \n",
    "                # 👇 qui, fuori dal for b\n",
    "                \n",
    "                all_cams_2d_per_band = np.mean(np.stack(per_band_cams), axis=0)  # (9,9)\n",
    "                \n",
    "                global_cams_2d[cls].append(all_cams_2d_per_band)\n",
    "            else:\n",
    "                raise RuntimeError(f\"activ.ndim inatteso: {activ.ndim}\")\n",
    "                \n",
    "                \n",
    "    '''\n",
    "    CASO MODELLO CON 3D PURO, dovrei fare: \n",
    "\n",
    "    1) per global_cams_3d vado ad ottenere una media, ossia una global_mean_cams_3d, che riassume il contributo GLOBALE della gradcam 3D aggregata\n",
    "    all'interno dell'intero volume 3D (che poi al massimo si può scorporare vedendo per ogni banda successivamante)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if isinstance(model, CNN3D_LSTM_FC):\n",
    "        \n",
    "        #1) global_cams_3d → media “globale” del volume 3D Grad‑CAM\n",
    "        # media sul numero di esempi per ogni classe → ottieni un array (D, H, W)\n",
    "        global_mean_cams_3d = {\n",
    "            cls: np.mean(np.stack(global_cams_3d[cls]), axis=0)  # da [ (1,D,H,W), … ] a (D,H,W)\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        \n",
    "        # ➜ CAM 2D globale per classe (media su TUTTE le bande dal VOLUME 3D)\n",
    "        global_2d_from3d = {\n",
    "            cls: np.mean(global_mean_cams_3d[cls], axis=0)  # (9,9)\n",
    "            for cls in [0, 1]\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        '''\n",
    "        2) poi, dentro a raw_power_per_band_3d (siccome è già suddivsa ogni potenza spettrale in 2D di ogni esempio, per ogni classe e per ogni banda !)\n",
    "        ottenere una media sulla potenza spettrale dei gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_raw_power_per_band_3d)\n",
    "        '''\n",
    "\n",
    "        #2) raw_power_per_band_3d → media della potenza raw per banda\n",
    "        #Hai già raccolto, per ogni cls e per ogni banda b, tutte le mappe 2D raw_power_per_band_3d[cls][b] (una per trial).\n",
    "        #La media diventa:\n",
    "\n",
    "        mean_raw_power_per_band_3d = {\n",
    "            cls: [ np.mean(np.stack(raw_power_per_band_3d[cls][b]), axis=0)\n",
    "                   for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "\n",
    "        # risultato: mean_raw_power_per_band_3d[cls][b] è (H,W)\n",
    "\n",
    "        '''\n",
    "        3) dentro a cams_per_band_3d, (siccome è già suddivsa ogni gradcam in 2D di ogni esempio, per ogni classe e per ogni banda !) \n",
    "        ottenere una media sulle gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_cam_3d_per_band) \n",
    "\n",
    "        '''\n",
    "\n",
    "        #3) cams_per_band_3d → media della Grad‑CAM per banda\n",
    "        #Analogamente hai raccolto tutte le slice 2D di Grad‑CAM in cams_per_band_3d[cls][b].\n",
    "        #La media diventa:\n",
    "\n",
    "        mean_cams_per_band_3d = {\n",
    "            cls: [ np.mean(np.stack(cams_per_band_3d[cls][b]), axis=0)\n",
    "                   for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        # mean_cam_3d_per_band[cls][b] ha shape (H,W)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Con queste tre strutture (global_mean_cams_3d, mean_raw_power_per_band_3d, mean_cam_3d_per_band) puoi\n",
    "\n",
    "        riga 1–2: istogrammi di mean_cam_3d_per_band[cls][b]\n",
    "\n",
    "        riga 3–4: heatmap di mean_cam_3d_per_band[cls][b]\n",
    "\n",
    "        riga 5–6: heatmap di mean_raw_power_per_band_3d[cls][b]\n",
    "\n",
    "        riga 7–8: slice di global_mean_cams_3d[cls] per ogni b\n",
    "\n",
    "        '''\n",
    "    \n",
    "    elif isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "    \n",
    "        '''\n",
    "        CASO MODELLO CONV SEPARABLE, dovrei fare: \n",
    "\n",
    "        1) per global_cams_2d vado ad ottenere una media, ossia una global_mean_cams_2d, che riassume il contributo GLOBALE della gradcam 2D aggregata\n",
    "        all'interno di TUTTE LE BANDE ASSIME (che mi dovrebbe dare quindi per OGNI CLASSE un plot unico, \n",
    "        e non come il global_mean_cams_3d, dove dovrei vedere in quel caso, invece, la stessa mappa di “rilevanza complessiva”, MA distribuita lungo la profondità,\n",
    "        ossia tra le bande e quindi potrei vedere se effettivamente io abbia una banda che è specificatamente più attiva di altre COMPLESSIVAMENTE...\n",
    "\n",
    "        Per la SeparableCNN2D ricostruisci un Grad‑CAM “3D” artificiale facendo 5 Grad‑CAM 2D una per ogni banda\n",
    "        '''\n",
    "\n",
    "        #global_cams_2d\n",
    "        #Qui “ricompatti” i 5 CAM 2D in un volume e poi medii, per ottenere una mappa 2D complessiva che tenga insieme l’informazione su tutte le bande.\n",
    "        #Durante il loop, dopo il masking e il calcolo di cam_2d, fai:\n",
    "\n",
    "        # cam_2d ha shape (1,9,9) → [0] è la matrice 9×9\n",
    "        #global_cams_2d[cls].append(cam_2d[0])\n",
    "\n",
    "        global_mean_cams_2d = {\n",
    "            cls: np.mean(np.stack(global_cams_2d[cls]), axis=0)\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        # global_mean_cams_2d[cls] shape = (9,9)\n",
    "\n",
    "        '''\n",
    "        2) poi, dentro a raw_power_per_band_2d (siccome è già suddivsa ogni potenza spettrale in 2D di ogni esempio, per ogni classe e per ogni banda !)\n",
    "        ottenere una media sulla potenza spettrale dei gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_raw_power_per_band_2d)\n",
    "        '''\n",
    "\n",
    "        #raw_power_per_band_2d\n",
    "        #Hai già in raw_power_per_band_2d[cls][b] tutte le mappe 2D di potenza (9×9) per trial, per ciascuna banda b.\n",
    "        #La media finale:\n",
    "\n",
    "\n",
    "        mean_raw_power_per_band_2d = {\n",
    "            cls: [ np.mean(np.stack(raw_power_per_band_2d[cls][b]), axis=0)\n",
    "                  for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "\n",
    "        # mean_raw_power_per_band_2d[cls][b] shape = (9,9)\n",
    "\n",
    "\n",
    "        '''\n",
    "        3) dentro a cams_per_band_2d, (siccome è già suddivsa ogni gradcam in 2D di ogni esempio, per ogni classe e per ogni banda !) \n",
    "        ottenere una media sulle gradcam di ogni trial della relativa classe, sulla relativa banda... (i.e., mean_cam_2d_per_band) \n",
    "\n",
    "        '''\n",
    "\n",
    "        #cams_per_band_2d\n",
    "        #Durante il masking loop appendi in cams_per_band_2d[cls][b] il CAM 2D (9×9) di ogni trial.\n",
    "        #La media finale:\n",
    "\n",
    "        mean_cams_per_band_2d = {\n",
    "            cls: [ np.mean(np.stack(cams_per_band_2d[cls][b]), axis=0)\n",
    "                   for b in range(n_bands) ]\n",
    "            for cls in [0,1]\n",
    "        }\n",
    "        # mean_cam_2d_per_band[cls][b] shape = (9,9)\n",
    "\n",
    "        '''\n",
    "        Con queste tre strutture —\n",
    "\n",
    "        mean_raw_power_per_band_2d (5 mappe 9×9),\n",
    "\n",
    "        mean_cam_2d_per_band (5 mappe 9×9),\n",
    "\n",
    "        global_mean_cams_2d (1 mappa 9×9) —\n",
    "\n",
    "        puoi costruire esattamente le stesse righe di plot che avevi per il caso 3D, solo che al posto di “slice” del volume userai le CAM 2D mascherate.\n",
    "\n",
    "\n",
    "        '''\n",
    "    \n",
    "    # Preleva la struttura corretta in base al modello\n",
    "    \n",
    "    if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        mean_cams_per_band = mean_cams_per_band_2d\n",
    "        mean_raw_power_per_band = mean_raw_power_per_band_2d\n",
    "        global_mean_cams = global_mean_cams_2d\n",
    "    \n",
    "    else:  # Caso per modello CNN3D\n",
    "        mean_cams_per_band = mean_cams_per_band_3d\n",
    "        mean_raw_power_per_band = mean_raw_power_per_band_3d\n",
    "        \n",
    "        #global_mean_cams = global_mean_cams_3d\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''NUOVA MODIFICA''' \n",
    "        global_mean_cams = global_2d_from3d\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # prima di salvare la figura, solo se richiesto vedi i valori delle potenze medie per banda e condizione sperimentale...\n",
    "    \n",
    "    #\"tag_names=[f\"{model.__class__.__name__} power\"]\")\n",
    "    \n",
    "    if debug:\n",
    "        if isinstance(model, CNN3D_LSTM_FC):\n",
    "            model_tag = f\"{model.__class__.__name__} power\"\n",
    "            check_negative_residuals(band_names,\n",
    "                                     mean_raw_power_per_band_3d,\n",
    "                                     model_tag)\n",
    "        else:\n",
    "            model_tag = f\"{model.__class__.__name__} power\"\n",
    "            check_negative_residuals(band_names,\n",
    "                                     mean_raw_power_per_band_2d,\n",
    "                                     model_tag)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Crea la figura dinamicamente in base al modello\n",
    "    if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        fig, axs = plt.subplots(8, 5, figsize=(24, 30))  # 2 righe per 5 colonne per modello 2D\n",
    "    else:\n",
    "        fig, axs = plt.subplots(8, 5, figsize=(24, 30))  # 5 righe per 2 colonne per modello 3D\n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "    title = (\n",
    "        f\"Grad-CAM Mapping over EEG Trials – Experimental Conditions: {exp_cond_display}\\n\\n\"\n",
    "        \"Row 1-2: Histogram of Mean Grad-CAM Raw Values for Each Class and Frequency Band\\n\"\n",
    "        \"Row 3-4: Normalized Mean Grad-CAM Heatmaps for Class 0 (top) and Class 1 (bottom)\\n\"\n",
    "        #\"Row 5-6: Log-Scaled Mean Raw Spectrogram for Class 0 (top) and Class 1 (bottom)\\n\"\n",
    "        \"Row 5-6: Log-Scaled Mean Raw Power Maps for Class 0 (top) and Class 1 (bottom)\\n\"\n",
    "        #\"Row 7-8: Global CAM per Class\"\n",
    "        \"Row 7: Global CAM per Class 0 (left side) and Class 1 (right side)\"\n",
    "        )\n",
    "    \n",
    "    plt.suptitle(title, fontsize=15)\n",
    "\n",
    "    # Spaziatura verticale per evitare sovrapposizione\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    plt.subplots_adjust(hspace = 0.7, wspace = 0.4)  # Fine tuning della spaziatura tra subplot\n",
    "    \n",
    "    #PER PLOT RIGA 1-2 \n",
    "    \n",
    "    from matplotlib.ticker import ScalarFormatter, MaxNLocator\n",
    "\n",
    "    # Crea un formatter per notazione scientifica\n",
    "    sci_formatter = ScalarFormatter(useMathText=True)\n",
    "    \n",
    "    #Questa chiamata serve a forzare il range entro cui usare la notazione scientifica\n",
    "    #Se non metti questo limite, il comportamento può variare leggermente a seconda della scala dei dati —\n",
    "    #a volte sarà decimale (0.0001), altre volte esponenziale (1e-4), e potrebbe non essere uniforme tra subplot.\n",
    "    \n",
    "    sci_formatter.set_powerlimits((-3, 3))  # usa 1e-xxx se valori sono piccoli\n",
    "\n",
    "    for b, b_name in enumerate(band_names):\n",
    "    \n",
    "        for j, cls in enumerate([0, 1]):\n",
    "            \n",
    "            # Calcola l'istogramma dei valori della heatmap media\n",
    "            # rispetto alle 2 classi in base alla banda di frequenza isolata\n",
    "            \n",
    "            ax = axs[0, b] if cls == 0 else axs[1, b]\n",
    "            \n",
    "            '''NEW UPDATED\n",
    "            Histograms are shown with per-band x-limits [0, max] to avoid binning artifacts when distributions are near-zero.\n",
    "            '''\n",
    "            vals = mean_cams_per_band[cls][b].ravel()\n",
    "            vals = vals[np.isfinite(vals)]  # safety\n",
    "            \n",
    "            if vals.size == 0:\n",
    "                ax.text(0.5, 0.5, \"No finite values\", ha=\"center\", va=\"center\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "            \n",
    "            #vmax_h = vals.max() if vals.size else 0.0\n",
    "            #vmax_h = max(vmax_h, 1e-12)  # evita range=(0,0)\n",
    "            \n",
    "            '''\n",
    "            2) Range robusto (upper bound): percentile invece del max\n",
    "            Cosa fa np.percentile(vals, 99.5)?\n",
    "\n",
    "            Prende un valore vmax_h tale che:\n",
    "\n",
    "            il 99.5% dei valori in vals è ≤ vmax_h\n",
    "            e solo lo 0.5% più grande sta sopra (outlier estremi)\n",
    "\n",
    "            Quindi usi come limite destro dell’istogramma non il massimo assoluto, ma un massimo “robusto”.\n",
    "\n",
    "            Perché aiuta?\n",
    "\n",
    "            Nel tuo errore succedeva che:\n",
    "            \n",
    "            max(vals) era grande (magari per pochi pixel),\n",
    "            ma il resto dei valori era piccolissimo / quasi tutto zero,\n",
    "            e con bins=\"auto\" Numpy provava a mettere troppi bin per “risolvere” la parte piccola → esplodeva.\n",
    "\n",
    "            Con vmax_h robusto:\n",
    "\n",
    "            “tagli” gli outlier (solo per l’istogramma),\n",
    "            quindi la scala x è più “umana” e non induce binning patologico.\n",
    "            max(vmax_h, 1e-12) a cosa serve?\n",
    "\n",
    "            Evita il caso in cui vmax_h venga 0 o quasi 0 (es. valori tutti 0):\n",
    "\n",
    "            se vmax_h=0, il range (0, vmax_h) diventa (0,0) e Matplotlib/Numpy si lamentano o producono bin degeneri.\n",
    "            1e-12 è un “minimo tecnico” solo per non rompere il plot.\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            # ✅ invece di usare il max (outlier), usa un upper bound robusto\n",
    "            vmax_h = np.percentile(vals, 99.5)\n",
    "            vmax_h = max(vmax_h, 1e-12)  # evita range=(0,0)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            \n",
    "            3) Bins adattivi + cap\n",
    "            \n",
    "            sqrt(vals.size) cosa significa?\n",
    "\n",
    "            È una regola classica per istogrammi:\n",
    "            più campioni ⇒ più bin (ma lentamente)\n",
    "            è molto stabile e non esplode\n",
    "            Esempio: se vals.size = 81 ⇒ sqrt=9 (ma poi lo porti almeno a 20 col cap minimo).\n",
    "            \n",
    "            Perché il cap 20–120?\n",
    "\n",
    "            È un “freno di sicurezza”:\n",
    "            min 20: evita istogrammi troppo “a blocchi” e poco informativi\n",
    "            max 120: evita istogrammi con centinaia/migliaia di bin che:\n",
    "\n",
    "            pesano tanto in rendering\n",
    "            e possono comunque diventare instabili in casi strani\n",
    "\n",
    "            Quindi ottieni un numero di bin “ragionevole” sempre.\n",
    "\n",
    "            '''\n",
    "            # ✅ bins adattivi + cap (niente \"auto\")\n",
    "            bins_h = int(np.sqrt(vals.size))\n",
    "            bins_h = max(20, min(bins_h, 120))\n",
    "            \n",
    "            '''OLD VERSION'''\n",
    "            #ax.hist(mean_cams_per_band[cls][b].flatten(), bins='auto', color='blue', edgecolor='black')\n",
    "            \n",
    "            '''NEW UPDATED'''\n",
    "            #ax.hist(vals, bins=\"auto\", range=(0, vmax_h), color=\"blue\", edgecolor=\"black\")\n",
    "            \n",
    "            '''\n",
    "            4) Plot dell’istogramma dentro quel range e con quei bin\n",
    "\n",
    "            Cosa fa in pratica?\n",
    "\n",
    "            Considera i valori vals\n",
    "            costruisce un istogramma in bins_h bin\n",
    "            solo nel range [0, vmax_h]\n",
    "            Se ci sono valori > vmax_h (quello 0.5% di outlier), vengono ignorati dall’istogramma (o meglio: “cadono fuori range” e non vengono conteggiati nei bin).\n",
    "            Questo è voluto: ti interessa rappresentare la massa principale.\n",
    "            \n",
    "            SINTESI:\n",
    "\n",
    "            Per questa banda e classe, fammi un istogramma che rappresenti bene il 99.5% dei valori, con un numero di bin che cresce con la quantità di dati\n",
    "            ma non diventa mai folle. Se non ho dati validi, non fare nulla.”\n",
    "\n",
    "            '''\n",
    "            ax.hist(vals, bins=bins_h, range=(0, vmax_h), color=\"blue\", edgecolor=\"black\")\n",
    "            ax.set_xlim(0, vmax_h)\n",
    "            \n",
    "            \n",
    "            ax.set_title(f\"{b_name} - Class {condition_names[cls]}\", fontsize=10)\n",
    "            ax.set_xlabel(\"Grad-CAM Value\")\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            \n",
    "            # ✅ Format tick con notazione scientifica\n",
    "            ax.xaxis.set_major_formatter(sci_formatter)\n",
    "            ax.xaxis.set_major_locator(MaxNLocator(4))      # max ~4 tick\n",
    "            \n",
    "            #ax.tick_params(axis='x', labelrotation=45, labelsize=6)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PER PLOT RIGA 3-4 \n",
    "    \n",
    "    '''\n",
    "    Concateno tutte le mean-CAM (cls 0+1, tutte le bande) in un unico array\n",
    "    in modo da confrontare le Gradcam tra classi e bande tra di loro! \n",
    "    '''\n",
    "    \n",
    "    all_mean_cams = np.concatenate([\n",
    "        mean_cams_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        mean_cams_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    vmin_cam = all_mean_cams.min()\n",
    "    vmax_cam = all_mean_cams.max()\n",
    "    \n",
    "    \n",
    "    for b, band in enumerate(band_names):                                 \n",
    "        \n",
    "        for j, cls in enumerate([0, 1]):\n",
    "            \n",
    "            ax = axs[2, b] if cls == 0 else axs[3, b]\n",
    "            \n",
    "            cam = mean_cams_per_band[cls][b] \n",
    "            \n",
    "            # Controlla se la forma è corretta per l'input di imshow\n",
    "            assert cam.ndim == 2, f\"Expected 2D array, got {cam.ndim}D array\"\n",
    "            \n",
    "            im = ax.imshow(\n",
    "                cam,\n",
    "                cmap = 'RdYlBu_r',\n",
    "                vmin = vmin_cam, \n",
    "                vmax = vmax_cam,\n",
    "                aspect = 'equal',\n",
    "                origin = 'upper'\n",
    "            )\n",
    "            \n",
    "            ticks = np.linspace(vmin_cam, vmax_cam, 6)\n",
    "\n",
    "            cbar = fig.colorbar(\n",
    "                im, ax=ax, orientation='horizontal', pad=0.12, ticks=ticks, format='%.1e')\n",
    "            \n",
    "            cbar.set_ticks(ticks)\n",
    "            cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "\n",
    "            ax.set_title(\n",
    "                f\"{band} - Class {condition_names[cls]}\",\n",
    "                fontsize=10\n",
    "            )\n",
    "\n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(\n",
    "                        x, y, name,\n",
    "                        ha='center', va='center',\n",
    "                        fontsize=6, color='black', weight='bold'\n",
    "                    )\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PER PLOT RIGA 5-6 \n",
    "    \n",
    "    '''\n",
    "    Concateno tutte lo spettogrammam medio logaritmico\n",
    "    da confrontare tra le classi per ogni banda tra di loro! \n",
    "    '''\n",
    "    \n",
    "    from matplotlib import colors\n",
    "\n",
    "    \n",
    "    # Log-trasform delle mappe di potenza per banda\n",
    "    log_mean_power_per_band = {\n",
    "        cls: [np.log1p(mean_raw_power_per_band[cls][b])\n",
    "              for b in range(n_bands)]\n",
    "        for cls in [0, 1]\n",
    "    }\n",
    "    \n",
    "    #Concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    \n",
    "    \n",
    "    # concateno tutte le log mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    all_mean_pow = np.concatenate([\n",
    "        log_mean_power_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        log_mean_power_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    \n",
    "    vmin_pow = all_mean_pow.min()\n",
    "    vmax_pow = all_mean_pow.max()\n",
    "    \n",
    "    \n",
    "    '''NEW UPDATE'''\n",
    "    gamma = 2.5\n",
    "    # NEW: power-law normalization globale (scegli gamma)\n",
    "    norm_pow = colors.PowerNorm(gamma = gamma , vmin=vmin_pow, vmax=vmax_pow)  # prova 1.5–3.0\n",
    "    \n",
    "    '''NEW UPDATE'''\n",
    "    # ✅ (consigliato) colormap percettiva\n",
    "    cmap_pow = \"magma\"  # oppure \"viridis\" / \"cividis\"\n",
    "\n",
    "    \n",
    "    #ticks = np.linspace(vmin_pow, vmax_pow, 6)\n",
    "    \n",
    "    '''NEW UPDATE'''\n",
    "    n_ticks = 6   # <-- metti 3 oppure 4\n",
    "    '''NEW UPDATE'''\n",
    "    u = np.linspace(0, 1, n_ticks)  # posizioni uniformi nello spazio colore\n",
    "    \n",
    "    '''NEW UPDATE'''\n",
    "    # se la tua versione di matplotlib supporta inverse():\n",
    "    try:\n",
    "        tick_vals = norm_pow.inverse(u)\n",
    "    except Exception:\n",
    "        # inversione manuale di PowerNorm\n",
    "        tick_vals = vmin_pow + (vmax_pow - vmin_pow) * (u ** (1.0 / gamma))\n",
    "        \n",
    "    \n",
    "    from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "    sci = ScalarFormatter(useMathText=True)\n",
    "    sci.set_powerlimits((-2, 2))          # forza 1eX fuori dall’intervallo 1e‑2 … 1e2\n",
    "\n",
    "    for b, band in enumerate(band_names):\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            ax = axs[4, b] if cls == 0 else axs[5, b]\n",
    "\n",
    "            log_power = log_mean_power_per_band[cls][b]\n",
    "\n",
    "            im = ax.imshow(\n",
    "                log_power,\n",
    "                \n",
    "                #cmap='jet',\n",
    "                #vmin=vmin_pow,\n",
    "                #vmax=vmax_pow,\n",
    "                \n",
    "                cmap=cmap_pow,\n",
    "                norm=norm_pow,      # ✅ NEW (invece di vmin/vmax)\n",
    "                \n",
    "                aspect='equal',\n",
    "                origin='upper'\n",
    "            )\n",
    "            \n",
    "            #cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.12, ticks = ticks, format= sci)#format='%.1e')\n",
    "            \n",
    "            '''NEW UPDATE'''\n",
    "            cbar = fig.colorbar(im, ax=ax, orientation=\"horizontal\", pad=0.12, ticks = tick_vals, format= sci)\n",
    "\n",
    "            cbar.ax.xaxis.set_major_formatter(sci) # <-- solo formatter\n",
    "            cbar.ax.tick_params(labelsize=6)\n",
    "            \n",
    "            # (opzionale ma utile se cambi ticks a posteriori)\n",
    "            cbar.update_ticks()\n",
    "\n",
    "            ax.set_title(f\"{band} Log Mean Power - Class {condition_names[cls]}\", fontsize=10)\n",
    "            \n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name, ha='center', va='center', fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "                \n",
    "                \n",
    "    '''\n",
    "    OLD PLOTS SPETTROGRAMMA RAW \n",
    "    \n",
    "    \n",
    "    #****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "     \n",
    "    #PER PLOT RIGA 5-6 (SCALA LINEARE)\n",
    "    \n",
    "    from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "    sci = ScalarFormatter(useMathText=True)\n",
    "    sci.set_powerlimits((-2, 2))          # forza 1eX fuori dall’intervallo 1e‑2 … 1e2\n",
    "\n",
    "    \n",
    "    #Concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    \n",
    "    \n",
    "    # concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    all_mean_pow = np.concatenate([\n",
    "        mean_raw_power_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        mean_raw_power_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    vmin_pow = all_mean_pow.min()\n",
    "    vmax_pow = all_mean_pow.max()\n",
    "    \n",
    "    \n",
    "    ticks = np.linspace(vmin_pow, vmax_pow, 6)\n",
    "    \n",
    "    # Riga 3: Mappa della potenza media rispetto a distribuzione congiunta (su ciascuna banda e classe)\n",
    "    for b, band in enumerate(band_names):\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            ax = axs[4, b] if cls == 0 else axs[5, b]\n",
    "            \n",
    "            power = mean_raw_power_per_band[cls][b] \n",
    "            \n",
    "            im = ax.imshow(\n",
    "                power, \n",
    "                cmap='jet',\n",
    "                vmin= vmin_pow,\n",
    "                vmax= vmax_pow,\n",
    "                aspect='equal',\n",
    "                origin='upper'\n",
    "            )\n",
    "            \n",
    "            #ticks = np.linspace(vmin_pow, vmax_pow, 6)\n",
    "            \n",
    "            cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.12, ticks = ticks, format= sci)#format='%.1e')\n",
    "            \n",
    "            #cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            #cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "            \n",
    "            cbar.ax.xaxis.set_major_formatter(sci)   # <-- solo formatter\n",
    "            cbar.ax.tick_params(labelsize=6)\n",
    "            \n",
    "            ax.set_title(f\"{band} Power - Class {condition_names[cls]}\", fontsize=10)\n",
    "            \n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name, ha='center', va='center', fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "                \n",
    "                \n",
    "    \n",
    "    #****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ****** ******\n",
    "    \n",
    "    #PER PLOT RIGA 5-6 (SCALA LOGARITMICA)\n",
    "        \n",
    "    \n",
    "    # ----- 1. calcola vmin_pow / vmax_pow -----\n",
    "    \n",
    "    #Concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    \n",
    "    \n",
    "    # concateno tutte le mean-power (cls 0+1, tutte le bande) in un unico array\n",
    "    all_mean_pow = np.concatenate([\n",
    "        mean_raw_power_per_band[0][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ] + [\n",
    "        mean_raw_power_per_band[1][b].flatten()\n",
    "        for b in range(n_bands)\n",
    "    ])\n",
    "    vmin_pow = all_mean_pow.min()\n",
    "    vmax_pow = all_mean_pow.max()\n",
    "    \n",
    "    #Filtra solo i valori strettamente > 0 (la scala log non accetta zeri o negativi).\n",
    "    #Perché? Se l’intero array fosse ≤ 0 (caso patologico) avremmo positive.size == 0.\n",
    "    positive = all_mean_pow[all_mean_pow > 0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #In pratica: stiamo abbassando il bordo inferiore del colormap di un 10 % rispetto al minimo positivo reale, \n",
    "    #così quei pixel non finiscono “incollati” al limite della color‑bar. Se preferisci usare un altro margine (5 %, 1 %) basta cambiare 0.9 in 0.95, 0.99, ecc.\n",
    "    #Se invece vuoi proprio che vada esattamente sul minimo, puoi togliere *0.9 (ma occhio ai warning di Matplotlib)\n",
    "    \n",
    "    #Il resto del blocco:\n",
    "\n",
    "    #Calcola vmax_pow dal massimo globale.\n",
    "    #Decide automaticamente use_log se il dynamic‑range supera 10³.\n",
    "    #Imposta una sola logica di plotting: quando use_log è True usa LogNorm, LogLocator e LogFormatterMathtext; altrimenti scala lineare + ScalarFormatter.\n",
    "\n",
    "    #I titoli aggiungono “(log10)” solo quando serve.\n",
    "    #Nota: quando use_log è True, passiamo vmin/vmax tramite LogNorm; quando è False, li passiamo direttamente a imshow con i parametri vmin=…, vmax=….\n",
    "    #Così la stessa funzione disegna correttamente entrambe le situazioni senza dover duplicare codice.\n",
    "    \n",
    "    \n",
    "    #1. Se esistono valori positivi, prende il più piccolo e lo moltiplica per 0.9 (−10 %).\n",
    "    # Obiettivo: Creare un piccolo margine: il vero minimo non cade esattamente sul bordo inferiore della scala log, evitando clip / warning.\n",
    "    \n",
    "    #2. Se non esistono, imposta un fallback sicur0\n",
    "    # Obiettivo: Garantire che vmin_pow > 0 in ogni caso (requisito di LogNorm).\n",
    "    \n",
    "    #vmin_pow = positive.min()*0.9 if positive.size else 1e-12\n",
    "    #vmin_pow = positive.min() if positive.size else 1e-12\n",
    "    \n",
    "    vmin_pow = positive.min()\n",
    "    \n",
    "    vmax_pow = all_mean_pow.max()\n",
    "\n",
    "    use_log  = vmax_pow / max(vmin_pow, 1e-12) > 1e3   # o il flag suggest_log\n",
    "\n",
    "    if use_log:\n",
    "        norm      = LogNorm(vmin=vmin_pow, vmax=vmax_pow)\n",
    "        locator   = LogLocator(base=10.0)\n",
    "        formatter = LogFormatterMathtext(base=10.0)\n",
    "    else:\n",
    "        norm      = None\n",
    "        locator   = None\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_powerlimits((-2, 2))         # 1e‑2 – 1e2 lineare\n",
    "\n",
    "    # ----- 2. plot -----\n",
    "    for b, band in enumerate(band_names):\n",
    "        for cls in (0, 1):\n",
    "            ax   = axs[4, b] if cls == 0 else axs[5, b]\n",
    "            pow_ = mean_raw_power_per_band[cls][b]\n",
    "\n",
    "            im = ax.imshow(pow_, cmap='jet', norm=norm,\n",
    "                           vmin=None if use_log else vmin_pow,\n",
    "                           vmax=None if use_log else vmax_pow,\n",
    "                           aspect='equal', origin='upper')\n",
    "\n",
    "            cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.12)\n",
    "            \n",
    "            if locator is not None:\n",
    "                cbar.locator   = locator\n",
    "            cbar.formatter = formatter\n",
    "            cbar.update_ticks()\n",
    "            cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "            scale = \"(log10)\" if use_log else \"\"\n",
    "            ax.set_title(f\"{band} Power {scale} – Class {condition_names[cls]}\",\n",
    "                         fontsize=10)\n",
    "            \n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name, ha='center', va='center', fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PER PLOT RIGA 7-8\n",
    "    \n",
    "    '''\n",
    "    Vorrei solo verificare allora l'ultima riga 7-8 per la differenza tra i due modelli, perchè: \n",
    "    \n",
    "    \n",
    "    1) Nel caso del modello 3d puro, ho ancora una fetta rappresentata, ossia\n",
    "\n",
    "    per global_cams_3d vado ad ottenere una media, ossia una global_mean_cams_3d, che riassume il contributo GLOBALE della gradcam 3D aggregata\n",
    "    all'interno dell'intero volume 3D (che poi al massimo si può scorporare vedendo per ogni banda successivamante) \n",
    "\n",
    "    ed è quello che vorrei fare per il modello Conv3D puro...\n",
    "\n",
    "    2) per il modello Conv Separabili invece, \n",
    "\n",
    "    per global_cams_2d vado ad ottenere una media, ossia una global_mean_cams_2d, che riassume il contributo GLOBALE della gradcam 2D aggregata\n",
    "    all'interno di TUTTE LE BANDE ASSIEME (che mi dovrebbe dare quindi per OGNI CLASSE un plot unico, \n",
    "    \n",
    "    e non come il global_mean_cams_3d, dove dovrei vedere in quel caso, invece, la stessa mappa di “rilevanza complessiva”, MA distribuita lungo la profondità,\n",
    "    ossia tra le bande e quindi potrei vedere se effettivamente io abbia una banda che è specificatamente più attiva di altre COMPLESSIVAMENTE ...\n",
    "\n",
    "    devo verificare che per queste righe 7-8, a seconda del modello, il codice sia corretto, in base a come so che \n",
    "\n",
    "    global_mean_cams_3d e global_mean_cams_2d sono in realtà adesso ossia \n",
    "    \n",
    "    global_mean_cams_3d[cls]\t(5, 9, 9)\tvolume medio 3D\n",
    "    global_mean_cams_2d[cls]\t(9, 9)\theatmap 2D “globale” su tutte le bande\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Costruiamo la distribuzione congiunta della media del singolo input multi-canale per ogni classe\n",
    "    '''\n",
    "    \n",
    "    all_global_mean_cams = np.concatenate([global_mean_cams[0].flatten(), global_mean_cams[1].flatten()])\n",
    "    \n",
    "    global_vmin_cam = all_global_mean_cams.min()\n",
    "    global_vmax_cam = all_global_mean_cams.max()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    In sintesi:\n",
    "\n",
    "    CNN3D: global_mean_cams_3d[cls] è già shape (5,9,9), quindi fai subito mat2d = global_mean_cams_3d[cls][b]\n",
    "    SeparableCNN2D: global_mean_cams_2d[cls] è shape (9,9), e la metti in axs[6, cls]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #'''Global CAM 2D: una mappa per classe, entrambe su riga 6'''\n",
    "    \n",
    "    if isinstance(model, SeparableCNN2D_LSTM_FC):\n",
    "        \n",
    "        #mean_cams_per_band = mean_cams_per_band_2d\n",
    "        #mean_raw_power_per_band = mean_raw_power_per_band_2d\n",
    "        global_mean_cams = global_mean_cams_2d\n",
    "        \n",
    "        # Global 2D: una sola heatmap per classe\n",
    "        for cls in [0, 1]:\n",
    "            ax = axs[6, cls]\n",
    "            mat2d = global_mean_cams[cls]  # (9,9)\n",
    "            im = ax.imshow(mat2d,\n",
    "                           cmap='RdYlBu_r',\n",
    "                           vmin=global_vmin_cam,\n",
    "                           vmax=global_vmax_cam,\n",
    "                           aspect='equal',\n",
    "                           origin='upper')\n",
    "            ticks = np.linspace(global_vmin_cam, global_vmax_cam, 6)\n",
    "            cbar = fig.colorbar(im, ax=ax,\n",
    "                                orientation='horizontal',\n",
    "                                pad=0.12,\n",
    "                                ticks=ticks,\n",
    "                                format='%.1e')\n",
    "            cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "            ax.set_title(f\"Global CAM 2D (across bands) – Class {condition_names[cls]}\", fontsize=10)\n",
    "\n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([]);  ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name,\n",
    "                            ha='center', va='center',\n",
    "                            fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        # spegni i subplot vuoti\n",
    "        for col in range(2, n_bands):\n",
    "            axs[6, col].axis(\"off\")\n",
    "        for col in range(n_bands):\n",
    "            axs[7, col].axis(\"off\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        '''\n",
    "        # Global 3D: una heatmap per banda e per classe\n",
    "        #mean_cams_per_band = mean_cams_per_band_3d\n",
    "        #mean_raw_power_per_band = mean_raw_power_per_band_3d\n",
    "        \n",
    "        global_mean_cams = global_mean_cams_3d\n",
    "        \n",
    "        for b, band in enumerate(band_names):\n",
    "            \n",
    "            #for cls in [0, 1]:\n",
    "            for j, cls in enumerate([0, 1]):\n",
    "                \n",
    "                #ax = axs[6 + cls, b]  # cls==0→riga6, cls==1→riga7\n",
    "                \n",
    "                ax = axs[6, b] if cls == 0 else axs[7, b]\n",
    "                \n",
    "                vol3d = global_mean_cams[cls]     # (5,9,9) --> perché? \n",
    "                                                  # Perché sopra è stato fatto 'global_cams_3d[cls].append(cam_vol[0])'\n",
    "                                                  # Quindi ogni dato non era più fatto da (B, D, W, H) dove B = 1 (ossia l'esempio stesso)\n",
    "                                                  # Per cui dopo in global_mean_cams_3d quando ho fatto la media, ho ottenuto una rappresentazione MEDIA\n",
    "                                                  # del gradcam 3D, PER OGNI BANDA. Quindi, quando prelevo la SINGOLA BANDA, basta che faccio lo 'slicing' ossia\n",
    "                                                  # mat2d = vol3d[b]  --> da (5,9,9) diventa --> (9,9)\n",
    "                \n",
    "                mat2d = vol3d[b]                  # slice b → (9,9)\n",
    "                \n",
    "                im = ax.imshow(mat2d,\n",
    "                               cmap='RdYlBu_r',\n",
    "                               vmin=global_vmin_cam,\n",
    "                               vmax=global_vmax_cam,\n",
    "                               aspect='equal',\n",
    "                               origin='upper')\n",
    "                ticks = np.linspace(global_vmin_cam, global_vmax_cam, 6)\n",
    "                cbar = fig.colorbar(im, ax=ax,\n",
    "                                    orientation='horizontal',\n",
    "                                    pad=0.12,\n",
    "                                    ticks=ticks,\n",
    "                                    format='%.1e')\n",
    "                cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "                cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "                ax.set_title(f\"{band} Global CAM 3D – Class {condition_names[cls]}\", fontsize=10)\n",
    "\n",
    "                if channel_names is not None:\n",
    "                    ax.set_xticks([]);  ax.set_yticks([])\n",
    "                    for name, (y, x) in channel_names.items():\n",
    "                        ax.text(x, y, name,\n",
    "                                ha='center', va='center',\n",
    "                                fontsize=6, color='black', weight='bold')\n",
    "                else:\n",
    "                    ax.axis(\"off\")\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # Global 3D: una heatmap per classe TRA le bande\n",
    "        '''\n",
    "        \n",
    "        # ➜ CNN3D: una sola mappa 2D globale per classe (media sulle bande)\n",
    "        #   global_2d_from3d[cls] ha shape (9,9)\n",
    "        \n",
    "        global_mean_cams = global_2d_from3d\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            ax = axs[6, cls]   # uso riga 7, colonne 0 e 1\n",
    "            mat2d = global_2d_from3d[cls]  # (9,9)\n",
    "\n",
    "            im = ax.imshow(\n",
    "                mat2d,\n",
    "                cmap='RdYlBu_r',\n",
    "                vmin=global_vmin_cam,\n",
    "                vmax=global_vmax_cam,\n",
    "                aspect='equal',\n",
    "                origin='upper'\n",
    "            )\n",
    "            ticks = np.linspace(global_vmin_cam, global_vmax_cam, 6)\n",
    "            cbar = fig.colorbar(\n",
    "                im, ax=ax,\n",
    "                orientation='horizontal',\n",
    "                pad=0.12,\n",
    "                ticks=ticks,\n",
    "                format='%.1e'\n",
    "            )\n",
    "            cbar.ax.xaxis.set_major_locator(FixedLocator(ticks))\n",
    "            cbar.set_ticklabels([f\"{t:.2f}\" for t in ticks])\n",
    "            ax.set_title(f\"Global CAM 3D (across bands) – Class {condition_names[cls]}\", fontsize=10)\n",
    "\n",
    "            if channel_names is not None:\n",
    "                ax.set_xticks([]); ax.set_yticks([])\n",
    "                for name, (y, x) in channel_names.items():\n",
    "                    ax.text(x, y, name,\n",
    "                            ha='center', va='center',\n",
    "                            fontsize=6, color='black', weight='bold')\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        # spegni gli altri subplot della riga 7–8\n",
    "        for col in range(2, n_bands):\n",
    "            axs[6, col].axis(\"off\")\n",
    "        for col in range(n_bands):\n",
    "            axs[7, col].axis(\"off\")\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    # ❸ — Ripristino allo stato precedente il modello ottimizzato trovato migliore, che aveva incluso anche layer LSTM\n",
    "    # ------------------------------------------------------------ ------------------------------------------------------------\n",
    "    \n",
    "    if needs_train_mode:\n",
    "        # ➌ ripristino layer singoli (i.e., riporto BN/Dropout dove stavano in eval mode)\n",
    "        for m, old_flag in saved:\n",
    "            m.train(old_flag)\n",
    "        # ➍ ripristino lo stato globale del modello (di nuovo ad .eval())\n",
    "        # i.e.,  come era stato passato in input alla funzione compute_gradcam_figure a partire 'load_best_run_results'!\n",
    "        \n",
    "        #Così simuli l’eval (Dropout off, BN congelato) pur essendo in train() per soddisfare CuDNN‑RNN.\n",
    "        model.train(was_training)\n",
    "        \n",
    "    \n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # ---- layout finale (dopo aver aggiunto TUTTO) ----\n",
    "    print(\">>> layout\", flush=True)\n",
    "    try:\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.95])   # <-- usa fig, non plt\n",
    "    except Exception as e:\n",
    "        print(\"tight_layout skipped:\", e, flush=True)\n",
    "        fig.subplots_adjust(left=0.04, right=0.98, bottom=0.05, top=0.92,\n",
    "                            wspace=0.4, hspace=0.7)\n",
    "                                \n",
    "    \n",
    "    #Passaggio 8: Salvataggio della figura\n",
    "    #Qui la figura viene salvata in un buffer di memoria, pronto per essere salvato o inviato altrove\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Passaggio 8: Salvataggio della figura in un buffer\n",
    "    # -------------\n",
    "    \n",
    "    print(\">>> savefig\", flush=True)\n",
    "\n",
    "    # Salva la figura in un buffer (che potrai poi passare a save_performance_results)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    fig_image = buf.getvalue()\n",
    "    buf.close()\n",
    "    plt.close(fig)\n",
    "    print(\">>> done\", flush=True)\n",
    "    return fig_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8347d-75b5-4f8b-b985-fe63c7eb8dd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **NUOVO LOOP PER DATI NON HYPER SU CNN2D, BiLSTM e Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a986fbd-755c-4f01-9568-cac47286c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import random\n",
    "#perché è importante numpy.random.seed()?\n",
    "#https://www.analyticsvidhya.com/blog/2021/12/what-does-numpy-random-seed-do/#:~:text=The%20numpy%20random%20seed%20is,displays%20the%20same%20random%20numbers.\n",
    "from numpy.random import seed\n",
    "\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "#importing librerie pytorch\n",
    "import torch \n",
    "import torch.nn as nn #neural network module\n",
    "import torch.optim as optim #ottimizzatore\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "#importing librerie numpy, pandas, scikit-learn e matplotlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66c5799c-2f4a-42e3-a253-8f5e659ee983",
   "metadata": {
    "tags": []
   },
   "source": [
    "#LOOP PER CARICARE I DATI NON HYPER\n",
    "data_dict = {}\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "\n",
    "    for data_type in [\"spectrograms\"]:\n",
    "        \n",
    "        for category in [\"familiar\", \"unfamiliar\"]:\n",
    "            \n",
    "            for subject_type in [\"th\", \"pt\"]:\n",
    "            \n",
    "                # Caricamento e suddivisione dei dati\n",
    "                if data_type == \"wavelet\":\n",
    "                    X, y = load_data(data_type, category, subject_type, wavelet_level=\"delta\")\n",
    "                else:\n",
    "                    X, y = load_data(data_type, category, subject_type)\n",
    "\n",
    "                #key = f\"{condition}/{data_type}_{category}_{subject_type}\"\n",
    "                key = f\"{condition}_{data_type}_{category}_{subject_type}\"\n",
    "                data_dict[key] = (X, y)\n",
    "\n",
    "                # Stampa di conferma\n",
    "                print(f\"Dataset caricato: \\033[1m{key}\\033[0m - Forma X: {X.shape}, Lunghezza y: {len(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e63823aa-83ba-4c46-bbd7-34a62cba1e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricato: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m - Forma X: (1586, 45, 61), Lunghezza y: 1586\n",
      "Dataset caricato: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m - Forma X: (1580, 45, 61), Lunghezza y: 1580\n",
      "Dataset caricato: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m - Forma X: (1667, 45, 61), Lunghezza y: 1667\n",
      "Dataset caricato: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m - Forma X: (1667, 45, 61), Lunghezza y: 1667\n",
      "Dataset caricato: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m - Forma X: (1586, 45, 61), Lunghezza y: 1586\n",
      "Dataset caricato: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m - Forma X: (1580, 45, 61), Lunghezza y: 1580\n",
      "Dataset caricato: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m - Forma X: (1667, 45, 61), Lunghezza y: 1667\n",
      "Dataset caricato: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m - Forma X: (1667, 45, 61), Lunghezza y: 1667\n",
      "Dataset caricato: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m - Forma X: (1586, 45, 61), Lunghezza y: 1586\n",
      "Dataset caricato: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m - Forma X: (1580, 45, 61), Lunghezza y: 1580\n",
      "Dataset caricato: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m - Forma X: (1667, 45, 61), Lunghezza y: 1667\n",
      "Dataset caricato: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m - Forma X: (1667, 45, 61), Lunghezza y: 1667\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    \n",
    "    # inizializza livello condition\n",
    "    data_dict.setdefault(condition, {})\n",
    "    \n",
    "    for data_type in [\"spectrograms\"]:\n",
    "        \n",
    "        # inizializza livello data_type\n",
    "        data_dict[condition].setdefault(data_type, {})\n",
    "        \n",
    "        for category in [\"familiar\", \"unfamiliar\"]:\n",
    "            \n",
    "            for subject_type in [\"th\", \"pt\"]:\n",
    "                \n",
    "                # Caricamento dati\n",
    "                if data_type == \"wavelet\":\n",
    "                    X, y = load_data(data_type, category, subject_type, wavelet_level=\"delta\")\n",
    "                else:\n",
    "                    X, y = load_data(data_type, category, subject_type)\n",
    "\n",
    "                # chiave del terzo livello (come si aspetta la funzione di conversione)\n",
    "                category_subject = f\"{category}_{subject_type}\"   # es. \"familiar_th\"\n",
    "\n",
    "                data_dict[condition][data_type][category_subject] = (X, y)\n",
    "\n",
    "                # stampa di controllo (solo per debug)\n",
    "                flat_key = f\"{condition}_{data_type}_{category}_{subject_type}\"\n",
    "                print(f\"Dataset caricato: \\033[1m{flat_key}\\033[0m - Forma X: {X.shape}, Lunghezza y: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dac599-c4c3-4d32-a7f7-ec50f598f1bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Creazione Griglia 2D per Interrogait - EEG Spectrograms - Electrodes x Frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d4001d7-8c8d-4bd5-9a21-8192074d1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "with open(f\"{path}EEG_channels_names.pkl\", \"rb\") as f:\n",
    "    EEG_channels_names = pickle.load(f)\n",
    "    \n",
    "# Caricare file xlsx con pickle\n",
    "path_xlsx = f'{path}EEG_grid_interrogait.xlsx'\n",
    "\n",
    "# Caricamento del file in un DataFrame\n",
    "EEG_file_interrogait = pd.read_excel(path_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2d35c88-e011-4d33-be97-4519a4de3df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electrode</th>\n",
       "      <th>grid_x</th>\n",
       "      <th>grid_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMPTY</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EMPTY</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EMPTY</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fp1</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fpz</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Oz</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>O2</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>EMPTY</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>EMPTY</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>EMPTY</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electrode  grid_x  grid_y\n",
       "0      EMPTY   0.000     0.0\n",
       "1      EMPTY   0.125     0.0\n",
       "2      EMPTY   0.250     0.0\n",
       "3        Fp1   0.375     0.0\n",
       "4        Fpz   0.500     0.0\n",
       "..       ...     ...     ...\n",
       "76        Oz   0.500     1.0\n",
       "77        O2   0.625     1.0\n",
       "78     EMPTY   0.750     1.0\n",
       "79     EMPTY   0.875     1.0\n",
       "80     EMPTY   1.000     1.0\n",
       "\n",
       "[81 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEG_file_interrogait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34b3a21b-c95a-403d-bceb-f0e0e1e56b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "def _build_grid_maps(\n",
    "    eeg_grid_df: pd.DataFrame,\n",
    "    eeg_channels_names: List[str],\n",
    "    grid_shape: Tuple[int, int] = (9, 9),\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea:\n",
    "      - label_grid: matrice (9x9) di etichette (elettrodi o 'EMPTY')\n",
    "      - electrode_grid_map: dict {elettrodo -> (y, x)} (solo elettrodi reali)\n",
    "      - placement_idx: matrice (9x9) di indici canale (>=0) o -1 per EMPTY/non presenti\n",
    "    \"\"\"\n",
    "    df = eeg_grid_df.copy()\n",
    "    df[\"Electrode\"] = df[\"Electrode\"].astype(str).str.strip()\n",
    "\n",
    "    H, W = grid_shape\n",
    "    label_grid = np.full((H, W), \"\", dtype=object)\n",
    "    electrode_grid_map = {}\n",
    "\n",
    "    # Mappa canale -> indice colonna in X_data\n",
    "    ch_to_idx = {ch: i for i, ch in enumerate(eeg_channels_names)}\n",
    "\n",
    "    # Matrice con indici canale (per riempimento veloce delle griglie)\n",
    "    placement_idx = np.full((H, W), -1, dtype=int)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        elec = row[\"Electrode\"]\n",
    "        x = int(round(row[\"grid_x\"] * (W - 1)))\n",
    "        y = int(round(row[\"grid_y\"] * (H - 1)))\n",
    "\n",
    "        label_grid[y, x] = \"\" if elec == \"EMPTY\" else elec\n",
    "\n",
    "        if elec != \"EMPTY\":\n",
    "            electrode_grid_map[elec] = (y, x)\n",
    "            if elec in ch_to_idx:\n",
    "                placement_idx[y, x] = ch_to_idx[elec]\n",
    "            # se l'elettrodo non è nella lista canali, placement resta -1 (verrà messo 0 in griglia)\n",
    "\n",
    "    # Controllo elettrodi presenti nell'Excel ma non nei dati\n",
    "    excel_elec = set(df[\"Electrode\"].unique()) - {\"EMPTY\"}\n",
    "    missing = sorted(elec for elec in excel_elec if elec not in ch_to_idx)\n",
    "    if missing:\n",
    "        print(\"⚠️ Elettrodi nel file Excel ma non presenti in EEG_channels_names:\", missing)\n",
    "\n",
    "    return label_grid, electrode_grid_map, placement_idx\n",
    "\n",
    "\n",
    "def convert_fft_images_to_2d_grids_all_freqs_interrogait(\n",
    "    data_dict: Dict[str, Dict[str, Dict[str, Tuple[np.ndarray, np.ndarray]]]],\n",
    "    eeg_grid_df: pd.DataFrame,\n",
    "    eeg_channels_names: List[str],\n",
    "    grid_shape: Tuple[int, int] = (9, 9),\n",
    "    fs: int = 250,\n",
    "    n_fft_points: int = 250,\n",
    "    bands: Dict[str, Tuple[float, float]] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Dict, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Converte OGNI X_data nella struttura annidata di `data_dict`:\n",
    "      (B, n_freqs, n_channels)  →  (B, 9, 9, 5)\n",
    "    sommando la potenza sulle frequenze per ciascuna banda EEG e mappando\n",
    "    i canali nelle posizioni (y,x) definite dal file Excel della griglia.\n",
    "\n",
    "    Struttura in input (immutata nelle chiavi):\n",
    "      data_dict[condition][data_type][category_subject] = (X_data, y_data)\n",
    "\n",
    "    In output mantiene la stessa struttura ma con X_data trasformato:\n",
    "      X_grid: (B, 9, 9, 5),  y invariato.\n",
    "\n",
    "    Ritorna:\n",
    "      - new_data_dict: stessa struttura annidata con X trasformati\n",
    "      - label_grid: matrice (9x9) con le etichette\n",
    "      - electrode_grid_map: dict {elettrodo -> (y, x)}\n",
    "    \"\"\"\n",
    "    if bands is None:\n",
    "        # Ordine fisso (profondità = 5)\n",
    "        bands = {\n",
    "            \"delta\": (1, 4),\n",
    "            \"theta\": (4, 8),\n",
    "            \"alpha\": (8, 13),\n",
    "            \"beta\":  (13, 30),\n",
    "            \"gamma\": (30, 45),\n",
    "        }\n",
    "    band_order = [\"delta\", \"theta\", \"alpha\", \"beta\", \"gamma\"]\n",
    "\n",
    "    # Precostruisco mappe della griglia\n",
    "    label_grid, electrode_grid_map, placement_idx = _build_grid_maps(\n",
    "        eeg_grid_df=eeg_grid_df,\n",
    "        eeg_channels_names=eeg_channels_names,\n",
    "        grid_shape=grid_shape,\n",
    "    )\n",
    "\n",
    "    H, W = grid_shape\n",
    "\n",
    "    # Trovo un esempio per determinare n_freqs effettivi (bins) e costruire le maschere\n",
    "    example_found = False\n",
    "    n_freqs_example = None\n",
    "    n_channels_example = None\n",
    "\n",
    "    for condition, data_types in data_dict.items():\n",
    "        for data_type, categories in data_types.items():\n",
    "            for category_subject, (X_data, y_data) in categories.items():\n",
    "                if X_data is not None and len(X_data) > 0:\n",
    "                    n_freqs_example = X_data.shape[1]\n",
    "                    n_channels_example = X_data.shape[2]\n",
    "                    example_found = True\n",
    "                    break\n",
    "            if example_found:\n",
    "                break\n",
    "        if example_found:\n",
    "            break\n",
    "\n",
    "    if not example_found:\n",
    "        raise ValueError(\"Impossibile determinare n_freqs/n_channels: data_dict è vuoto?\")\n",
    "\n",
    "    # Frequenze in Hz per i bins RFFT (tronco ai primi n_freqs effettivi)\n",
    "    all_freqs_full = np.fft.rfftfreq(n_fft_points, d=1.0 / fs)\n",
    "    all_freqs = all_freqs_full[:n_freqs_example]\n",
    "\n",
    "    # Maschere per ciascuna banda sull'asse delle frequenze\n",
    "    band_masks = {\n",
    "        b: (all_freqs >= fmin) & (all_freqs <= fmax) for b, (fmin, fmax) in bands.items()\n",
    "    }\n",
    "\n",
    "    # Avvisi utili\n",
    "    if verbose:\n",
    "        print(f\"fs={fs} Hz, n_fft_points={n_fft_points}\")\n",
    "        print(f\"n_freqs in X_data = {n_freqs_example} (verranno usati i primi {n_freqs_example} bins di rfftfreq)\")\n",
    "        print(\"Bande usate:\", {b: bands[b] for b in band_order})\n",
    "\n",
    "    # Trasformazione\n",
    "    new_data_dict = {}\n",
    "    for condition, data_types in data_dict.items():\n",
    "        new_data_dict.setdefault(condition, {})\n",
    "        for data_type, categories in data_types.items():\n",
    "            new_data_dict[condition].setdefault(data_type, {})\n",
    "\n",
    "            for category_subject, (X_data, y_data) in categories.items():\n",
    "                \n",
    "                # X_data: (B, n_freqs, n_channels)\n",
    "                if X_data is None or X_data.size == 0:\n",
    "                    new_data_dict[condition][data_type][category_subject] = (X_data, y_data)\n",
    "                    continue\n",
    "\n",
    "                B, n_freqs, n_channels = X_data.shape\n",
    "                if n_freqs != n_freqs_example:\n",
    "                    # Le maschere sono state costruite su n_freqs_example; se differisce, le rigenero on-the-fly\n",
    "                    all_freqs_local = np.fft.rfftfreq(n_fft_points, d=1.0 / fs)[:n_freqs]\n",
    "                    band_masks_local = {\n",
    "                        b: (all_freqs_local >= bands[b][0]) & (all_freqs_local <= bands[b][1])\n",
    "                        for b in band_order\n",
    "                    }\n",
    "                else:\n",
    "                    band_masks_local = band_masks\n",
    "\n",
    "                if n_channels != len(eeg_channels_names):\n",
    "                    print(\n",
    "                        f\"⚠️ Attenzione: n_channels={n_channels} \"\n",
    "                        f\"diverso da len(EEG_channels_names)={len(eeg_channels_names)} \"\n",
    "                        f\"per {condition} / {data_type} / {category_subject}. \"\n",
    "                        f\"Userò SOLO i canali presenti in placement_idx (gli altri verranno ignorati).\"\n",
    "                    )\n",
    "\n",
    "                # Output per questo blocco: (B, H, W, 5)\n",
    "                X_out = np.zeros((B, H, W, len(band_order)), dtype=X_data.dtype)\n",
    "\n",
    "                # Precompute posizione valide nella griglia (non EMPTY)\n",
    "                valid_pos = placement_idx >= 0\n",
    "                idx_lin = placement_idx[valid_pos]  # indici canale per le posizioni valide\n",
    "                yy, xx = np.where(valid_pos)       # coordinate y,x da riempire\n",
    "\n",
    "                for b in range(B):\n",
    "                    # Per ciascuna banda: somma lungo le frequenze → vettore (n_channels,)\n",
    "                    per_band_grids = []\n",
    "                    sample = X_data[b]  # (n_freqs, n_channels)\n",
    "\n",
    "                    for bi, band_name in enumerate(band_order):\n",
    "                        mask = band_masks_local[band_name]\n",
    "                        if not np.any(mask):\n",
    "                            # nessun bin in banda → griglia a zero\n",
    "                            continue\n",
    "\n",
    "                        # potenza totale per canale nella banda\n",
    "                        band_power_per_ch = sample[mask, :].sum(axis=0)  # (n_channels,)\n",
    "\n",
    "                        # riempi griglia rapidamente con indicizzazione\n",
    "                        grid = np.zeros((H, W), dtype=sample.dtype)\n",
    "                        # assegna solo posizioni con elettrodi mappati (placement_idx >= 0)\n",
    "                        grid[yy, xx] = band_power_per_ch[idx_lin]\n",
    "\n",
    "                        X_out[b, :, :, bi] = grid\n",
    "\n",
    "                new_data_dict[condition][data_type][category_subject] = (X_out, y_data)\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"[OK] {condition} / {data_type} / {category_subject} : \"\n",
    "                        f\"{X_data.shape}  →  {X_out.shape}\"\n",
    "                    )\n",
    "\n",
    "    return new_data_dict, label_grid, electrode_grid_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e58622b3-fcc5-4bbb-a17b-5fa4bef28b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fs=250 Hz, n_fft_points=250\n",
      "n_freqs in X_data = 45 (verranno usati i primi 45 bins di rfftfreq)\n",
      "Bande usate: {'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30), 'gamma': (30, 45)}\n",
      "[OK] th_resp_vs_pt_resp / spectrograms / familiar_th : (1586, 45, 61)  →  (1586, 9, 9, 5)\n",
      "[OK] th_resp_vs_pt_resp / spectrograms / familiar_pt : (1580, 45, 61)  →  (1580, 9, 9, 5)\n",
      "[OK] th_resp_vs_pt_resp / spectrograms / unfamiliar_th : (1667, 45, 61)  →  (1667, 9, 9, 5)\n",
      "[OK] th_resp_vs_pt_resp / spectrograms / unfamiliar_pt : (1667, 45, 61)  →  (1667, 9, 9, 5)\n",
      "[OK] th_resp_vs_shared_resp / spectrograms / familiar_th : (1586, 45, 61)  →  (1586, 9, 9, 5)\n",
      "[OK] th_resp_vs_shared_resp / spectrograms / familiar_pt : (1580, 45, 61)  →  (1580, 9, 9, 5)\n",
      "[OK] th_resp_vs_shared_resp / spectrograms / unfamiliar_th : (1667, 45, 61)  →  (1667, 9, 9, 5)\n",
      "[OK] th_resp_vs_shared_resp / spectrograms / unfamiliar_pt : (1667, 45, 61)  →  (1667, 9, 9, 5)\n",
      "[OK] pt_resp_vs_shared_resp / spectrograms / familiar_th : (1586, 45, 61)  →  (1586, 9, 9, 5)\n",
      "[OK] pt_resp_vs_shared_resp / spectrograms / familiar_pt : (1580, 45, 61)  →  (1580, 9, 9, 5)\n",
      "[OK] pt_resp_vs_shared_resp / spectrograms / unfamiliar_th : (1667, 45, 61)  →  (1667, 9, 9, 5)\n",
      "[OK] pt_resp_vs_shared_resp / spectrograms / unfamiliar_pt : (1667, 45, 61)  →  (1667, 9, 9, 5)\n"
     ]
    }
   ],
   "source": [
    "# 3) converti TUTTI i blocchi del tuo data_dict\n",
    "data_dict, label_grid, electrode_grid_map = convert_fft_images_to_2d_grids_all_freqs_interrogait(\n",
    "    data_dict,\n",
    "    eeg_grid_df=EEG_file_interrogait,\n",
    "    eeg_channels_names=EEG_channels_names,\n",
    "    grid_shape=(9, 9),\n",
    "    fs=250,\n",
    "    n_fft_points=250,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447ef93-55a7-40f8-8dc4-dffb52852a09",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Implementazione: Carico Dati** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "764ec91b-3712-4374-bab2-045a1e44225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) FLAT: torniamo al formato { key: (X, y) } come prima\n",
    "data_dict_flat = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9043ded-da01-43a8-84b0-90a8102cb750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FLAT] th_resp_vs_pt_resp_spectrograms_familiar_th -> X: (1586, 9, 9, 5), y: (1586,)\n",
      "[FLAT] th_resp_vs_pt_resp_spectrograms_familiar_pt -> X: (1580, 9, 9, 5), y: (1580,)\n",
      "[FLAT] th_resp_vs_pt_resp_spectrograms_unfamiliar_th -> X: (1667, 9, 9, 5), y: (1667,)\n",
      "[FLAT] th_resp_vs_pt_resp_spectrograms_unfamiliar_pt -> X: (1667, 9, 9, 5), y: (1667,)\n",
      "[FLAT] th_resp_vs_shared_resp_spectrograms_familiar_th -> X: (1586, 9, 9, 5), y: (1586,)\n",
      "[FLAT] th_resp_vs_shared_resp_spectrograms_familiar_pt -> X: (1580, 9, 9, 5), y: (1580,)\n",
      "[FLAT] th_resp_vs_shared_resp_spectrograms_unfamiliar_th -> X: (1667, 9, 9, 5), y: (1667,)\n",
      "[FLAT] th_resp_vs_shared_resp_spectrograms_unfamiliar_pt -> X: (1667, 9, 9, 5), y: (1667,)\n",
      "[FLAT] pt_resp_vs_shared_resp_spectrograms_familiar_th -> X: (1586, 9, 9, 5), y: (1586,)\n",
      "[FLAT] pt_resp_vs_shared_resp_spectrograms_familiar_pt -> X: (1580, 9, 9, 5), y: (1580,)\n",
      "[FLAT] pt_resp_vs_shared_resp_spectrograms_unfamiliar_th -> X: (1667, 9, 9, 5), y: (1667,)\n",
      "[FLAT] pt_resp_vs_shared_resp_spectrograms_unfamiliar_pt -> X: (1667, 9, 9, 5), y: (1667,)\n"
     ]
    }
   ],
   "source": [
    "for condition, data_types in data_dict.items():          # es. \"th_resp_vs_pt_resp\"\n",
    "    for data_type, categories in data_types.items():            # es. \"spectrograms\"\n",
    "        for category_subject, (X_grid, y_data) in categories.items():  # es. \"familiar_th\"\n",
    "\n",
    "            # Ricostruiamo la chiave piatta come la usavi prima\n",
    "            key = f\"{condition}_{data_type}_{category_subject}\"\n",
    "            # es: \"th_resp_vs_pt_resp_spectrograms_familiar_th\"\n",
    "\n",
    "            data_dict_flat[key] = (X_grid, y_data)\n",
    "\n",
    "            # opzionale: print di debug\n",
    "            print(\n",
    "                f\"[FLAT] {key} -> X: {X_grid.shape}, y: {y_data.shape}\"\n",
    "            )\n",
    "\n",
    "# Sovrascrivi data_dict così il resto del codice resta IDENTICO\n",
    "data_dict = data_dict_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef343673-50e8-47c4-a97f-2f164a9c082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['th_resp_vs_pt_resp_spectrograms_familiar_th', 'th_resp_vs_pt_resp_spectrograms_familiar_pt', 'th_resp_vs_pt_resp_spectrograms_unfamiliar_th', 'th_resp_vs_pt_resp_spectrograms_unfamiliar_pt', 'th_resp_vs_shared_resp_spectrograms_familiar_th', 'th_resp_vs_shared_resp_spectrograms_familiar_pt', 'th_resp_vs_shared_resp_spectrograms_unfamiliar_th', 'th_resp_vs_shared_resp_spectrograms_unfamiliar_pt', 'pt_resp_vs_shared_resp_spectrograms_familiar_th', 'pt_resp_vs_shared_resp_spectrograms_familiar_pt', 'pt_resp_vs_shared_resp_spectrograms_unfamiliar_th', 'pt_resp_vs_shared_resp_spectrograms_unfamiliar_pt'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5922bfaa-9bf0-4c34-9c1f-e9ce39eb1163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1586, 9, 9, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "perfetto ora, siccome ho creato data_dict nel modo di cui sopra, \n",
    "ora dentro ogni chiave, \n",
    "ci sono già tutte le chiavi associate correttamente, per estrarmi i dati e labels corrispondenti di quella combinazione di fattori lì.\n",
    "\n",
    "infatti dentro ogni chiave c'è una tupla, con 2 elementi, il primo è l'array dei dati, il secondo è l'array delle labels\n",
    "'''\n",
    "\n",
    "data_dict['th_resp_vs_pt_resp_spectrograms_familiar_th'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fa50e-a8ac-4d5a-8faa-165c8423959a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Implementazione: Richiamo Reti Ottimizzate dopo W&B** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8781ebb4-af72-48a4-ace8-4778f41290d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_fam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_unfam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_fam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_unfam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_fam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_unfam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_fam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_unfam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_fam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_unfam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_fam\u001b[0m\n",
      "Cartella creata: \u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_unfam\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''NEW VERSION'''\n",
    "\n",
    "# Percorso base per il salvataggio\n",
    "#base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "#_params_hyperparams\n",
    "#base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "                                        #spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\n",
    "\n",
    "    \n",
    "#base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB_GradCAM_Checks\"    \n",
    "\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks\"    \n",
    "\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Tipologie di dati\n",
    "data_types = [\"spectrograms\"]\n",
    "\n",
    "# Subfolders per tipologia di soggetto\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "# Creazione della struttura delle cartelle\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "            \n",
    "            print(f\"Cartella creata: \\033[1m{path}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecd9541f-898c-4f6a-9fc7-0aa070b0b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/stefano/Interrogait/all_datas/'\n",
    "\n",
    "#with open(f\"{path}EEG_channels_names.pkl\", \"rb\") as f:\n",
    "    #EEG_channels_names = pickle.load(f)\n",
    "    \n",
    "with open(f\"{path}electrode_grid_map_interrogait.pkl\", \"rb\") as f:\n",
    "    EEG_channels_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c2fbb15-e1eb-40d4-8872-a5da87778e0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "EEG_channels = EEG_channels_names \n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies_params_hyperparams\"\n",
    "                                        #WB_spectrograms_best_results_channels_frequencies_params_hyperparams/\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    for model_name in [\"CNN2D\"]:\n",
    "        \n",
    "            \n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        # Prova a caricare la configurazione e i pesi ottimali dal file .pkl\n",
    "        \n",
    "        '''\n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "        pesi del modello e i suoi iper-parametri\n",
    "        \n",
    "        Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "        '''\n",
    "        \n",
    "        config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "        \n",
    "        if config is None:\n",
    "            raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "        \n",
    "        '''\n",
    "        Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "        MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "        '''\n",
    "        \n",
    "        # Parsifica la chiave una volta sola e memorizza i valori\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "        \n",
    "        '''\n",
    "        Dpodiché, \n",
    "        \n",
    "        1) si carica i vari valori degli iper-parametri,\n",
    "        2) si esegue la standardizzazione se servisse,\n",
    "        3) prepara il modello per la divisione in train_loader etc.,\n",
    "        4) si carica la configurazione dei pesi del modello, \n",
    "        5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "        \n",
    "        6) esegue il training e il test e poi\n",
    "        \n",
    "        7) si salva il tutto nella path corrispondente...\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "        DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "        \n",
    "        model_batch_size = config[\"batch_size\"]\n",
    "        model_n_epochs = config[\"n_epochs\"]\n",
    "        model_patience = config[\"patience\"]\n",
    "        model_lr = config[\"lr\"]\n",
    "        model_weight_decay = config[\"weight_decay\"]\n",
    "        model_standardization = config[\"standardization\"]\n",
    "        \n",
    "        print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, weight_decay= \\033[1m{model_weight_decay}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "        \n",
    "        # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "        models_info[model_key] = {\"standardization\": model_standardization}\n",
    "        \n",
    "        \n",
    "        '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "        IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "        '''\n",
    "\n",
    "        if model_standardization:\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "        \n",
    "        # Sposta il modello sulla GPU (se disponibile)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        \n",
    "        # Preparazione dei dataloaders\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "        \n",
    "        # Inizializzazione del modello\n",
    "        if model_name == \"CNN2D\":\n",
    "            model = CNN2D(input_channels = 61, num_classes=2)\n",
    "\n",
    "        \n",
    "        # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "        if best_weights is not None:\n",
    "            try:\n",
    "                model.load_state_dict(best_weights)\n",
    "                print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # Definizione del criterio di perdita\n",
    "        criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "        \n",
    "        # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "        \n",
    "        print(f\"🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "    \n",
    "        print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        '''\n",
    "        GRADCAM COMPUTATION PER IL MODELLO CNN2D\n",
    "        \n",
    "        La funzione compute_gradcam_figure estrae due campioni (uno per ogni classe) e crea una figura con le due righe richieste.\n",
    "        \n",
    "        Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "        'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "        \n",
    "        La funzione 'save_performance_results' è stata modificata \n",
    "        per gestire ANCHE questo nuovo input dell'immagine \n",
    "        \n",
    "        (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "        seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "        \n",
    "        - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "        - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "        - provenienza del dato stesso (i.e., familiar_th)\n",
    "        )\n",
    "        \n",
    "        Spiegazione:\n",
    "        \n",
    "        La funzione compute_gradcam_figure eseguire il calcolo di GradCAM (vedi dettagli nella sua funzione)\n",
    "        e alla fine ritornerà in output una variabile \n",
    "        \n",
    "        'fig_image' che sarà poi assegnata alla variabile 'gradcam_image',\n",
    "        che è un oggetto buffer, che contiene i dati binari dell'immagine in formato PNG\n",
    "        (poiché abbiamo usato plt.savefig con format='png'). \n",
    "        \n",
    "        Quindi, quando passi gradcam_image (cioè fig_image) alla funzione 'save_performance_results',\n",
    "        viene scritto direttamente su disco come file PNG.\n",
    "        \n",
    "        Non c'è bisogno di ri-aprire o convertire ulteriormente, a meno che tu non voglia manipolare l'immagine in seguito.\n",
    "        Quindi, la soluzione è corretta così com'è:\n",
    "        il buffer viene salvato come file PNG nella directory specificata, \n",
    "        e successivamente potrai aprirlo con una libreria come cv2 o PIL se necessario.        \n",
    "        \n",
    "        Quindi, gradcam_image (i.e., fig_image) viene quindi passato correttamente dentro al loop di training e test, \n",
    "        tramite 'save_performance_results', come input, \n",
    "        che salverà quindi poi l'immagine nella path corrispondente \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "        gradcam_image = None\n",
    "        \n",
    "        if model_name == \"CNN2D\":\n",
    "            gradcam_image = compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device, channel_names = EEG_channels)\n",
    "            if gradcam_image is not None:\n",
    "                print(f\"Creazione di \\033[1mGradCAM Image\\033[0m per il modello \\033[1m{model_name}\\033[0m.\")\n",
    "                \n",
    "        print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        save_performance_results(model_name,\n",
    "                                 my_train_results,\n",
    "                                 my_test_results,\n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder = save_path_folder,\n",
    "                                 gradcam_image = gradcam_image)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        N.B\n",
    "        \n",
    "        gradcam_image = None avverrà solo all'inizio cioè per il primo modello CNN2D, che verrà testato con una certa combinazione di dati mi sa.. \n",
    "        ma servirebbe tracciare in qualche modo \n",
    "\n",
    "        1) o che la gradcam_image di ogni combinazione venga ri-azzerata alla fine loop\n",
    "        2) o che venga monitorato che gradcam_image di una combinazione di dati già analizzata venga esclusa poi\n",
    "        (o messa in un set) in modo che rivenga per errore sovrascritta più volte.. \n",
    "        \n",
    "        Forse la strada più veloce potrebbe essere la soluzione 1)\n",
    "        \n",
    "        La soluzione più veloce e semplice è reimpostare la variabile gradcam_image a None alla fine dell'iterazione per ogni combinazione di dati\n",
    "        (cioè, all'interno del ciclo esterno che itera su key). \n",
    "         \n",
    "        In questo modo, per ogni nuovo dataset la variabile viene \"azzera\" e viene calcolata l'immagine GradCAM solo per quella combinazione, \n",
    "        evitando di sovrascrivere accidentalmente i risultati già calcolati per combinazioni precedenti.\n",
    "         \n",
    "        Un'altra possibilità sarebbe tenere traccia delle chiavi (o combinazioni) per cui hai già calcolato la GradCAM,\n",
    "        ad esempio usando un set, e saltare il calcolo se la combinazione è già presente. \n",
    "        \n",
    "        Tuttavia, se ogni combinazione deve avere la sua immagine, \n",
    "        la soluzione più semplice è quella di reimpostare gradcam_image = None alla fine dell'iterazione.\n",
    "        \n",
    "        Quindi, per esempio, alla fine del ciclo per ogni dataset (key) potresti fare:\n",
    "        (VEDI SOTTO)\n",
    "        \n",
    "        In questo modo, ti assicuri che per ogni nuova combinazione la variabile sia pulita e pronta per essere ricalcolata, \n",
    "        senza rischio di sovrascrivere o confondere i risultati\n",
    "        '''\n",
    "        \n",
    "        # Reimposta gradcam_image a None per la prossima combinazione di dati\n",
    "        gradcam_image = None\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a89e073e-2c9b-4235-9507-3af4cceec49f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "EEG_channels = EEG_channels_names \n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies_params_hyperparams\"\n",
    "                                        #WB_spectrograms_best_results_channels_frequencies_params_hyperparams/\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    for model_name in [\"CNN2D\"]:\n",
    "        \n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        '''\n",
    "        La variabile folder_path che stai mostrando si crea utilizzando una funzione chiamata parse_combination_key(dataset_key), \n",
    "        che sembra restituire una lista di elementi. Successivamente, si selezionano i primi tre elementi con [:3] per costruire un percorso di directory \n",
    "        (usando os.path.join). \n",
    "        In questa parte del codice non si sta effettivamente escludendo le ultime 3 lettere, \n",
    "        ma piuttosto si stanno estraendo i primi tre elementi dalla lista restituita da parse_combination_key\n",
    "        '''\n",
    "        \n",
    "        # Determina la cartella in cui si trovano i file .pkl per questa combinazione corrente\n",
    "        folder_path = os.path.join(base_folder, *parse_combination_key(key)[:3])\n",
    "        \n",
    "        # Trova il file migliore (con il migliore max_val_acc) nella cartella\n",
    "        best_file = scan_folder_for_best_model(folder_path)\n",
    "        \n",
    "        if best_file is None:\n",
    "            raise ValueError(f\"Nessun file .pkl trovato per {model_name} su {key}\")\n",
    "        \n",
    "        #print(model_data.keys())\n",
    "        #dict_keys(['optimized_model', 'state_dict', 'model_config', 'config'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd4fb95b-96b3-4e1f-afa8-645b406b076f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "EEG_channels = EEG_channels_names \n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies_params_hyperparams\"\n",
    "                                        #WB_spectrograms_best_results_channels_frequencies_params_hyperparams/\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB_GradCAM_Checks\"\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    for model_name in [\"CNN2D\"]:\n",
    "        \n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        '''\n",
    "        La variabile folder_path che stai mostrando si crea utilizzando una funzione chiamata parse_combination_key(dataset_key), \n",
    "        che sembra restituire una lista di elementi. Successivamente, si selezionano i primi tre elementi con [:3] per costruire un percorso di directory \n",
    "        (usando os.path.join). \n",
    "        In questa parte del codice non si sta effettivamente escludendo le ultime 3 lettere, \n",
    "        ma piuttosto si stanno estraendo i primi tre elementi dalla lista restituita da parse_combination_key\n",
    "        '''\n",
    "        \n",
    "        # Determina la cartella in cui si trovano i file .pkl per questa combinazione corrente\n",
    "        folder_path = os.path.join(base_folder, *parse_combination_key(key)[:3])\n",
    "        \n",
    "        # Trova il file migliore (con il migliore max_val_acc) nella cartella\n",
    "        best_file = scan_folder_for_best_model(folder_path)\n",
    "        \n",
    "        if best_file is None:\n",
    "            raise ValueError(f\"Nessun file .pkl trovato per {model_name} su {key}\")\n",
    "        \n",
    "        ''' \n",
    "        OLD VERSION\n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "        pesi del modello e i suoi iper-parametri\n",
    "        \n",
    "        Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "        '''\n",
    "        \n",
    "        # Prova a caricare la configurazione del modello dal file .pkl, con: \n",
    "        #1) pesi ottimali dal file .pkl\n",
    "        #2) parametri del modello\n",
    "        #3) iper-parametri del modello\n",
    "        \n",
    "        #config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "        #if config is None:\n",
    "        #    raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "    \n",
    "        '''\n",
    "        NEW VERSION\n",
    "        \n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        Adesso, rispetto a PRIMA, ne ho 1 di variabile di output e non più 2!\n",
    "        '''\n",
    "    \n",
    "        model_data = load_config_if_available(key, model_name, base_folder, best_file)\n",
    "        \n",
    "        if model_data is None:\n",
    "            raise ValueError(f\"Nessun file .pkl valido trovato per {model_name} su {key}\")\n",
    "        \n",
    "       \n",
    "        \n",
    "        '''\n",
    "        OLD VERSION\n",
    "        Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "        MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "        '''\n",
    "        \n",
    "        # Parsifica la chiave una volta sola e memorizza i valori\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "        \n",
    "        '''\n",
    "        Dpodiché, \n",
    "        \n",
    "        1) si carica i vari valori degli iper-parametri,\n",
    "        2) si esegue la standardizzazione se servisse,\n",
    "        3) prepara il modello per la divisione in train_loader etc.,\n",
    "        4) si carica la configurazione dei pesi del modello, \n",
    "        5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "        \n",
    "        6) esegue il training e il test e poi\n",
    "        \n",
    "        7) si salva il tutto nella path corrispondente...\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "        DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "        \n",
    "        #Mi estraggo i singoli dizionari dal file del modello!\n",
    "        \n",
    "        best_model = model_data[\"optimized_model\"]\n",
    "        #best_state_dict = model_data[\"state_dict\"]\n",
    "        best_model_config = model_data[\"model_config\"]\n",
    "        best_training_config = model_data[\"config\"]\n",
    "        \n",
    "        # Stampa le configurazioni caricate\n",
    "        print(f\"\\n\\033[1mConfigurazione modello estratta\\033[0m: {best_model_config}\")\n",
    "        print(f\"\\033[1mIperparametri estratti\\033[0m: {best_training_config}\\n\")\n",
    "        \n",
    "        model_batch_size = best_training_config[\"batch_size\"]\n",
    "        model_n_epochs = best_training_config[\"n_epochs\"]\n",
    "        model_patience = best_training_config[\"patience\"]\n",
    "        model_lr = best_training_config[\"lr\"]\n",
    "        model_weight_decay = best_training_config[\"weight_decay\"]\n",
    "        model_standardization = best_training_config[\"standardization\"]\n",
    "        \n",
    "        # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "        models_info[model_key] = {\"standardization\": model_standardization}\n",
    "        \n",
    "        \n",
    "        '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "        IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "        '''\n",
    "\n",
    "        if model_standardization:\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "\n",
    "        # Sposta il modello sulla GPU (se disponibile)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Preparazione dei dataloaders\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "        \n",
    "        # Inizializzazione del modello\n",
    "        if model_name == \"CNN2D\":\n",
    "            \n",
    "            # Carica il modello migliore usando i dati estratti\n",
    "            #model = load_best_cnn2d(best_model, best_state_dict, best_model_config, best_training_config)\n",
    "            model = load_best_cnn2d(best_model, best_model_config, best_training_config)\n",
    "            \n",
    "        \n",
    "        # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "        #if best_weights is not None:\n",
    "        #    try:\n",
    "        #       model.load_state_dict(best_weights)\n",
    "        #        print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "        #   except Exception as e:\n",
    "        #        print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "        #        continue\n",
    "        \n",
    "        \n",
    "        # Definizione del criterio di perdita\n",
    "        criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "        \n",
    "        # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "        \n",
    "        print(f\"\\n🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "    \n",
    "        print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        '''\n",
    "        GRADCAM COMPUTATION PER IL MODELLO CNN2D\n",
    "        \n",
    "        La funzione compute_gradcam_figure estrae due campioni (uno per ogni classe) e crea una figura con le due righe richieste.\n",
    "        \n",
    "        Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "        'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "        \n",
    "        La funzione 'save_performance_results' è stata modificata \n",
    "        per gestire ANCHE questo nuovo input dell'immagine \n",
    "        \n",
    "        (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "        seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "        \n",
    "        - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "        - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "        - provenienza del dato stesso (i.e., familiar_th)\n",
    "        )\n",
    "        \n",
    "        Spiegazione:\n",
    "        \n",
    "        La funzione compute_gradcam_figure eseguire il calcolo di GradCAM (vedi dettagli nella sua funzione)\n",
    "        e alla fine ritornerà in output una variabile \n",
    "        \n",
    "        'fig_image' che sarà poi assegnata alla variabile 'gradcam_image',\n",
    "        che è un oggetto buffer, che contiene i dati binari dell'immagine in formato PNG\n",
    "        (poiché abbiamo usato plt.savefig con format='png'). \n",
    "        \n",
    "        Quindi, quando passi gradcam_image (cioè fig_image) alla funzione 'save_performance_results',\n",
    "        viene scritto direttamente su disco come file PNG.\n",
    "        \n",
    "        Non c'è bisogno di ri-aprire o convertire ulteriormente, a meno che tu non voglia manipolare l'immagine in seguito.\n",
    "        Quindi, la soluzione è corretta così com'è:\n",
    "        il buffer viene salvato come file PNG nella directory specificata, \n",
    "        e successivamente potrai aprirlo con una libreria come cv2 o PIL se necessario.        \n",
    "        \n",
    "        Quindi, gradcam_image (i.e., fig_image) viene quindi passato correttamente dentro al loop di training e test, \n",
    "        tramite 'save_performance_results', come input, \n",
    "        che salverà quindi poi l'immagine nella path corrispondente \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "        gradcam_image = None\n",
    "        \n",
    "        if model_name == \"CNN2D\":\n",
    "            gradcam_image = compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device, EEG_channels)\n",
    "            if gradcam_image is not None:\n",
    "                print(f\"Creazione di \\033[1mGradCAM Image\\033[0m per il modello \\033[1m{model_name}\\033[0m.\")\n",
    "                \n",
    "        print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        save_performance_results(model_name,\n",
    "                                 my_train_results,\n",
    "                                 my_test_results,\n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder = save_path_folder,\n",
    "                                 gradcam_image = gradcam_image)\n",
    "        \n",
    "        '''\n",
    "        N.B\n",
    "        \n",
    "        gradcam_image = None avverrà solo all'inizio cioè per il primo modello CNN2D, che verrà testato con una certa combinazione di dati mi sa.. \n",
    "        ma servirebbe tracciare in qualche modo \n",
    "\n",
    "        1) o che la gradcam_image di ogni combinazione venga ri-azzerata alla fine loop\n",
    "        2) o che venga monitorato che gradcam_image di una combinazione di dati già analizzata venga esclusa poi\n",
    "        (o messa in un set) in modo che rivenga per errore sovrascritta più volte.. \n",
    "        \n",
    "        Forse la strada più veloce potrebbe essere la soluzione 1)\n",
    "        \n",
    "        La soluzione più veloce e semplice è reimpostare la variabile gradcam_image a None alla fine dell'iterazione per ogni combinazione di dati\n",
    "        (cioè, all'interno del ciclo esterno che itera su key). \n",
    "         \n",
    "        In questo modo, per ogni nuovo dataset la variabile viene \"azzera\" e viene calcolata l'immagine GradCAM solo per quella combinazione, \n",
    "        evitando di sovrascrivere accidentalmente i risultati già calcolati per combinazioni precedenti.\n",
    "         \n",
    "        Un'altra possibilità sarebbe tenere traccia delle chiavi (o combinazioni) per cui hai già calcolato la GradCAM,\n",
    "        ad esempio usando un set, e saltare il calcolo se la combinazione è già presente. \n",
    "        \n",
    "        Tuttavia, se ogni combinazione deve avere la sua immagine, \n",
    "        la soluzione più semplice è quella di reimpostare gradcam_image = None alla fine dell'iterazione.\n",
    "        \n",
    "        Quindi, per esempio, alla fine del ciclo per ogni dataset (key) potresti fare:\n",
    "        (VEDI SOTTO)\n",
    "        \n",
    "        In questo modo, ti assicuri che per ogni nuova combinazione la variabile sia pulita e pronta per essere ricalcolata, \n",
    "        senza rischio di sovrascrivere o confondere i risultati\n",
    "        '''\n",
    "        \n",
    "        # Reimposta gradcam_image a None per la prossima combinazione di dati\n",
    "        gradcam_image = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab492b8f-bc2c-483f-8e45-02575172688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VERSIONE NUOVA UFFICIALE\n",
    "\n",
    "\n",
    "Ecco come puoi correggere solo il calcolo dell’AUC–ROC sul training set a posteriori, \n",
    "lasciando invariato tutto il resto di load_best_run_results. \n",
    "\n",
    "\n",
    "L’idea è:\n",
    "\n",
    "1) Estrarre la history normale da W&B (che contiene il vecchio train_auc)\n",
    "2) Individuare best_epoch\n",
    "3) Caricare il modello migliore da disco\n",
    "4) Rifare un passaggio solo sullo train_loader per ottenere le vere probabilità e ricalcolare la ROC–AUC\n",
    "5) Sovrascrivere il vecchio valore auc_train_history[best_epoch] e aggiornare best_metrics[\"train_auc\"]\n",
    "\n",
    "\n",
    "\n",
    "Cosa è cambiato\n",
    "\n",
    "1) Ti ho inserito un passaggio 6) in cui ricalcoli l’AUC–ROC vero del train set, usando torch.softmax(…,dim=1)[:,1].\n",
    "2) Sostituisci il vecchio auc_train_history[best_epoch] col valore corretto.\n",
    "3) Ricomponi best_metrics[\"train_auc\"] con true_auc_train.\n",
    "\n",
    "Da qui in poi, puoi chiamare subito dopo la tua testing(...) per ottenere anche tutte le metriche sul test set e salvare la tabella finale in cui:\n",
    "\n",
    "“Train” = best_metrics[\"train_*\"] (ora con AUC corretta)\n",
    "\n",
    "“Test” = test_results[\"test_performances\"]\n",
    "\n",
    "Ecco fatto: nessun re‑training, solo un passaggio aggiuntivo per correggere il calcolo dell’AUC–ROC sul train set.\n",
    "\n",
    "\n",
    "\n",
    "Quindi il punto 6\n",
    "\n",
    "# --- 6) Ricalcolo vero train AUC–ROC sul train_loader ---\n",
    "\n",
    "serve per ri-calcolarsi correttamente l'auc roc al train set nell'epoca in cui sul val set ho ottenuto la migliore validation accuracy, \n",
    "che corrisponde quindi al modello salvato dentro il best_model che io ri-prelevo quando poi lo do in pasto al test set?\n",
    "\n",
    "\n",
    "Esattamente: quel passaggio 6):\n",
    "\n",
    "Riprende il modello caricato dal file .pkl (che è proprio il best_model scelto sull’epoca di miglior val_accuracy),\n",
    "\n",
    "Lo mette in eval() e senza gradienti scorre tutto il train_loader,\n",
    "\n",
    "Calcola le probabilità (softmax(:,1)) e da quelle ricava la vera ROC–AUC per il train set,\n",
    "\n",
    "Infine sovrascrive auc_train_history[best_epoch] e aggiorna best_metrics[\"train_auc\"] con questo valore corretto.\n",
    "\n",
    "In questo modo la tua colonna “Train” nella tabella conterrà davvero l’AUC–ROC calcolata sulle probabilità del modello nella stessa epoca \n",
    "in cui hai ottenuto la migliore validazione, cioè esattamente quei pesi che poi passerai al test set.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from wandb import Api\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "1) questa serve per plottare le metriche di loss e accuracy in ogni modello e condizione sperimentale\n",
    "per salvarla dentro al dizionario 'training_plot' come buffer di memoria\n",
    "'''\n",
    "\n",
    "\n",
    "def plot_training_results(loss_train_history, loss_val_history, accuracy_train_history, accuracy_val_history):\n",
    "    \n",
    "    '''\n",
    "    # Creazione di una figura con 2 subplot\n",
    "    '''\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n",
    "\n",
    "    #Plot della loss\n",
    "    ax[0].plot(loss_train_history, label='Train Loss', color='blue')\n",
    "    ax[0].plot(loss_val_history, label='Validation Loss', color='orange')\n",
    "    #ax[0].set_title(f'Loss during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[0].set_title(f'Loss during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[0].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[0].set_ylabel('Loss', fontsize=12)    # Dimensione font asse y\n",
    "    ax[0].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # Plot dell'accuracy\n",
    "    ax[1].plot(accuracy_train_history, label='Train Accuracy', color='blue')\n",
    "    ax[1].plot(accuracy_val_history, label='Validation Accuracy', color='orange')\n",
    "    #ax[1].set_title(f'Accuracy during Training: {exp_cond_1} vs {exp_cond_2}', fontsize=16)  # Titolo più grande\n",
    "    ax[1].set_title(f'Accuracy during Training: ', fontsize=12)  # Titolo più grande\n",
    "    ax[1].set_xlabel('Epochs', fontsize=12)  # Dimensione font asse x\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=12)  # Dimensione font asse y\n",
    "    ax[1].legend(fontsize=12)  # Dimensione font legenda\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    # Regolare la spaziatura tra i subplot\n",
    "    plt.tight_layout()  # Alternativa: fig.subplots_adjust(hspace=0.3)\n",
    "    \n",
    "    #plt.close(fig)\n",
    "    \n",
    "    '''\n",
    "    # Salvare il plot in un buffer di memoria\n",
    "    '''\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')  # Salviamo il plot in formato PNG\n",
    "    buf.seek(0)  # Torniamo all'inizio del buffer\n",
    "\n",
    "    # Convertire il buffer in un'immagine PIL (opzionale, per visualizzarla)\n",
    "    img = Image.open(buf)\n",
    "\n",
    "    # Aggiungere i dati dell'immagine nel dizionario\n",
    "    plot_image_data = buf.getvalue()  # Otteniamo i dati binari dell'immagine\n",
    "    buf.close()\n",
    "    \n",
    "    # Ritorniamo i dati dell'immagine da salvare nel dizionario\n",
    "    return plot_image_data\n",
    "\n",
    "\n",
    "'''\n",
    "2) questa serve per estrarmi le stringhe per ricostruire il nome del progetto su W&B per \n",
    "poi estrarmi le metriche ottenute sul training e validation \n",
    "da salvare sempre dentro al dizionario 'training_plot' \n",
    "'''\n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_key(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    Il formato atteso è:\n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ \n",
    "    \"spectrograms\" _ \n",
    "    \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "\n",
    "\n",
    "'''CELLA DI ESEMPIO PER VERIFICARE SE QUESTA FUNZIONE FACESSE IL PARSING DELLE STRINGHE DELLE COMBINAZIONI DI FATTORI CORRETTAMENTE'''\n",
    "\n",
    "# Test\n",
    "#combination_key = \"rest_vs_left_fist_spectrograms_familiar_th\"\n",
    "#condition_experiment, data_type, subject_key = parse_combination_key(combination_key)\n",
    "\n",
    "#print(\"Condizione:\", condition_experiment)\n",
    "#print(\"Data Type:\", data_type)\n",
    "#print(\"Soggetto:\", subject_key)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Ecco come puoi correggere solo il calcolo dell’AUC–ROC sul training set a posteriori, lasciando invariato tutto il resto di load_best_run_results. \n",
    "\n",
    "\n",
    "L’idea è:\n",
    "\n",
    "1) Estrarre la history normale da W&B (che contiene il vecchio train_auc)\n",
    "2) Individuare best_epoch\n",
    "3) Caricare il modello migliore da disco\n",
    "4) Rifare un passaggio solo sullo train_loader per ottenere le vere probabilità e ricalcolare la ROC–AUC\n",
    "5) Sovrascrivere il vecchio valore auc_train_history[best_epoch] e aggiornare best_metrics[\"train_auc\"]\n",
    "\n",
    "\n",
    "\n",
    "Cosa è cambiato\n",
    "\n",
    "1) Ti ho inserito un passaggio 6) in cui ricalcoli l’AUC–ROC vero del train set, usando torch.softmax(…,dim=1)[:,1].\n",
    "2) Sostituisci il vecchio auc_train_history[best_epoch] col valore corretto.\n",
    "3) Ricomponi best_metrics[\"train_auc\"] con true_auc_train.\n",
    "\n",
    "Da qui in poi, puoi chiamare subito dopo la tua testing(...) per ottenere anche tutte le metriche sul test set e salvare la tabella finale in cui:\n",
    "\n",
    "“Train” = best_metrics[\"train_*\"] (ora con AUC corretta)\n",
    "\n",
    "“Test” = test_results[\"test_performances\"]\n",
    "\n",
    "Ecco fatto: nessun re‑training, solo un passaggio aggiuntivo per correggere il calcolo dell’AUC–ROC sul train set.\n",
    "\n",
    "\n",
    "\n",
    "Quindi il punto 6\n",
    "\n",
    "# --- 6) Ricalcolo vero train AUC–ROC sul train_loader ---\n",
    "\n",
    "serve per ri-calcolarsi correttamente l'auc roc al train set nell'epoca in cui sul val set ho ottenuto la migliore validation accuracy, \n",
    "che corrisponde quindi al modello salvato dentro il best_model che io ri-prelevo quando poi lo do in pasto al test set?\n",
    "\n",
    "\n",
    "Esattamente: quel passaggio 6):\n",
    "\n",
    "Riprende il modello caricato dal file .pkl (che è proprio il best_model scelto sull’epoca di miglior val_accuracy),\n",
    "\n",
    "Lo mette in eval() e senza gradienti scorre tutto il train_loader,\n",
    "\n",
    "Calcola le probabilità (softmax(:,1)) e da quelle ricava la vera ROC–AUC per il train set,\n",
    "\n",
    "Infine sovrascrive auc_train_history[best_epoch] e aggiorna best_metrics[\"train_auc\"] con questo valore corretto.\n",
    "\n",
    "In questo modo la tua colonna “Train” nella tabella conterrà davvero l’AUC–ROC calcolata sulle probabilità del modello nella stessa epoca \n",
    "in cui hai ottenuto la migliore validazione, cioè esattamente quei pesi che poi passerai al test set.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "3) Dopodiché, comincia la funzione di load_best_run_results che, \n",
    "per ogni progetto e sweep del relativo modello,\n",
    "\n",
    "si va ad estrarre le metriche del train (corregge il calcolo del train_auc)\n",
    "e si calcola anche per il validation phase la confusion matrix e classification report\n",
    "\n",
    "\n",
    "4) dopodichè dovrebbe richiamare la funzione di \n",
    "\"plot_training_results\" in modo che poi si salvi i plot di training e validation (sia loss che accuracy)\n",
    "in modo che si salvi tutto in una immagine come buffer che viene spuntato fuori da quella funzione \n",
    "\n",
    "e poi inserito come valore dentro al dizionario training_results che sarà l'output di \"load_best_run_results\" \n",
    "\n",
    "\n",
    "quindi qui sotto mi manca richiamare la funzione \"plot_training_results\" con una variabile tipo training_plot = plot_training_results che avrà come argomenti\n",
    "\n",
    "queste liste qua salvate come colonne del df creato dentro a 'load_best_run_results!'\n",
    "\n",
    "\n",
    "loss_train_history     = df[\"train_loss\"].tolist()\n",
    "loss_val_history       = df[\"val_loss\"].tolist()\n",
    "accuracy_train_history = df[\"train_accuracy\"].tolist()\n",
    "accuracy_val_history   = df[\"val_accuracy\"].tolist()\n",
    "\n",
    "\n",
    "5) dopodiché mi serve caricare tutte queste info dentro al dizionario train_results, che sarà l'output di load_best_run_results... \n",
    "e su questo ho dei dubbi su quali chiavi del dizionario tenere separate oppure se \"unirne\" qualcuna, aggregando tutte le info del sweep_config assieme, \n",
    "sia che siano veri iper-parametri (learning rate etc) o parametri architetturali della rete (anche se avevano valori fissi) il più delle volte se vedi\n",
    "\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        # --- setup generale ---\n",
    "        \"model_name\":   {\"value\": \"CNN2D\"},\n",
    "        \"n_epochs\":     {\"value\": 100},\n",
    "        \"patience\":     {\"value\": 12},\n",
    "        \"batch_size\":   {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"value\": True},   # fisso a True\n",
    "\n",
    "        # --- ottimizzatore ---\n",
    "        \"lr\":           {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":        {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\":        {\"values\": [0.99, 0.995]},\n",
    "        \"eps\":          {\"values\": [1e-8, 1e-7]},\n",
    "\n",
    "        # --- iperparametri architettura CNN2D (fissi) ---\n",
    "        \"conv_out_channels\": {\"value\": 16},\n",
    "\n",
    "        \"conv_k1_h\": {\"value\": 3}, \"conv_k1_w\": {\"value\": 5},\n",
    "        \"conv_k2_h\": {\"value\": 3}, \"conv_k2_w\": {\"value\": 5},\n",
    "        \"conv_k3_h\": {\"value\": 3}, \"conv_k3_w\": {\"value\": 5},\n",
    "\n",
    "        \"conv_s1_h\": {\"value\": 1}, \"conv_s1_w\": {\"value\": 2},\n",
    "        \"conv_s2_h\": {\"value\": 1}, \"conv_s2_w\": {\"value\": 2},\n",
    "        \"conv_s3_h\": {\"value\": 1}, \"conv_s3_w\": {\"value\": 2},\n",
    "\n",
    "        \"pool_p1_h\": {\"value\": 1}, \"pool_p1_w\": {\"value\": 2},\n",
    "        \"pool_p2_h\": {\"value\": 1}, \"pool_p2_w\": {\"value\": 2},\n",
    "        \"pool_p3_h\": {\"value\": 1}, \"pool_p3_w\": {\"value\": 1},\n",
    "\n",
    "        \"pool_type\":  {\"value\": \"max\", \"avg\"},     # se vuoi fissarlo; se vuoi provarlo, usa {\"values\":[\"max\",\"avg\"]}\n",
    "        \"fc1_units\":  {\"value\": 12},\n",
    "        \"cnn_act1\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act2\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act3\":   {\"value\": \"relu\"},\n",
    "        \"dropout\":    {\"value\": 0.5}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def load_best_run_results(\n",
    "    key, # es. \"rest_vs_left_fist_spectrograms_familiar_th\"\n",
    "    model, # # <-- istanza PyTorch già caricata con i pesi best es. \"CNN3D_LSTM_FC\"\n",
    "    \n",
    "    sweep_config,      # <— qui richiamo lo sweep config del modello corrispondente\n",
    "    \n",
    "    data_loaders, # dict con DataLoader per \"train\" e \"val\"\n",
    "    entity= \"my_wb_entity\"): # entity = \"stefano‑bargione‑universit‑di‑roma‑tor‑vergata\"\n",
    "    \n",
    "    \n",
    "    # --- 1) Parse key e ricava project name ---\n",
    "    exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "    \n",
    "    \n",
    "    '''CAMBIATA PER DATI INTERROGAIT IN RAPPRESENTAZIONE TIME DOMAIN 1D'''\n",
    "    #project = f\"{exp_cond}_{data_type}_channels_freqs_new_3d_grid_multiband\"\n",
    "    #project = f\"{exp_cond}_{data_type}_time_freqs_new_imagery_3d_grid_multiband\"\n",
    "    \n",
    "    project = f\"{exp_cond}_{data_type}_channels_freqs_{category_subject}\"\n",
    "    \n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "\n",
    "    '''SE ESTRAGGO SWEEP ID A POSTERIORI DAL PROGETTO\n",
    "\n",
    "    1) Prendo tutte le run del progetto e modello corrispondente\n",
    "    2) Filtro solo quelle con config[\"model_name\"] == model_name.\n",
    "    3) Controllo che ce ne sia almeno una (altrimenti errore).\n",
    "    4) Costruisce un set di tutti gli r.sweep e verifica che sia esattamente uno (altrimenti errore).\n",
    "    5) Estrae quello unico (.pop()) e lo stampa insieme al numero di run.\n",
    "    6) Infine, seleziona la singola best_run sulla base di val_accuracy.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # 2) Recupero tutte le run del progetto\n",
    "    api  = Api()\n",
    "    runs = api.runs(f\"{entity}/{project}\")\n",
    "\n",
    "    # 3) filtro solo quelle del modello giusto\n",
    "    runs_filtered = [r for r in runs if r.config.get(\"model_name\", \"\") == model_name]\n",
    "    n_runs = len(runs_filtered)\n",
    "\n",
    "    if n_runs == 0:\n",
    "        raise RuntimeError(f\"Nessuna run trovata per progetto `{project}` e modello `{model_name}`\")\n",
    "\n",
    "    # 4) controllo che le run filtrate appartengano tutte allo stesso sweep\n",
    "    unique_sweeps = {r.sweep for r in runs_filtered}\n",
    "    if len(unique_sweeps) != 1:\n",
    "        raise RuntimeError(\n",
    "            f\"Trovati più sweep per progetto `{project}` e modello `{model_name}`: {unique_sweeps}\"\n",
    "        )\n",
    "\n",
    "    # 5) estraggo lo sweep_id\n",
    "    sweep_id_unico = unique_sweeps.pop()\n",
    "    #print(f\"✓ Trovate \\033[1m{n_runs}\\033[0m runs in progetto `{project}` e modello `{model_name}`, sweep: `{sweep_id_unico}`\")\n",
    "    print(f\"✓ Trovate \\033[1m{n_runs}\\033[0m runs\\n\")\n",
    "    print(f\"✓ Progetto \\033[1m`{project}`\\033[0m\\n\")\n",
    "    print(f\"✓ Modello \\033[1m`{model_name}`\\033[0m\\n\")\n",
    "    print(f\"✓ Sweep \\033[1m`{sweep_id_unico}`\\033[0m\\n\\n\")\n",
    "\n",
    "    # 6) scelgo la run con val_accuracy massima\n",
    "    best_run = max(runs_filtered, key=lambda r: r.summary.get(\"val_accuracy\", 0.0))\n",
    "\n",
    "    # --- 7) Estraggo tutta la history (compresi i train_auc sbagliati) ---\n",
    "    df = best_run.history(\n",
    "        keys=[\n",
    "          \"train_loss\",\"train_accuracy\",\"train_precision\",\n",
    "          \"train_recall\",\"train_f1\",\"train_auc\",\n",
    "          \"val_loss\",\"val_accuracy\"\n",
    "        ],\n",
    "        pandas=True\n",
    "    )\n",
    "    # converto in liste\n",
    "    loss_train_history     = df[\"train_loss\"].tolist()\n",
    "    loss_val_history       = df[\"val_loss\"].tolist()\n",
    "    accuracy_train_history = df[\"train_accuracy\"].tolist()\n",
    "    accuracy_val_history   = df[\"val_accuracy\"].tolist()\n",
    "    precision_train_history= df[\"train_precision\"].tolist()\n",
    "    recall_train_history   = df[\"train_recall\"].tolist()\n",
    "    f1_train_history       = df[\"train_f1\"].tolist()\n",
    "    auc_train_history      = df[\"train_auc\"].tolist()\n",
    "\n",
    "    # best_epoch (su val_accuracy)\n",
    "    best_epoch = int(df[\"val_accuracy\"].idxmax())\n",
    "\n",
    "    # --- 8) Prendo il modello ottimizzato .pkl corrispondente passato in input ---\n",
    "    device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- 9) Ricalcolo vero train AUC–ROC sul train_loader ---\n",
    "    y_t_train, y_s_train = [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loaders[\"train\"]:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs  = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            y_s_train.extend(probs)\n",
    "            y_t_train.extend(y.numpy())\n",
    "            \n",
    "    true_auc_train = roc_auc_score(np.array(y_t_train), np.array(y_s_train))\n",
    "\n",
    "    # Sovrascrivo il vecchio valore sbagliato\n",
    "    auc_train_history[best_epoch] = true_auc_train\n",
    "\n",
    "    # Ricostruisco best_metrics\n",
    "    best_metrics = {\n",
    "      \"train_loss\":       [round(loss_train_history[best_epoch],4)],\n",
    "      \"train_accuracy\":   [round(accuracy_train_history[best_epoch],4)],\n",
    "      \"train_precision\":  [round(precision_train_history[best_epoch],4)],\n",
    "      \"train_recall\":     [round(recall_train_history[best_epoch],4)],\n",
    "      \"train_f1_score\":   [round(f1_train_history[best_epoch],4)],\n",
    "      \"train_auc\":        [round(true_auc_train,4)]\n",
    "    }\n",
    "\n",
    "    #Solo una nota: qui non serve per training che l'auc abbia l'average='weighted' \n",
    "    #perché è binario e stai usando score continui.\n",
    "    #anche se sopra lo avevi messo in \"training_sweep\".\n",
    "    \n",
    "    #Per le altre metriche (precision, recall, f1_score invece) l'average andava bene!\n",
    "    #Anche in binario: average='weighted' = fai la media pesata per supporto delle metriche per ciascuna classe (0 e 1). \n",
    "    #È sensato se hai sbilanciamento e vuoi che le metriche riflettano anche quanto è frequente ciascuna classe. \n",
    "    \n",
    "    #L’unica cosa da essere consapevoli è che non stai riportando “F1 della classe positiva”, \n",
    "    #ma una F1 complessiva pesata sulle due classi. \n",
    "    #Ma va bene, basta essere coerenti e chiari nel testo della tesi/paper.\"\n",
    "    \n",
    "\n",
    "    # --- 10) Ricreo confusion matrix e classification report su val set ---\n",
    "    \n",
    "    #Per il validation set, invece, rifai il calcolo:\n",
    "    \n",
    "    #y_t_val = true labels (0/1).\n",
    "    #y_p_val = predizioni binarie (0/1), usate per accuracy / precision / recall / f1.\n",
    "    #y_s_val = score continui (probabilità o logit della classe 1), usati per il calcolo dell'AUC-ROC:\n",
    "    \n",
    "    #Quindi diventerà ---> val_auc = roc_auc_score(y_t_val, y_s_val)\n",
    "    #Quindi qui \"y_s_val\" è semplicemente la lista di p(y=1) per ogni campione di validation.\n",
    "    \n",
    "    y_t_val, y_p_val, y_s_val = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loaders[\"val\"]:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs  = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            preds  = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            \n",
    "            y_p_val.extend(preds) # predizioni 0/1\n",
    "            y_s_val.extend(probs) # score continui per AUC\n",
    "            y_t_val.extend(y.numpy()) \n",
    "            \n",
    "    confusion_matrix_val = confusion_matrix(y_t_val, y_p_val)\n",
    "    classification_report_val = classification_report(y_t_val, y_p_val, output_dict=False)\n",
    "    \n",
    "    # Metriche Validation\n",
    "    #val_accuracy = accuracy_score(y_t_val, y_p_val)\n",
    "    val_precision = precision_score(y_t_val, y_p_val, average='weighted')\n",
    "    val_recall    = recall_score(y_t_val, y_p_val, average='weighted')\n",
    "    val_f1        = f1_score(y_t_val, y_p_val, average='weighted')\n",
    "    \n",
    "    try:\n",
    "        val_auc = roc_auc_score(y_t_val, y_s_val)   # <-- NOTHING average=... qui\n",
    "    except ValueError:\n",
    "        print(\"⚠️ AUC non calcolabile: nel val set c'è una sola classe.\")\n",
    "        val_auc = np.nan\n",
    "    \n",
    "    # Val performances alla best_epoch\n",
    "    '''\n",
    "    Qui la cosa importante è: il modello con cui stai facendo il forward su data_loaders[\"val\"] \n",
    "    è il best model, cioè quello che hai caricato da .pkl e che dovrebbe corrispondere esattamente \n",
    "    ai pesi di best_epoch per quella run.\n",
    "    \n",
    "    Per cui, val_loss e val_accuracy che salvi nel dict sono proprio quelli loggati all’epoca best_epoch durante il training.\n",
    "    Questi sono coerenti con “la migliore epoca secondo val_accuracy”.\n",
    "    \n",
    "    Mentre le altre metriche (precision, recall, f1_score, son ricalcolate in base al best model che aveva ottenuto\n",
    "    a quella epoca specifica la migliore val_accuracy!\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    validation_performances = {\n",
    "        # dalla history di W&B (loss/acc per quella epoch)\n",
    "        \"val_loss\":       [round(loss_val_history[best_epoch],4)],\n",
    "        \"val_accuracy\":   [round(accuracy_val_history[best_epoch],4)],\n",
    "        \n",
    "        # dalle metriche ricalcolate con il best_model\n",
    "        \"val_precision\":  [round(val_precision,4)],\n",
    "        \"val_recall\":     [round(val_recall,4)],\n",
    "        \"val_f1_score\":   [round(val_f1,4)],\n",
    "        \"val_auc\":        [round(val_auc,4)],\n",
    "    }\n",
    "    \n",
    "        \n",
    "    # --- 10) Plot delle curve loss/accuracy tra train e test ---\n",
    "    training_plot = plot_training_results(\n",
    "        loss_train_history,\n",
    "        loss_val_history,\n",
    "        accuracy_train_history,\n",
    "        accuracy_val_history\n",
    "    )\n",
    "\n",
    "    # --- 11) Composizione del dict finale identico a `training()` ---\n",
    "    \n",
    "    # Restituire tutti i risultati in un dizionario\n",
    "    train_results = {\n",
    "        \"training_performances\": best_metrics,  # Aggiungi il dizionario delle performance\n",
    "        \n",
    "        \"loss_train_history\": loss_train_history,\n",
    "        \"loss_val_history\": loss_val_history,\n",
    "        \n",
    "        \"accuracy_train_history\": accuracy_train_history,\n",
    "        \"accuracy_val_history\": accuracy_val_history,\n",
    "        \n",
    "        \"best_model\": model,\n",
    "        \n",
    "        # VALIDATION\n",
    "        \"validation_performances\": validation_performances,\n",
    "        \n",
    "        \"confusion_matrix\": confusion_matrix_val,\n",
    "        \"classification_report\": classification_report_val,\n",
    "    \n",
    "        \"hyperparams\" : {k: best_run.config[k] for k in best_run.config.keys() if k in sweep_config[\"parameters\"]},\n",
    "            \n",
    "        \"training_plot\": training_plot  # Salviamo il buffer con il plot\n",
    "    }\n",
    "    \n",
    "    '''\n",
    "    Ho questo errore \"Errore “cudnn RNN backward can only be called in training mode”\" solo con i dati di \n",
    "    left_fist_vs_right_fist, per il modello SeparableCNN2D_LSTM_FC, \n",
    "    mentre con i dati delle altre condizioni sperimentali, ossia:\n",
    "    \n",
    "    rest_vs_left_fist o rest_vs_right_fist, sempre per il modello SeparableCNN2D_LSTM_FC,non succede ... come mai solo con l'ultimo succede? \n",
    "    \n",
    "    cioè dove dovrei aver lasciato il modello caricato in eval.() ?\n",
    "    \n",
    "    probabilmente qui nella funzione load_best_train_results!?\n",
    "    \n",
    "    quindi qui poi alla fine dovrei rimettere il modello in un'altra modalità alla fine della funzione? \n",
    "    perché in sostanza, dovrebbe succedere che in sostanza... non succede nulla per lo stesso modello per  gli altri dati, \n",
    "    perché ogni volta che ne prendo uno lo porto in eval e vabbè.. ma poi il problema succede solo per l'ultimo caso solo, \n",
    "    perché forse l'ultimo proprio, ossia solo SeparableCNN2D_LSTM_FC usa proprio il layer LSTM e quindi da errore là,\n",
    "    perché dentro a load_best_train_results è rimasto in .eval() ed ha il layer LSTM e quindi dà errore?\n",
    "    \n",
    "    \n",
    "    \n",
    "    Perché l’errore appare “solo” con l’ultima combinazione\n",
    "\n",
    "    1. load_best_run_results() termina con:\n",
    "\n",
    "    model.to(device).eval()   # ← il modello rimane in eval()\n",
    "    \n",
    "    2. In compute_gradcam_figure() tu usi il best model che hai messo in train_results[\"best_model\"] (quello appena impostato in eval()), poi esegui:\n",
    "\n",
    "   \n",
    "    output = model(sample_input)\n",
    "    ...\n",
    "    target.backward()         # <-- gradiente attraverso l’LSTM\n",
    "    3. Il kernel CuDNN per gli RNN (LSTM/GRU) rifiuta il backward quando il modulo è in modalità inference (eval()), e solleva:\n",
    "\n",
    "    \n",
    "    RuntimeError: cudnn RNN backward can only be called in training mode\n",
    "    \n",
    "    4. Per le combinazioni precedenti con lo stesso modello “SeparableCNN2D_LSTM_FC” non è esploso perché, con ogni probabilità, \n",
    "    use_lstm=False nelle relative run migliori (quindi l’LSTM non c’è e CuDNN non interviene).\n",
    "    \n",
    "    Nell’ultima combinazione invece la best‑run ha use_lstm=True, quindi compare l’LSTM e l’errore salta fuori.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    return train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d732f33-b6cc-4759-9684-07d55b90b988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        # --- setup generale ---\n",
    "        \"model_name\":   {\"value\": \"CNN2D\"},\n",
    "        \"n_epochs\":     {\"value\": 100},\n",
    "        \"patience\":     {\"value\": 12},\n",
    "        \"batch_size\":   {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"value\": True},   # fisso a True\n",
    "\n",
    "        # --- ottimizzatore ---\n",
    "        \"lr\":           {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":        {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\":        {\"values\": [0.99, 0.995]},\n",
    "        \"eps\":          {\"values\": [1e-8, 1e-7]},\n",
    "\n",
    "        # --- iperparametri architettura CNN2D (fissi) ---\n",
    "        \"conv_out_channels\": {\"value\": 16},\n",
    "\n",
    "        \"conv_k1_h\": {\"value\": 3}, \"conv_k1_w\": {\"value\": 5},\n",
    "        \"conv_k2_h\": {\"value\": 3}, \"conv_k2_w\": {\"value\": 5},\n",
    "        \"conv_k3_h\": {\"value\": 3}, \"conv_k3_w\": {\"value\": 5},\n",
    "\n",
    "        \"conv_s1_h\": {\"value\": 1}, \"conv_s1_w\": {\"value\": 2},\n",
    "        \"conv_s2_h\": {\"value\": 1}, \"conv_s2_w\": {\"value\": 2},\n",
    "        \"conv_s3_h\": {\"value\": 1}, \"conv_s3_w\": {\"value\": 2},\n",
    "\n",
    "        \"pool_p1_h\": {\"value\": 1}, \"pool_p1_w\": {\"value\": 2},\n",
    "        \"pool_p2_h\": {\"value\": 1}, \"pool_p2_w\": {\"value\": 2},\n",
    "        \"pool_p3_h\": {\"value\": 1}, \"pool_p3_w\": {\"value\": 1},\n",
    "\n",
    "        \"pool_type\":  {\"value\": \"max\", \"avg\"},     # se vuoi fissarlo; se vuoi provarlo, usa {\"values\":[\"max\",\"avg\"]}\n",
    "        \"fc1_units\":  {\"value\": 12},\n",
    "        \"cnn_act1\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act2\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act3\":   {\"value\": \"relu\"},\n",
    "        \"dropout\":    {\"value\": 0.5}\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "932cdcb9-4c31-49a7-bacf-2b2c48209d3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "sweep_config_cnn2d = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        # --- setup generale ---\n",
    "        \"model_name\":   {\"value\": \"CNN2D\"},\n",
    "        \"n_epochs\":     {\"value\": 100},\n",
    "        \"patience\":     {\"value\": 12},\n",
    "        \"batch_size\":   {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"value\": True},   # fisso a True\n",
    "\n",
    "        # --- ottimizzatore ---\n",
    "        \"lr\":           {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"beta1\":        {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\":        {\"values\": [0.99, 0.995]},\n",
    "        \"eps\":          {\"values\": [1e-8, 1e-7]},\n",
    "\n",
    "        # --- iperparametri architettura CNN2D (fissi) ---\n",
    "        \"conv_out_channels\": {\"value\": 16},\n",
    "\n",
    "        \"conv_k1_h\": {\"value\": 3}, \"conv_k1_w\": {\"value\": 5},\n",
    "        \"conv_k2_h\": {\"value\": 3}, \"conv_k2_w\": {\"value\": 5},\n",
    "        \"conv_k3_h\": {\"value\": 3}, \"conv_k3_w\": {\"value\": 5},\n",
    "\n",
    "        \"conv_s1_h\": {\"value\": 1}, \"conv_s1_w\": {\"value\": 2},\n",
    "        \"conv_s2_h\": {\"value\": 1}, \"conv_s2_w\": {\"value\": 2},\n",
    "        \"conv_s3_h\": {\"value\": 1}, \"conv_s3_w\": {\"value\": 2},\n",
    "\n",
    "        \"pool_p1_h\": {\"value\": 1}, \"pool_p1_w\": {\"value\": 2},\n",
    "        \"pool_p2_h\": {\"value\": 1}, \"pool_p2_w\": {\"value\": 2},\n",
    "        \"pool_p3_h\": {\"value\": 1}, \"pool_p3_w\": {\"value\": 1},\n",
    "\n",
    "        \"pool_type\":  {\"value\": [\"max\", \"avg\"]},     # se vuoi fissarlo; se vuoi provarlo, usa {\"values\":[\"max\",\"avg\"]}\n",
    "        \"fc1_units\":  {\"value\": 12},\n",
    "        \"cnn_act1\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act2\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act3\":   {\"value\": \"relu\"},\n",
    "        \"dropout\":    {\"value\": 0.5}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df88451-0339-448f-84f7-2cc77d2deca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        num_classes: int,\n",
    "\n",
    "        # da sweep: numero di feature map di base\n",
    "        conv_out_channels: int,\n",
    "\n",
    "        # da sweep: kernel size H×W per i 3 blocchi\n",
    "        conv_k1_h: int, conv_k1_w: int,\n",
    "        conv_k2_h: int, conv_k2_w: int,\n",
    "        conv_k3_h: int, conv_k3_w: int,\n",
    "\n",
    "        # da sweep: stride H×W per i 3 blocchi\n",
    "        conv_s1_h: int, conv_s1_w: int,\n",
    "        conv_s2_h: int, conv_s2_w: int,\n",
    "        conv_s3_h: int, conv_s3_w: int,\n",
    "\n",
    "        # da sweep: pool kernel H×W per i 3 blocchi\n",
    "        pool_p1_h: int, pool_p1_w: int,\n",
    "        pool_p2_h: int, pool_p2_w: int,\n",
    "        pool_p3_h: int, pool_p3_w: int,\n",
    "\n",
    "        # da sweep: tipo di pooling\n",
    "        pool_type: str,  # \"max\" o \"avg\"\n",
    "\n",
    "        # fully‑connected\n",
    "        fc1_units: int,\n",
    "        dropout: float,\n",
    "\n",
    "        # attivazioni per i 3 blocchi\n",
    "        cnn_act1: str,\n",
    "        cnn_act2: str,\n",
    "        cnn_act3: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        mapping = {'relu': F.relu, 'selu': F.selu, 'elu': F.elu}\n",
    "        self.act_fns = [\n",
    "            mapping[cnn_act1],\n",
    "            mapping[cnn_act2],\n",
    "            mapping[cnn_act3],\n",
    "        ]\n",
    "        \n",
    "        # calcolo padding “quasi‐same” per ciascun blocco\n",
    "        p1_h = (conv_k1_h - 1) // 2\n",
    "        p1_w = (conv_k1_w - 1) // 2\n",
    "        p2_h = (conv_k2_h - 1) // 2\n",
    "        p2_w = (conv_k2_w - 1) // 2\n",
    "        p3_h = (conv_k3_h - 1) // 2\n",
    "        p3_w = (conv_k3_w - 1) // 2\n",
    "        \n",
    "        # Primo blocco\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            input_channels, conv_out_channels,\n",
    "            kernel_size = (conv_k1_h, conv_k1_w),\n",
    "            stride = (conv_s1_h, conv_s1_w),\n",
    "            #padding='same'\n",
    "            padding = (p1_h, p1_w)\n",
    "        )\n",
    "        self.bn1   = nn.BatchNorm2d(conv_out_channels)\n",
    "        self.pool1 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p1_h, pool_p1_w))\n",
    "\n",
    "        # Secondo blocco (×2 feature map)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            conv_out_channels, conv_out_channels*2,\n",
    "            kernel_size=(conv_k2_h, conv_k2_w),\n",
    "            stride=(conv_s2_h, conv_s2_w),\n",
    "            #padding='same'\n",
    "            padding = (p2_h, p2_w) \n",
    "        )\n",
    "        self.bn2   = nn.BatchNorm2d(conv_out_channels*2)\n",
    "        self.pool2 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p2_h, pool_p2_w))\n",
    "\n",
    "        # Terzo blocco (×3 feature map)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            conv_out_channels*2, conv_out_channels*3,\n",
    "            kernel_size=(conv_k3_h, conv_k3_w),\n",
    "            stride=(conv_s3_h, conv_s3_w),\n",
    "            #padding='same'\n",
    "            padding = (p3_h, p3_w)\n",
    "        )\n",
    "        self.bn3   = nn.BatchNorm2d(conv_out_channels*3)\n",
    "        self.pool3 = (nn.MaxPool2d if pool_type=='max' else nn.AvgPool2d)((pool_p3_h, pool_p3_w))\n",
    "\n",
    "        # FC finale\n",
    "        self.fc1     = nn.LazyLinear(fc1_units)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2     = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input Iniziale\n",
    "        #x: (batch, frequenze, canali)\n",
    "        \n",
    "        #🔁 Prima:\n",
    "        \n",
    "        # Sappaimo che x abbia forma (batch_size, 45, 61)\n",
    "        # Se i 61 sono i canali, allora occorre trasporre le dimensioni:\n",
    "        \n",
    "        # Permutiamo per ottenere (batch, canali, frequenze)\n",
    "        #x = x.permute(0, 2, 1)  # Ora ha forma (batch_size, 61, 45)\n",
    "        \n",
    "        # Aggiungiamo una dimensione extra per adattarlo alla convoluzione 2D\n",
    "        #x = x.unsqueeze(3)  # Ora ha forma (batch_size, 61, 45, 1)\n",
    "        \n",
    "        #✅ Ora:\n",
    "        #Siccome i dati arrivano come (B, 45, 61) — cioè frequenze × canali, non serve permutare. Ti basta:\n",
    "        \n",
    "        # Aggiungiamo una dimensione per il canale \"immagine\"\n",
    "        x = x.unsqueeze(1)  # → (B, 1, 45, 61)\n",
    "            \n",
    "        # Passaggio attraverso il primo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = self.act_fns[0](x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = self.act_fns[1](x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato convoluzionale, BatchNorm e pooling\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Batch Normalization\n",
    "        x = self.act_fns[2](x)\n",
    "       \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten per preparare i dati per gli strati fully connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # Passaggio attraverso il primo strato fully connected\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "       \n",
    "        # Dropout per evitare overfitting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Passaggio attraverso il secondo strato fully connected\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "       \n",
    "       \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37366f79-e835-41a0-b220-0ef58c348139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "        \"conv_k1_h\": {\"value\": 3}, \"conv_k1_w\": {\"value\": 5},\n",
    "        \"conv_k2_h\": {\"value\": 3}, \"conv_k2_w\": {\"value\": 5},\n",
    "        \"conv_k3_h\": {\"value\": 3}, \"conv_k3_w\": {\"value\": 5},\n",
    "\n",
    "        \"conv_s1_h\": {\"value\": 1}, \"conv_s1_w\": {\"value\": 2},\n",
    "        \"conv_s2_h\": {\"value\": 1}, \"conv_s2_w\": {\"value\": 2},\n",
    "        \"conv_s3_h\": {\"value\": 1}, \"conv_s3_w\": {\"value\": 2},\n",
    "\n",
    "        \"pool_p1_h\": {\"value\": 1}, \"pool_p1_w\": {\"value\": 2},\n",
    "        \"pool_p2_h\": {\"value\": 1}, \"pool_p2_w\": {\"value\": 2},\n",
    "        \"pool_p3_h\": {\"value\": 1}, \"pool_p3_w\": {\"value\": 1},\n",
    "\n",
    "        \"pool_type\":  {\"values\": [\"max\", \"avg\"]},     # se vuoi fissarlo; se vuoi provarlo, usa {\"values\":[\"max\",\"avg\"]}\n",
    "        \"fc1_units\":  {\"value\": 12},\n",
    "        \"cnn_act1\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act2\":   {\"value\": \"relu\"},\n",
    "        \"cnn_act3\":   {\"value\": \"relu\"},\n",
    "        \"dropout\":    {\"value\": 0.5}\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        model = CNN2D(\n",
    "                input_channels   = 1,\n",
    "                num_classes      = num_classes,\n",
    "                conv_out_channels= config.conv_out_channels,\n",
    "\n",
    "                conv_k1_h = config.conv_k1_h, \n",
    "                conv_k1_w = config.conv_k1_w,\n",
    "\n",
    "                conv_k2_h = config.conv_k2_h, \n",
    "                conv_k2_w = config.conv_k2_w,\n",
    "\n",
    "                conv_k3_h = config.conv_k3_h,\n",
    "                conv_k3_w = config.conv_k3_w,\n",
    "\n",
    "                conv_s1_h = config.conv_s1_h, \n",
    "                conv_s1_w = config.conv_s1_w,\n",
    "\n",
    "                conv_s2_h = config.conv_s2_h,\n",
    "                conv_s2_w = config.conv_s2_w,\n",
    "\n",
    "                conv_s3_h = config.conv_s3_h,\n",
    "                conv_s3_w = config.conv_s3_w,\n",
    "\n",
    "                pool_p1_h = config.pool_p1_h,\n",
    "                pool_p1_w = config.pool_p1_w,\n",
    "\n",
    "                pool_p2_h = config.pool_p2_h,\n",
    "                pool_p2_w = config.pool_p2_w,\n",
    "\n",
    "                pool_p3_h = config.pool_p3_h,\n",
    "                pool_p3_w = config.pool_p3_w,\n",
    "\n",
    "                pool_type = config.pool_type,\n",
    "\n",
    "                fc1_units = config.fc1_units,\n",
    "                dropout   = config.dropout,\n",
    "\n",
    "                cnn_act1  = config.cnn_act1,\n",
    "                cnn_act2  = config.cnn_act2,\n",
    "                cnn_act3  = config.cnn_act3,\n",
    "            )\n",
    "            \n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ce27bf6-d8b6-43e5-a425-af6cb4545439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 – Sweep config per ciascun modello\n",
    "sweep_config_cnn3d = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"CNN3D_LSTM_FC\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"values\": [True, False]},\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"use_lstm\": {\"values\": [True, False]},\n",
    "        \"lstm_hidden\": {\"values\": [32]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_config_cnn_sep = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        \"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        \"n_epochs\": {\"value\": 100},\n",
    "        \"patience\": {\"value\": 12},\n",
    "        \"model_name\": {\"values\": [\"SeparableCNN2D_LSTM_FC\"]},\n",
    "        \"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        \"standardization\": {\"values\": [True, False]},\n",
    "        \"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        \"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        \"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \"use_lstm\": {\"values\": [True, False]},\n",
    "        \"lstm_hidden\": {\"values\": [32]},\n",
    "        \"dropout\": {\"values\": [0.5]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1382ec7-4ea4-46d6-a5ab-2d18117c1ef8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "EEG_channels = EEG_channels_names \n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies_params_hyperparams\"\n",
    "                                        #WB_spectrograms_best_results_channels_frequencies_params_hyperparams/\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB_GradCAM_Checks\"\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    for model_name in [\"CNN2D\"]:\n",
    "        \n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        '''\n",
    "        La variabile folder_path che stai mostrando si crea utilizzando una funzione chiamata parse_combination_key(dataset_key), \n",
    "        che sembra restituire una lista di elementi. Successivamente, si selezionano i primi tre elementi con [:3] per costruire un percorso di directory \n",
    "        (usando os.path.join). \n",
    "        In questa parte del codice non si sta effettivamente escludendo le ultime 3 lettere, \n",
    "        ma piuttosto si stanno estraendo i primi tre elementi dalla lista restituita da parse_combination_key\n",
    "        '''\n",
    "        \n",
    "        # Determina la cartella in cui si trovano i file .pkl per questa combinazione corrente\n",
    "        folder_path = os.path.join(base_folder, *parse_combination_key(key)[:3])\n",
    "        \n",
    "        # Trova il file migliore (con il migliore max_val_acc) nella cartella\n",
    "        best_file = scan_folder_for_best_model(folder_path)\n",
    "        \n",
    "        if best_file is None:\n",
    "            raise ValueError(f\"Nessun file .pkl trovato per {model_name} su {key}\")\n",
    "        \n",
    "        ''' \n",
    "        OLD VERSION\n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "        pesi del modello e i suoi iper-parametri\n",
    "        \n",
    "        Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "        '''\n",
    "        \n",
    "        # Prova a caricare la configurazione del modello dal file .pkl, con: \n",
    "        #1) pesi ottimali dal file .pkl\n",
    "        #2) parametri del modello\n",
    "        #3) iper-parametri del modello\n",
    "        \n",
    "        #config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "        #if config is None:\n",
    "        #    raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "    \n",
    "        '''\n",
    "        NEW VERSION\n",
    "        \n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        Adesso, rispetto a PRIMA, ne ho 1 di variabile di output e non più 2!\n",
    "        '''\n",
    "    \n",
    "        model_data = load_config_if_available(key, model_name, base_folder, best_file)\n",
    "        \n",
    "        if model_data is None:\n",
    "            raise ValueError(f\"Nessun file .pkl valido trovato per {model_name} su {key}\")\n",
    "        \n",
    "       \n",
    "        \n",
    "        '''\n",
    "        OLD VERSION\n",
    "        Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "        MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "        '''\n",
    "        \n",
    "        # Parsifica la chiave una volta sola e memorizza i valori\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "        \n",
    "        '''\n",
    "        Dpodiché, \n",
    "        \n",
    "        1) si carica i vari valori degli iper-parametri,\n",
    "        2) si esegue la standardizzazione se servisse,\n",
    "        3) prepara il modello per la divisione in train_loader etc.,\n",
    "        4) si carica la configurazione dei pesi del modello, \n",
    "        5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "        \n",
    "        6) esegue il training e il test e poi\n",
    "        \n",
    "        7) si salva il tutto nella path corrispondente...\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "        DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "        \n",
    "        #Mi estraggo i singoli dizionari dal file del modello!\n",
    "        \n",
    "        best_model = model_data[\"optimized_model\"]\n",
    "        #best_state_dict = model_data[\"state_dict\"]\n",
    "        best_model_config = model_data[\"model_config\"]\n",
    "        best_training_config = model_data[\"config\"]\n",
    "        \n",
    "        # Stampa le configurazioni caricate\n",
    "        print(f\"\\n\\033[1mConfigurazione modello estratta\\033[0m: {best_model_config}\")\n",
    "        print(f\"\\033[1mIperparametri estratti\\033[0m: {best_training_config}\\n\")\n",
    "        \n",
    "        model_batch_size = best_training_config[\"batch_size\"]\n",
    "        model_n_epochs = best_training_config[\"n_epochs\"]\n",
    "        model_patience = best_training_config[\"patience\"]\n",
    "        model_lr = best_training_config[\"lr\"]\n",
    "        model_weight_decay = best_training_config[\"weight_decay\"]\n",
    "        model_standardization = best_training_config[\"standardization\"]\n",
    "        \n",
    "        # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "        models_info[model_key] = {\"standardization\": model_standardization}\n",
    "        \n",
    "        \n",
    "        '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "        IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "        '''\n",
    "\n",
    "        if model_standardization:\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "\n",
    "        # Sposta il modello sulla GPU (se disponibile)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Preparazione dei dataloaders\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "        \n",
    "        # Inizializzazione del modello\n",
    "        if model_name == \"CNN2D\":\n",
    "            \n",
    "            # Carica il modello migliore usando i dati estratti\n",
    "            #model = load_best_cnn2d(best_model, best_state_dict, best_model_config, best_training_config)\n",
    "            model = load_best_cnn2d(best_model, best_model_config, best_training_config)\n",
    "            \n",
    "        \n",
    "        # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "        #if best_weights is not None:\n",
    "        #    try:\n",
    "        #       model.load_state_dict(best_weights)\n",
    "        #        print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "        #   except Exception as e:\n",
    "        #        print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "        #        continue\n",
    "        \n",
    "        \n",
    "        # Definizione del criterio di perdita\n",
    "        criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "        \n",
    "        # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "        \n",
    "        print(f\"\\n🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "    \n",
    "        print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        '''\n",
    "        GRADCAM COMPUTATION PER IL MODELLO CNN2D\n",
    "        \n",
    "        La funzione compute_gradcam_figure estrae due campioni (uno per ogni classe) e crea una figura con le due righe richieste.\n",
    "        \n",
    "        Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "        'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "        \n",
    "        La funzione 'save_performance_results' è stata modificata \n",
    "        per gestire ANCHE questo nuovo input dell'immagine \n",
    "        \n",
    "        (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "        seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "        \n",
    "        - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "        - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "        - provenienza del dato stesso (i.e., familiar_th)\n",
    "        )\n",
    "        \n",
    "        Spiegazione:\n",
    "        \n",
    "        La funzione compute_gradcam_figure eseguire il calcolo di GradCAM (vedi dettagli nella sua funzione)\n",
    "        e alla fine ritornerà in output una variabile \n",
    "        \n",
    "        'fig_image' che sarà poi assegnata alla variabile 'gradcam_image',\n",
    "        che è un oggetto buffer, che contiene i dati binari dell'immagine in formato PNG\n",
    "        (poiché abbiamo usato plt.savefig con format='png'). \n",
    "        \n",
    "        Quindi, quando passi gradcam_image (cioè fig_image) alla funzione 'save_performance_results',\n",
    "        viene scritto direttamente su disco come file PNG.\n",
    "        \n",
    "        Non c'è bisogno di ri-aprire o convertire ulteriormente, a meno che tu non voglia manipolare l'immagine in seguito.\n",
    "        Quindi, la soluzione è corretta così com'è:\n",
    "        il buffer viene salvato come file PNG nella directory specificata, \n",
    "        e successivamente potrai aprirlo con una libreria come cv2 o PIL se necessario.        \n",
    "        \n",
    "        Quindi, gradcam_image (i.e., fig_image) viene quindi passato correttamente dentro al loop di training e test, \n",
    "        tramite 'save_performance_results', come input, \n",
    "        che salverà quindi poi l'immagine nella path corrispondente \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "        gradcam_image = None\n",
    "        \n",
    "        if model_name == \"CNN2D\":\n",
    "            gradcam_image = compute_gradcam_figure(model, test_loader, exp_cond, data_type, category_subject, device, EEG_channels)\n",
    "            if gradcam_image is not None:\n",
    "                print(f\"Creazione di \\033[1mGradCAM Image\\033[0m per il modello \\033[1m{model_name}\\033[0m.\")\n",
    "                \n",
    "        print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        save_performance_results(model_name,\n",
    "                                 my_train_results,\n",
    "                                 my_test_results,\n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder = save_path_folder,\n",
    "                                 gradcam_image = gradcam_image)\n",
    "        \n",
    "        '''\n",
    "        N.B\n",
    "        \n",
    "        gradcam_image = None avverrà solo all'inizio cioè per il primo modello CNN2D, che verrà testato con una certa combinazione di dati mi sa.. \n",
    "        ma servirebbe tracciare in qualche modo \n",
    "\n",
    "        1) o che la gradcam_image di ogni combinazione venga ri-azzerata alla fine loop\n",
    "        2) o che venga monitorato che gradcam_image di una combinazione di dati già analizzata venga esclusa poi\n",
    "        (o messa in un set) in modo che rivenga per errore sovrascritta più volte.. \n",
    "        \n",
    "        Forse la strada più veloce potrebbe essere la soluzione 1)\n",
    "        \n",
    "        La soluzione più veloce e semplice è reimpostare la variabile gradcam_image a None alla fine dell'iterazione per ogni combinazione di dati\n",
    "        (cioè, all'interno del ciclo esterno che itera su key). \n",
    "         \n",
    "        In questo modo, per ogni nuovo dataset la variabile viene \"azzera\" e viene calcolata l'immagine GradCAM solo per quella combinazione, \n",
    "        evitando di sovrascrivere accidentalmente i risultati già calcolati per combinazioni precedenti.\n",
    "         \n",
    "        Un'altra possibilità sarebbe tenere traccia delle chiavi (o combinazioni) per cui hai già calcolato la GradCAM,\n",
    "        ad esempio usando un set, e saltare il calcolo se la combinazione è già presente. \n",
    "        \n",
    "        Tuttavia, se ogni combinazione deve avere la sua immagine, \n",
    "        la soluzione più semplice è quella di reimpostare gradcam_image = None alla fine dell'iterazione.\n",
    "        \n",
    "        Quindi, per esempio, alla fine del ciclo per ogni dataset (key) potresti fare:\n",
    "        (VEDI SOTTO)\n",
    "        \n",
    "        In questo modo, ti assicuri che per ogni nuova combinazione la variabile sia pulita e pronta per essere ricalcolata, \n",
    "        senza rischio di sovrascrivere o confondere i risultati\n",
    "        '''\n",
    "        \n",
    "        # Reimposta gradcam_image a None per la prossima combinazione di dati\n",
    "        gradcam_image = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716de8c-9de1-48ef-8317-b4b41801d7a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "PER CNN2D\n",
    "'''\n",
    "\n",
    "\n",
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "EEG_channels = EEG_channels_names \n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies\"\n",
    "                                        #WB_spectrograms_best_results_channels_frequencies_params_hyperparams/\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks\"\n",
    "\n",
    "\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    CREO COPIA TEST_LOADER_RAW PER I PLOT DEL POWER RAW PER BANDA E CLASSE\n",
    "    '''\n",
    "    # 1) salva una copia RAW dei soli dati di test PRIMA di standardizzare\n",
    "    X_test_raw = X_test.copy()\n",
    "    y_test_raw = y_test.copy()\n",
    "    \n",
    "    # 2) tensori\n",
    "    X_raw_tensor = torch.tensor(X_test_raw, dtype=torch.float32)\n",
    "    y_raw_tensor = torch.tensor(y_test_raw, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    '''ATTENZIONE MODIFICA QUI'''\n",
    "    \n",
    "    #for model_name in [\"CNN1D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    for model_name in [\"CNN2D\"]:\n",
    "\n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        # Prova a caricare la configurazione e i pesi ottimali dal file .pkl\n",
    "        \n",
    "        '''\n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "        pesi del modello e i suoi iper-parametri\n",
    "        \n",
    "        Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "        '''\n",
    "        \n",
    "        config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "        \n",
    "        if config is None:\n",
    "            raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "        \n",
    "        '''\n",
    "        Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "        MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "        '''\n",
    "        \n",
    "        # Parsifica la chiave una volta sola e memorizza i valori\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "        \n",
    "        '''\n",
    "        Dpodiché, \n",
    "        \n",
    "        1) si carica i vari valori degli iper-parametri,\n",
    "        2) si esegue la standardizzazione se servisse,\n",
    "        3) prepara il modello per la divisione in train_loader etc.,\n",
    "        4) si carica la configurazione dei pesi del modello, \n",
    "        5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "        \n",
    "        6) esegue il training e il test e poi\n",
    "        \n",
    "        7) si salva il tutto nella path corrispondente...\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Ricordati di aggiungere le varie variabili associate alla definizione dinamica dei valori dei parametri model-specific qui!\n",
    "    \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "        DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "        \n",
    "        \n",
    "        model_lr = config[\"lr\"]\n",
    "        model_weight_decay = config[\"weight_decay\"]\n",
    "        model_n_epochs = config[\"n_epochs\"]\n",
    "        model_patience = config[\"patience\"]\n",
    "        \n",
    "        \n",
    "        model_batch_size = config[\"batch_size\"]\n",
    "        model_standardization = config[\"standardization\"]\n",
    "        \n",
    "        #model_n_epochs = config[\"n_epochs\"]\n",
    "        #model_patience = config[\"patience\"]\n",
    "        \n",
    "        #model_lr = config[\"lr\"]\n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        model_beta1 =  config[\"beta1\"]\n",
    "        model_beta2 =  config[\"beta2\"]\n",
    "        model_eps = config[\"eps\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #model_weight_decay = config[\"weight_decay\"]\n",
    "        #model_standardization = config[\"standardization\"]\n",
    "        \n",
    "        #print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, weight_decay= \\033[1m{model_weight_decay}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "        print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, model_beta1= \\033[1m{model_beta1}\\033[0m,  model_beta2= \\033[1m{model_beta2}\\033[0m,  model_eps= \\033[1m{model_eps}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "        \n",
    "        # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "        models_info[model_key] = {\"standardization\": model_standardization}\n",
    "        \n",
    "        \n",
    "        # 3) dataset & loader per test set (per plots power raw) –‑  IMPORTANTISSIMO: shuffle=False\n",
    "        raw_dataset = TensorDataset(X_raw_tensor, y_raw_tensor)\n",
    "        test_loader_raw = DataLoader(raw_dataset,\n",
    "                             batch_size=model_batch_size,\n",
    "                             shuffle=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "        IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "        '''\n",
    "\n",
    "        if model_standardization:\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "        \n",
    "        # Sposta il modello sulla GPU (se disponibile)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        \n",
    "        # Preparazione dei dataloaders\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "        \n",
    "\n",
    "        '''\n",
    "        # ====== MODELLO CNN2D FREQUENCY x CHANNELS 2D ======\n",
    "        \n",
    "        PRENDO LA SHAPE DEI DATI PER FORNIRE VALORI GIUSTI PER OGNI INPUt DI CIASCUNA RETE\n",
    "    \n",
    "        # Appena caricato X_train, X_val, X_test, etc.\n",
    "        # X_train.shape == (N, freq_bins, channels)\n",
    "\n",
    "        _, freq_bins, channels = X_train.shape\n",
    "\n",
    "        #NEW VERSION\n",
    "        if config.model_name == \"CNN2D\":\n",
    "\n",
    "            #model = CNN2D(\n",
    "                #input_channels   = 1,\n",
    "                #num_classes      = 2,\n",
    "                #conv_out_channels= config.conv_out_channels,\n",
    "                #conv2d_kernel_size = tuple(config.conv2d_kernel_size),\n",
    "                #conv2d_stride      = tuple(config.conv2d_stride),\n",
    "                #pool_type        = config.pool_type,\n",
    "                #pool2d_kernel_size = tuple(config.pool2d_kernel_size),\n",
    "                #fc1_units        = config.fc1_units,\n",
    "                #dropout          = config.dropout,\n",
    "                #activations      = tuple(config.activations)\n",
    "            #)\n",
    "            #print(f\"\\nInizializzazione Modello \\033[1mCNN2D\\033[0m\")\n",
    "        \n",
    "    \n",
    "        model = CNN2D(\n",
    "                input_channels   = 1,\n",
    "                num_classes      = num_classes,\n",
    "                conv_out_channels= config.conv_out_channels,\n",
    "\n",
    "                conv_k1_h = config.conv_k1_h, \n",
    "                conv_k1_w = config.conv_k1_w,\n",
    "\n",
    "                conv_k2_h = config.conv_k2_h, \n",
    "                conv_k2_w = config.conv_k2_w,\n",
    "\n",
    "                conv_k3_h = config.conv_k3_h,\n",
    "                conv_k3_w = config.conv_k3_w,\n",
    "\n",
    "                conv_s1_h = config.conv_s1_h, \n",
    "                conv_s1_w = config.conv_s1_w,\n",
    "\n",
    "                conv_s2_h = config.conv_s2_h,\n",
    "                conv_s2_w = config.conv_s2_w,\n",
    "\n",
    "                conv_s3_h = config.conv_s3_h,\n",
    "                conv_s3_w = config.conv_s3_w,\n",
    "\n",
    "                pool_p1_h = config.pool_p1_h,\n",
    "                pool_p1_w = config.pool_p1_w,\n",
    "\n",
    "                pool_p2_h = config.pool_p2_h,\n",
    "                pool_p2_w = config.pool_p2_w,\n",
    "\n",
    "                pool_p3_h = config.pool_p3_h,\n",
    "                pool_p3_w = config.pool_p3_w,\n",
    "\n",
    "                pool_type = config.pool_type,\n",
    "\n",
    "                fc1_units = config.fc1_units,\n",
    "                dropout   = config.dropout,\n",
    "\n",
    "                cnn_act1  = config.cnn_act1,\n",
    "                cnn_act2  = config.cnn_act2,\n",
    "                cnn_act3  = config.cnn_act3,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ...\n",
    "    \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        '''PARAMETRI MODEL-SPECIFIC DI CNN2D, ma richiamati al momenti della inizializzazione dei relativi sweep_config! '''\n",
    "        \n",
    "        #model_dropout = config['dropout']\n",
    "        #model_bidirectional = config['bidirectional']\n",
    "        #model_d_model = config['d_model']\n",
    "        #model_num_heads = config['num_heads']\n",
    "        #model_num_layers  = config['num_layers']\n",
    "        \n",
    "        \n",
    "        # Per caricare la shape dei dati 2D da X_train, X_val, X_test, etc.\n",
    "        \n",
    "        #Prelevo le dimensioni di frequencies e channels dei miei dati \n",
    "        \n",
    "        _, freq_bins, channels = X_train.shape\n",
    "        \n",
    "        # Nome dei tre sweep config usati\n",
    "        \n",
    "        # sweep_config_cnn2d\n",
    "\n",
    "        if model_name == \"CNN2D\":\n",
    "            \n",
    "            #Sweep Config CNN2D (sweep_config_cnn2d)\n",
    "            \n",
    "            sweep_config = sweep_config_cnn2d\n",
    "        \n",
    "            #Numero di classi da riconoscere (es. binaria)\n",
    "            num_classes = 2\n",
    "            \n",
    "            model_conv_out_channels = config[\"conv_out_channels\"]\n",
    "            \n",
    "            model_conv_k1_h =config[\"conv_k1_h\"]\n",
    "            model_conv_k1_w =config[\"conv_k1_w\"]\n",
    "            \n",
    "            model_conv_k2_h =config[\"conv_k2_h\"]\n",
    "            model_conv_k2_w =config[\"conv_k2_w\"]\n",
    "            \n",
    "            model_conv_k3_h =config[\"conv_k3_h\"]\n",
    "            model_conv_k3_w =config[\"conv_k3_w\"]\n",
    "            \n",
    "            model_conv_s1_h =config[\"conv_s1_h\"]\n",
    "            model_conv_s1_w =config[\"conv_s1_w\"]\n",
    "            \n",
    "            model_conv_s2_h =config[\"conv_s2_h\"]\n",
    "            model_conv_s2_w =config[\"conv_s2_w\"]\n",
    "            \n",
    "            model_conv_s3_h =config[\"conv_s3_h\"]\n",
    "            model_conv_s3_w =config[\"conv_s3_w\"]\n",
    "            \n",
    "            model_pool_p1_h = config[\"pool_p1_h\"]\n",
    "            model_pool_p1_w = config[\"pool_p1_w\"]\n",
    "            \n",
    "            model_pool_p2_h = config[\"pool_p2_h\"]\n",
    "            model_pool_p2_w = config[\"pool_p2_w\"]\n",
    "            \n",
    "            model_pool_p3_h = config[\"pool_p3_h\"]\n",
    "            model_pool_p3_w = config[\"pool_p3_w\"]\n",
    "            \n",
    "            \n",
    "            model_pool_type =config[\"pool_type\"]\n",
    "            \n",
    "            \n",
    "            model_fc1_units = config[\"fc1_units\"]\n",
    "            \n",
    "            model_dropout = config[\"dropout\"]\n",
    "            \n",
    "            model_cnn_act1 = config[\"cnn_act1\"]\n",
    "            model_cnn_act2 = config[\"cnn_act2\"]\n",
    "            model_cnn_act3 = config[\"cnn_act3\"]\n",
    "            \n",
    "        \n",
    "            model = CNN2D(\n",
    "                input_channels   = 1,\n",
    "                num_classes      = num_classes,\n",
    "                conv_out_channels= model_conv_out_channels, \n",
    "                \n",
    "                conv_k1_h        = model_conv_k1_h,\n",
    "                conv_k1_w        = model_conv_k1_w,\n",
    "                \n",
    "                conv_k2_h        = model_conv_k2_h,\n",
    "                conv_k2_w        = model_conv_k2_w,\n",
    "                \n",
    "                conv_k3_h        = model_conv_k3_h,\n",
    "                conv_k3_w        = model_conv_k3_w,\n",
    "                \n",
    "                \n",
    "                conv_s1_h        = model_conv_s1_h,\n",
    "                conv_s1_w        = model_conv_s1_w,\n",
    "                \n",
    "                conv_s2_h        = model_conv_s2_h,\n",
    "                conv_s2_w        = model_conv_s2_w,\n",
    "                \n",
    "                conv_s3_h        = model_conv_s3_h,\n",
    "                conv_s3_w        = model_conv_s3_w,\n",
    "                \n",
    "            \n",
    "                pool_p1_h        = model_pool_p1_h,\n",
    "                pool_p1_w        = model_pool_p1_w,\n",
    "                \n",
    "                pool_p2_h        = model_pool_p2_h,\n",
    "                pool_p2_w        = model_pool_p2_w,\n",
    "                \n",
    "                pool_p3_h        = model_pool_p3_h,\n",
    "                pool_p3_w        = model_pool_p3_w,\n",
    "                \n",
    "                \n",
    "                pool_type        = model_pool_type,\n",
    "                \n",
    "                \n",
    "                fc1_units        = model_fc1_units,\n",
    "                dropout          = model_dropout,\n",
    "                \n",
    "                cnn_act1         = model_cnn_act1,\n",
    "                cnn_act2         = model_cnn_act2,\n",
    "                cnn_act3         = model_cnn_act3,\n",
    "            )\n",
    "    \n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "        \n",
    "        # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "        if best_weights is not None:\n",
    "            try:\n",
    "                model.load_state_dict(best_weights)\n",
    "                print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        # Definizione del criterio di perdita\n",
    "        criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        \n",
    "         # 10) ottimizzatore + scheduler + early stopping\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr    = model_lr,\n",
    "            betas = (model_beta1, model_beta2),\n",
    "            eps   = model_eps,\n",
    "            weight_decay = model_weight_decay\n",
    "            \n",
    "        )\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode     = \"min\",   # monitoriamo val_loss\n",
    "            factor   = 0.1,\n",
    "            patience = 8,\n",
    "            verbose  = True\n",
    "        )\n",
    "        early_stopping = EarlyStopping(patience=model_patience, mode=\"min\")\n",
    "        \n",
    "        \n",
    "        #criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        \n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        #print(f\"🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        #my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "    \n",
    "        #print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        #my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        '''NEW VERSION'''\n",
    "        # --- dopo model.load_state_dict(best_weights) e criterion = nn.CrossEntropyLoss() ---\n",
    "\n",
    "        # 1) prepara i data_loaders per train/val\n",
    "        data_loaders = {\n",
    "            \"train\": train_loader,\n",
    "            \"val\":   val_loader\n",
    "        }\n",
    "        \n",
    "        print(f\"🏋️‍♂️Salvo le metriche del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m a seguito della ottimizzazione su W&B ...\")\n",
    "        # 2) richiama la funzione che pesca da W&B la best‐run e corregge la train AUC\n",
    "        \n",
    "        #ATTENZIONE al potenziale problema di stringa, non di API: \n",
    "\n",
    "        #i due esempi che hai postato in realtà usano diversi caratteri “‑” (uno è il classico ASCII U+002D, l’altro è un non‑breaking hyphen U+2011 o simili), quindi quando chiami\n",
    "        \n",
    "        #entity = \"stefano‑bargione‑universit‑di‑roma‑tor‑vergata\"\n",
    "        #stai passando un nome che W&B non riconosce (e quindi api.projects(entity=…) torna vuoto), mentre con\n",
    "\n",
    "        #entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "        #funziona perché lì usi i semplici - ASCII.\n",
    "\n",
    "        my_train_results = load_best_run_results(\n",
    "            key=key,\n",
    "            model = model,\n",
    "            sweep_config = sweep_config,\n",
    "            data_loaders = data_loaders,\n",
    "            entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L’entity che passi a Api().runs(f\"{entity}/{project}\") è semplicemente il tuo account (o l’organizzazione) su W&B,\n",
    "        cioè la parte che compare subito prima del nome del progetto nell’URL.\n",
    "\n",
    "        Per esempio, se quando apri il tuo progetto su W&B vedi un indirizzo del tipo\n",
    "        \n",
    "        -> https://wandb.ai/steclab/some_project_name, allora entity = \"steclab\".\n",
    "        \n",
    "        Se invece lavori sotto un’organizzazione \n",
    "        \n",
    "        -> “cool‑team”, e l’URL è https://wandb.ai/cool-team/some_project_name, allora userai entity = \"cool-team\".\n",
    "\n",
    "        Puoi verificarlo:\n",
    "\n",
    "        Accedi a wandb.ai e vai sul progetto.\n",
    "        Leggi la prima parte dell’URL (tra wandb.ai/ e il /project_name).\n",
    "        Copiala esattamente come stringa in entity.\n",
    "\n",
    "        Così il tuo Api().runs(f\"{entity}/{project}\") andrà a pescare proprio le run che hai lanciato tu.\n",
    "\n",
    "        my_train_results = load_best_run_results(\n",
    "            key= key,\n",
    "            model = model,\n",
    "            sweep_config = sweep_config,\n",
    "            data_loaders = data_loaders,\n",
    "            entity= \"mio-entity\"\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        # 3) usa il best_model caricato dentro `train_results` e chiama il testing\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "                        \n",
    "        #print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        #save_performance_results(model_name,\n",
    "                                 #my_train_results,\n",
    "                                 #my_test_results,\n",
    "                                 #key,\n",
    "                                 #exp_cond,\n",
    "                                 #model_standardization,\n",
    "                                 #base_folder = save_path_folder)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #++++++++++++++++++++++++ ++++++++++++ ++++++++++++ ++++++++++++ ++++++++++++ ++++++++++++ ++++++++++++\n",
    "        \n",
    "        '''\n",
    "        GRADCAM COMPUTATION PER IL MODELLO CNN3D e ConvSep\n",
    "        \n",
    "        La funzione compute_gradcam_figure estrae i campioni (per ogni classe) e crea una figura con le due righe richieste.\n",
    "        \n",
    "        Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "        'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "        \n",
    "        La funzione 'save_performance_results' è stata modificata \n",
    "        per gestire ANCHE questo nuovo input dell'immagine \n",
    "        \n",
    "        (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "        seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "        \n",
    "        - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "        - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "        - provenienza del dato stesso (i.e., familiar_th)\n",
    "        )\n",
    "        \n",
    "        Spiegazione:\n",
    "        \n",
    "        La funzione compute_gradcam_figure eseguire il calcolo di GradCAM (vedi dettagli nella sua funzione)\n",
    "        e alla fine ritornerà in output una variabile \n",
    "        \n",
    "        'fig_image' che sarà poi assegnata alla variabile 'gradcam_image',\n",
    "        che è un oggetto buffer, che contiene i dati binari dell'immagine in formato PNG\n",
    "        (poiché abbiamo usato plt.savefig con format='png'). \n",
    "        \n",
    "        Quindi, quando passi gradcam_image (cioè fig_image) alla funzione 'save_performance_results',\n",
    "        viene scritto direttamente su disco come file PNG.\n",
    "        \n",
    "        Non c'è bisogno di ri-aprire o convertire ulteriormente, a meno che tu non voglia manipolare l'immagine in seguito.\n",
    "        Quindi, la soluzione è corretta così com'è:\n",
    "        il buffer viene salvato come file PNG nella directory specificata, \n",
    "        e successivamente potrai aprirlo con una libreria come cv2 o PIL se necessario.        \n",
    "        \n",
    "        Quindi, gradcam_image (i.e., fig_image) viene quindi passato correttamente dentro al loop di training e test, \n",
    "        tramite 'save_performance_results', come input, \n",
    "        che salverà quindi poi l'immagine nella path corrispondente \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "        gradcam_image = None\n",
    "        \n",
    "        #if model_name == \"CNN2D\":\n",
    "        \n",
    "        '''ATTENZIONE MODIFICA QUI'''\n",
    "        \n",
    "        #if model_name in (\"CNN3D_LSTM_FC\", \"SeparableCNN2D_LSTM_FC\"):\n",
    "        if model_name == \"CNN2D\":\n",
    "            \n",
    "            #def compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device, channel_names = None):\n",
    "            \n",
    "            '''ATTENZIONE HO AGGIUNTO IL TEST LOADER RAW PER VISUALIZZAZIONE SPETTROGRAMMI GREZZI (test_loader_raw)'''\n",
    "            gradcam_image = compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device, EEG_channels_names)\n",
    "            if gradcam_image is not None:\n",
    "                print(f\"Creazione di \\033[1mGradCAM Image\\033[0m per il modello \\033[1m{model_name}\\033[0m.\")\n",
    "                \n",
    "        print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        save_performance_results(model_name,\n",
    "                                 my_train_results,\n",
    "                                 my_test_results,\n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder = save_path_folder,\n",
    "                                 gradcam_image = gradcam_image)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        N.B\n",
    "        \n",
    "        gradcam_image = None avverrà solo all'inizio cioè per il primo modello, che verrà testato con una certa combinazione di dati.. \n",
    "        ma servirebbe tracciare in qualche modo \n",
    "\n",
    "        1) o che la gradcam_image di ogni combinazione venga ri-azzerata alla fine loop\n",
    "        2) o che venga monitorato che gradcam_image di una combinazione di dati già analizzata venga esclusa poi\n",
    "        (o messa in un set) in modo che rivenga per errore sovrascritta più volte.. \n",
    "        \n",
    "        Forse la strada più veloce potrebbe essere la soluzione 1)\n",
    "        \n",
    "        La soluzione più veloce e semplice è re-impostare la variabile gradcam_image a None alla fine dell'iterazione per ogni combinazione di dati\n",
    "        (cioè, all'interno del ciclo esterno che itera su key). \n",
    "         \n",
    "        In questo modo, per ogni nuovo dataset la variabile viene \"azzera\" e viene calcolata l'immagine GradCAM solo per quella combinazione, \n",
    "        evitando di sovrascrivere accidentalmente i risultati già calcolati per combinazioni precedenti.\n",
    "         \n",
    "        Un'altra possibilità sarebbe tenere traccia delle chiavi (o combinazioni) per cui hai già calcolato la GradCAM,\n",
    "        ad esempio usando un set, e saltare il calcolo se la combinazione è già presente. \n",
    "        \n",
    "        Tuttavia, se ogni combinazione deve avere la sua immagine, \n",
    "        la soluzione più semplice è quella di reimpostare gradcam_image = None alla fine dell'iterazione.\n",
    "        \n",
    "        Quindi, per esempio, alla fine del ciclo per ogni dataset (key) potresti fare:\n",
    "        (VEDI SOTTO)\n",
    "        \n",
    "        In questo modo, ti assicuri che per ogni nuova combinazione la variabile sia pulita e pronta per essere ricalcolata, \n",
    "        senza rischio di sovrascrivere o confondere i risultati\n",
    "        '''\n",
    "        \n",
    "        # Reimposta gradcam_image a None per la prossima combinazione di dati\n",
    "        gradcam_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7ca8b-e61e-4e98-a69a-cc2f4154a6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m, \tShape X: \u001b[1m(1586, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1586,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1014, 9, 9, 5)\u001b[0m, Val: \u001b[1m(254, 9, 9, 5)\u001b[0m, Test: \u001b[1m(318, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/familiar_th/CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_familiar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m64\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.005\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m203\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_th/fzz3drlj (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8696: 100%|██████████████████████████████| 5/5 [00:00<00:00, 265.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5377\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63       168\n",
      "           1       0.52      0.31      0.38       150\n",
      "\n",
      "    accuracy                           0.54       318\n",
      "   macro avg       0.53      0.53      0.51       318\n",
      "weighted avg       0.53      0.54      0.51       318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_fam/CNN3D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_th_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_fam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_th_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/familiar_th/SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_familiar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0001\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_th/ihg03ri0 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6530: 100%|██████████████████████████████| 7/7 [00:00<00:00, 317.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5472\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.48      0.53       168\n",
      "           1       0.52      0.62      0.56       150\n",
      "\n",
      "    accuracy                           0.55       318\n",
      "   macro avg       0.55      0.55      0.55       318\n",
      "weighted avg       0.55      0.55      0.55       318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_fam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_th_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_fam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_th_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m, \tShape X: \u001b[1m(1580, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1580,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1011, 9, 9, 5)\u001b[0m, Val: \u001b[1m(253, 9, 9, 5)\u001b[0m, Test: \u001b[1m(316, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/familiar_pt/CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_familiar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m96\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.01\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m203\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_pt/a96emyne (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6955: 100%|██████████████████████████████| 4/4 [00:00<00:00, 237.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4557\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.10      0.17       173\n",
      "           1       0.45      0.89      0.60       143\n",
      "\n",
      "    accuracy                           0.46       316\n",
      "   macro avg       0.48      0.49      0.38       316\n",
      "weighted avg       0.49      0.46      0.36       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_fam/CNN3D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_pt_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_fam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_pt_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/familiar_pt/SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_familiar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m96\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m1e-05\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_familiar_pt/9yjfwm0a (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7092: 100%|██████████████████████████████| 4/4 [00:00<00:00, 284.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4747\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.49      0.50       173\n",
      "           1       0.43      0.46      0.44       143\n",
      "\n",
      "    accuracy                           0.47       316\n",
      "   macro avg       0.47      0.47      0.47       316\n",
      "weighted avg       0.48      0.47      0.48       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_familiar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_fam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_pt_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_fam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_pt_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m, \tShape X: \u001b[1m(1667, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1667,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1066, 9, 9, 5)\u001b[0m, Val: \u001b[1m(267, 9, 9, 5)\u001b[0m, Test: \u001b[1m(334, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/unfamiliar_th/CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_unfamiliar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m32\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.005\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_th/std34429 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9306: 100%|████████████████████████████| 11/11 [00:00<00:00, 259.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5868\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.50      0.57       181\n",
      "           1       0.54      0.69      0.61       153\n",
      "\n",
      "    accuracy                           0.59       334\n",
      "   macro avg       0.60      0.60      0.59       334\n",
      "weighted avg       0.60      0.59      0.58       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_unfam/CNN3D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_th_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_unfam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_th_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/unfamiliar_th/SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_unfamiliar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m64\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.001\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_th/5ubasx8h (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8232: 100%|██████████████████████████████| 6/6 [00:00<00:00, 318.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5539\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.59      0.59       181\n",
      "           1       0.51      0.51      0.51       153\n",
      "\n",
      "    accuracy                           0.55       334\n",
      "   macro avg       0.55      0.55      0.55       334\n",
      "weighted avg       0.55      0.55      0.55       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_unfam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_th_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/th_unfam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_th_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m, \tShape X: \u001b[1m(1667, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1667,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1066, 9, 9, 5)\u001b[0m, Val: \u001b[1m(267, 9, 9, 5)\u001b[0m, Test: \u001b[1m(334, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/unfamiliar_pt/CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_unfamiliar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m32\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.005\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_pt/bzamyw4w (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8807: 100%|████████████████████████████| 11/11 [00:00<00:00, 292.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5689\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.54      0.57       181\n",
      "           1       0.53      0.61      0.56       153\n",
      "\n",
      "    accuracy                           0.57       334\n",
      "   macro avg       0.57      0.57      0.57       334\n",
      "weighted avg       0.58      0.57      0.57       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_unfam/CNN3D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_pt_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_unfam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_pt_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_pt_resp/spectrograms/unfamiliar_pt/SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_unfamiliar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m5e-05\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_pt_resp_spectrograms_channels_freqs_unfamiliar_pt/j4mxeqyt (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6930: 100%|██████████████████████████████| 7/7 [00:00<00:00, 231.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5539\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.54       181\n",
      "           1       0.51      0.63      0.57       153\n",
      "\n",
      "    accuracy                           0.55       334\n",
      "   macro avg       0.56      0.56      0.55       334\n",
      "weighted avg       0.57      0.55      0.55       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_pt_resp_spectrograms_unfamiliar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_unfam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_pt_resp_spectrograms_pt_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_pt_resp/spectrograms/pt_unfam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_pt_resp_spectrograms_pt_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m, \tShape X: \u001b[1m(1586, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1586,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1014, 9, 9, 5)\u001b[0m, Val: \u001b[1m(254, 9, 9, 5)\u001b[0m, Test: \u001b[1m(318, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/familiar_th/CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_familiar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m64\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.005\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th/72khtdjy (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8353: 100%|██████████████████████████████| 5/5 [00:00<00:00, 249.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.25      0.35       168\n",
      "           1       0.48      0.78      0.60       150\n",
      "\n",
      "    accuracy                           0.50       318\n",
      "   macro avg       0.52      0.52      0.47       318\n",
      "weighted avg       0.52      0.50      0.46       318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_fam/CNN3D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_th_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_fam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_th_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/familiar_th/SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_familiar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0005\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th/ez2xh4al (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8207: 100%|██████████████████████████████| 7/7 [00:00<00:00, 325.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5189\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.24      0.35       168\n",
      "           1       0.49      0.83      0.62       150\n",
      "\n",
      "    accuracy                           0.52       318\n",
      "   macro avg       0.55      0.54      0.48       318\n",
      "weighted avg       0.56      0.52      0.48       318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "/tmp/ipykernel_734141/710077813.py:979: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axs = plt.subplots(8, 5, figsize=(24, 30))  # 2 righe per 5 colonne per modello 2D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_fam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_th_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_fam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_th_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m, \tShape X: \u001b[1m(1580, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1580,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1011, 9, 9, 5)\u001b[0m, Val: \u001b[1m(253, 9, 9, 5)\u001b[0m, Test: \u001b[1m(316, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/familiar_pt/CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_familiar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m64\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.001\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt/b1qwsd9r (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_734141/776537650.py:86: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(2, 1, figsize=(10, 8))  # 2 righe, 1 colonna, dimensione figura\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7122: 100%|██████████████████████████████| 5/5 [00:00<00:00, 198.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4620\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.07      0.12       173\n",
      "           1       0.45      0.94      0.61       143\n",
      "\n",
      "    accuracy                           0.46       316\n",
      "   macro avg       0.51      0.50      0.37       316\n",
      "weighted avg       0.52      0.46      0.34       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_fam/CNN3D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_pt_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_fam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_pt_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/familiar_pt/SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_familiar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0005\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt/0prdjv6c (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5836: 100%|██████████████████████████████| 7/7 [00:00<00:00, 344.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5190\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.32      0.42       173\n",
      "           1       0.48      0.76      0.59       143\n",
      "\n",
      "    accuracy                           0.52       316\n",
      "   macro avg       0.55      0.54      0.50       316\n",
      "weighted avg       0.56      0.52      0.50       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_fam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_pt_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_fam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_pt_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m, \tShape X: \u001b[1m(1667, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1667,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1066, 9, 9, 5)\u001b[0m, Val: \u001b[1m(267, 9, 9, 5)\u001b[0m, Test: \u001b[1m(334, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/unfamiliar_th/CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_unfamiliar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m96\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.001\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th/zvfaed33 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8048: 100%|██████████████████████████████| 4/4 [00:00<00:00, 180.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5030\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.17      0.27       181\n",
      "           1       0.48      0.90      0.62       153\n",
      "\n",
      "    accuracy                           0.50       334\n",
      "   macro avg       0.57      0.53      0.45       334\n",
      "weighted avg       0.58      0.50      0.43       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_unfam/CNN3D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_th_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_unfam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_th_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/unfamiliar_th/SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_unfamiliar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m32\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.01\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th/wvox3tp8 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7065: 100%|████████████████████████████| 11/11 [00:00<00:00, 324.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5659\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60       181\n",
      "           1       0.53      0.53      0.53       153\n",
      "\n",
      "    accuracy                           0.57       334\n",
      "   macro avg       0.56      0.56      0.56       334\n",
      "weighted avg       0.57      0.57      0.57       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_unfam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_th_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/th_unfam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_th_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m, \tShape X: \u001b[1m(1667, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1667,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1066, 9, 9, 5)\u001b[0m, Val: \u001b[1m(267, 9, 9, 5)\u001b[0m, Test: \u001b[1m(334, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/unfamiliar_pt/CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_unfamiliar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m32\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.01\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt/zolaocpr (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7491: 100%|████████████████████████████| 11/11 [00:00<00:00, 305.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4910\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.13      0.22       181\n",
      "           1       0.47      0.92      0.62       153\n",
      "\n",
      "    accuracy                           0.49       334\n",
      "   macro avg       0.56      0.52      0.42       334\n",
      "weighted avg       0.57      0.49      0.40       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_unfam/CNN3D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_pt_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_unfam/GradCAM_results_CNN3D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_pt_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/th_resp_vs_shared_resp/spectrograms/unfamiliar_pt/SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_unfamiliar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m96\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0001\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/th_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt/k6hi6l04 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6949: 100%|██████████████████████████████| 4/4 [00:00<00:00, 269.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5659\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.38      0.49       181\n",
      "           1       0.52      0.78      0.62       153\n",
      "\n",
      "    accuracy                           0.57       334\n",
      "   macro avg       0.60      0.58      0.56       334\n",
      "weighted avg       0.60      0.57      0.55       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mth_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_unfam/SeparableCNN2D_LSTM_FC_performances_th_resp_vs_shared_resp_spectrograms_pt_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/th_resp_vs_shared_resp/spectrograms/pt_unfam/GradCAM_results_SeparableCNN2D_LSTM_FC_th_resp_vs_shared_resp_spectrograms_pt_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m, \tShape X: \u001b[1m(1586, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1586,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1014, 9, 9, 5)\u001b[0m, Val: \u001b[1m(254, 9, 9, 5)\u001b[0m, Test: \u001b[1m(318, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/familiar_th/CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_familiar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m96\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0005\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th/6f7q5vbg (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8490: 100%|██████████████████████████████| 4/4 [00:00<00:00, 177.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.3994\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.14      0.19       168\n",
      "           1       0.42      0.69      0.52       150\n",
      "\n",
      "    accuracy                           0.40       318\n",
      "   macro avg       0.38      0.42      0.36       318\n",
      "weighted avg       0.37      0.40      0.35       318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_fam/CNN3D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_th_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_fam/GradCAM_results_CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_th_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/familiar_th/SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_familiar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m96\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.001\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_th/kwiflzxd (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7146: 100%|██████████████████████████████| 4/4 [00:00<00:00, 268.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4717\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.02       168\n",
      "           1       0.47      0.99      0.64       150\n",
      "\n",
      "    accuracy                           0.47       318\n",
      "   macro avg       0.49      0.50      0.33       318\n",
      "weighted avg       0.49      0.47      0.31       318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_fam/SeparableCNN2D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_th_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_fam/GradCAM_results_SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_th_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m, \tShape X: \u001b[1m(1580, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1580,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1011, 9, 9, 5)\u001b[0m, Val: \u001b[1m(253, 9, 9, 5)\u001b[0m, Test: \u001b[1m(316, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/familiar_pt/CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_familiar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m32\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m5e-05\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt/73gndas9 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7173: 100%|████████████████████████████| 10/10 [00:00<00:00, 296.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4272\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.10      0.16       173\n",
      "           1       0.43      0.83      0.57       143\n",
      "\n",
      "    accuracy                           0.43       316\n",
      "   macro avg       0.42      0.46      0.36       316\n",
      "weighted avg       0.42      0.43      0.34       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_fam/CNN3D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_pt_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_fam/GradCAM_results_CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_pt_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/familiar_pt/SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_familiar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0005\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_familiar_pt/pqzzxcn8 (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6756: 100%|██████████████████████████████| 7/7 [00:00<00:00, 310.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4525\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.16      0.24       173\n",
      "           1       0.44      0.81      0.57       143\n",
      "\n",
      "    accuracy                           0.45       316\n",
      "   macro avg       0.47      0.48      0.41       316\n",
      "weighted avg       0.47      0.45      0.39       316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_familiar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_fam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_fam/SeparableCNN2D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_pt_fam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_fam/GradCAM_results_SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_pt_fam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m, \tShape X: \u001b[1m(1667, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1667,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1066, 9, 9, 5)\u001b[0m, Val: \u001b[1m(267, 9, 9, 5)\u001b[0m, Test: \u001b[1m(334, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/unfamiliar_th/CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_unfamiliar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m64\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0001\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th/csm9yy6p (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7939: 100%|██████████████████████████████| 6/6 [00:00<00:00, 233.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4162\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.28      0.34       181\n",
      "           1       0.40      0.58      0.47       153\n",
      "\n",
      "    accuracy                           0.42       334\n",
      "   macro avg       0.42      0.43      0.41       334\n",
      "weighted avg       0.42      0.42      0.40       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_unfam/CNN3D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_th_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_unfam/GradCAM_results_CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_th_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/unfamiliar_th/SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_unfamiliar_th.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.005\u001b[0m, model_beta1= \u001b[1m0.9\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-07\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_th/m9cknnnf (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9419: 100%|██████████████████████████████| 7/7 [00:00<00:00, 296.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4042\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.49      0.47       181\n",
      "           1       0.33      0.30      0.32       153\n",
      "\n",
      "    accuracy                           0.40       334\n",
      "   macro avg       0.39      0.40      0.39       334\n",
      "weighted avg       0.40      0.40      0.40       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_th\u001b[0m, Subfolder ottenuto: \u001b[1mth_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_unfam/SeparableCNN2D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_th_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/th_unfam/GradCAM_results_SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_th_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "Estrazione Dati per il dataset: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m, \tShape X: \u001b[1m(1667, 9, 9, 5)\u001b[0m, Shape y: \u001b[1m(1667,)\u001b[0m\n",
      "Dataset Splitting: Train: \u001b[1m(1066, 9, 9, 5)\u001b[0m, Val: \u001b[1m(267, 9, 9, 5)\u001b[0m, Test: \u001b[1m(334, 9, 9, 5)\u001b[0m\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m e il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/unfamiliar_pt/CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_unfamiliar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mCNN3D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m\n",
      "Parametri per \u001b[1mCNN3D_LSTM_FC\u001b[0m: batch_size= \u001b[1m32\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.005\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.99\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mCNN3D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`CNN3D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt/uv2q0pgs (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7344: 100%|████████████████████████████| 11/11 [00:00<00:00, 301.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4341\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.13      0.20       181\n",
      "           1       0.44      0.79      0.56       153\n",
      "\n",
      "    accuracy                           0.43       334\n",
      "   macro avg       0.43      0.46      0.38       334\n",
      "weighted avg       0.43      0.43      0.37       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n",
      ">>> savefig\n",
      ">>> done\n",
      "Creazione di \u001b[1mGradCAM Image\u001b[0m per il modello \u001b[1mCNN3D_LSTM_FC\u001b[0m.\n",
      "Salvataggio dei risultati per \u001b[1mCNN3D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n",
      "\n",
      "DEBUG - Chiave: \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m, Subfolder ottenuto: \u001b[1mpt_unfam\u001b[0m\n",
      "\n",
      "🔬Risultati salvati con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_unfam/CNN3D_LSTM_FC_performances_pt_resp_vs_shared_resp_spectrograms_pt_unfam_std.pkl\u001b[0m\n",
      "\n",
      "\n",
      "📸Immagine \u001b[1mGradCAM salvata\u001b[0m con successo 👍 in: \n",
      "\u001b[1m/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks/pt_resp_vs_shared_resp/spectrograms/pt_unfam/GradCAM_results_CNN3D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_pt_unfam_std.png\u001b[0m\n",
      "\n",
      "\n",
      "Preparazione dati per il dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m e il modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m...\n",
      "🕵️‍♂️🔍Caricamento file .pkl: \u001b[1m/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies/pt_resp_vs_shared_resp/spectrograms/unfamiliar_pt/SeparableCNN2D_LSTM_FC_pt_resp_vs_shared_resp_spectrograms_unfamiliar_pt.pkl\u001b[0m\n",
      "✅ File .pkl trovato per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m su \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m\n",
      "Parametri per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m: batch_size= \u001b[1m48\u001b[0m, n_epochs= \u001b[1m100\u001b[0m, patience= \u001b[1m12\u001b[0m, lr= \u001b[1m0.0001\u001b[0m, model_beta1= \u001b[1m0.95\u001b[0m,  model_beta2= \u001b[1m0.995\u001b[0m,  model_eps= \u001b[1m1e-08\u001b[0m, standardization= \u001b[1mTrue\u001b[0m\n",
      "\u001b[1mSÌ Standardizzazione Dati!\u001b[0m\n",
      "\n",
      "Inizializzazione Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m\n",
      "📊 Modello \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m inizializzato con \u001b[01i pesi ottimizzati\u001b[0m tramite hyper-parameter tuning su \u001b[1mWeight & Biases\u001b[0m\n",
      "🏋️‍♂️Salvo le metriche del training per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m a seguito della ottimizzazione su W&B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trovate \u001b[1m202\u001b[0m runs\n",
      "\n",
      "✓ Progetto \u001b[1m`pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt`\u001b[0m\n",
      "\n",
      "✓ Modello \u001b[1m`SeparableCNN2D_LSTM_FC`\u001b[0m\n",
      "\n",
      "✓ Sweep \u001b[1m`<Sweep stefano-bargione-universit-di-roma-tor-vergata/pt_resp_vs_shared_resp_spectrograms_channels_freqs_unfamiliar_pt/25e9pu8h (RUNNING)>`\u001b[0m\n",
      "\n",
      "\n",
      "Avvio del testing per \u001b[1mSeparableCNN2D_LSTM_FC\u001b[0m sul dataset \u001b[1mpt_resp_vs_shared_resp_spectrograms_unfamiliar_pt\u001b[0m...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7283: 100%|██████████████████████████████| 7/7 [00:00<00:00, 316.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.4251\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.34      0.39       181\n",
      "           1       0.40      0.53      0.46       153\n",
      "\n",
      "    accuracy                           0.43       334\n",
      "   macro avg       0.43      0.43      0.42       334\n",
      "weighted avg       0.43      0.43      0.42       334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/stefano/anaconda3/envs/new_pd_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> layout\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PER CNN3D PURA o CNN2D CONV SEP\n",
    "'''\n",
    "\n",
    "# Imposta il seme per la riproducibilità\n",
    "\n",
    "#Imposta il seme per i generatori casuali di PyTorch (per operazioni sui tensori e inizializzazione dei pesi dei modelli).\n",
    "#Importante se vuoi garantire che l'addestramento del modello produca gli stessi risultati in diverse esecuzioni.\n",
    "torch.manual_seed(32)\n",
    "\n",
    "#Imposta il seme per NumPy, utile se NumPy viene usato per operazioni casuali (ad es. shuffling dei dati, inizializzazione di matrici, ecc.).\n",
    "#Importante se usi NumPy per il preprocessing dei dati e vuoi riproducibilità.\n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "#mposta il seme per il modulo random di Python (utile se si usano funzioni di randomizzazione di Python puro).\n",
    "#Importante solo se usi random per operazioni come mescolamento di liste.\n",
    "random.seed(32)\n",
    "\n",
    "#Imposta il seme per i generatori casuali su GPU, se disponibile.\n",
    "#Utile se stai eseguendo il codice su una GPU per garantire riproducibilità anche in quel contesto.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(32)\n",
    "\n",
    "       \n",
    "'''\n",
    "\n",
    "In questo caso, \n",
    "\n",
    "il set processed_datasets traccia i dataset già elaborati, \n",
    "e il set processed_models tiene traccia delle combinazioni già effettuate (modello + dataset). \n",
    "\n",
    "In questo modo, puoi escludere un dataset dal training se è già stato utilizzato in precedenza, \n",
    "anche se usato con un modello differente.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dizionario per tracciare la standardizzazione usata per ogni combinazione d\n",
    "# Dizionario per salvare informazioni sul modello (es. se i dati sono standardizzati)\n",
    "models_info = {}\n",
    "\n",
    "EEG_channels = EEG_channels_names \n",
    "\n",
    "# Set per tenere traccia dei dataset già elaborati\n",
    "processed_datasets = set()\n",
    "\n",
    "# Set per tenere traccia delle combinazioni già elaborate\n",
    "processed_models = set()\n",
    "\n",
    "\n",
    "# Path delle performance dei modelli ottimizzati con weight and biases\n",
    "# Path per trovare le best performances di ogni modello per ogni combinazione dei dati\n",
    "base_folder = \"/home/stefano/Interrogait/WB_spectrograms_best_results_channels_frequencies\"\n",
    "                                        #WB_spectrograms_best_results_channels_frequencies_params_hyperparams/\n",
    "\n",
    "# Path di salvataggio delle performance dei modelli dopo estrazione best models da base_folder\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_post_WB\"\n",
    "\n",
    "#save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_hyperparams_post_WB\"\n",
    "save_path_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks\"\n",
    "\n",
    "# --- LOOP PRINCIPALE (con minime modifiche) ---\n",
    "for key, (X_data, y_data) in data_dict.items():\n",
    "    \n",
    "    print(f\"\\n\\nEstrazione Dati per il dataset: \\033[1m{key}\\033[0m, \\tShape X: \\033[1m{X_data.shape}\\033[0m, Shape y: \\033[1m{y_data.shape}\\033[0m\")\n",
    "    \n",
    "    if key in processed_datasets:\n",
    "        print(f\"ATTENZIONE: Il dataset {key} è già stato elaborato! Salto iterazione...\")\n",
    "        continue\n",
    "        \n",
    "    processed_datasets.add(key)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X_data, y_data)\n",
    "    print(f\"Dataset Splitting: Train: \\033[1m{X_train.shape}\\033[0m, Val: \\033[1m{X_val.shape}\\033[0m, Test: \\033[1m{X_test.shape}\\033[0m\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    CREO COPIA TEST_LOADER_RAW PER I PLOT DEL POWER RAW PER BANDA E CLASSE\n",
    "    '''\n",
    "    # 1) salva una copia RAW dei soli dati di test PRIMA di standardizzare\n",
    "    X_test_raw = X_test.copy()\n",
    "    y_test_raw = y_test.copy()\n",
    "    \n",
    "    # 2) tensori\n",
    "    X_raw_tensor = torch.tensor(X_test_raw, dtype=torch.float32)\n",
    "    y_raw_tensor = torch.tensor(y_test_raw, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    #for model_name in [\"CNN2D\", \"BiLSTM\", \"Transformer\"]:\n",
    "    \n",
    "    '''ATTENZIONE MODIFICA QUI'''\n",
    "    \n",
    "    #for model_name in [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"]:\n",
    "    for model_name in [\"CNN3D_LSTM_FC\", \"SeparableCNN2D_LSTM_FC\"]:\n",
    "\n",
    "        model_key = f\"{model_name}_{key}\"\n",
    "        if model_key in processed_models:\n",
    "            print(f\"ATTENZIONE: Il modello {model_name} per il dataset {key} è già stato addestrato! Salto iterazione...\")\n",
    "            continue\n",
    "        processed_models.add(model_key)\n",
    "        \n",
    "        print(f\"\\nPreparazione dati per il dataset \\033[1m{key}\\033[0m e il modello \\033[1m{model_name}\\033[0m...\")\n",
    "        \n",
    "        # Prova a caricare la configurazione e i pesi ottimali dal file .pkl\n",
    "        \n",
    "        '''\n",
    "        load_config_if_available --> prende in input 'key' che è la chiave composita (i.e, th_resp_vs_pt_resp_1_20_familiar_th)\n",
    "        parse_combination_key --> prende in input 'key' che suddivide la chiave composita in stringhe separate\n",
    "        \n",
    "        exp_cond, data_type, category_subject che sfrutto per crearmi la directory path che mi servirà per caricarmi \n",
    "        pesi del modello e i suoi iper-parametri\n",
    "        \n",
    "        Diciamo che in questo caso, sfrutto 'parse_combination_key per qualcosa che serve a 'load_config_if_available' in modo IMPLICITO..\n",
    "        '''\n",
    "        \n",
    "        config, best_weights = load_config_if_available(key, model_name, base_folder)\n",
    "        \n",
    "        if config is None:\n",
    "            raise ValueError(f\"\\033[1mNessun file .pkl trovato per {model_name} su {key}\\033[0m. Non posso procedere senza la configurazione ottimale.\")\n",
    "        \n",
    "        '''\n",
    "        Successivamente, queste variabili vengono invece create in maniera ESPLICITA per fasi successive del loop\n",
    "        MA in questo caso, parsifica la chiave una VOLTA SOLA e memorizza i valori!\n",
    "        '''\n",
    "        \n",
    "        # Parsifica la chiave una volta sola e memorizza i valori\n",
    "        exp_cond, data_type, category_subject = parse_combination_key(key)\n",
    "        \n",
    "        '''\n",
    "        Dpodiché, \n",
    "        \n",
    "        1) si carica i vari valori degli iper-parametri,\n",
    "        2) si esegue la standardizzazione se servisse,\n",
    "        3) prepara il modello per la divisione in train_loader etc.,\n",
    "        4) si carica la configurazione dei pesi del modello, \n",
    "        5) assegna i vari valori degli iper-parametri del modello corrente per la combinazione di dati correntemente iterata \n",
    "        \n",
    "        6) esegue il training e il test e poi\n",
    "        \n",
    "        7) si salva il tutto nella path corrispondente...\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PER DARE UNIFORMITÀ AL CODICE, CAMBIO IL NOME DELLE VARIABILI, CHE CONTENGONO I VALORI OTTIMIZZATI \n",
    "        DA FORNIRE IN INPUT ALLE VARIE FUNZIONI CHE SONO RICHIAMATE NEL LOOP'''\n",
    "        \n",
    "        \n",
    "        #\"lr\": {\"values\": [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]},\n",
    "        #\"weight_decay\": {\"values\": [0, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "        #\"n_epochs\": {\"value\": 100},\n",
    "        #\"patience\": {\"value\": 12},\n",
    "        \n",
    "        #\"model_name\": {\"values\": [\"SeparableCNN2D_LSTM_FC\"]},\n",
    "        #\"batch_size\": {\"values\": [32, 48, 64, 96]},\n",
    "        #\"standardization\": {\"values\": [True, False]},\n",
    "        #\"beta1\": {\"values\": [0.9, 0.95]},\n",
    "        #\"beta2\": {\"values\": [0.99, 0.995]},\n",
    "        #\"eps\": {\"values\": [1e-8, 1e-7]},\n",
    "        \n",
    "        #\"use_lstm\": {\"values\": [True, False]},\n",
    "        #\"lstm_hidden\": {\"values\": [32]},\n",
    "        #\"dropout\": {\"values\": [0.5]},\n",
    "        \n",
    "        \n",
    "        model_lr = config[\"lr\"]\n",
    "        model_weight_decay = config[\"weight_decay\"]\n",
    "        model_n_epochs = config[\"n_epochs\"]\n",
    "        model_patience = config[\"patience\"]\n",
    "        \n",
    "        \n",
    "        model_batch_size = config[\"batch_size\"]\n",
    "        model_standardization = config[\"standardization\"]\n",
    "    \n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        model_beta1 =  config[\"beta1\"]\n",
    "        model_beta2 =  config[\"beta2\"]\n",
    "        model_eps = config[\"eps\"]\n",
    "        \n",
    "\n",
    "        '''Per CNN3D_LSTM_FC e SeparableCNN2D_LSTM_FC'''\n",
    "        model_use_lstm      = config[\"use_lstm\"]\n",
    "        model_lstm_hidden   = config[\"lstm_hidden\"]\n",
    "        model_dropout       = config[\"dropout\"]\n",
    "    \n",
    "            \n",
    "        #print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, weight_decay= \\033[1m{model_weight_decay}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "        print(f\"Parametri per \\033[1m{model_name}\\033[0m: batch_size= \\033[1m{model_batch_size}\\033[0m, n_epochs= \\033[1m{model_n_epochs}\\033[0m, patience= \\033[1m{model_patience}\\033[0m, lr= \\033[1m{model_lr}\\033[0m, model_beta1= \\033[1m{model_beta1}\\033[0m,  model_beta2= \\033[1m{model_beta2}\\033[0m,  model_eps= \\033[1m{model_eps}\\033[0m, standardization= \\033[1m{model_standardization}\\033[0m\")\n",
    "        \n",
    "        # Salva nel dizionario se per quella combinazione è stata applicata la standardizzazione ai dati\n",
    "        models_info[model_key] = {\"standardization\": model_standardization}\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 3) dataset & loader per test set (per plots power raw) –‑  IMPORTANTISSIMO: shuffle=False\n",
    "        raw_dataset = TensorDataset(X_raw_tensor, y_raw_tensor)\n",
    "        test_loader_raw = DataLoader(raw_dataset,\n",
    "                             batch_size=model_batch_size,\n",
    "                             shuffle=False)\n",
    "        \n",
    "        '''PER MANTENERE LA STESSA LOGICA DEL CODICE (ANCHE SE POTREI INSERIRLA DENTRO PREPARE_DATA_FOR_MODEL MODIFICANDO LA FUNZIONE (SI VEDA IN CELLA SOPRA COME)\n",
    "        IMPONGONO LA STANDARDIZZAZIONE PRIMA DI QUESTA FUNZIONE\n",
    "        '''\n",
    "\n",
    "        if model_standardization:\n",
    "            X_train, X_val, X_test = standardize_data(X_train, X_val, X_test)\n",
    "            print(f\"\\033[1mSÌ Standardizzazione Dati!\\033[0m\")\n",
    "        else:\n",
    "            print(f\"\\033[1mNO Standardizzazione Dati!\\033[0m\")\n",
    "        \n",
    "        # Sposta il modello sulla GPU (se disponibile)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        \n",
    "        # Preparazione dei dataloaders\n",
    "        train_loader, val_loader, test_loader, class_weights_tensor = prepare_data_for_model(\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test, model_type = model_name, batch_size = model_batch_size)\n",
    "        \n",
    "\n",
    "        # Inizializzazione del modello\n",
    "        #if model_name == \"CNN2D\":\n",
    "        #    model = CNN2D(input_channels=64, num_classes=2)\n",
    "        \n",
    "        #if model_name == \"CNN2D_LSTM_FC\":\n",
    "            \n",
    "            #model = CNN2D_LSTM_FC(n_freq = 45, input_channels=64, num_classes=2, dropout = 0.2)\n",
    "            #model = CNN2D_LSTM_FC(input_channels = 5, num_classes=2, dropout=0.2)\n",
    "            \n",
    "        #elif model_name == \"BiLSTM\":\n",
    "        #    model = ReadMEndYou(input_size= 64 * 81, hidden_sizes=[24, 48, 62], output_size=2, bidirectional=True)\n",
    "        #elif model_name == \"Transformer\":\n",
    "        #    model = ReadMYMind(d_model=16, num_heads=4, num_layers=2, num_classes=2, channels=64, freqs=81)\n",
    "        \n",
    "        #elif model_name == \"TopomapNet\":\n",
    "            #model = TopomapNet(\n",
    "                #input_channels=5,\n",
    "                #num_classes=2,\n",
    "                #base_channels=model_base_channels,\n",
    "                #use_lstm=model_use_lstm,\n",
    "                #lstm_hidden=model_lstm_hidden,\n",
    "                #dropout=model_dropout\n",
    "            #)\n",
    "        \n",
    "        '''OCCHIO QUI CAMBIATO PER GRIGLIA 3D'''\n",
    "        if model_name == \"CNN3D_LSTM_FC\":\n",
    "            \n",
    "            sweep_config = sweep_config_cnn3d\n",
    "            \n",
    "            model = CNN3D_LSTM_FC(\n",
    "                num_classes=2,\n",
    "                dropout=model_dropout,\n",
    "                hidden_size=model_lstm_hidden,\n",
    "                use_lstm=model_use_lstm)\n",
    "\n",
    "            print(f\"\\nInizializzazione Modello \\033[1mCNN3D_LSTM_FC\\033[0m\")\n",
    "        \n",
    "        elif model_name == \"SeparableCNN2D_LSTM_FC\":\n",
    "            \n",
    "            sweep_config = sweep_config_cnn_sep\n",
    "            \n",
    "            model = SeparableCNN2D_LSTM_FC(\n",
    "                num_classes=2,\n",
    "                dropout=model_dropout,\n",
    "                hidden_size=model_lstm_hidden,\n",
    "                use_lstm=model_use_lstm\n",
    "            )\n",
    "            print(f\"\\nInizializzazione Modello \\033[1mSeparableCNN2D_LSTM_FC\\033[0m\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Modello {model_name} non riconosciuto.\")\n",
    "        \n",
    "        \n",
    "        # Se abbiamo caricato i pesi ottimali, li carichiamo nel modello\n",
    "        if best_weights is not None:\n",
    "            try:\n",
    "                model.load_state_dict(best_weights)\n",
    "                print(f\"📊 Modello \\033[1m{model_name}\\033[0m inizializzato con \\033[01i pesi ottimizzati\\033[0m tramite hyper-parameter tuning su \\033[1mWeight & Biases\\033[0m\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️Errore nel caricamento dei pesi per {model_name} su {key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        # Definizione del criterio di perdita\n",
    "        #criterion = nn.CrossEntropyLoss(weight = class_weights_tensor)\n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        # Definizione dell'ottimizzatore con i parametri aggiornati\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr = model_lr, weight_decay = model_weight_decay)\n",
    "        \n",
    "        '''NUOVE MODIFICHE'''\n",
    "        \n",
    "         # 10) ottimizzatore + scheduler + early stopping\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr    = model_lr,\n",
    "            betas = (model_beta1, model_beta2),\n",
    "            eps   = model_eps, \n",
    "            weight_decay = model_weight_decay\n",
    "        )\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode     = \"min\",   # monitoriamo val_loss\n",
    "            factor   = 0.1,\n",
    "            patience = 8,\n",
    "            verbose  = True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #early_stopping = EarlyStopping(patience=model_patience, mode=\"min\")\n",
    "        \n",
    "        '''OLD VERSION'''\n",
    "        #print(f\"🏋️‍♂️Avvio del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        #my_train_results = training(model, train_loader, val_loader, optimizer, criterion, n_epochs = model_n_epochs, patience = model_patience)\n",
    "        \n",
    "        '''NEW VERSION'''\n",
    "        # --- dopo model.load_state_dict(best_weights) e criterion = nn.CrossEntropyLoss() ---\n",
    "\n",
    "        # 1) prepara i data_loaders per train/val\n",
    "        data_loaders = {\n",
    "            \"train\": train_loader,\n",
    "            \"val\":   val_loader\n",
    "        }\n",
    "        \n",
    "        print(f\"🏋️‍♂️Salvo le metriche del training per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m a seguito della ottimizzazione su W&B ...\")\n",
    "        # 2) richiama la funzione che pesca da W&B la best‐run e corregge la train AUC\n",
    "        \n",
    "        #ATTENZIONE al potenziale problema di stringa, non di API: \n",
    "\n",
    "        #i due esempi che hai postato in realtà usano diversi caratteri “‑” (uno è il classico ASCII U+002D, l’altro è un non‑breaking hyphen U+2011 o simili), quindi quando chiami\n",
    "        \n",
    "        #entity = \"stefano‑bargione‑universit‑di‑roma‑tor‑vergata\"\n",
    "        #stai passando un nome che W&B non riconosce (e quindi api.projects(entity=…) torna vuoto), mentre con\n",
    "\n",
    "        #entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "        #funziona perché lì usi i semplici - ASCII.\n",
    "\n",
    "        my_train_results = load_best_run_results(\n",
    "            key=key,\n",
    "            model = model,\n",
    "            sweep_config = sweep_config,\n",
    "            data_loaders = data_loaders,\n",
    "            entity = \"stefano-bargione-universit-di-roma-tor-vergata\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        L’entity che passi a Api().runs(f\"{entity}/{project}\") è semplicemente il tuo account (o l’organizzazione) su W&B,\n",
    "        cioè la parte che compare subito prima del nome del progetto nell’URL.\n",
    "\n",
    "        Per esempio, se quando apri il tuo progetto su W&B vedi un indirizzo del tipo\n",
    "        \n",
    "        -> https://wandb.ai/steclab/some_project_name, allora entity = \"steclab\".\n",
    "        \n",
    "        Se invece lavori sotto un’organizzazione \n",
    "        \n",
    "        -> “cool‑team”, e l’URL è https://wandb.ai/cool-team/some_project_name, allora userai entity = \"cool-team\".\n",
    "\n",
    "        Puoi verificarlo:\n",
    "\n",
    "        Accedi a wandb.ai e vai sul progetto.\n",
    "        Leggi la prima parte dell’URL (tra wandb.ai/ e il /project_name).\n",
    "        Copiala esattamente come stringa in entity.\n",
    "\n",
    "        Così il tuo Api().runs(f\"{entity}/{project}\") andrà a pescare proprio le run che hai lanciato tu.\n",
    "\n",
    "        my_train_results = load_best_run_results(\n",
    "            key= key,\n",
    "            model = model,\n",
    "            sweep_config = sweep_config,\n",
    "            data_loaders = data_loaders,\n",
    "            entity= \"mio-entity\"\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        print(f\"Avvio del testing per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        # 3) usa il best_model caricato dentro `train_results` e chiama il testing\n",
    "        my_test_results = testing(my_train_results, test_loader, criterion)\n",
    "        \n",
    "        '''\n",
    "        GRADCAM COMPUTATION PER IL MODELLO CNN3D e ConvSep\n",
    "        \n",
    "        La funzione compute_gradcam_figure estrae i campioni (per ogni classe) e crea una figura con le due righe richieste.\n",
    "        \n",
    "        Il parametro gradcam_image (un buffer binario o un'immagine) viene passato alla funzione di salvataggio, \n",
    "        'save_performance_results', in modo da essere salvato nella path corretta. \n",
    "        \n",
    "        La funzione 'save_performance_results' è stata modificata \n",
    "        per gestire ANCHE questo nuovo input dell'immagine \n",
    "        \n",
    "        (ossia, per salvare il file con un nome che inizia con 'GradCAM_results_'\n",
    "        seguito da tutte le altre stringhe corrispondenti alla combinazione di fattori che costituiscono il dataset corrente:\n",
    "        \n",
    "        - coppia di condizioni sperimentali da cui provengono i dati (i.e., th_resp_vs_pt_resp )\n",
    "        - tipologia di dato EEG prelevato (i.e., spectrograms) \n",
    "        - provenienza del dato stesso (i.e., familiar_th)\n",
    "        )\n",
    "        \n",
    "        Spiegazione:\n",
    "        \n",
    "        La funzione compute_gradcam_figure eseguire il calcolo di GradCAM (vedi dettagli nella sua funzione)\n",
    "        e alla fine ritornerà in output una variabile \n",
    "        \n",
    "        'fig_image' che sarà poi assegnata alla variabile 'gradcam_image',\n",
    "        che è un oggetto buffer, che contiene i dati binari dell'immagine in formato PNG\n",
    "        (poiché abbiamo usato plt.savefig con format='png'). \n",
    "        \n",
    "        Quindi, quando passi gradcam_image (cioè fig_image) alla funzione 'save_performance_results',\n",
    "        viene scritto direttamente su disco come file PNG.\n",
    "        \n",
    "        Non c'è bisogno di ri-aprire o convertire ulteriormente, a meno che tu non voglia manipolare l'immagine in seguito.\n",
    "        Quindi, la soluzione è corretta così com'è:\n",
    "        il buffer viene salvato come file PNG nella directory specificata, \n",
    "        e successivamente potrai aprirlo con una libreria come cv2 o PIL se necessario.        \n",
    "        \n",
    "        Quindi, gradcam_image (i.e., fig_image) viene quindi passato correttamente dentro al loop di training e test, \n",
    "        tramite 'save_performance_results', come input, \n",
    "        che salverà quindi poi l'immagine nella path corrispondente \n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Se il modello è CNN2D, calcola anche GradCAM per la visualizzazione\n",
    "        gradcam_image = None\n",
    "        \n",
    "        #if model_name == \"CNN2D\":\n",
    "        \n",
    "        '''ATTENZIONE MODIFICA QUI'''\n",
    "        \n",
    "        #if model_name == \"CNN2D_LSTM_FC\":\n",
    "        \n",
    "        if model_name in (\"CNN3D_LSTM_FC\", \"SeparableCNN2D_LSTM_FC\"):\n",
    "            \n",
    "            gradcam_image = compute_gradcam_figure(model, test_loader, test_loader_raw, exp_cond, data_type, category_subject, device, EEG_channels_names, debug = False)\n",
    "            if gradcam_image is not None:\n",
    "                print(f\"Creazione di \\033[1mGradCAM Image\\033[0m per il modello \\033[1m{model_name}\\033[0m.\")\n",
    "                \n",
    "        print(f\"Salvataggio dei risultati per \\033[1m{model_name}\\033[0m sul dataset \\033[1m{key}\\033[0m...\")\n",
    "        save_performance_results(model_name,\n",
    "                                 my_train_results,\n",
    "                                 my_test_results,\n",
    "                                 key,\n",
    "                                 exp_cond,\n",
    "                                 model_standardization,\n",
    "                                 base_folder = save_path_folder,\n",
    "                                 gradcam_image = gradcam_image)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        N.B\n",
    "        \n",
    "        gradcam_image = None avverrà solo all'inizio cioè per il primo modello, che verrà testato con una certa combinazione di dati.. \n",
    "        ma servirebbe tracciare in qualche modo \n",
    "\n",
    "        1) o che la gradcam_image di ogni combinazione venga ri-azzerata alla fine loop\n",
    "        2) o che venga monitorato che gradcam_image di una combinazione di dati già analizzata venga esclusa poi\n",
    "        (o messa in un set) in modo che rivenga per errore sovrascritta più volte.. \n",
    "        \n",
    "        Forse la strada più veloce potrebbe essere la soluzione 1)\n",
    "        \n",
    "        La soluzione più veloce e semplice è re-impostare la variabile gradcam_image a None alla fine dell'iterazione per ogni combinazione di dati\n",
    "        (cioè, all'interno del ciclo esterno che itera su key). \n",
    "         \n",
    "        In questo modo, per ogni nuovo dataset la variabile viene \"azzera\" e viene calcolata l'immagine GradCAM solo per quella combinazione, \n",
    "        evitando di sovrascrivere accidentalmente i risultati già calcolati per combinazioni precedenti.\n",
    "         \n",
    "        Un'altra possibilità sarebbe tenere traccia delle chiavi (o combinazioni) per cui hai già calcolato la GradCAM,\n",
    "        ad esempio usando un set, e saltare il calcolo se la combinazione è già presente. \n",
    "        \n",
    "        Tuttavia, se ogni combinazione deve avere la sua immagine, \n",
    "        la soluzione più semplice è quella di reimpostare gradcam_image = None alla fine dell'iterazione.\n",
    "        \n",
    "        Quindi, per esempio, alla fine del ciclo per ogni dataset (key) potresti fare:\n",
    "        (VEDI SOTTO)\n",
    "        \n",
    "        In questo modo, ti assicuri che per ogni nuova combinazione la variabile sia pulita e pronta per essere ricalcolata, \n",
    "        senza rischio di sovrascrivere o confondere i risultati\n",
    "        '''\n",
    "        \n",
    "        # Reimposta gradcam_image a None per la prossima combinazione di dati\n",
    "        gradcam_image = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92cb3b-1a8e-41ef-a262-abc5601fe141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ipykernel.connect import get_connection_file\n",
    "\n",
    "print(\"PID:\", os.getpid())\n",
    "print(\"Conn file:\", get_connection_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb3a7c-d0d3-4cd4-81bf-cacb361ae2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"finito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d577ed-7eb0-49d1-9c50-a57451f8e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf5e9d-2664-4f14-9231-fadaf34cab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/home/stefano/Interrogait/models_info_spectrograms_EEG_GradCAM_Checks.pkl', 'wb') as f:\n",
    "#    pickle.dump(models_info, f)\n",
    "\n",
    "with open('/home/stefano/Interrogait/spectrograms_EEG_channels_freqs_params_GradCAM_Checks.pkl', 'wb') as f:\n",
    "    pickle.dump(models_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c6996-1398-4a15-adc9-e4070f958278",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **CREAZIONE DELLE TABLES CON INTEGRAZIONE DELLE PERFORMANCE TRAINING & TEST DEI MODELLI DENTRO DATAFRAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f6d32-8813-47d1-8b6e-4d41df604a80",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Integrazioni Performance Training e Test del Modello dentro DataFrame - OLD APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18246eb8-3f44-4adc-9ac0-5d029eb68539",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **OLD BEST APPROACH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c9a65-6ea2-4064-bee7-dbe2e631b4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Definiamo le path\n",
    "paths = {\n",
    "    \"TH_FAM\": \"/home/stefano/Interrogait/PRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/TH_FAM_UNSCALED/\",\n",
    "    \"PT_FAM\": \"/home/stefano/Interrogait/PRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/PT_FAM_UNSCALED/\",\n",
    "    \"TH_UNFAM\": \"/home/stefano/Interrogait/PRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/TH_UNFAM_UNSCALED/\",\n",
    "    \"PT_UNFAM\": \"/home/stefano/Interrogait/PPRE_WB_OPTIMIZATION_MODELS_RESULTS _TIME_DOMAIN/Model_Results/PT_UNFAM_UNSCALED/\"\n",
    "}\n",
    "\n",
    "\n",
    "# Identificatori delle triplette\n",
    "identifiers = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Dizionario per salvare i risultati\n",
    "all_models_dict = {}\n",
    "\n",
    "# Iteriamo su ogni path\n",
    "for condition, path in paths.items():\n",
    "    models_dict = {identifier: {} for identifier in identifiers}  # Dizionario per i modelli della path corrente\n",
    "    \n",
    "    # Controlliamo che la directory esista\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory non trovata: {path}\")\n",
    "        continue\n",
    "    \n",
    "    # Otteniamo la lista di file nella directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filtriamo e carichiamo i file per ciascun identificatore\n",
    "    for identifier in identifiers:\n",
    "        for file in files:\n",
    "            if file.endswith(f\"{identifier}.pkl\"):  # Controlliamo se il file termina con l'identificatore\n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        models_dict[identifier][file] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "    \n",
    "    # Salviamo il dizionario della path corrente nel dizionario principale\n",
    "    all_models_dict[condition] = models_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4e091-3fb6-44b4-b20e-94ee5f2fd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora all_models_dict contiene i dati strutturati per ogni path e identificatore\n",
    "# Stampa i tipi di ogni sotto-dizionario\n",
    "for path_key, identifier_dict in all_models_dict.items():\n",
    "    print(f\"Path: {path_key} - Tipo: {type(identifier_dict)}\")\n",
    "    for identifier, model_dict in identifier_dict.items():\n",
    "        print(f\"  Identifier: {identifier} - Tipo: {type(model_dict)}\")\n",
    "        for model, data in model_dict.items():\n",
    "            print(f\"    Model: {model} - Tipo: {type(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfbf5e-696d-407e-8cba-2b5b56a752ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_dict.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79ac5507-cc13-40d4-86bf-9180652ce5c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#METTENDO TUTTE LE FEATURE WAVELET ASSIEME NELLA STESSA TABLE\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Definiamo le path\n",
    "paths = {\n",
    "    \"TH_FAM\": \"/home/stefano/Interrogait/Model_Results/TH_FAM_UNSCALED/\",\n",
    "    \"PT_FAM\": \"/home/stefano/Interrogait/Model_Results/PT_FAM_UNSCALED/\",\n",
    "    \"TH_UNFAM\": \"/home/stefano/Interrogait/Model_Results/TH_UNFAM_UNSCALED/\",\n",
    "    \"PT_UNFAM\": \"/home/stefano/Interrogait/Model_Results/PT_UNFAM_UNSCALED/\"\n",
    "}\n",
    "\n",
    "\n",
    "# Identificatori delle triplette\n",
    "identifiers = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Dizionario per salvare i risultati\n",
    "all_models_dict = {}\n",
    "\n",
    "# Iteriamo su ogni path\n",
    "for condition, path in paths.items():\n",
    "    models_dict = {identifier: {} for identifier in identifiers}  # Dizionario per i modelli della path corrente\n",
    "    \n",
    "    # Controlliamo che la directory esista\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory non trovata: {path}\")\n",
    "        continue\n",
    "    \n",
    "    # Otteniamo la lista di file nella directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filtriamo e carichiamo i file per ciascun identificatore\n",
    "    for identifier in identifiers:\n",
    "        for file in files:\n",
    "            if file.endswith(f\"{identifier}.pkl\"):  # Controlliamo se il file termina con l'identificatore\n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        models_dict[identifier][file] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "    \n",
    "    # Salviamo il dizionario della path corrente nel dizionario principale\n",
    "    all_models_dict[condition] = models_dict\n",
    "\n",
    "    \n",
    "# Definizione delle metriche in ordine\n",
    "metrics = [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]\n",
    "\n",
    "# Creiamo una struttura dati iniziale\n",
    "df_data = {\"Metriche\": metrics}\n",
    "\n",
    "# Iteriamo su ogni condizione in all_models_dict\n",
    "for condition, models_dict in all_models_dict.items():\n",
    "    print(f\"\\nProcessing condition: {condition}\\n\")\n",
    "\n",
    "    # Iteriamo su ogni identificatore dentro il dizionario di modelli\n",
    "    for identifier, models in models_dict.items():\n",
    "        print(f\"  Processing identifier: {identifier}\")\n",
    "\n",
    "        # Iteriamo su ogni modello per questa combinazione condition-identifier\n",
    "        for model_name, model_data in models.items():\n",
    "            \n",
    "            name_model = model_name.split(\"_\")[0]  # Prende solo la parte prima del primo '_' \n",
    "            \n",
    "            print(f\"    Processing model: {name_model}\")\n",
    "\n",
    "            # Supponiamo che 'model_data' contenga i risultati del modello\n",
    "            try:\n",
    "                # Puoi accedere ai risultati di training e testing (se esistono)\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "                \n",
    "                 # Converte i valori da liste a float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training EEG {identifier})\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing EEG {identifier})\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "                \n",
    "            # Creazione del DataFrame finale\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            # Creazione del DataFrame finale\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            # Crea una figura per il plot\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))  # Imposta le dimensioni della figura\n",
    "\n",
    "            # Nasconde l'asse per non visualizzare i numeri\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Rimuovi l'indice dal DataFrame\n",
    "            df_without_index = df_performances.reset_index(drop=True)\n",
    "\n",
    "            # Usa pandas per creare una tabella nel grafico\n",
    "            tabla = table(ax, df_without_index, loc='center', colWidths=[0.2]*len(df_performances.columns))\n",
    "\n",
    "            # Personalizza l'aspetto della tabella\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(10)\n",
    "            tabla.scale(2, 2)  # Scala per una migliore visibilità\n",
    "\n",
    "            # Metti i nomi delle colonne in grassetto\n",
    "            for key, cell in tabla.get_celld().items():\n",
    "                if key[0] == 0:  # Se la riga è la prima (intestazioni delle colonne)\n",
    "                    cell.set_text_props(weight='bold')  # Metti in grassetto\n",
    "\n",
    "            # Aumenta la larghezza della colonna se c'è uno strabordamento\n",
    "            tabla.auto_set_column_width([0, 1, 2, 3, 4, 5])  # Modifica questa lista in base al numero di colonne\n",
    "\n",
    "            # Rimuovi la colonna dell'indice (indice di riga)\n",
    "            tabla.get_celld().pop((0, 0))  # Rimuove l'indice dalla tabella\n",
    "\n",
    "            # Creazione della directory se non esiste\n",
    "            output_dir = paths[condition]  # Usa la path dinamica corrispondente alla condition\n",
    "\n",
    "            file_name = f\"{condition}_{identifier}_models.png\"\n",
    "\n",
    "            img_file_path = os.path.join(output_dir, file_name)  # Nome della path del \n",
    "\n",
    "            # Salva la figura\n",
    "            fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "            print(f\"Tabella salvata in: {img_file_path}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89226e-44d1-4377-bd7d-75e2306dae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "# Definiamo le path\n",
    "paths = {\n",
    "    \"TH_FAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/TH_FAM_UNSCALED/\",\n",
    "    \"PT_FAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/PT_FAM_UNSCALED/\",\n",
    "    \"TH_UNFAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/TH_UNFAM_UNSCALED/\",\n",
    "    \"PT_UNFAM_UNSCALED\": \"/home/stefano/Interrogait/Model_Results/PT_UNFAM_UNSCALED/\"\n",
    "}\n",
    "\n",
    "# Identificatori delle triplette\n",
    "identifiers = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Iteriamo su ogni path\n",
    "for condition, path in paths.items():\n",
    "    \n",
    "    # Dizionario per i modelli della path corrente\n",
    "    models_dict = {identifier: {} for identifier in identifiers}\n",
    "    \n",
    "    # Controlliamo che la directory esista\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Directory non trovata: {path}\")\n",
    "        continue\n",
    "    \n",
    "    # Otteniamo la lista di file nella directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    # Filtriamo e carichiamo i file per ciascun identificatore\n",
    "    for identifier in identifiers:\n",
    "        for file in files:\n",
    "            if file.endswith(f\"{identifier}.pkl\"):  # Controlliamo se il file termina con l'identificatore\n",
    "                file_path = os.path.join(path, file)\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        models_dict[identifier][file] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "    # Ora creiamo un file separato per ogni identificatore\n",
    "    for identifier in identifiers:\n",
    "        df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "        \n",
    "        print(f\"\\nProcessing condition: {condition}, identifier: {identifier}\\n\")\n",
    "\n",
    "        # Iteriamo sui modelli relativi a questo identificatore\n",
    "        for model_name, model_data in models_dict[identifier].items():\n",
    "            name_model = model_name.split(\"_\")[0]  # Prende solo la parte prima del primo '_'\n",
    "            print(f\"    Processing model: {name_model}\")\n",
    "\n",
    "            try:\n",
    "                # Recupera i risultati di training e testing\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "\n",
    "                # Converti i valori in float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training)\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing)\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "        # Creazione del DataFrame per l'identificatore specifico\n",
    "        df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "        # Crea un'immagine della tabella\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Usa pandas per creare una tabella nel grafico\n",
    "        tabla = table(ax, df_performances, loc='center', colWidths=[0.2]*len(df_performances.columns))\n",
    "\n",
    "        # Personalizza la tabella\n",
    "        tabla.auto_set_font_size(True)\n",
    "        tabla.set_fontsize(10)\n",
    "        tabla.scale(2, 2)\n",
    "\n",
    "        # Evidenzia i nomi delle colonne\n",
    "        for key, cell in tabla.get_celld().items():\n",
    "            if key[0] == 0:  # Se la riga è la prima (intestazioni delle colonne)\n",
    "                cell.set_text_props(weight='bold')  # Grassetto\n",
    "\n",
    "        # Creazione della directory se non esiste\n",
    "        output_dir = paths[condition]\n",
    "        file_name = f\"{condition}_{identifier}_models.png\"\n",
    "        img_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # Salva l'immagine della tabella\n",
    "        fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)  # Chiudi la figura per liberare memoria\n",
    "\n",
    "        print(f\"Tabella salvata in: {img_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6a695-c1b8-4790-bca4-9ed6cf110dde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Integrazioni Performance Training e Test del Modello dentro DataFrame - NEW APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca7e2a-35e8-4b6b-8646-cb9e87faa38e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Spiegazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1db04-ae2e-4938-ab62-6da5fe44950b",
   "metadata": {},
   "source": [
    "Ok in questo modo, model_standardization_dict dovrebbe andare a salvarsi se, i dati per quella combinazione di fattori, rispetto ad uno specifico modello, siano stati standardizzati o meno.\n",
    "\n",
    "Di conseguenza, dentro questo loop\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pandas.plotting import table\n",
    "\n",
    "    # Base folder\n",
    "    base_folder = \"/home/stefano/Interrogait/time_domain_best_models_post_WB\"\n",
    "\n",
    "    # Condizioni sperimentali\n",
    "    experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "    # Tipologie di dati\n",
    "    data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "    # Subfolders per tipologia di soggetto\n",
    "    subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "    # Dizionario per salvare tutti i modelli\n",
    "    all_models = {}\n",
    "\n",
    "    # Caricamento dei modelli\n",
    "    for condition in experimental_conditions:\n",
    "        for data_type in data_types:\n",
    "            for subfolder in subfolders:\n",
    "\n",
    "                path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "\n",
    "                if not os.path.exists(path):\n",
    "                    print(f\"Directory non trovata: {path}\")\n",
    "                    continue\n",
    "\n",
    "                # Creiamo la chiave per questa combinazione\n",
    "                key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "                all_models[key] = {}\n",
    "\n",
    "                # Otteniamo la lista di file nella directory\n",
    "                files = os.listdir(path)\n",
    "\n",
    "                # Filtriamo e carichiamo i file .pkl\n",
    "                for file in files:\n",
    "                    if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                        file_path = os.path.join(path, file)\n",
    "                        try:\n",
    "                            with open(file_path, \"rb\") as f:\n",
    "                                all_models[key][file] = pickle.load(f)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "    # Creazione delle tabelle di performance\n",
    "    for key, models_dict in all_models.items():\n",
    "\n",
    "        # Otteniamo le informazioni dalla chiave\n",
    "        #condition, data_type, subfolder = key.split(\"_\", 2)\n",
    "        condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "\n",
    "        print(f\"\\nProcessing: \\033[1m{condition}\\033[0m - \\033[1m{data_type}\\033[0m - \\033[1m{subfolder}\\033[0m\\n\")\n",
    "\n",
    "        # Creazione della tabella\n",
    "        df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "        # Iteriamo sui modelli caricati\n",
    "        for model_name, model_data in models_dict.items():\n",
    "            name_model = model_name.split(\"_\")[0]  # Nome modello\n",
    "            print(f\"    Processing model: \\033[1m{name_model}\\033[0m\")\n",
    "\n",
    "            try:\n",
    "                # Recupera i risultati di training e testing\n",
    "                train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "\n",
    "                # Converti i valori in float\n",
    "                train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "\n",
    "\n",
    "                # Aggiungi le metriche di training\n",
    "                df_data[f\"{name_model} (Training)\"] = [\n",
    "                    train_scores[\"train_accuracy\"],\n",
    "                    train_scores[\"train_loss\"],\n",
    "                    train_scores[\"train_precision\"],\n",
    "                    train_scores[\"train_recall\"],\n",
    "                    train_scores[\"train_f1_score\"],\n",
    "                    train_scores[\"train_auc\"],\n",
    "                ]\n",
    "\n",
    "                # Aggiungi le metriche di test\n",
    "                df_data[f\"{name_model} (Testing)\"] = [\n",
    "                    test_scores[\"test_accuracy\"],\n",
    "                    test_scores[\"test_loss\"],\n",
    "                    test_scores[\"test_precision\"],\n",
    "                    test_scores[\"test_recall\"],\n",
    "                    test_scores[\"test_f1_score\"],\n",
    "                    test_scores[\"test_auc\"],\n",
    "                ]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "        # Creazione del DataFrame\n",
    "        #df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "        # Crea un'immagine della tabella\n",
    "        #fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        #ax.axis('off')\n",
    "        #tabla = table(ax, df_performances, loc='center', colWidths=[0.2] * len(df_performances.columns))\n",
    "        #tabla.auto_set_font_size(True)\n",
    "        #tabla.set_fontsize(10)\n",
    "        #tabla.scale(2, 2)\n",
    "\n",
    "        # Evidenzia i nomi delle colonne\n",
    "        #for key, cell in tabla.get_celld().items():\n",
    "        #    if key[0] == 0:\n",
    "        #        cell.set_text_props(weight='bold')\n",
    "\n",
    "        # Salva l'immagine della tabella\n",
    "        path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "        file_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "        img_file_path = os.path.join(path, file_name)\n",
    "        #fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "        #plt.close(fig)\n",
    "\n",
    "        print(f\"\\nTabella dei dati di \\033[1m{key}\\033[0m salvati in: \\n\\033[1m{img_file_path}\\033[0m\")\n",
    "\n",
    "\n",
    "vorrei provare ad iterare con \"zip\", sia all_models che su model_standardization_dict ...? (che forse dovrebbero avere la stessa struttura, che renderebbe possibile questa cosa...?)\n",
    "\n",
    "E, nel momento in cui si aggiungono le metriche del training e test del relativo modello, controllare rispetto a model_standardization_dict (di cui si ha la chiave per accedere all' informazione su se quel modello, per quella combinazioni di fattori che compongono quel dato) se il dato sia stato standardizzato... \n",
    "\n",
    "Se questo è VERO, allora nella colonna del dataframe che si riferisce al modello... vorrei che ci mettessi accanto, alla stringa che si riferisce al nome del modello (name_model) un asterisco, SOLO SE, per quel modello, allenato con quella combinazioni di fattori che compongono quel dato, i dati siano stati standardizzati...\n",
    "\n",
    "chiaro?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c418b-9136-4c49-8fd4-9332cfd7220c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Implementazione "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271895bc-bc75-421e-8494-83d44c7046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "path = '/home/stefano/Interrogait/'\n",
    "\n",
    "with open(f\"{path}spectrograms_EEG_channels_freqs_params_GradCAM_Checks.pkl\", \"rb\") as f:\n",
    "    models_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2d9f3-230c-41d6-b04e-edabb7086f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In questo codice:\n",
    "\n",
    "model_info.get('standardization', False) cerca la chiave 'standardization' all'interno di ogni sottodizionario. \n",
    "Se non esiste, restituirà False come valore di default.\n",
    "Se standardization è True, stampa la chiave associata.\n",
    "'''\n",
    "\n",
    "# Ciclo attraverso le chiavi di 'models_info'\n",
    "for key, model_info in models_info.items():\n",
    "    # Controllo se 'standardization' è True\n",
    "    if model_info.get('standardization', False):  # Default a False nel caso in cui non esista la chiave\n",
    "        print(key)  # Stampa la chiave\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eba6e2-1402-47e2-8581-07a16c75b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb34e11-33b7-4c8a-9fdf-28d1a6791733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, model_info in all_models.items():\n",
    "#    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c91b0-26a4-4235-81b0-de832ccd1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Siccome la stringa associata alla category subject è diversa tra i due.. \n",
    "\n",
    "familiar_th  familiar_pt unfamiliar_pt unfamiliar_pt  da un lato (models_info)\n",
    "th_fam, pt_fam, th_unfam, pt_unfam  dall'altro (all_models)\n",
    "\n",
    "la corrispondenza non avverrà mai... per cui, si deve fare il mapping corrispondente tra \n",
    "le stringhe di uno e dell'altro, in modo che models_info cambi come parte della stringa della sua chiave da queste \n",
    "\n",
    "familiar_th  familiar_pt unfamiliar_pt unfamiliar_pt\n",
    "a queste\n",
    "th_fam, pt_fam, th_unfam, pt_unfam \n",
    "\n",
    "'''\n",
    "\n",
    "mapping_subject = {\n",
    "    \"familiar_th\": \"th_fam\",\n",
    "    \"familiar_pt\": \"pt_fam\",\n",
    "    \"unfamiliar_th\": \"th_unfam\",\n",
    "    \"unfamiliar_pt\": \"pt_unfam\"\n",
    "}\n",
    "\n",
    "# Creiamo un nuovo dizionario con le chiavi corrette\n",
    "updated_models_info = {}\n",
    "\n",
    "for key, value in models_info.items():\n",
    "    for old_suffix, new_suffix in mapping_subject.items():\n",
    "        if key.endswith(old_suffix):\n",
    "            new_key = key.replace(old_suffix, new_suffix)\n",
    "            updated_models_info[new_key] = value\n",
    "            break  # Evita sostituzioni multiple se una è già stata fatta\n",
    "    else:\n",
    "        # Se nessuna sostituzione è stata fatta, mantieni la chiave originale\n",
    "        updated_models_info[key] = value\n",
    "\n",
    "# Sostituisci il vecchio dizionario con quello aggiornato\n",
    "models_info = updated_models_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a3dc5-4f68-4944-80b4-a369be42e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3c37f-67a4-4db4-9933-4559930f79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Ciclo attraverso le chiavi di 'models_info' AGGIORNATO!'''\n",
    "\n",
    "for key, model_info in models_info.items():\n",
    "    # Controllo se 'standardization' è True\n",
    "    if model_info.get('standardization', False):  # Default a False nel caso in cui non esista la chiave\n",
    "        print(key)  # Stampa la chiavi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3c040-e212-4e2a-9966-e2e525e4c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parsing della chiave e costruzione del path:\n",
    "Usando la funzione parse_combination_key si estraggono \n",
    "\n",
    "exp_cond, data_type e category_subject dalla chiave del dataset. \n",
    "\n",
    "Questi vengono usati per costruire il percorso in cui cercare i file .pkl.\n",
    "'''\n",
    "\n",
    "# Funzione per parsare la chiave\n",
    "def parse_combination_models_keys(combination_key):\n",
    "    \"\"\"\n",
    "    Estrae (exp_cond, data_type, category_subject) da combination_key.\n",
    "    \n",
    "    Il formato atteso PRIMA è:\n",
    "    \n",
    "    \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ spectrograms\" _ \"familiar_th|familiar_pt|unfamiliar_th|unfamiliar_pt\"\n",
    "    \n",
    "    Il formato atteso ORA è:\n",
    "    \n",
    "     \"th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp\" _ spectrograms\" _ \"th_fam|th_unfam|pt_fam|pt_unfam\"\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    match = re.match(\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\", \n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()  # (exp_cond, data_type, category_subject)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "        \n",
    "    return exp_cond, data_type, category_subject"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76e8a17c-8c12-45f3-9b30-1df003b2df82",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''CODICE DI PROVA PER POPOLAMENTO CORRETTO DEL DIZIONARIO ALL_MODELS'''\n",
    "\n",
    "# Dizionario per salvare tutti i modelli\n",
    "all_models = {}\n",
    "\n",
    "# Iteriamo su ogni combinazione\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Creiamo la chiave per questa combinazione\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            # Otteniamo la lista di file nella directory\n",
    "            files = os.listdir(path)\n",
    "            \n",
    "            # Filtriamo e carichiamo i file .pkl\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "# Controlliamo le chiavi principali del dizionario\n",
    "print(all_models.keys())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2585d1de-6daa-48eb-9f3b-e1d793b84cb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "\n",
    "OLD APPROACH\n",
    "\n",
    "Adesso replichiamo l'approccio usato prima, ma stavolta integrado tutte le combinazioni di dati. \n",
    "Andiamo a\n",
    "\n",
    "1) iterare sulla struttura delle directory a partire da base_folder, \n",
    "2) caricare i modelli .pkl per ogni combinazione di fattori che compongono i dati\n",
    "3) creare un DataFrame che raccolga le metriche di tutti i modelli relativi alla stessa combinazione di dati. \n",
    "\n",
    "Infine, salviamo questa tabella come immagine all'interno della cartella corrispondente\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "# Base folder\n",
    "base_folder = \"/home/stefano/Interrogait/time_domain_best_models_post_WB\"\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Tipologie di dati\n",
    "data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "\n",
    "# Subfolders per tipologia di soggetto\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "# Iteriamo su ogni combinazione\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Dizionario per i modelli della path corrente\n",
    "            models_dict = {}\n",
    "            \n",
    "            # Otteniamo la lista di file nella directory\n",
    "            files = os.listdir(path)\n",
    "            \n",
    "            # Filtriamo e carichiamo i file .pkl\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            models_dict[file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "                        \n",
    "            '''\n",
    "            # Creazione della tabella\n",
    "            df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "            \n",
    "            print(f\"\\nProcessing: {condition} - {data_type} - {subfolder}\\n\")\n",
    "            \n",
    "            # Iteriamo sui modelli\n",
    "            for model_name, model_data in models_dict.items():\n",
    "                name_model = model_name.split(\"_\")[0]  # Nome modello\n",
    "                print(f\"    Processing model: {name_model}\")\n",
    "                \n",
    "                try:\n",
    "                    # Recupera i risultati di training e testing\n",
    "                    train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "                    test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "                    \n",
    "                    # Converti i valori in float\n",
    "                    train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "                    test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "                    \n",
    "                    # Aggiungi le metriche di training\n",
    "                    df_data[f\"{name_model} (Training)\"] = [\n",
    "                        train_scores.get(\"train_accuracy\", float('nan')),\n",
    "                        train_scores.get(\"train_loss\", float('nan')),\n",
    "                        train_scores.get(\"train_precision\", float('nan')),\n",
    "                        train_scores.get(\"train_recall\", float('nan')),\n",
    "                        train_scores.get(\"train_f1_score\", float('nan')),\n",
    "                        train_scores.get(\"train_auc\", float('nan')),\n",
    "                    ]\n",
    "                    \n",
    "                    # Aggiungi le metriche di test\n",
    "                    df_data[f\"{name_model} (Testing)\"] = [\n",
    "                        test_scores.get(\"test_accuracy\", float('nan')),\n",
    "                        test_scores.get(\"test_loss\", float('nan')),\n",
    "                        test_scores.get(\"test_precision\", float('nan')),\n",
    "                        test_scores.get(\"test_recall\", float('nan')),\n",
    "                        test_scores.get(\"test_f1_score\", float('nan')),\n",
    "                        test_scores.get(\"test_auc\", float('nan')),\n",
    "                    ]\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "            \n",
    "            # Creazione del DataFrame\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "            \n",
    "            # Crea un'immagine della tabella\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.axis('off')\n",
    "            tabla = table(ax, df_performances, loc='center', colWidths=[0.2]*len(df_performances.columns))\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(10)\n",
    "            tabla.scale(2, 2)\n",
    "            \n",
    "            # Evidenzia i nomi delle colonne\n",
    "            for key, cell in tabla.get_celld().items():\n",
    "                if key[0] == 0:\n",
    "                    cell.set_text_props(weight='bold')\n",
    "            \n",
    "            # Salva l'immagine della tabella\n",
    "            file_name = f\"{condition}_{data_type}_{subfolder}_models.png\"\n",
    "            img_file_path = os.path.join(path, file_name)\n",
    "            fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            print(f\"Tabella salvata in: {img_file_path}\")\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05689030-0755-437c-9420-65fe2bc8fbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NEW APPROACH \n",
    "\n",
    "Adesso replichiamo l'approccio usato prima, ma stavolta integrado tutte le combinazioni di dati. \n",
    "Andiamo a\n",
    "\n",
    "1) iterare sulla struttura delle directory a partire da base_folder, \n",
    "2) caricare i modelli .pkl per ogni combinazione di fattori che compongono i dati\n",
    "3) creare un DataFrame che raccolga le metriche di tutti i modelli relativi alla stessa combinazione di dati. \n",
    "\n",
    "Infine, salviamo questa tabella come immagine all'interno della cartella corrispondente\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "\n",
    "# Base folder\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks\"\n",
    "\n",
    "\n",
    "# Condizioni sperimentali\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "\n",
    "# Tipologie di dati\n",
    "data_types = [\"spectrograms\"]\n",
    "\n",
    "# Subfolders per tipologia di soggetto\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "# Dizionario per salvare tutti i modelli\n",
    "all_models = {}\n",
    "\n",
    "# Caricamento dei modelli\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            \n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "            \n",
    "            # Creiamo la chiave per questa combinazione\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            # Otteniamo la lista di file nella directory\n",
    "            files = os.listdir(path)\n",
    "            \n",
    "            # Filtriamo e carichiamo i file .pkl\n",
    "            for file in files:\n",
    "                if file.endswith(\".pkl\"):  # Controlliamo se è un file modello\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "# Creazione delle tabelle di performance\n",
    "for key, models_dict in all_models.items():\n",
    "    \n",
    "    # Otteniamo le informazioni dalla chiave\n",
    "    condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "    \n",
    "    print(f\"\\nProcessing: \\033[1m{condition}\\033[0m - \\033[1m{data_type}\\033[0m - \\033[1m{subfolder}\\033[0m\\n\")\n",
    "    \n",
    "    # Creazione della tabella\n",
    "    df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "    # Iteriamo sui modelli caricati\n",
    "    for model_name, model_data in models_dict.items():\n",
    "        \n",
    "        # Estrai il nome del modello dal file (ad esempio, \"CNN1D\" da \"CNN1D_performances_...pkl\")\n",
    "        name_model = model_name.split(\"_\")[0]\n",
    "        \n",
    "        print(f\"    Processing model: \\033[1m{name_model}\\033[0m\")\n",
    "        \n",
    "        # Costruisci la chiave utilizzata nel dizionario models_info\n",
    "        \n",
    "        '''\n",
    "        Nota: occorrerà che il formato della chiave sia consistente tra i due loop.\n",
    "        \n",
    "        Ad esempio, se nel primo loop era f\"{key}_{model_name}\", qui potresti dover fare:\n",
    "        model_key = f\"{key}_{name_model}\"\n",
    "        \n",
    "        Oppure, se nel primo loop era f\"{model_name}_{key}\", qui potresti dover fare:\n",
    "        model_key = f\"{name_model}_{key}\"\n",
    "        \n",
    "        '''\n",
    "        model_key = f\"{name_model}_{key}\"\n",
    "        \n",
    "        # Controlla se i dati sono stati standardizzati per questo modello\n",
    "        standardization_flag = models_info.get(model_key, {}).get(\"standardization\", False)\n",
    "        \n",
    "        if standardization_flag:\n",
    "            suffix = \"*\" \n",
    "        else:\n",
    "            suffix = \"\" \n",
    "        \n",
    "        try:\n",
    "            # Recupera i risultati di training e testing\n",
    "            train_scores = model_data.get('my_train_results', {}).get('training_performances', {})\n",
    "            test_scores = model_data.get('my_test_results', {}).get('test_performances', {})\n",
    "            \n",
    "            # Converti i valori in float\n",
    "            train_scores = {key: float(value[0]) for key, value in train_scores.items()}\n",
    "            test_scores = {key: float(value[0]) for key, value in test_scores.items()}\n",
    "            \n",
    "            \n",
    "            # Aggiunge le metriche di training, modificando il nome della colonna se è vera la condizione\n",
    "            col_train = f\"{name_model} (Training){suffix}\"  # Usa suffix qui per il nome\n",
    "            \n",
    "            df_data[f\"{col_train}\"] = [\n",
    "                train_scores[\"train_accuracy\"],\n",
    "                train_scores[\"train_loss\"],\n",
    "                train_scores[\"train_precision\"],\n",
    "                train_scores[\"train_recall\"],\n",
    "                train_scores[\"train_f1_score\"],\n",
    "                train_scores[\"train_auc\"],\n",
    "            ]\n",
    "\n",
    "            # Aggiunge le metriche di training, modificando il nome della colonna se è vera la condizione\n",
    "            col_test = f\"{name_model} (Test){suffix}\"  # Usa suffix qui per il nome\n",
    "            \n",
    "            df_data[f\"{col_test}\"] = [\n",
    "                test_scores[\"test_accuracy\"],\n",
    "                test_scores[\"test_loss\"],\n",
    "                test_scores[\"test_precision\"],\n",
    "                test_scores[\"test_recall\"],\n",
    "                test_scores[\"test_f1_score\"],\n",
    "                test_scores[\"test_auc\"],\n",
    "            ]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Errore nell'elaborazione di {model_name}: {e}\")\n",
    "\n",
    "    # Creazione del DataFrame\n",
    "    df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "    # Crea un'immagine della tabella\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Aggiunta del titolo\n",
    "    title = f\"DL Models performances for Exp Conditions: {condition}, EEG data: {data_type}, Subject: {subfolder}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    tabla = table(ax, df_performances, loc='center', colWidths=[0.2] * len(df_performances.columns))\n",
    "    tabla.auto_set_font_size(True)\n",
    "    tabla.set_fontsize(10)\n",
    "    tabla.scale(2, 2)\n",
    "\n",
    "    # Evidenzia i nomi delle colonne\n",
    "    for key, cell in tabla.get_celld().items():\n",
    "        if key[0] == 0:\n",
    "            cell.set_text_props(weight='bold')\n",
    "\n",
    "    # Salva l'immagine della tabella\n",
    "    path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "    file_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "    img_file_path = os.path.join(path, file_name)\n",
    "    fig.savefig(img_file_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"\\nTabella dei dati di \\033[1m{condition}_{data_type}_{subfolder}\\033[0m salvati in: \\n\\033[1m{img_file_path}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704d304-7c4b-4519-82ed-81299593294a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Integrazioni in Tabelle AGGREGATE delle Performance Training e Test del Modello dentro DataFrame - NEW APPROACH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67d531-7424-4c71-a0e7-c541bb3a73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perfetto ora va. ma io vorrei anche rendere le tabelle ancora più informative.. ossia\n",
    "\n",
    "vorrei ricreare lo stesso codice ma questa volta anziché avere una tabella specifica SOLO\n",
    "per un certo tipo di condizione sperimentale, tipo di dato e soggetto...\n",
    "\n",
    "io vorrei provare quanto meno ad 'allargare' le tabelle, nel senso di mettere nella stessa tabella\n",
    "la stessa condizione sperimentale e tipo di dato, per tutti e 3 i modelli, \n",
    "\n",
    "ma confrontando però la performance dello STESSO MODELLO per gli STESSI TIPI DI CONDIZIONE SPERIMENTALE, TIPO DI DATO e TIPI DI SOGGETTI (ossia RUOLO nel task)\n",
    "... ossia ad esempio\n",
    "\n",
    "\n",
    "A) Ossia.. quindi, farei prima i RUOLI di th_fam e th_unfam ...ossia\n",
    "\n",
    "per 'th_resp_vs_pt_resp' (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_45\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per th_fam \n",
    "2) sia per th_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp', (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_20\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per th_fam \n",
    "2) sia per th_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp', (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati delta_wavelet\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per th_fam \n",
    "2) sia per th_unfam.. \n",
    "\n",
    "in modo da avere un confronto diretto visivo per la stessa condizione sperimentale, stesso tipo di feature dei dati EEG usata, \n",
    "rispetto allo stesso modello, ma confrontando però la performance tra i due soggetti che hanno fatto lo STESSO RUOLO nei 2 gruppi (controllo e sperimentale).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "B) Allo stesso modo.. quindi, farei la STESSA COSA anche per i RUOLI di pt_fam e pt_unfam...ossia\n",
    "\n",
    " \n",
    "per 'th_resp_vs_pt_resp' (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_45\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per pt_fam \n",
    "2) sia per pt_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp', (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!), per i dati 1_20\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test'\n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per pt_fam \n",
    "2) sia per pt_unfam.. \n",
    "\n",
    "\n",
    "\n",
    "poi per 'th_resp_vs_pt_resp' (e così come poi per 'th_resp_vs_shared_resp' e 'pt_resp_vs_shared_resp'!!!!),, per i dati delta_wavelet\n",
    "\n",
    "io vorrei che nella stessa tabella, ci fossero le performance di tutti e 3 i modelli sia in fase di training che di test' \n",
    "(CNN1D, poi BiLSTM ed infine per Transformer...) ma\n",
    "\n",
    "1) sia per pt_fam \n",
    "2) sia per pt_unfam.. \n",
    "  \n",
    "magari, nella prima riga metto le performance di training e test dei modelli che son con \"_fam\" \n",
    "e invece sotto le stesse performance dello stesso modello, condizione e tipo di dato, per chi è \"_unfam\", \n",
    "\n",
    "\n",
    "in modo da distinguire in base alla riga quali sono le performance di uno rispetto a quelle dell'altro soggetto, \n",
    "che avrà svolto lo stesso ruolo ma nel gruppo o di controllo o sperimentale...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa805e-eb19-4679-91f1-54bd93be595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Yes! idea chiarissima. Senza stravolgere il tuo codice, \n",
    "aggiungi un secondo pass che costruisce (e salva) le tabelle aggregate per ruolo per ogni (condizione, data_type). \n",
    "\n",
    "Le colonne restano i 3 modelli × (Training/Test), le righe diventano le metriche replicate per i due soggetti del ruolo (fam / unfam). \n",
    "\n",
    "Il simbolo * per la standardizzazione lo mettiamo dentro la cella (così può cambiare tra fam e unfam).\n",
    "\n",
    "Incolla questo blocco dopo aver popolato all_models (puoi tenere anche le tabelle “singole” che già fai):\n",
    "\n",
    "\n",
    "\n",
    "# ===== TABELLE AGGREGATE PER RUOLO (th_fam vs th_unfam e pt_fam vs pt_unfam) =====\n",
    "\n",
    "MODEL_ORDER = [\"CNN1D\", \"BiLSTM\", \"Transformer\"]\n",
    "METRICS = [\n",
    "    (\"Accuracy\",  \"train_accuracy\", \"test_accuracy\"),\n",
    "    (\"Loss\",      \"train_loss\",     \"test_loss\"),\n",
    "    (\"Precision\", \"train_precision\",\"test_precision\"),\n",
    "    (\"Recall\",    \"train_recall\",   \"test_recall\"),\n",
    "    (\"F1-Score\",  \"train_f1_score\", \"test_f1_score\"),\n",
    "    (\"AUC-ROC\",   \"train_auc\",      \"test_auc\"),\n",
    "]\n",
    "\n",
    "def find_model_blob(all_models, condition, data_type, subfolder, model_prefix):\n",
    "    \"\"\"Ritorna il dict salvato a disco per il modello richiesto (o None).\"\"\"\n",
    "    key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "    if key not in all_models:\n",
    "        return None\n",
    "    for fname, blob in all_models[key].items():\n",
    "        # match robusto: inizia con \"<MODEL>_\"\n",
    "        if fname.startswith(model_prefix + \"_\"):\n",
    "            return blob\n",
    "    return None\n",
    "\n",
    "def fmt(v, star=False):\n",
    "    try:\n",
    "        val = float(v)\n",
    "        s = f\"{val:.3f}\"\n",
    "    except Exception:\n",
    "        s = \"-\"\n",
    "    return s + (\"*\" if star else \"\")\n",
    "\n",
    "# Gruppi di ruolo\n",
    "ROLE_GROUPS = {\n",
    "    \"THroles\": [\"th_fam\", \"th_unfam\"],\n",
    "    \"PTroles\": [\"pt_fam\", \"pt_unfam\"],\n",
    "}\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for role_label, subs in ROLE_GROUPS.items():\n",
    "\n",
    "            # Costruisci righe: una sezione per sub='..._fam' e una per '..._unfam'\n",
    "            rows = []\n",
    "            # Colonne: 3 modelli × (Training/Test)\n",
    "            columns = [\"Metriche\"]\n",
    "            for m in MODEL_ORDER:\n",
    "                columns.append(f\"{m} (Training)\")\n",
    "                columns.append(f\"{m} (Test)\")\n",
    "\n",
    "            df_data = {c: [] for c in columns}\n",
    "\n",
    "            for subfolder in subs:\n",
    "                # intestazione “visiva” delle righe: preferisci th_fam/th_unfam ecc.\n",
    "                for label, tr_key, te_key in METRICS:\n",
    "                    df_data[\"Metriche\"].append(f\"{subfolder} — {label}\")\n",
    "\n",
    "                    for m in MODEL_ORDER:\n",
    "                        # recupero blob salvato per quel subfolder/modello\n",
    "                        blob = find_model_blob(all_models, condition, data_type, subfolder, m)\n",
    "\n",
    "                        # standardization flag (per cella)\n",
    "                        mi_key = f\"{m}_{condition}_{data_type}_{subfolder}\"\n",
    "                        std_flag = bool(models_info.get(mi_key, {}).get(\"standardization\", False))\n",
    "\n",
    "                        if blob is None:\n",
    "                            # niente file -> celle vuote\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            tr = blob.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "                            te = blob.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "                            # training_performances/test_performances hanno valori come liste [val]\n",
    "                            tr_val = tr.get(tr_key, [None])[0]\n",
    "                            te_val = te.get(te_key, [None])[0]\n",
    "\n",
    "                            df_data[f\"{m} (Training)\"].append(fmt(tr_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Test)\"].append(fmt(te_val, star=std_flag))\n",
    "                        except Exception:\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "\n",
    "            # DataFrame e salvataggio\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            ax.axis('off')\n",
    "\n",
    "            title = f\"DL Models performances — {condition} — EEG: {data_type} — {role_label}\"\n",
    "            ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "            tabla = table(ax, df_performances, loc='center',\n",
    "                          colWidths=[0.25] + [0.12]*(len(df_performances.columns)-1))\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(9)\n",
    "            tabla.scale(1.2, 1.2)\n",
    "\n",
    "            for k, cell in tabla.get_celld().items():\n",
    "                if k[0] == 0:\n",
    "                    cell.set_text_props(weight='bold')\n",
    "\n",
    "            out_dir = os.path.join(base_folder, condition, data_type)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "            out_path = os.path.join(out_dir, out_name)\n",
    "            fig.savefig(out_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Tabella aggregata salvata: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bf9e7-84f4-4206-aa14-0d0d4db5f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perfetto — qui sotto trovi il tuo script “chiavi in mano” con il secondo pass che genera anche le tabelle aggregate per ruolo \n",
    "\n",
    "(THroles = th_fam/th_unfam, PTroles = pt_fam/pt_unfam) per ogni coppia (condizione, data_type).\n",
    "\n",
    "Ho aggiunto:\n",
    "\n",
    "parse_combination_models_keys() con wavelet_delta nel regex.\n",
    "\n",
    "Caricamento “robusto” di models_info (se non esiste, procede senza *).\n",
    "\n",
    "Funzioni di supporto find_model_blob() e fmt().\n",
    "\n",
    "Secondo pass che salva i PNG ..._{condition}_{data_type}_{THroles|PTroles}.png nella cartella di quella coppia.\n",
    "\n",
    "\n",
    "Se vuoi nascondere completamente il vecchio primo pass e tenere solo i comparativi per ruolo, basta commentare/bloccare la sezione “Pass 1”.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Puoi commentare tutto il “Pass 1” senza problemi: il “Pass 2” non dipende da quello.\n",
    "Il “Pass 2” usa solo:\n",
    "\n",
    "all_models (riempito nel blocco di caricamento iniziale, prima del Pass 1),\n",
    "\n",
    "models_info (per mettere l’asterisco * se standardizzato),\n",
    "\n",
    "le funzioni helper (find_model_blob, fmt) e le costanti (MODEL_ORDER, ROLE_GROUPS, ecc.).\n",
    "Quindi, finché lasci il caricamento di all_models e gli helper, funziona da solo.\n",
    "\n",
    "Sì, le tabelle aggregate del Pass 2 vengono salvate esattamente nel percorso costruito da queste righe:\n",
    "\n",
    "out_dir = os.path.join(base_folder, condition, data_type)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "\n",
    "\n",
    "quindi avrai file tipo:\n",
    "\n",
    "/home/stefano/Interrogait/time_domain_1D_best_models_post_WB/th_resp_vs_pt_resp/1_45/models_performances_th_resp_vs_pt_resp_1_45_THroles.png\n",
    "\n",
    "/home/stefano/Interrogait/time_domain_1D_best_models_post_WB/pt_resp_vs_shared_resp/wavelet_delta/models_performances_pt_resp_vs_shared_resp_wavelet_delta_PTroles.png\n",
    "\n",
    "Se preferisci tenerle in una sottocartella tipo aggregated/, cambia così:\n",
    "\n",
    "out_dir = os.path.join(base_folder, condition, data_type, \"aggregated\")\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def parse_combination_models_keys(combination_key: str):\n",
    "    \"\"\"\n",
    "    Ritorna (exp_cond, data_type, category_subject) da chiavi tipo:\n",
    "    th_resp_vs_pt_resp_1_45_th_fam\n",
    "    \"\"\"\n",
    "    match = re.match(\n",
    "        #r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(1_20|1_45|wavelet_delta)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        r\"^(th_resp_vs_pt_resp|pt_resp_vs_shared_resp|th_resp_vs_shared_resp)_(spectrograms)_(th_fam|th_unfam|pt_fam|pt_unfam)$\",\n",
    "        combination_key\n",
    "    )\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        raise ValueError(f\"Formato non valido: {combination_key}\")\n",
    "\n",
    "# Carica models_info (flag standardization per cella). Se non c'è, prosegue senza '*'\n",
    "try:\n",
    "    with open(\"/home/stefano/Interrogait/spectrograms_EEG_channels_freqs_params_GradCAM_Checks.pkl\", \"rb\") as f:\n",
    "        models_info = pickle.load(f)\n",
    "except Exception:\n",
    "    print(\"⚠️  models_info non trovato/caricabile: le tabelle verranno create senza indicatore * di standardizzazione.\")\n",
    "    models_info = {}\n",
    "\n",
    "def find_model_blob(all_models, condition, data_type, subfolder, model_prefix):\n",
    "    \"\"\"Ritorna il dict salvato a disco per il modello richiesto (o None) cercando per filename prefix.\"\"\"\n",
    "    key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "    if key not in all_models:\n",
    "        return None\n",
    "    for fname, blob in all_models[key].items():\n",
    "        if fname.startswith(model_prefix + \"_\"):\n",
    "            return blob\n",
    "    return None\n",
    "\n",
    "def fmt(v, star=False):\n",
    "    \"\"\"Formatta un valore numerico a 3 decimali e aggiunge '*' se standardizzato.\"\"\"\n",
    "    try:\n",
    "        val = float(v)\n",
    "        s = f\"{val:.3f}\"\n",
    "    except Exception:\n",
    "        s = \"-\"\n",
    "    return s + (\"*\" if star else \"\")\n",
    "\n",
    "# ---------- Config ----------\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks\" \n",
    "\n",
    "experimental_conditions = [\"th_resp_vs_pt_resp\", \"th_resp_vs_shared_resp\", \"pt_resp_vs_shared_resp\"]\n",
    "#data_types = [\"1_20\", \"1_45\", \"wavelet_delta\"]\n",
    "data_types = [\"spectrograms\"]\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "#MODEL_ORDER = [\"CNN2D_LSTM_TF\", \"BiLSTM\", \"Transformer\"]\n",
    "\n",
    "MODEL_ORDER = [\"CNN2D\"]\n",
    "\n",
    "METRICS = [\n",
    "    (\"Accuracy\",  \"train_accuracy\", \"test_accuracy\"),\n",
    "    (\"Loss\",      \"train_loss\",     \"test_loss\"),\n",
    "    (\"Precision\", \"train_precision\",\"test_precision\"),\n",
    "    (\"Recall\",    \"train_recall\",   \"test_recall\"),\n",
    "    (\"F1-Score\",  \"train_f1_score\", \"test_f1_score\"),\n",
    "    (\"AUC-ROC\",   \"train_auc\",      \"test_auc\"),\n",
    "]\n",
    "\n",
    "ROLE_GROUPS = {\n",
    "    \"Observer_Role\": [\"th_fam\", \"th_unfam\"],\n",
    "    \"Receiver_Role\": [\"pt_fam\", \"pt_unfam\"],\n",
    "}\n",
    "\n",
    "\n",
    "# --- aggiungi in alto (vicino a ROLE_GROUPS / MODEL_ORDER) ---\n",
    "DISPLAY_LABELS = {\n",
    "    \"th_fam\":   \"observer_fam\",\n",
    "    \"th_unfam\": \"observer_unfam\",\n",
    "    \"pt_fam\":   \"receiver_fam\",\n",
    "    \"pt_unfam\": \"receiver_unfam\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Caricamento modelli ----------\n",
    "all_models = {}\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            for file in os.listdir(path):\n",
    "                if file.endswith(\".pkl\"):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "# ---------- Pass 1: tabella per singola combinazione (come avevi) ----------\n",
    "\n",
    "'''\n",
    "for key, models_dict in all_models.items():\n",
    "    condition, data_type, subfolder = parse_combination_models_keys(key)\n",
    "    print(f\"\\nProcessing: {condition} - {data_type} - {subfolder}\\n\")\n",
    "\n",
    "    df_data = {\"Metriche\": [\"Accuracy\", \"Loss\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC-ROC\"]}\n",
    "\n",
    "    for filename, model_data in models_dict.items():\n",
    "        name_model = filename.split(\"_\")[0]  # CNN1D / BiLSTM / Transformer\n",
    "\n",
    "        # Standardization: usa models_info[colonna] a livello di modello-subfolder\n",
    "        model_key = f\"{name_model}_{key}\"\n",
    "        standardization_flag = bool(models_info.get(model_key, {}).get(\"standardization\", False))\n",
    "        suffix = \"*\" if standardization_flag else \"\"\n",
    "\n",
    "        try:\n",
    "            train_scores = model_data.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "            test_scores  = model_data.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "            # convert list -> float\n",
    "            train_scores = {k: float(v[0]) for k, v in train_scores.items()}\n",
    "            test_scores  = {k: float(v[0]) for k, v in test_scores.items()}\n",
    "\n",
    "            col_train = f\"{name_model} (Training){suffix}\"\n",
    "            col_test  = f\"{name_model} (Test){suffix}\"\n",
    "\n",
    "            df_data[col_train] = [\n",
    "                train_scores.get(\"train_accuracy\", float(\"nan\")),\n",
    "                train_scores.get(\"train_loss\", float(\"nan\")),\n",
    "                train_scores.get(\"train_precision\", float(\"nan\")),\n",
    "                train_scores.get(\"train_recall\", float(\"nan\")),\n",
    "                train_scores.get(\"train_f1_score\", float(\"nan\")),\n",
    "                train_scores.get(\"train_auc\", float(\"nan\")),\n",
    "            ]\n",
    "            df_data[col_test] = [\n",
    "                test_scores.get(\"test_accuracy\", float(\"nan\")),\n",
    "                test_scores.get(\"test_loss\", float(\"nan\")),\n",
    "                test_scores.get(\"test_precision\", float(\"nan\")),\n",
    "                test_scores.get(\"test_recall\", float(\"nan\")),\n",
    "                test_scores.get(\"test_f1_score\", float(\"nan\")),\n",
    "                test_scores.get(\"test_auc\", float(\"nan\")),\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"    Errore nell'elaborazione di {filename}: {e}\")\n",
    "\n",
    "    df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.axis(\"off\")\n",
    "    title = f\"DL Models performances for Exp Conditions: {condition}, EEG data: {data_type}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "    tabla = table(ax, df_performances, loc=\"center\", colWidths=[0.2] * len(df_performances.columns))\n",
    "    tabla.auto_set_font_size(True)\n",
    "    tabla.set_fontsize(10)\n",
    "    tabla.scale(2, 2)\n",
    "\n",
    "    for kcell, cell in tabla.get_celld().items():\n",
    "        if kcell[0] == 0:\n",
    "            cell.set_text_props(weight=\"bold\")\n",
    "\n",
    "    out_dir = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_name = f\"models_performances_{condition}_{data_type}_{subfolder}.png\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Tabella singola salvata: {out_path}\")\n",
    "'''\n",
    "\n",
    "\n",
    "# ---------- Pass 2: tabelle aggregate per ruolo ----------\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for role_label, subs in ROLE_GROUPS.items():\n",
    "\n",
    "            columns = [\"Metriche\"]\n",
    "            for m in MODEL_ORDER:\n",
    "                columns.append(f\"{m} (Training)\")\n",
    "                columns.append(f\"{m} (Test)\")\n",
    "\n",
    "            df_data = {c: [] for c in columns}\n",
    "\n",
    "            for idx_sub, subfolder in enumerate(subs):\n",
    "                \n",
    "                '''CONVERSIONE LABELS DEL RUOLO --> da th_fam a observer_fam etc'''\n",
    "                # solo per la visualizzazione converto th_fam->observer_fam, ecc.\n",
    "                subfolder_disp = DISPLAY_LABELS.get(subfolder, subfolder)\n",
    "                \n",
    "                # per ciascun subfolder (fam / unfam) aggiungo le 6 metriche\n",
    "                for label, tr_key, te_key in METRICS:\n",
    "                    \n",
    "                    #df_data[\"Metriche\"].append(f\"{subfolder} — {label}\")\n",
    "                    \n",
    "                    # usa il suffisso \"display\" SOLO per la label della riga\n",
    "                    df_data[\"Metriche\"].append(f\"{subfolder_disp} — {label}\")\n",
    "                    \n",
    "\n",
    "                    for m in MODEL_ORDER:\n",
    "                        blob = find_model_blob(all_models, condition, data_type, subfolder, m)\n",
    "\n",
    "                        mi_key = f\"{m}_{condition}_{data_type}_{subfolder}\"\n",
    "                        std_flag = bool(models_info.get(mi_key, {}).get(\"standardization\", False))\n",
    "\n",
    "                        if blob is None:\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            tr = blob.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "                            te = blob.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "                            tr_val = tr.get(tr_key, [None])[0]\n",
    "                            te_val = te.get(te_key, [None])[0]\n",
    "\n",
    "                            df_data[f\"{m} (Training)\"].append(fmt(tr_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Test)\"].append(fmt(te_val, star=std_flag))\n",
    "                        except Exception:\n",
    "                            df_data[f\"{m} (Training)\"].append(\"-\")\n",
    "                            df_data[f\"{m} (Test)\"].append(\"-\")\n",
    "\n",
    "                # riga separatrice tra fam e unfam (opzionale ma utile visivamente)\n",
    "                if idx_sub == 0:\n",
    "                    df_data[\"Metriche\"].append(\"\")  # riga vuota\n",
    "                    for m in MODEL_ORDER:\n",
    "                        df_data[f\"{m} (Training)\"].append(\"\")\n",
    "                        df_data[f\"{m} (Test)\"].append(\"\")\n",
    "\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            SHOW_ONLY = False  # <- True per visualizzare, False per salvare\n",
    "            \n",
    "            \n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            title = f\"DL Models performances — {condition} — EEG feature: {data_type} — {role_label}\"\n",
    "            ax.set_title(title, fontsize=12, fontweight=\"bold\", pad=20)\n",
    "\n",
    "            tabla = table(\n",
    "                ax,\n",
    "                df_performances,\n",
    "                loc=\"center\",\n",
    "                colWidths=[0.25] + [0.12] * (len(df_performances.columns) - 1),\n",
    "            )\n",
    "            tabla.auto_set_font_size(True)\n",
    "            tabla.set_fontsize(9)\n",
    "            tabla.scale(1.2, 1.2)\n",
    "\n",
    "            for kcell, cell in tabla.get_celld().items():\n",
    "                if kcell[0] == 0:\n",
    "                    cell.set_text_props(weight=\"bold\")\n",
    "            \n",
    "            \n",
    "            '''Con \"aggregated\", io aggiungo una sotto-cartella ancora alla path di salvataggio delle tabelle'''\n",
    "            \n",
    "            if SHOW_ONLY:\n",
    "                plt.show()\n",
    "                print(f\"Tabella aggregata di: models_performances_{condition}_{data_type}_{role_label}.png\")\n",
    "            else:\n",
    "                \n",
    "                out_dir = os.path.join(base_folder, condition, data_type, \"aggregated\")\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "                out_path = os.path.join(out_dir, out_name)\n",
    "                fig.savefig(out_path, bbox_inches=\"tight\", dpi=300)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"Tabella aggregata salvata: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d15d0-6e1f-4fc1-9a6e-58f96e240b90",
   "metadata": {},
   "source": [
    "#### Implementazione : Versione dal 24 novembre 2025 - Versione Aggregata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ee5c3-3ec2-4568-8da0-e6f83c8071f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''METRICHE PRIMA DI TUTTI I MODELLI SUL TRAIN ... POI SUL VALIDATION ...  E POI SUL TEST SET'''\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def find_model_blob(all_models, condition, data_type, subfolder, model_prefix):\n",
    "    \"\"\"Ritorna il dict salvato a disco per il modello richiesto (o None) cercando per filename prefix.\"\"\"\n",
    "    key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "    if key not in all_models:\n",
    "        return None\n",
    "    for fname, blob in all_models[key].items():\n",
    "        if fname.startswith(model_prefix + \"_\"):\n",
    "            return blob\n",
    "    return None\n",
    "\n",
    "\n",
    "def fmt(v, star=False):\n",
    "    \"\"\"Formatta un valore numerico a 3 decimali e aggiunge '*' se standardizzato (se vuoi).\"\"\"\n",
    "    try:\n",
    "        val = float(v)\n",
    "        s = f\"{val:.3f}\"\n",
    "    except Exception:\n",
    "        s = \"-\"\n",
    "    return s + (\"\" if star else \"\")\n",
    "\n",
    "\n",
    "def format_col_label(label: str) -> str:\n",
    "    \"\"\"\n",
    "    Converte 'CNN1D (Training)' -> 'CNN1D\\n(Training)' per header a due righe.\n",
    "    Lascia 'Metrics' invariato.\n",
    "    \"\"\"\n",
    "    if label == \"Metrics\":\n",
    "        return label\n",
    "    if \"(\" in label and label.endswith(\")\"):\n",
    "        model, phase = label.split(\"(\", 1)\n",
    "        model = model.strip()\n",
    "        phase = \"(\" + phase.strip()\n",
    "        return f\"{model}\\n{phase}\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def pretty_condition_name(cond: str) -> str:\n",
    "    \"\"\"\n",
    "    th_resp_vs_pt_resp        -> 'observer resp vs receiver resp'\n",
    "    th_resp_vs_shared_resp    -> 'observer resp vs shared resp'\n",
    "    pt_resp_vs_shared_resp    -> 'receiver resp vs shared resp'\n",
    "    \"\"\"\n",
    "    token_map = {\n",
    "        \"th_resp\": \"observer resp\",\n",
    "        \"pt_resp\": \"receiver resp\",\n",
    "        \"shared_resp\": \"shared resp\",\n",
    "    }\n",
    "    parts = cond.split(\"_vs_\")\n",
    "    pretty_parts = [token_map.get(p, p.replace(\"_\", \" \")) for p in parts]\n",
    "    return \" vs \".join(pretty_parts)\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "base_folder = \"/home/stefano/Interrogait/spectrograms_best_models_channels_frequencies_params_post_WB_GradCAM_Checks\" \n",
    "\n",
    "experimental_conditions = [\n",
    "    \"th_resp_vs_pt_resp\",\n",
    "    \"th_resp_vs_shared_resp\",\n",
    "    \"pt_resp_vs_shared_resp\",\n",
    "]\n",
    "\n",
    "#data_types = [\"1_20\", \"1_45\", \"wavelet\"]\n",
    "data_types = [\"spectrograms\"]\n",
    "subfolders = [\"th_fam\", \"th_unfam\", \"pt_fam\", \"pt_unfam\"]\n",
    "\n",
    "#MODEL_ORDER = [\"CNN3D_LSTM_FC\", \"SeparableCNN2D_LSTM_FC\"]\n",
    "\n",
    "MODEL_ORDER = [\"CNN3D_LSTM\", \"SeparableCNN2D_LSTM\"]\n",
    "\n",
    "PHASES = [\"Training\", \"Validation\", \"Test\"]   # <--- aggiunto per chiarezza\n",
    "\n",
    "# (label, train_key, val_key, test_key)\n",
    "METRICS = [\n",
    "    (\"Accuracy\",  \"train_accuracy\",  \"val_accuracy\",  \"test_accuracy\"),\n",
    "    (\"Loss\",      \"train_loss\",      \"val_loss\",      \"test_loss\"),\n",
    "    (\"Precision\", \"train_precision\", \"val_precision\", \"test_precision\"),\n",
    "    (\"Recall\",    \"train_recall\",    \"val_recall\",    \"test_recall\"),\n",
    "    (\"F1-Score\",  \"train_f1_score\",  \"val_f1_score\",  \"test_f1_score\"),\n",
    "    (\"AUC-ROC\",   \"train_auc\",       \"val_auc\",       \"test_auc\"),\n",
    "]\n",
    "\n",
    "ROLE_GROUPS = {\n",
    "    \"Observer_Role\": [\"th_fam\", \"th_unfam\"],\n",
    "    \"Receiver_Role\": [\"pt_fam\", \"pt_unfam\"],\n",
    "}\n",
    "\n",
    "# Etichette che appariranno nella colonna \"Metrics\"\n",
    "DISPLAY_LABELS = {\n",
    "    \"th_fam\":   \"observers familiar group\",\n",
    "    \"th_unfam\": \"observers unfamiliar group\",\n",
    "    \"pt_fam\":   \"receivers familiar group\",\n",
    "    \"pt_unfam\": \"receivers unfamiliar group\",\n",
    "}\n",
    "\n",
    "# Etichette per il tipo di dato nel titolo\n",
    "\n",
    "DATA_LABELS = {\n",
    "    \"spectrograms\": \"Electrodes x Frequency\"\n",
    "}\n",
    "SHOW_ONLY = False  # cambia in False per salvare i PNG\n",
    "#SHOW_ONLY = True  # cambia in False per salvare i PNG\n",
    "\n",
    "# ---------- Carica models_info se esiste ----------\n",
    "try:\n",
    "    with open(\"/home/stefano/Interrogait/spectrograms_EEG_channels_freqs_params_GradCAM_Checks.pkl\", \"rb\") as f:\n",
    "        models_info = pickle.load(f)\n",
    "except Exception:\n",
    "    print(\"⚠️  models_info non trovato/caricabile: nessun indicatore di standardizzazione (*).\")\n",
    "    models_info = {}\n",
    "\n",
    "# ---------- Caricamento modelli ----------\n",
    "all_models = {}\n",
    "\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(base_folder, condition, data_type, subfolder)\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Directory non trovata: {path}\")\n",
    "                continue\n",
    "\n",
    "            key = f\"{condition}_{data_type}_{subfolder}\"\n",
    "            all_models[key] = {}\n",
    "\n",
    "            for file in os.listdir(path):\n",
    "                if file.endswith(\".pkl\"):\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    try:\n",
    "                        with open(file_path, \"rb\") as f:\n",
    "                            all_models[key][file] = pickle.load(f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Errore nel caricamento di {file}: {e}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#  PASS 2: tabelle aggregate per ruolo (Observer / Receiver)\n",
    "# =========================================================\n",
    "for condition in experimental_conditions:\n",
    "    for data_type in data_types:\n",
    "        for role_label, subs in ROLE_GROUPS.items():\n",
    "\n",
    "            print(f\"\\nProcessing aggregate table: {condition} - {data_type} - {role_label}\")\n",
    "\n",
    "            # ---------- COSTRUZIONE DF ----------\n",
    "            # ORA: prima tutte le colonne Train (tutti i modelli),\n",
    "            #      poi tutte le colonne Validation, poi Test.\n",
    "            columns = [\"Metrics\"]\n",
    "            for phase in PHASES:                      # <- loop su Training / Validation / Test\n",
    "                for m in MODEL_ORDER:                 #    e dentro sui modelli\n",
    "                    columns.append(f\"{m} ({phase})\")\n",
    "\n",
    "            df_data = {c: [] for c in columns}\n",
    "\n",
    "            for idx_sub, subfolder in enumerate(subs):\n",
    "\n",
    "                subfolder_disp = DISPLAY_LABELS.get(subfolder, subfolder)\n",
    "\n",
    "                for label, tr_key, val_key, te_key in METRICS:\n",
    "\n",
    "                    df_data[\"Metrics\"].append(f\"{subfolder_disp} — {label}\")\n",
    "\n",
    "                    for m in MODEL_ORDER:\n",
    "                        blob = find_model_blob(all_models, condition, data_type, subfolder, m)\n",
    "\n",
    "                        mi_key = f\"{m}_{condition}_{data_type}_{subfolder}\"\n",
    "                        std_flag = bool(models_info.get(mi_key, {}).get(\"standardization\", False))\n",
    "\n",
    "                        if blob is None:\n",
    "                            for phase in PHASES:\n",
    "                                df_data[f\"{m} ({phase})\"].append(\"-\")\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            tr = blob.get(\"my_train_results\", {}).get(\"training_performances\", {})\n",
    "                            va = blob.get(\"my_train_results\", {}).get(\"validation_performances\", {})\n",
    "                            te = blob.get(\"my_test_results\", {}).get(\"test_performances\", {})\n",
    "\n",
    "                            tr_val = tr.get(tr_key,  [None])[0]\n",
    "                            va_val = va.get(val_key, [None])[0]\n",
    "                            te_val = te.get(te_key, [None])[0]\n",
    "\n",
    "                            df_data[f\"{m} (Training)\"].append(fmt(tr_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Validation)\"].append(fmt(va_val, star=std_flag))\n",
    "                            df_data[f\"{m} (Test)\"].append(fmt(te_val, star=std_flag))\n",
    "\n",
    "                        except Exception:\n",
    "                            for phase in PHASES:\n",
    "                                df_data[f\"{m} ({phase})\"].append(\"-\")\n",
    "\n",
    "                # riga vuota di separazione (fam / unfam)\n",
    "                if idx_sub == 0:\n",
    "                    df_data[\"Metrics\"].append(\"\")\n",
    "                    for phase in PHASES:\n",
    "                        for m in MODEL_ORDER:\n",
    "                            df_data[f\"{m} ({phase})\"].append(\"\")\n",
    "\n",
    "            df_performances = pd.DataFrame(df_data)\n",
    "\n",
    "            # =========================\n",
    "            #  PREPARAZIONE PER PLOT\n",
    "            # =========================\n",
    "            df_display = df_performances.copy()\n",
    "\n",
    "            col_weights = []\n",
    "            for col in df_display.columns:\n",
    "                header_len = len(str(col))\n",
    "                body_max = df_display[col].astype(str).map(len).max()\n",
    "                col_weights.append(max(header_len, body_max))\n",
    "\n",
    "            col_weights = np.array(col_weights, dtype=float)\n",
    "            col_weights[0] *= 1.4  # \"Metrics\" più larga\n",
    "            col_widths = (col_weights / col_weights.sum()) * 0.98\n",
    "\n",
    "            # ---------- FIGURA & AX ----------\n",
    "            fig, ax = plt.subplots(figsize=(14, 8))\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            cond_pretty = pretty_condition_name(condition)\n",
    "            data_pretty = DATA_LABELS.get(data_type, data_type)\n",
    "            role_pretty = role_label.replace(\"_\", \" \")\n",
    "\n",
    "            line1 = \"Deep Learning Models performances for Brain Decoding of Sense of Responsibility\"\n",
    "            line2 = f\"Experimental Conditions: {cond_pretty} — EEG Spectrograms: {data_pretty} — Subject Cohort: {role_pretty}\"\n",
    "\n",
    "            ax.set_title(\n",
    "                f\"{line1}\\n{line2}\",\n",
    "                fontsize=11,\n",
    "                pad=6,\n",
    "            )\n",
    "\n",
    "            col_labels = [format_col_label(c) for c in df_display.columns]\n",
    "\n",
    "            tabla = ax.table(\n",
    "                cellText=df_display.values,\n",
    "                colLabels=col_labels,\n",
    "                loc=\"upper center\",\n",
    "                cellLoc=\"center\",\n",
    "                colWidths=col_widths.tolist(),\n",
    "            )\n",
    "\n",
    "            tabla.auto_set_font_size(False)\n",
    "            base_fontsize = 6\n",
    "            header_fontsize = 6\n",
    "\n",
    "            tabla.set_fontsize(base_fontsize)\n",
    "            tabla.scale(1.1, 1.1)\n",
    "\n",
    "            for (row, col), cell in tabla.get_celld().items():\n",
    "                if row == 0:\n",
    "                    cell.set_text_props(weight=\"bold\", fontsize=header_fontsize)\n",
    "\n",
    "            if SHOW_ONLY:\n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "                #print(f\"Tabella aggregata mostrata: {condition} - {data_type} - {role_label}\")\n",
    "                #out_dir = os.path.join(base_folder, condition, data_type)\n",
    "                #os.makedirs(out_dir, exist_ok=True)\n",
    "                #out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "                #out_path = os.path.join(out_dir, out_name)\n",
    "                #print(f'out_path: {out_path}') \n",
    "            else:\n",
    "                out_dir = os.path.join(base_folder, condition, data_type, \"aggregated\")\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                out_name = f\"models_performances_{condition}_{data_type}_{role_label}.png\"\n",
    "                out_path = os.path.join(out_dir, out_name)\n",
    "                print(f'out_path: \\033[1m{out_path}\\033[0m') \n",
    "\n",
    "                fig.savefig(out_path, bbox_inches=\"tight\", dpi=300)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"Tabella aggregata salvata: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ea119-e211-4187-95ac-283ba37c4976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "                                        QUI IL LOOP LO ESEGUO SU OGNI SINGOLO SWEEP DI OGNI COMBINAZIONE DI FATTORI!!!\n",
    "                                                            \n",
    "                                                                    VERSIONE B\n",
    "                                        \n",
    "Questa volta, invece, andiamo ad iterare rispetto a \n",
    "\n",
    "- sweep_tuple, che la tuple che contiene\n",
    "\n",
    "1) relativo codice stringa univoco dello Sweep ID\n",
    "2  la sua combination_key, che ri-associa allo Sweep ID la combinazione di fattori della relativa condizione sperimentale\n",
    "\n",
    "\n",
    "PRIMA FACEVO IN QUESTO MODO\n",
    "\n",
    "for sweep_id in sweep_ids[condition][data_type][category_subject]:\n",
    "    print(f\"\\033[1mInizio l'agent\\033[0m per sweep_id: \\033[1m{sweep_id}\\033[0m\")\n",
    "    \n",
    "ORA INVECE ITERO SULLA TUPLA!\n",
    "\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            for sweep_tuple in sweep_ids[condition][data_type][data_tuples]:\n",
    "        \n",
    "\n",
    "VERSIONE B (SEMPLIFICATA!)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                                    POST-AGGIORNAMENTO \n",
    "                                                                    \n",
    "Conclusioni\n",
    "Le funzioni di versioning (get_model_config_key e update_model_version) \n",
    "sembrano corrette e funzionano come previsto per generare \n",
    "una rappresentazione univoca della configurazione interna e assegnare versioni progressive.\n",
    "\n",
    "L'integrazione nel loop di training è quasi completa; ho segnalato alcuni dettagli (come la virgola mancante) \n",
    "e suggerito di verificare che tutte le variabili (ad esempio, cp, wandb, standardize_data, prepare_data_for_model, EarlyStopping, ecc.) \n",
    "siano definite o importate correttamente.\n",
    "\n",
    "Il meccanismo di versioning ti consentirà di tenere traccia delle diverse configurazioni (versioni) per ogni combinazione di dati,\n",
    "aggiornando il salvataggio del modello se la validation accuracy migliora.\n",
    "\n",
    "'''\n",
    "\n",
    "#                                                                      IMPORTANTE\n",
    "#Questa struttura garantisce che ogni sweep abbia una gestione separata delle versioni del modello, senza conflitti tra sweep differenti\n",
    "#e ogni esecuzione di training_sweep può aggiornare e usare correttamente il dizionario model_versions per tracciare le versioni dei modelli.\n",
    "\n",
    "import time  # Importa il modulo time\n",
    "\n",
    "# Crea un dizionario per tenere traccia delle versioni del modello per questo sweep\n",
    "model_versions = {}\n",
    "                  \n",
    "# Registra il tempo di inizio\n",
    "start_time = time.time()\n",
    "\n",
    "for condition in sweep_ids:\n",
    "    for data_type in sweep_ids[condition]:\n",
    "        for category_subject in sweep_ids[condition][data_type]:\n",
    "            \n",
    "            for sweep_tuple in sweep_ids[condition][data_type][category_subject]:\n",
    "                \n",
    "                # Esegui l'unpacking della tupla per ottenere solo il primo elemento della tupla (sweep_id, combination_key)\n",
    "                sweep_id, combination_key = sweep_tuple\n",
    "                \n",
    "                # Un modo efficace per \"catturare\" il contesto (come sweep_id e le altre variabili) \n",
    "                # per ogni iterazione è definire una funzione wrapper locale all'interno del ciclo\n",
    "                # In questo modo, ogni volta che chiami l'agente, il wrapper avrà già i parametri specifici per quella combinazione\n",
    "                \n",
    "                # Definiamo una funzione wrapper che \"cattura\" lo sweep_id e le altre variabili\n",
    "                def make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_versions):\n",
    "                    def train_wrapper():\n",
    "                        \n",
    "                        # Qui chiamiamo la funzione di training con i parametri appropriati\n",
    "                        #print(f\"\\nSetto il training per lo Sweep ID \\033[1m{condition}_{data_type}_{category_subject}\\033[0m con sweep_id {sweep_id}\")\n",
    "                        \n",
    "                        print(f\"\\nSetting Up Training per lo Sweep ID \\033[1m{sweep_id}\\033[0m --> \\033[1m{combination_key}\\033[0m\")\n",
    "                        training_sweep(\n",
    "                            data_dict_preprocessed, \n",
    "                            sweep_config,\n",
    "                            sweep_ids,\n",
    "                            sweep_id,\n",
    "                            sweep_tuple,\n",
    "                            best_models,  # Best models viene aggiornato all'interno della funzione,\n",
    "                            model_versions # Passa model_versions come argomento\n",
    "                        )\n",
    "                    return train_wrapper\n",
    "                \n",
    "                # Crea la funzione wrapper per l'agent\n",
    "                agent_function = make_train_wrapper(sweep_id, sweep_tuple, condition, data_type, category_subject, model_versions)\n",
    "                \n",
    "                # NOTA: non assegno il valore di wandb.agent a best_models, lascio che training_sweep aggiorni best_models internamente!\n",
    "                '''DEVI INSERIRE PER L'AGENTE COME PARAMETRO IL NOME DELLA CONDIZIONE SPERIMENTALE DEL PROGETTO SU  W&B\n",
    "                   ALTRIMENTI CERCA LO SWEEP NEL PROGETTO SBAGLIATO '''\n",
    "                \n",
    "                print(f\"Inizio l'\\033[1magent\\033[0m per \\033[1msweep_id\\033[0m \\tN°: \\033[1m{sweep_tuple}\\033[0m\")\n",
    "                wandb.agent(sweep_id, function=agent_function, project = f\"{condition}_spectrograms_channels_freqs_params_hyperparams\", count=15)\n",
    "                \n",
    "                    \n",
    "                print(f\"\\nLo sweep id corrente \\033[1m{sweep_id}\\033[0m ha la combinazione di fattori stringhe: \\033[1m{condition}; {data_type}; {category_subject}\\033[0m\\n\")\n",
    "\n",
    "# Registra il tempo di fine\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcola il tempo totale\n",
    "total_time = end_time - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "# Stampa il tempo totale in formato leggibile\n",
    "print(f\"\\nTempo totale impiegato: \\033[1m{hours} ore, {minutes} minuti e {seconds} secondi\\033[0m.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe54b56-225c-4058-a331-c87f7c1c78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_versions['CNN2D_th_resp_vs_pt_resp_spectrograms_familiar_th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd1333-e3fe-4bf7-91f5-e6908356e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finito Training su W&B !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f91bf-38b8-4e37-a97f-abe7538ad7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa il numero totale di sweeps\n",
    "#print(f\"Numero totale di sweeps che verranno eseguiti: {total_sweeps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e83890-7811-4aa6-b77f-53ec7c0f21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweep_ids.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc76d83c-e1f4-404b-a2b2-609d4e0eef9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "''CODICE DI ULTERIORE MODIFICA, PER VERIFICARE VISIVAMENTE LA COERENZA TRA \n",
    "\n",
    "LE SHAPE DEI DATI DEL RELATIVO SOTTO-DIZIONARIO ESTRATTO DA DATA_DICT_PREPROCESSED,\n",
    "A SEGUITO DELLA VERIFICA DI COERENZA TRA \n",
    "\n",
    "1) LA COMBINATION KEY DELLO SWEEP ID CORRENTE E \n",
    "2) LA COMBINATION KEY TROVATA NEL SOTTO-DIZIONARIO DI DATA_DICT_PREPROCESSED\n",
    "'''\n",
    "\n",
    "#Questo codice estrae ogni elemento della tupla data_tuple e,\n",
    "#se l'elemento è un array numpy, \n",
    "#stampa la sua forma. \n",
    "#Ogni print è separato da una linea di trattini per una visualizzazione chiara.\n",
    "\n",
    "#Assicurati che data_dict_preprocessed contenga effettivamente i dati nella struttura indicata \n",
    "#e che ogni elemento della tupla sia un array numpy o un oggetto che possa essere controllato tramite .shape.\n",
    "\n",
    "# Estrai la tupla dalla struttura data_dict_preprocessed\n",
    "data_tuple = data_dict_preprocessed['pt_resp_vs_shared_resp']['wavelet']['unfamiliar_pt']\n",
    "\n",
    "# Itera sulla tupla e stampa la forma di ciascun elemento\n",
    "for idx, data in enumerate(data_tuple):\n",
    "    print(f\"Shape of element {idx + 1}:\")\n",
    "\n",
    "    # Verifica se l'elemento è un array numpy, e se lo è, stampane la shape\n",
    "    if isinstance(data, np.ndarray):\n",
    "        print(data.shape)\n",
    "    else:\n",
    "        print(\"Element is not a numpy array\")\n",
    "    \n",
    "    print(\"-\" * 50)  # Separatore per chiarezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f293e-fbc8-490a-8c59-36dd25123491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne_env",
   "language": "python",
   "name": "mne_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
